{"pr_number": 4029, "pr_title": "Support check committed txns before catalog drop meta, like db, table, partition etc", "pr_createdAt": "2020-07-06T11:30:18Z", "pr_url": "https://github.com/apache/incubator-doris/pull/4029", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3Mjk0NA==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450272944", "bodyText": "Add a Preconditions.checkState( db.isWriteLockHeldByCurrentThread()) to NOTICE.", "author": "morningman", "createdAt": "2020-07-06T14:46:19Z", "path": "fe/src/main/java/org/apache/doris/alter/MaterializedViewHandler.java", "diffHunk": "@@ -716,7 +716,6 @@ public void processBatchDropRollup(List<AlterClause> dropRollupClauses, Database\n \n     public void processDropMaterializedView(DropMaterializedViewStmt dropMaterializedViewStmt, Database db,\n             OlapTable olapTable) throws DdlException, MetaNotFoundException {\n-        db.writeLock();", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/alter/MaterializedViewHandler.java b/fe/src/main/java/org/apache/doris/alter/MaterializedViewHandler.java\ndeleted file mode 100644\nindex 10479d918..000000000\n--- a/fe/src/main/java/org/apache/doris/alter/MaterializedViewHandler.java\n+++ /dev/null\n\n@@ -1,1227 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.alter;\n-\n-import org.apache.doris.alter.AlterJob.JobState;\n-import org.apache.doris.analysis.AddRollupClause;\n-import org.apache.doris.analysis.AlterClause;\n-import org.apache.doris.analysis.CancelAlterTableStmt;\n-import org.apache.doris.analysis.CancelStmt;\n-import org.apache.doris.analysis.CreateMaterializedViewStmt;\n-import org.apache.doris.analysis.DropMaterializedViewStmt;\n-import org.apache.doris.analysis.DropRollupClause;\n-import org.apache.doris.analysis.MVColumnItem;\n-import org.apache.doris.catalog.AggregateType;\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.catalog.Column;\n-import org.apache.doris.catalog.Database;\n-import org.apache.doris.catalog.KeysType;\n-import org.apache.doris.catalog.MaterializedIndex;\n-import org.apache.doris.catalog.MaterializedIndex.IndexState;\n-import org.apache.doris.catalog.OlapTable;\n-import org.apache.doris.catalog.OlapTable.OlapTableState;\n-import org.apache.doris.catalog.Partition;\n-import org.apache.doris.catalog.PrimitiveType;\n-import org.apache.doris.catalog.Replica;\n-import org.apache.doris.catalog.Table;\n-import org.apache.doris.catalog.Tablet;\n-import org.apache.doris.catalog.TabletInvertedIndex;\n-import org.apache.doris.catalog.TabletMeta;\n-import org.apache.doris.catalog.Type;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.Config;\n-import org.apache.doris.common.DdlException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.FeConstants;\n-import org.apache.doris.common.MetaNotFoundException;\n-import org.apache.doris.common.util.ListComparator;\n-import org.apache.doris.common.util.PropertyAnalyzer;\n-import org.apache.doris.common.util.Util;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.persist.BatchDropInfo;\n-import org.apache.doris.persist.DropInfo;\n-import org.apache.doris.persist.EditLog;\n-import org.apache.doris.qe.ConnectContext;\n-import org.apache.doris.qe.OriginStatement;\n-import org.apache.doris.thrift.TStorageFormat;\n-import org.apache.doris.thrift.TStorageMedium;\n-\n-import com.google.common.base.Preconditions;\n-import com.google.common.base.Strings;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.LinkedHashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-import java.util.concurrent.ConcurrentHashMap;\n-\n-/*\n- * MaterializedViewHandler is responsible for ADD/DROP materialized view.\n- * For compatible with older version, it is also responsible for ADD/DROP rollup.\n- * In function level, the mv completely covers the rollup in the future.\n- * In grammar level, there is some difference between mv and rollup.\n- */\n-public class MaterializedViewHandler extends AlterHandler {\n-    private static final Logger LOG = LogManager.getLogger(MaterializedViewHandler.class);\n-    public static final String NEW_STORAGE_FORMAT_INDEX_NAME_PREFIX = \"__v2_\";\n-\n-    public MaterializedViewHandler() {\n-        super(\"materialized view\");\n-    }\n-\n-    // for batch submit rollup job, tableId -> jobId\n-    // keep table's not final state job size. The job size determine's table's state, = 0 means table is normal, otherwise is rollup\n-    private Map<Long, Set<Long>> tableNotFinalStateJobMap = new ConcurrentHashMap<>();\n-    // keep table's running job,used for concurrency limit\n-    // table id -> set of running job ids\n-    private Map<Long, Set<Long>> tableRunningJobMap = new ConcurrentHashMap<>();\n-\n-    @Override\n-    public void addAlterJobV2(AlterJobV2 alterJob) {\n-        super.addAlterJobV2(alterJob);\n-        addAlterJobV2ToTableNotFinalStateJobMap(alterJob);\n-    }\n-\n-    protected void batchAddAlterJobV2(List<AlterJobV2> alterJobV2List) {\n-        for (AlterJobV2 alterJobV2 : alterJobV2List) {\n-            addAlterJobV2(alterJobV2);\n-        }\n-    }\n-\n-    // return true iff job is actually added this time\n-    private boolean addAlterJobV2ToTableNotFinalStateJobMap(AlterJobV2 alterJobV2) {\n-        if (alterJobV2.isDone()) {\n-            LOG.warn(\"try to add a final job({}) to a unfinal set. db: {}, tbl: {}\",\n-                    alterJobV2.getJobId(), alterJobV2.getDbId(), alterJobV2.getTableId());\n-            return false;\n-        }\n-\n-        Long tableId = alterJobV2.getTableId();\n-        Long jobId = alterJobV2.getJobId();\n-\n-        synchronized (tableNotFinalStateJobMap) {\n-            Set<Long> tableNotFinalStateJobIdSet = tableNotFinalStateJobMap.get(tableId);\n-            if (tableNotFinalStateJobIdSet == null) {\n-                tableNotFinalStateJobIdSet = new HashSet<>();\n-                tableNotFinalStateJobMap.put(tableId, tableNotFinalStateJobIdSet);\n-            }\n-            return tableNotFinalStateJobIdSet.add(jobId);\n-        }\n-    }\n-\n-    /**\n-     *\n-     * @param alterJobV2\n-     * @return true iif we really removed a job from tableNotFinalStateJobMap,\n-     *         and there is no running job of this table\n-     *         false otherwise.\n-     */\n-    private boolean removeAlterJobV2FromTableNotFinalStateJobMap(AlterJobV2 alterJobV2) {\n-        Long tableId = alterJobV2.getTableId();\n-        Long jobId = alterJobV2.getJobId();\n-\n-        synchronized (tableNotFinalStateJobMap) {\n-            Set<Long> tableNotFinalStateJobIdset = tableNotFinalStateJobMap.get(tableId);\n-            if (tableNotFinalStateJobIdset == null) {\n-                // This could happen when this job is already removed before.\n-                // return false, so that we will not set table's to NORMAL again.\n-                return false;\n-            }\n-            tableNotFinalStateJobIdset.remove(jobId);\n-            if (tableNotFinalStateJobIdset.size() == 0) {\n-                tableNotFinalStateJobMap.remove(tableId);\n-                return true;\n-            }\n-            return false;\n-        }\n-    }\n-\n-    /**\n-     * There are 2 main steps in this function.\n-     * Step1: validate the request.\n-     *   Step1.1: semantic analysis: the name of olapTable must be same as the base table name in addMVClause.\n-     *   Step1.2: base table validation: the status of base table and partition could be NORMAL.\n-     *   Step1.3: materialized view validation: the name and columns of mv is checked.\n-     * Step2: create mv job\n-     * @param addMVClause\n-     * @param db\n-     * @param olapTable\n-     * @throws DdlException\n-     */\n-    public void processCreateMaterializedView(CreateMaterializedViewStmt addMVClause, Database db, OlapTable olapTable)\n-            throws DdlException, AnalysisException {\n-\n-        if (olapTable.existTempPartitions()) {\n-            throw new DdlException(\"Can not alter table when there are temp partitions in table\");\n-        }\n-\n-        // Step1.1: semantic analysis\n-        // TODO(ML): support the materialized view as base index\n-        if (!addMVClause.getBaseIndexName().equals(olapTable.getName())) {\n-            throw new DdlException(\"The name of table in from clause must be same as the name of alter table\");\n-        }\n-        // Step1.2: base table validation\n-        String baseIndexName = addMVClause.getBaseIndexName();\n-        String mvIndexName = addMVClause.getMVName();\n-        LOG.info(\"process add materialized view[{}] based on [{}]\", mvIndexName, baseIndexName);\n-\n-        // avoid conflict against with batch add rollup job\n-        Preconditions.checkState(olapTable.getState() == OlapTableState.NORMAL);\n-\n-        long baseIndexId = checkAndGetBaseIndex(baseIndexName, olapTable);\n-        // Step1.3: mv clause validation\n-        List<Column> mvColumns = checkAndPrepareMaterializedView(addMVClause, olapTable);\n-\n-        // Step2: create mv job\n-        RollupJobV2 rollupJobV2 = createMaterializedViewJob(mvIndexName, baseIndexName, mvColumns, addMVClause\n-                .getProperties(), olapTable, db, baseIndexId, addMVClause.getMVKeysType(), addMVClause.getOrigStmt());\n-\n-        addAlterJobV2(rollupJobV2);\n-\n-        olapTable.setState(OlapTableState.ROLLUP);\n-\n-        Catalog.getCurrentCatalog().getEditLog().logAlterJob(rollupJobV2);\n-        LOG.info(\"finished to create materialized view job: {}\", rollupJobV2.getJobId());\n-    }\n-\n-    /**\n-     * There are 2 main steps.\n-     * Step1: validate the request\n-     *   Step1.1: base table validation: the status of base table and partition could be NORMAL.\n-     *   Step1.2: rollup validation: the name and columns of rollup is checked.\n-     * Step2: create rollup job\n-     * @param alterClauses\n-     * @param db\n-     * @param olapTable\n-     * @throws DdlException\n-     * @throws AnalysisException\n-     */\n-    public void processBatchAddRollup(List<AlterClause> alterClauses, Database db, OlapTable olapTable) throws DdlException, AnalysisException {\n-        Map<String, RollupJobV2> rollupNameJobMap = new LinkedHashMap<>();\n-        // save job id for log\n-        Set<Long> logJobIdSet = new HashSet<>();\n-\n-        try {\n-            // 1 check and make rollup job\n-            for (AlterClause alterClause : alterClauses) {\n-                AddRollupClause addRollupClause = (AddRollupClause) alterClause;\n-\n-                // step 1 check whether current alter is change storage format\n-                String rollupIndexName = addRollupClause.getRollupName();\n-                boolean changeStorageFormat = false;\n-                if (rollupIndexName.equalsIgnoreCase(olapTable.getName())) {\n-                    // for upgrade test to create segment v2 rollup index by using the sql:\n-                    // alter table table_name add rollup table_name (columns) properties (\"storage_format\" = \"v2\");\n-                    Map<String, String> properties = addRollupClause.getProperties();\n-                    if (properties == null || !properties.containsKey(PropertyAnalyzer.PROPERTIES_STORAGE_FORMAT)\n-                            || !properties.get(PropertyAnalyzer.PROPERTIES_STORAGE_FORMAT).equalsIgnoreCase(\"v2\")) {\n-                        throw new DdlException(\"Table[\" + olapTable.getName() + \"] can not \" +\n-                                \"add segment v2 rollup index without setting storage format to v2.\");\n-                    }\n-                    rollupIndexName = NEW_STORAGE_FORMAT_INDEX_NAME_PREFIX + olapTable.getName();\n-                    changeStorageFormat = true;\n-                }\n-\n-                // get base index schema\n-                String baseIndexName = addRollupClause.getBaseRollupName();\n-                if (baseIndexName == null) {\n-                    // use table name as base table name\n-                    baseIndexName = olapTable.getName();\n-                }\n-\n-                // step 2 alter clause validation\n-                // step 2.1 check whether base index already exists in catalog\n-                long baseIndexId = checkAndGetBaseIndex(baseIndexName, olapTable);\n-\n-                // step 2.2  check rollup schema\n-                List<Column> rollupSchema = checkAndPrepareMaterializedView(addRollupClause, olapTable, baseIndexId, changeStorageFormat);\n-\n-                // step 3 create rollup job\n-                RollupJobV2 alterJobV2 = createMaterializedViewJob(rollupIndexName, baseIndexName, rollupSchema, addRollupClause.getProperties(),\n-                        olapTable, db, baseIndexId, olapTable.getKeysType(), null);\n-\n-                rollupNameJobMap.put(addRollupClause.getRollupName(), alterJobV2);\n-                logJobIdSet.add(alterJobV2.getJobId());\n-            }\n-        } catch (Exception e) {\n-            // remove tablet which has already inserted into TabletInvertedIndex\n-            TabletInvertedIndex tabletInvertedIndex = Catalog.getCurrentInvertedIndex();\n-            for (RollupJobV2 rollupJobV2 : rollupNameJobMap.values()) {\n-                for(MaterializedIndex index : rollupJobV2.getPartitionIdToRollupIndex().values()) {\n-                    for (Tablet tablet : index.getTablets()) {\n-                        tabletInvertedIndex.deleteTablet(tablet.getId());\n-                    }\n-                }\n-            }\n-            throw e;\n-        }\n-\n-        // set table' state to ROLLUP before adding rollup jobs.\n-        // so that when the AlterHandler thread run the jobs, it will see the expected table's state.\n-        // ATTN: This order is not mandatory, because database lock will protect us,\n-        // but this order is more reasonable\n-        olapTable.setState(OlapTableState.ROLLUP);\n-\n-        // 2 batch submit rollup job\n-        List<AlterJobV2> rollupJobV2List = new ArrayList<>(rollupNameJobMap.values());\n-        batchAddAlterJobV2(rollupJobV2List);\n-\n-        BatchAlterJobPersistInfo batchAlterJobV2 = new BatchAlterJobPersistInfo(rollupJobV2List);\n-        Catalog.getCurrentCatalog().getEditLog().logBatchAlterJob(batchAlterJobV2);\n-        LOG.info(\"finished to create materialized view job: {}\", logJobIdSet);\n-    }\n-\n-    /**\n-     * Step1: All replicas of the materialized view index will be created in meta and added to TabletInvertedIndex\n-     * Step2: Set table's state to ROLLUP.\n-     *\n-     * @param mvName\n-     * @param baseIndexName\n-     * @param mvColumns\n-     * @param properties\n-     * @param olapTable\n-     * @param db\n-     * @param baseIndexId\n-     * @throws DdlException\n-     * @throws AnalysisException\n-     */\n-    private RollupJobV2 createMaterializedViewJob(String mvName, String baseIndexName,\n-            List<Column> mvColumns, Map<String, String> properties, OlapTable\n-            olapTable, Database db, long baseIndexId, KeysType mvKeysType, OriginStatement origStmt)\n-            throws DdlException, AnalysisException {\n-        if (mvKeysType == null) {\n-            // assign rollup index's key type, same as base index's\n-            mvKeysType = olapTable.getKeysType();\n-        }\n-        // get rollup schema hash\n-        int mvSchemaHash = Util.schemaHash(0 /* init schema version */, mvColumns, olapTable.getCopiedBfColumns(),\n-                                           olapTable.getBfFpp());\n-        // get short key column count\n-        short mvShortKeyColumnCount = Catalog.calcShortKeyColumnCount(mvColumns, properties);\n-        // get timeout\n-        long timeoutMs = PropertyAnalyzer.analyzeTimeout(properties, Config.alter_table_timeout_second) * 1000;\n-\n-        // create rollup job\n-        long dbId = db.getId();\n-        long tableId = olapTable.getId();\n-        int baseSchemaHash = olapTable.getSchemaHashByIndexId(baseIndexId);\n-        Catalog catalog = Catalog.getCurrentCatalog();\n-        long jobId = catalog.getNextId();\n-        long mvIndexId = catalog.getNextId();\n-        RollupJobV2 mvJob = new RollupJobV2(jobId, dbId, tableId, olapTable.getName(), timeoutMs,\n-                                            baseIndexId, mvIndexId, baseIndexName, mvName,\n-                                            mvColumns, baseSchemaHash, mvSchemaHash,\n-                                            mvKeysType, mvShortKeyColumnCount, origStmt);\n-        String newStorageFormatIndexName = NEW_STORAGE_FORMAT_INDEX_NAME_PREFIX + olapTable.getName();\n-        if (mvName.equals(newStorageFormatIndexName)) {\n-            mvJob.setStorageFormat(TStorageFormat.V2);\n-        }\n-\n-        /*\n-         * create all rollup indexes. and set state.\n-         * After setting, Tables' state will be ROLLUP\n-         */\n-        List<Tablet> addedTablets = Lists.newArrayList();\n-        for (Partition partition : olapTable.getPartitions()) {\n-            long partitionId = partition.getId();\n-            TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(partitionId).getStorageMedium();\n-            // index state is SHADOW\n-            MaterializedIndex mvIndex = new MaterializedIndex(mvIndexId, IndexState.SHADOW);\n-            MaterializedIndex baseIndex = partition.getIndex(baseIndexId);\n-            TabletMeta mvTabletMeta = new TabletMeta(dbId, tableId, partitionId, mvIndexId, mvSchemaHash, medium);\n-            short replicationNum = olapTable.getPartitionInfo().getReplicationNum(partitionId);\n-            for (Tablet baseTablet : baseIndex.getTablets()) {\n-                long baseTabletId = baseTablet.getId();\n-                long mvTabletId = catalog.getNextId();\n-\n-                Tablet newTablet = new Tablet(mvTabletId);\n-                mvIndex.addTablet(newTablet, mvTabletMeta);\n-                addedTablets.add(newTablet);\n-\n-                mvJob.addTabletIdMap(partitionId, mvTabletId, baseTabletId);\n-                List<Replica> baseReplicas = baseTablet.getReplicas();\n-\n-                int healthyReplicaNum = 0;\n-                for (Replica baseReplica : baseReplicas) {\n-                    long mvReplicaId = catalog.getNextId();\n-                    long backendId = baseReplica.getBackendId();\n-                    if (baseReplica.getState() == Replica.ReplicaState.CLONE\n-                            || baseReplica.getState() == Replica.ReplicaState.DECOMMISSION\n-                            || baseReplica.getLastFailedVersion() > 0) {\n-                        LOG.info(\"base replica {} of tablet {} state is {}, and last failed version is {}, skip creating rollup replica\",\n-                                baseReplica.getId(), baseTabletId, baseReplica.getState(), baseReplica.getLastFailedVersion());\n-                        continue;\n-                    }\n-                    Preconditions.checkState(baseReplica.getState() == Replica.ReplicaState.NORMAL, baseReplica.getState());\n-                    // replica's init state is ALTER, so that tablet report process will ignore its report\n-                    Replica mvReplica = new Replica(mvReplicaId, backendId, Replica.ReplicaState.ALTER,\n-                                                    Partition.PARTITION_INIT_VERSION, Partition\n-                                                            .PARTITION_INIT_VERSION_HASH,\n-                                                    mvSchemaHash);\n-                    newTablet.addReplica(mvReplica);\n-                    healthyReplicaNum++;\n-                } // end for baseReplica\n-\n-                if (healthyReplicaNum < replicationNum / 2 + 1) {\n-                    /*\n-                     * TODO(cmy): This is a bad design.\n-                     * Because in the rollup job, we will only send tasks to the rollup replicas that have been created,\n-                     * without checking whether the quorum of replica number are satisfied.\n-                     * This will cause the job to fail until we find that the quorum of replica number\n-                     * is not satisfied until the entire job is done.\n-                     * So here we check the replica number strictly and do not allow to submit the job\n-                     * if the quorum of replica number is not satisfied.\n-                     */\n-                    for (Tablet tablet : addedTablets) {\n-                        Catalog.getCurrentInvertedIndex().deleteTablet(tablet.getId());\n-                    }\n-                    throw new DdlException(\"tablet \" + baseTabletId + \" has few healthy replica: \" + healthyReplicaNum);\n-                }\n-            } // end for baseTablets\n-\n-            mvJob.addMVIndex(partitionId, mvIndex);\n-\n-            LOG.debug(\"create materialized view index {} based on index {} in partition {}\",\n-                      mvIndexId, baseIndexId, partitionId);\n-        } // end for partitions\n-\n-        LOG.info(\"finished to create materialized view job: {}\", mvJob.getJobId());\n-        return mvJob;\n-    }\n-\n-    private List<Column> checkAndPrepareMaterializedView(CreateMaterializedViewStmt addMVClause, OlapTable olapTable)\n-            throws DdlException {\n-        // check if mv index already exists\n-        if (olapTable.hasMaterializedIndex(addMVClause.getMVName())) {\n-            throw new DdlException(\"Materialized view[\" + addMVClause.getMVName() + \"] already exists\");\n-        }\n-        // check if rollup columns are valid\n-        // a. all columns should exist in base rollup schema\n-        // b. For aggregate table, mv columns with aggregate function should be same as base schema\n-        // c. For aggregate table, the column which is the key of base table should be the key of mv as well.\n-        // update mv columns\n-        List<MVColumnItem> mvColumnItemList = addMVClause.getMVColumnItemList();\n-        List<Column> newMVColumns = Lists.newArrayList();\n-        int numOfKeys = 0;\n-        for (MVColumnItem mvColumnItem : mvColumnItemList) {\n-            String mvColumnName = mvColumnItem.getName();\n-            Column baseColumn = olapTable.getColumn(mvColumnName);\n-            if (baseColumn == null) {\n-                throw new DdlException(\"Column[\" + mvColumnName + \"] does not exist\");\n-            }\n-            if (mvColumnItem.isKey()) {\n-                ++numOfKeys;\n-            }\n-            AggregateType baseAggregationType = baseColumn.getAggregationType();\n-            AggregateType mvAggregationType = mvColumnItem.getAggregationType();\n-            if (olapTable.getKeysType().isAggregationFamily()) {\n-                if (baseColumn.isKey() && !mvColumnItem.isKey()) {\n-                    throw new DdlException(\"The column[\" + mvColumnName + \"] must be the key of materialized view\");\n-                }\n-                if (baseAggregationType != mvAggregationType) {\n-                    throw new DdlException(\"The aggregation type of column[\" + mvColumnName + \"] must be same as \"\n-                                                   + \"the aggregate type of base column in aggregate table\");\n-                }\n-                if (baseAggregationType != null && baseAggregationType.isReplaceFamily()\n-                        && olapTable.getKeysNum() != numOfKeys) {\n-                    throw new DdlException(\"The materialized view should contain all keys of base table if there is a\"\n-                                                   + \" REPLACE value\");\n-                }\n-            }\n-            if (olapTable.getKeysType() == KeysType.DUP_KEYS && mvAggregationType != null\n-                    && mvAggregationType.isReplaceFamily()) {\n-                throw new DdlException(\"The aggregation type of REPLACE AND REPLACE IF NOT NULL is forbidden in \"\n-                                               + \"duplicate table\");\n-            }\n-            Column newMVColumn = new Column(baseColumn);\n-            newMVColumn.setIsKey(mvColumnItem.isKey());\n-            newMVColumn.setAggregationType(mvAggregationType, mvColumnItem.isAggregationTypeImplicit());\n-            if (mvColumnItem.getDefineExpr() != null) {\n-                if (mvAggregationType.equals(AggregateType.BITMAP_UNION)) {\n-                    newMVColumn.setType(Type.BITMAP);\n-                } else if (mvAggregationType.equals(AggregateType.HLL_UNION)){\n-                    newMVColumn.setType(Type.HLL);\n-                } else {\n-                    throw new DdlException(\"The define expr of column is only support bitmap_union or hll_union\");\n-                }\n-                newMVColumn.setIsKey(false);\n-                newMVColumn.setIsAllowNull(false);\n-                newMVColumn.setDefineExpr(mvColumnItem.getDefineExpr());\n-            }\n-            newMVColumns.add(newMVColumn);\n-        }\n-        return newMVColumns;\n-    }\n-\n-    public List<Column> checkAndPrepareMaterializedView(AddRollupClause addRollupClause, OlapTable olapTable,\n-                                                        long baseIndexId, boolean changeStorageFormat)\n-            throws DdlException{\n-        String rollupIndexName = addRollupClause.getRollupName();\n-        List<String> rollupColumnNames = addRollupClause.getColumnNames();\n-        if (changeStorageFormat) {\n-            String newStorageFormatIndexName = NEW_STORAGE_FORMAT_INDEX_NAME_PREFIX + olapTable.getName();\n-            rollupIndexName = newStorageFormatIndexName;\n-            List<Column> columns = olapTable.getSchemaByIndexId(baseIndexId);\n-            // create the same schema as base table\n-            rollupColumnNames.clear();\n-            for (Column column : columns) {\n-                rollupColumnNames.add(column.getName());\n-            }\n-        }\n-\n-        // 2. check if rollup index already exists\n-        if (olapTable.hasMaterializedIndex(rollupIndexName)) {\n-            throw new DdlException(\"Rollup index[\" + rollupIndexName + \"] already exists\");\n-        }\n-\n-        // 3. check if rollup columns are valid\n-        // a. all columns should exist in base rollup schema\n-        // b. value after key\n-        // c. if rollup contains REPLACE column, all keys on base index should be included.\n-        List<Column> rollupSchema = new ArrayList<Column>();\n-        // check (a)(b)\n-        boolean meetValue = false;\n-        boolean hasKey = false;\n-        boolean meetReplaceValue = false;\n-        KeysType keysType = olapTable.getKeysType();\n-        Map<String, Column> baseColumnNameToColumn = Maps.newHashMap();\n-        for (Column column : olapTable.getSchemaByIndexId(baseIndexId)) {\n-            baseColumnNameToColumn.put(column.getName(), column);\n-        }\n-        if (keysType.isAggregationFamily()) {\n-            int keysNumOfRollup = 0;\n-            for (String columnName : rollupColumnNames) {\n-                Column oneColumn = baseColumnNameToColumn.get(columnName);\n-                if (oneColumn == null) {\n-                    throw new DdlException(\"Column[\" + columnName + \"] does not exist\");\n-                }\n-                if (oneColumn.isKey() && meetValue) {\n-                    throw new DdlException(\"Invalid column order. value should be after key\");\n-                }\n-                if (oneColumn.isKey()) {\n-                    keysNumOfRollup += 1;\n-                    hasKey = true;\n-                } else {\n-                    meetValue = true;\n-                    if (oneColumn.getAggregationType().isReplaceFamily()) {\n-                        meetReplaceValue = true;\n-                    }\n-                }\n-                rollupSchema.add(oneColumn);\n-            }\n-\n-            if (!hasKey) {\n-                throw new DdlException(\"No key column is found\");\n-            }\n-\n-            if (KeysType.UNIQUE_KEYS == keysType || meetReplaceValue) {\n-                // rollup of unique key table or rollup with REPLACE value\n-                // should have all keys of base table\n-                if (keysNumOfRollup != olapTable.getKeysNum()) {\n-                    if (KeysType.UNIQUE_KEYS == keysType) {\n-                        throw new DdlException(\"Rollup should contains all unique keys in basetable\");\n-                    } else {\n-                        throw new DdlException(\"Rollup should contains all keys if there is a REPLACE value\");\n-                    }\n-                }\n-            }\n-        } else if (KeysType.DUP_KEYS == keysType) {\n-            // supplement the duplicate key\n-            if (addRollupClause.getDupKeys() == null || addRollupClause.getDupKeys().isEmpty()) {\n-                // check the column meta\n-                for (int i = 0; i < rollupColumnNames.size(); i++) {\n-                    String columnName = rollupColumnNames.get(i);\n-                    Column baseColumn = baseColumnNameToColumn.get(columnName);\n-                    if (baseColumn == null) {\n-                        throw new DdlException(\"Column[\" + columnName + \"] does not exist in base index\");\n-                    }\n-                    Column rollupColumn = new Column(baseColumn);\n-                    rollupSchema.add(rollupColumn);\n-                }\n-                if (changeStorageFormat) {\n-                    return rollupSchema;\n-                }\n-                // Supplement key of MV columns\n-                int theBeginIndexOfValue = 0;\n-                int keySizeByte = 0;\n-                for (; theBeginIndexOfValue < rollupSchema.size(); theBeginIndexOfValue++) {\n-                    Column column = rollupSchema.get(theBeginIndexOfValue);\n-                    keySizeByte += column.getType().getIndexSize();\n-                    if (theBeginIndexOfValue + 1 > FeConstants.shortkey_max_column_count\n-                            || keySizeByte > FeConstants.shortkey_maxsize_bytes) {\n-                        if (theBeginIndexOfValue == 0 && column.getType().getPrimitiveType().isCharFamily()) {\n-                            column.setIsKey(true);\n-                            theBeginIndexOfValue++;\n-                        }\n-                        break;\n-                    }\n-                    if (column.getType().isFloatingPointType()) {\n-                        break;\n-                    }\n-                    if (column.getType().getPrimitiveType() == PrimitiveType.VARCHAR) {\n-                        column.setIsKey(true);\n-                        theBeginIndexOfValue++;\n-                        break;\n-                    }\n-                    column.setIsKey(true);\n-                }\n-                if (theBeginIndexOfValue == 0) {\n-                    throw new DdlException(\"The first column could not be float or double\");\n-                }\n-                // Supplement value of MV columns\n-                for (; theBeginIndexOfValue < rollupSchema.size(); theBeginIndexOfValue++) {\n-                    Column rollupColumn = rollupSchema.get(theBeginIndexOfValue);\n-                    rollupColumn.setIsKey(false);\n-                    rollupColumn.setAggregationType(AggregateType.NONE, true);\n-                }\n-            } else {\n-                /*\n-                 * eg.\n-                * Base Table's schema is (k1,k2,k3,k4,k5) dup key (k1,k2,k3).\n-                * The following rollup is allowed:\n-                * 1. (k1) dup key (k1)\n-                * 2. (k2,k3) dup key (k2)\n-                * 3. (k1,k2,k3) dup key (k1,k2)\n-                *\n-                * The following rollup is forbidden:\n-                * 1. (k1) dup key (k2)\n-                * 2. (k2,k3) dup key (k3,k2)\n-                * 3. (k1,k2,k3) dup key (k2,k3)\n-                */\n-                // user specify the duplicate keys for rollup index\n-                List<String> dupKeys = addRollupClause.getDupKeys();\n-                if (dupKeys.size() > rollupColumnNames.size()) {\n-                    throw new DdlException(\"Num of duplicate keys should less than or equal to num of rollup columns.\");\n-                }\n-\n-                for (int i = 0; i < rollupColumnNames.size(); i++) {\n-                    String rollupColName = rollupColumnNames.get(i);\n-                    boolean isKey = false;\n-                    if (i < dupKeys.size()) {\n-                        String dupKeyName = dupKeys.get(i);\n-                        if (!rollupColName.equalsIgnoreCase(dupKeyName)) {\n-                            throw new DdlException(\"Duplicate keys should be the prefix of rollup columns\");\n-                        }\n-                        isKey = true;\n-                    }\n-\n-                    Column baseColumn = baseColumnNameToColumn.get(rollupColName);\n-                    if (baseColumn == null) {\n-                        throw new DdlException(\"Column[\" + rollupColName + \"] does not exist\");\n-                    }\n-\n-                    if (isKey && meetValue) {\n-                        throw new DdlException(\"Invalid column order. key should before all values: \" + rollupColName);\n-                    }\n-\n-                    Column oneColumn = new Column(baseColumn);\n-                    if (isKey) {\n-                        hasKey = true;\n-                        oneColumn.setIsKey(true);\n-                        oneColumn.setAggregationType(null, false);\n-                    } else {\n-                        meetValue = true;\n-                        oneColumn.setIsKey(false);\n-                        oneColumn.setAggregationType(AggregateType.NONE, true);\n-                    }\n-                    rollupSchema.add(oneColumn);\n-                }\n-            }\n-        }\n-        return rollupSchema;\n-    }\n-\n-    /**\n-     *\n-     * @param baseIndexName\n-     * @param olapTable\n-     * @return\n-     * @throws DdlException\n-     */\n-    private long checkAndGetBaseIndex(String baseIndexName, OlapTable olapTable) throws DdlException {\n-        // up to here, table's state can only be NORMAL\n-        Preconditions.checkState(olapTable.getState() == OlapTableState.NORMAL, olapTable.getState().name());\n-\n-        Long baseIndexId = olapTable.getIndexIdByName(baseIndexName);\n-        if (baseIndexId == null) {\n-            throw new DdlException(\"Base index[\" + baseIndexName + \"] does not exist\");\n-        }\n-        // check state\n-        for (Partition partition : olapTable.getPartitions()) {\n-            MaterializedIndex baseIndex = partition.getIndex(baseIndexId);\n-            // up to here. index's state should only be NORMAL\n-            Preconditions.checkState(baseIndex.getState() == IndexState.NORMAL, baseIndex.getState().name());\n-        }\n-        return baseIndexId;\n-    }\n-\n-    public void processBatchDropRollup(List<AlterClause> dropRollupClauses, Database db, OlapTable olapTable)\n-            throws DdlException, MetaNotFoundException {\n-        db.writeLock();\n-        try {\n-            // check drop rollup index operation\n-            for (AlterClause alterClause : dropRollupClauses) {\n-                DropRollupClause dropRollupClause = (DropRollupClause) alterClause;\n-                checkDropMaterializedView(dropRollupClause.getRollupName(), olapTable);\n-            }\n-\n-            // drop data in memory\n-            Set<Long> indexIdSet = new HashSet<>();\n-            Set<String> rollupNameSet = new HashSet<>();\n-            for (AlterClause alterClause : dropRollupClauses) {\n-                DropRollupClause dropRollupClause = (DropRollupClause) alterClause;\n-                String rollupIndexName = dropRollupClause.getRollupName();\n-                long rollupIndexId = dropMaterializedView(rollupIndexName, olapTable);\n-                indexIdSet.add(rollupIndexId);\n-                rollupNameSet.add(rollupIndexName);\n-            }\n-\n-            // batch log drop rollup operation\n-            EditLog editLog = Catalog.getCurrentCatalog().getEditLog();\n-            long dbId = db.getId();\n-            long tableId = olapTable.getId();\n-            editLog.logBatchDropRollup(new BatchDropInfo(dbId, tableId, indexIdSet));\n-            LOG.info(\"finished drop rollup index[{}] in table[{}]\", String.join(\"\", rollupNameSet), olapTable.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void processDropMaterializedView(DropMaterializedViewStmt dropMaterializedViewStmt, Database db,\n-            OlapTable olapTable) throws DdlException, MetaNotFoundException {\n-        try {\n-            String mvName = dropMaterializedViewStmt.getMvName();\n-            // Step1: check drop mv index operation\n-            checkDropMaterializedView(mvName, olapTable);\n-            // Step2; drop data in memory\n-            long mvIndexId = dropMaterializedView(mvName, olapTable);\n-            // Step3: log drop mv operation\n-            EditLog editLog = Catalog.getCurrentCatalog().getEditLog();\n-            editLog.logDropRollup(new DropInfo(db.getId(), olapTable.getId(), mvIndexId));\n-            LOG.info(\"finished drop materialized view [{}] in table [{}]\", mvName, olapTable.getName());\n-        } catch (MetaNotFoundException e) {\n-            if (dropMaterializedViewStmt.isIfExists()) {\n-                LOG.info(e.getMessage());\n-            } else {\n-                throw e;\n-            }\n-        }\n-    }\n-\n-    /**\n-     * Make sure we got db write lock before using this method.\n-     * Up to here, table's state can only be NORMAL.\n-     *\n-     * @param mvName\n-     * @param olapTable\n-     */\n-    private void checkDropMaterializedView(String mvName, OlapTable olapTable)\n-            throws DdlException, MetaNotFoundException {\n-        Preconditions.checkState(olapTable.getState() == OlapTableState.NORMAL, olapTable.getState().name());\n-        if (mvName.equals(olapTable.getName())) {\n-            throw new DdlException(\"Cannot drop base index by using DROP ROLLUP or DROP MATERIALIZED VIEW.\");\n-        }\n-\n-        if (!olapTable.hasMaterializedIndex(mvName)) {\n-            throw new MetaNotFoundException(\n-                    \"Materialized view [\" + mvName + \"] does not exist in table [\" + olapTable.getName() + \"]\");\n-        }\n-\n-        long mvIndexId = olapTable.getIndexIdByName(mvName);\n-        int mvSchemaHash = olapTable.getSchemaHashByIndexId(mvIndexId);\n-        Preconditions.checkState(mvSchemaHash != -1);\n-\n-        for (Partition partition : olapTable.getPartitions()) {\n-            MaterializedIndex materializedIndex = partition.getIndex(mvIndexId);\n-            Preconditions.checkNotNull(materializedIndex);\n-        }\n-    }\n-\n-    /**\n-     * Return mv index id which has been dropped\n-     *\n-     * @param mvName\n-     * @param olapTable\n-     * @return\n-     */\n-    private long dropMaterializedView(String mvName, OlapTable olapTable) {\n-        long mvIndexId = olapTable.getIndexIdByName(mvName);\n-        TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-        for (Partition partition : olapTable.getPartitions()) {\n-            MaterializedIndex rollupIndex = partition.getIndex(mvIndexId);\n-            // delete rollup index\n-            partition.deleteRollupIndex(mvIndexId);\n-            // remove tablets from inverted index\n-            for (Tablet tablet : rollupIndex.getTablets()) {\n-                long tabletId = tablet.getId();\n-                invertedIndex.deleteTablet(tabletId);\n-            }\n-        }\n-        olapTable.deleteIndexInfo(mvName);\n-        return mvIndexId;\n-    }\n-\n-    public void replayDropRollup(DropInfo dropInfo, Catalog catalog) {\n-        Database db = catalog.getDb(dropInfo.getDbId());\n-        db.writeLock();\n-        try {\n-            long tableId = dropInfo.getTableId();\n-            long rollupIndexId = dropInfo.getIndexId();\n-\n-            TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-            OlapTable olapTable = (OlapTable) db.getTable(tableId);\n-            for (Partition partition : olapTable.getPartitions()) {\n-                MaterializedIndex rollupIndex = partition.deleteRollupIndex(rollupIndexId);\n-\n-                if (!Catalog.isCheckpointThread()) {\n-                    // remove from inverted index\n-                    for (Tablet tablet : rollupIndex.getTablets()) {\n-                        invertedIndex.deleteTablet(tablet.getId());\n-                    }\n-                }\n-            }\n-\n-            String rollupIndexName = olapTable.getIndexNameById(rollupIndexId);\n-            olapTable.deleteIndexInfo(rollupIndexName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-        LOG.info(\"replay drop rollup {}\", dropInfo.getIndexId());\n-    }\n-\n-    @Override\n-    protected void runAfterCatalogReady() {\n-        super.runAfterCatalogReady();\n-        runOldAlterJob();\n-        runAlterJobV2();\n-    }\n-\n-    private Map<Long, AlterJobV2> getAlterJobsCopy() {\n-        return new HashMap<>(alterJobsV2);\n-    }\n-\n-    private void removeJobFromRunningQueue(AlterJobV2 alterJob) {\n-        synchronized (tableRunningJobMap) {\n-            Set<Long> runningJobIdSet = tableRunningJobMap.get(alterJob.getTableId());\n-            if (runningJobIdSet != null) {\n-                runningJobIdSet.remove(alterJob.getJobId());\n-                if (runningJobIdSet.size() == 0) {\n-                    tableRunningJobMap.remove(alterJob.getTableId());\n-                }\n-            }\n-        }\n-    }\n-\n-    private void changeTableStatus(long dbId, long tableId, OlapTableState olapTableState) {\n-        Database db = Catalog.getCurrentCatalog().getDb(dbId);\n-        if (db == null) {\n-            LOG.warn(\"db {} has been dropped when changing table {} status after rollup job done\",\n-                    dbId, tableId);\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            OlapTable tbl = (OlapTable) db.getTable(tableId);\n-            if (tbl == null || tbl.getState() == olapTableState) {\n-                return;\n-            }\n-            tbl.setState(olapTableState);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    // replay the alter job v2\n-    @Override\n-    public void replayAlterJobV2(AlterJobV2 alterJob) {\n-        super.replayAlterJobV2(alterJob);\n-        if (!alterJob.isDone()) {\n-            addAlterJobV2ToTableNotFinalStateJobMap(alterJob);\n-            changeTableStatus(alterJob.getDbId(), alterJob.getTableId(), OlapTableState.ROLLUP);\n-        } else {\n-            if (removeAlterJobV2FromTableNotFinalStateJobMap(alterJob)) {\n-                changeTableStatus(alterJob.getDbId(), alterJob.getTableId(), OlapTableState.NORMAL);\n-            }\n-        }\n-    }\n-\n-    /**\n-     *  create tablet and alter tablet in be is thread safe,so we can run rollup job for one table concurrently\n-     */\n-    private void runAlterJobWithConcurrencyLimit(RollupJobV2 rollupJobV2) {\n-        if (rollupJobV2.isDone()) {\n-            return;\n-        }\n-\n-        if (rollupJobV2.isTimeout()) {\n-            // in run(), the timeout job will be cancelled.\n-            rollupJobV2.run();\n-            return;\n-        }\n-\n-        // check if rollup job can be run within limitation.\n-        long tblId = rollupJobV2.getTableId();\n-        long jobId = rollupJobV2.getJobId();\n-        boolean shouldJobRun = false;\n-        synchronized (tableRunningJobMap) {\n-            Set<Long> tableRunningJobSet = tableRunningJobMap.get(tblId);\n-            if (tableRunningJobSet == null) {\n-                tableRunningJobSet = new HashSet<>();\n-                tableRunningJobMap.put(tblId, tableRunningJobSet);\n-            }\n-\n-            // current job is already in running\n-            if (tableRunningJobSet.contains(jobId)) {\n-                shouldJobRun = true;\n-            } else if (tableRunningJobSet.size() < Config.max_running_rollup_job_num_per_table) {\n-                // add current job to running queue\n-                tableRunningJobSet.add(jobId);\n-                shouldJobRun = true;\n-            } else {\n-                LOG.debug(\"number of running alter job {} in table {} exceed limit {}. job {} is suspended\",\n-                        tableRunningJobSet.size(), rollupJobV2.getTableId(),\n-                        Config.max_running_rollup_job_num_per_table, rollupJobV2.getJobId());\n-                shouldJobRun = false;\n-            }\n-        }\n-\n-        if (shouldJobRun) {\n-            rollupJobV2.run();\n-        }\n-    }\n-\n-    private void runAlterJobV2() {\n-        for (Map.Entry<Long, AlterJobV2> entry : getAlterJobsCopy().entrySet()) {\n-            RollupJobV2 alterJob = (RollupJobV2) entry.getValue();\n-            // run alter job\n-            runAlterJobWithConcurrencyLimit(alterJob);\n-            // the following check should be right after job's running, so that the table's state\n-            // can be changed to NORMAL immediately after the last alter job of the table is done.\n-            //\n-            // ATTN(cmy): there is still a short gap between \"job finish\" and \"table become normal\",\n-            // so if user send next alter job right after the \"job finish\",\n-            // it may encounter \"table's state not NORMAL\" error.\n-\n-            if (alterJob.isDone()) {\n-                onJobDone(alterJob);\n-            }\n-        }\n-    }\n-\n-    // remove job from running queue and state map, also set table's state to NORMAL if this is\n-    // the last running job of the table.\n-    private void onJobDone(AlterJobV2 alterJob) {\n-        removeJobFromRunningQueue(alterJob);\n-        if (removeAlterJobV2FromTableNotFinalStateJobMap(alterJob)) {\n-            changeTableStatus(alterJob.getDbId(), alterJob.getTableId(), OlapTableState.NORMAL);\n-        }\n-    }\n-\n-    @Deprecated\n-    private void runOldAlterJob() {\n-        List<AlterJob> cancelledJobs = Lists.newArrayList();\n-        List<AlterJob> finishedJobs = Lists.newArrayList();\n-\n-        for (AlterJob alterJob : alterJobs.values()) {\n-            RollupJob rollupJob = (RollupJob) alterJob;\n-            if (rollupJob.getState() != JobState.FINISHING\n-                    && rollupJob.getState() != JobState.FINISHED\n-                    && rollupJob.getState() != JobState.CANCELLED) {\n-                // cancel the old alter table job\n-                cancelledJobs.add(rollupJob);\n-                continue;\n-            }\n-\n-            if (rollupJob.getTransactionId() < 0) {\n-                // it means this is an old type job and current version is real time load version\n-                // then kill this job\n-                cancelledJobs.add(rollupJob);\n-                continue;\n-            }\n-            JobState state = rollupJob.getState();\n-            switch (state) {\n-                case PENDING: {\n-                    // if rollup job's status is PENDING, we need to send tasks.\n-                    if (!rollupJob.sendTasks()) {\n-                        cancelledJobs.add(rollupJob);\n-                        LOG.warn(\"sending rollup job[\" + rollupJob.getTableId() + \"] tasks failed. cancel it.\");\n-                    }\n-                    break;\n-                }\n-                case RUNNING: {\n-                    if (rollupJob.isTimeout()) {\n-                        cancelledJobs.add(rollupJob);\n-                    } else {\n-                        int res = rollupJob.tryFinishJob();\n-                        if (res == -1) {\n-                            // cancel rollup\n-                            cancelledJobs.add(rollupJob);\n-                            LOG.warn(\"cancel rollup[{}] cause bad rollup job[{}]\",\n-                                     ((RollupJob) rollupJob).getRollupIndexName(), rollupJob.getTableId());\n-                        }\n-                    }\n-                    break;\n-                }\n-                case FINISHING: {\n-                    // check previous load job finished\n-                    if (rollupJob.isPreviousLoadFinished()) {\n-                        // if all previous load job finished, then send clear alter tasks to all related be\n-                        LOG.info(\"previous txn finished, try to send clear txn task\");\n-                        int res = rollupJob.checkOrResendClearTasks();\n-                        if (res != 0) {\n-                            LOG.info(\"send clear txn task return {}\", res);\n-                            if (res == -1) {\n-                                LOG.warn(\"rollup job is in finishing state, but could not finished, \"\n-                                        + \"just finish it, maybe a fatal error {}\", rollupJob);\n-                            }\n-                            finishedJobs.add(rollupJob);\n-                        }\n-                    } else {\n-                        LOG.info(\"previous load jobs are not finished. can not finish rollup job: {}\",\n-                                rollupJob.getTableId());\n-                    }\n-                    break;\n-                }\n-                case FINISHED: {\n-                    break;\n-                }\n-                case CANCELLED: {\n-                    // the alter job could be cancelled in 3 ways\n-                    // 1. the table or db is dropped\n-                    // 2. user cancels the job\n-                    // 3. the job meets errors when running\n-                    // for the previous 2 scenarios, user will call jobdone to finish the job and set its state to cancelled\n-                    // so that there exists alter job whose state is cancelled\n-                    // for the third scenario, the thread will add to cancelled job list and will be dealt by call jobdone\n-                    // Preconditions.checkState(false);\n-                    break;\n-                }\n-                default:\n-                    Preconditions.checkState(false);\n-                    break;\n-            }\n-        } // end for jobs\n-\n-        // handle cancelled rollup jobs\n-        for (AlterJob rollupJob : cancelledJobs) {\n-            Database db = Catalog.getCurrentCatalog().getDb(rollupJob.getDbId());\n-            if (db == null) {\n-                cancelInternal(rollupJob, null, null);\n-                continue;\n-            }\n-\n-            db.writeLock();\n-            try {\n-                OlapTable olapTable = (OlapTable) db.getTable(rollupJob.getTableId());\n-                rollupJob.cancel(olapTable, \"cancelled\");\n-            } finally {\n-                db.writeUnlock();\n-            }\n-            jobDone(rollupJob);\n-        }\n-\n-        // handle finished rollup jobs\n-        for (AlterJob alterJob : finishedJobs) {\n-            alterJob.setState(JobState.FINISHED);\n-            // remove from alterJobs.\n-            // has to remove here, because the job maybe finished and it still in alter job list,\n-            // then user could submit schema change task, and auto load to two table flag will be set false.\n-            // then schema change job will be failed.\n-            alterJob.finishJob();\n-            jobDone(alterJob);\n-            Catalog.getCurrentCatalog().getEditLog().logFinishRollup((RollupJob) alterJob);\n-        }\n-    }\n-\n-    @Override\n-    public List<List<Comparable>> getAlterJobInfosByDb(Database db) {\n-        List<List<Comparable>> rollupJobInfos = new LinkedList<List<Comparable>>();\n-\n-        getOldAlterJobInfos(db, rollupJobInfos);\n-        getAlterJobV2Infos(db, rollupJobInfos);\n-\n-        // sort by\n-        // \"JobId\", \"TableName\", \"CreateTime\", \"FinishedTime\", \"BaseIndexName\", \"RollupIndexName\"\n-        ListComparator<List<Comparable>> comparator = new ListComparator<List<Comparable>>(0, 1, 2, 3, 4, 5);\n-        Collections.sort(rollupJobInfos, comparator);\n-\n-        return rollupJobInfos;\n-    }\n-\n-    private void getAlterJobV2Infos(Database db, List<List<Comparable>> rollupJobInfos) {\n-        ConnectContext ctx = ConnectContext.get();\n-        for (AlterJobV2 alterJob : alterJobsV2.values()) {\n-            if (alterJob.getDbId() != db.getId()) {\n-                continue;\n-            }\n-            if (ctx != null) {\n-                if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ctx, db.getFullName(), alterJob.getTableName(), PrivPredicate.ALTER)) {\n-                    continue;\n-                }\n-            }\n-            alterJob.getInfo(rollupJobInfos);\n-        }\n-    }\n-\n-    @Deprecated\n-    private void getOldAlterJobInfos(Database db, List<List<Comparable>> rollupJobInfos) {\n-        List<AlterJob> jobs = Lists.newArrayList();\n-        // lock to perform atomically\n-        lock();\n-        try {\n-            for (AlterJob alterJob : this.alterJobs.values()) {\n-                if (alterJob.getDbId() == db.getId()) {\n-                    jobs.add(alterJob);\n-                }\n-            }\n-\n-            for (AlterJob alterJob : this.finishedOrCancelledAlterJobs) {\n-                if (alterJob.getDbId() == db.getId()) {\n-                    jobs.add(alterJob);\n-                }\n-            }\n-        } finally {\n-            unlock();\n-        }\n-\n-        db.readLock();\n-        try {\n-            for (AlterJob selectedJob : jobs) {\n-                OlapTable olapTable = (OlapTable) db.getTable(selectedJob.getTableId());\n-                if (olapTable == null) {\n-                    continue;\n-                }\n-\n-                selectedJob.getJobInfo(rollupJobInfos, olapTable);\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-    }\n-\n-    @Override\n-    public void process(List<AlterClause> alterClauses, String clusterName, Database db, OlapTable olapTable)\n-            throws DdlException, AnalysisException, MetaNotFoundException {\n-\n-        if (olapTable.existTempPartitions()) {\n-            throw new DdlException(\"Can not alter table when there are temp partitions in table\");\n-        }\n-\n-        Optional<AlterClause> alterClauseOptional = alterClauses.stream().findAny();\n-        if (alterClauseOptional.isPresent()) {\n-            if (alterClauseOptional.get() instanceof AddRollupClause) {\n-                processBatchAddRollup(alterClauses, db, olapTable);\n-            } else  if (alterClauseOptional.get() instanceof DropRollupClause) {\n-                processBatchDropRollup(alterClauses, db, olapTable);\n-            } else {\n-                Preconditions.checkState(false);\n-            }\n-        }\n-    }\n-\n-    @Override\n-    public void cancel(CancelStmt stmt) throws DdlException {\n-        CancelAlterTableStmt cancelAlterTableStmt = (CancelAlterTableStmt) stmt;\n-\n-        String dbName = cancelAlterTableStmt.getDbName();\n-        String tableName = cancelAlterTableStmt.getTableName();\n-        Preconditions.checkState(!Strings.isNullOrEmpty(dbName));\n-        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));\n-\n-        Database db = Catalog.getCurrentCatalog().getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        AlterJob rollupJob = null;\n-        List<AlterJobV2> rollupJobV2List = new ArrayList<>();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-            if (!(table instanceof OlapTable)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_NOT_OLAP_TABLE, tableName);\n-            }\n-            OlapTable olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.ROLLUP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not under ROLLUP. \"\n-                        + \"Use 'ALTER TABLE DROP ROLLUP' if you want to.\");\n-            }\n-\n-            // find from new alter jobs first\n-            if (cancelAlterTableStmt.getAlterJobIdList() != null) {\n-                for (Long jobId : cancelAlterTableStmt.getAlterJobIdList()) {\n-                    AlterJobV2 alterJobV2 = getUnfinishedAlterJobV2ByJobId(jobId);\n-                    if (alterJobV2 == null)\n-                        continue;\n-                    rollupJobV2List.add(getUnfinishedAlterJobV2ByJobId(jobId));\n-                }\n-            } else {\n-                rollupJobV2List = getUnfinishedAlterJobV2ByTableId(olapTable.getId());\n-            }\n-            if (rollupJobV2List.size() == 0) {\n-                rollupJob = getAlterJob(olapTable.getId());\n-                Preconditions.checkNotNull(rollupJob, olapTable.getId());\n-                if (rollupJob.getState() == JobState.FINISHED\n-                        || rollupJob.getState() == JobState.FINISHING\n-                        || rollupJob.getState() == JobState.CANCELLED) {\n-                    throw new DdlException(\"job is already \" + rollupJob.getState().name() + \", can not cancel it\");\n-                }\n-                rollupJob.cancel(olapTable, \"user cancelled\");\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-\n-        // alter job v2's cancel must be called outside the database lock\n-        if (rollupJobV2List.size() != 0) {\n-            for (AlterJobV2 alterJobV2 : rollupJobV2List) {\n-                alterJobV2.cancel(\"user cancelled\");\n-                if (alterJobV2.isDone()) {\n-                    onJobDone(alterJobV2);\n-                }\n-            }\n-            return;\n-        }\n-\n-        // handle old alter job\n-        if (rollupJob != null && rollupJob.getState() == JobState.CANCELLED) {\n-            jobDone(rollupJob);\n-        }\n-    }\n-\n-    // just for ut\n-    public Map<Long, Set<Long>> getTableRunningJobMap() {\n-        return tableRunningJobMap;\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NDU2OA==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450274568", "bodyText": "There are still some transactions in the COMMITTED state waiting to be completed. The database cannot be dropped. If you want to force drop(cannot be recovered), please use \"DROPP database\";", "author": "morningman", "createdAt": "2020-07-06T14:48:39Z", "path": "fe/src/main/java/org/apache/doris/catalog/Catalog.java", "diffHunk": "@@ -2666,6 +2666,13 @@ public void dropDb(DropDbStmt stmt) throws DdlException {\n             Database db = this.fullNameToDb.get(dbName);\n             db.writeLock();\n             try {\n+                if (stmt.isNeedCheckCommitedTxns()) {\n+                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), null, null)) {\n+                        throw new DdlException(\"There are still some commited txns cannot be aborted in db [\"", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/catalog/Catalog.java b/fe/src/main/java/org/apache/doris/catalog/Catalog.java\ndeleted file mode 100755\nindex 5370a6543..000000000\n--- a/fe/src/main/java/org/apache/doris/catalog/Catalog.java\n+++ /dev/null\n\n@@ -1,6784 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.catalog;\n-\n-import org.apache.doris.alter.Alter;\n-import org.apache.doris.alter.AlterJob;\n-import org.apache.doris.alter.AlterJob.JobType;\n-import org.apache.doris.alter.AlterJobV2;\n-import org.apache.doris.alter.DecommissionBackendJob.DecommissionType;\n-import org.apache.doris.alter.MaterializedViewHandler;\n-import org.apache.doris.alter.SchemaChangeHandler;\n-import org.apache.doris.alter.SystemHandler;\n-import org.apache.doris.analysis.AddPartitionClause;\n-import org.apache.doris.analysis.AddRollupClause;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt.CheckType;\n-import org.apache.doris.analysis.AdminSetConfigStmt;\n-import org.apache.doris.analysis.AdminSetReplicaStatusStmt;\n-import org.apache.doris.analysis.AlterClause;\n-import org.apache.doris.analysis.AlterClusterStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt.QuotaType;\n-import org.apache.doris.analysis.AlterDatabaseRename;\n-import org.apache.doris.analysis.AlterSystemStmt;\n-import org.apache.doris.analysis.AlterTableStmt;\n-import org.apache.doris.analysis.AlterViewStmt;\n-import org.apache.doris.analysis.BackupStmt;\n-import org.apache.doris.analysis.CancelAlterSystemStmt;\n-import org.apache.doris.analysis.CancelAlterTableStmt;\n-import org.apache.doris.analysis.CancelBackupStmt;\n-import org.apache.doris.analysis.ColumnRenameClause;\n-import org.apache.doris.analysis.CreateClusterStmt;\n-import org.apache.doris.analysis.CreateDbStmt;\n-import org.apache.doris.analysis.CreateFunctionStmt;\n-import org.apache.doris.analysis.CreateMaterializedViewStmt;\n-import org.apache.doris.analysis.CreateTableStmt;\n-import org.apache.doris.analysis.CreateUserStmt;\n-import org.apache.doris.analysis.CreateViewStmt;\n-import org.apache.doris.analysis.DecommissionBackendClause;\n-import org.apache.doris.analysis.DistributionDesc;\n-import org.apache.doris.analysis.DropClusterStmt;\n-import org.apache.doris.analysis.DropDbStmt;\n-import org.apache.doris.analysis.DropFunctionStmt;\n-import org.apache.doris.analysis.DropMaterializedViewStmt;\n-import org.apache.doris.analysis.DropPartitionClause;\n-import org.apache.doris.analysis.DropTableStmt;\n-import org.apache.doris.analysis.FunctionName;\n-import org.apache.doris.analysis.InstallPluginStmt;\n-import org.apache.doris.analysis.KeysDesc;\n-import org.apache.doris.analysis.LinkDbStmt;\n-import org.apache.doris.analysis.MigrateDbStmt;\n-import org.apache.doris.analysis.PartitionDesc;\n-import org.apache.doris.analysis.PartitionRenameClause;\n-import org.apache.doris.analysis.RangePartitionDesc;\n-import org.apache.doris.analysis.RecoverDbStmt;\n-import org.apache.doris.analysis.RecoverPartitionStmt;\n-import org.apache.doris.analysis.RecoverTableStmt;\n-import org.apache.doris.analysis.ReplacePartitionClause;\n-import org.apache.doris.analysis.RestoreStmt;\n-import org.apache.doris.analysis.RollupRenameClause;\n-import org.apache.doris.analysis.ShowAlterStmt.AlterType;\n-import org.apache.doris.analysis.SingleRangePartitionDesc;\n-import org.apache.doris.analysis.TableName;\n-import org.apache.doris.analysis.TableRef;\n-import org.apache.doris.analysis.TableRenameClause;\n-import org.apache.doris.analysis.TruncateTableStmt;\n-import org.apache.doris.analysis.UninstallPluginStmt;\n-import org.apache.doris.analysis.UserDesc;\n-import org.apache.doris.analysis.UserIdentity;\n-import org.apache.doris.backup.BackupHandler;\n-import org.apache.doris.catalog.ColocateTableIndex.GroupId;\n-import org.apache.doris.catalog.Database.DbState;\n-import org.apache.doris.catalog.DistributionInfo.DistributionInfoType;\n-import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n-import org.apache.doris.catalog.MaterializedIndex.IndexState;\n-import org.apache.doris.catalog.OlapTable.OlapTableState;\n-import org.apache.doris.catalog.Replica.ReplicaState;\n-import org.apache.doris.catalog.Replica.ReplicaStatus;\n-import org.apache.doris.catalog.Table.TableType;\n-import org.apache.doris.clone.ColocateTableBalancer;\n-import org.apache.doris.clone.DynamicPartitionScheduler;\n-import org.apache.doris.clone.TabletChecker;\n-import org.apache.doris.clone.TabletScheduler;\n-import org.apache.doris.clone.TabletSchedulerStat;\n-import org.apache.doris.cluster.BaseParam;\n-import org.apache.doris.cluster.Cluster;\n-import org.apache.doris.cluster.ClusterNamespace;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.Config;\n-import org.apache.doris.common.ConfigBase;\n-import org.apache.doris.common.DdlException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.FeConstants;\n-import org.apache.doris.common.FeMetaVersion;\n-import org.apache.doris.common.MarkedCountDownLatch;\n-import org.apache.doris.common.MetaNotFoundException;\n-import org.apache.doris.common.Pair;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.common.io.Text;\n-import org.apache.doris.common.util.Daemon;\n-import org.apache.doris.common.util.DynamicPartitionUtil;\n-import org.apache.doris.common.util.MasterDaemon;\n-import org.apache.doris.common.util.PrintableMap;\n-import org.apache.doris.common.util.PropertyAnalyzer;\n-import org.apache.doris.common.util.QueryableReentrantLock;\n-import org.apache.doris.common.util.SmallFileMgr;\n-import org.apache.doris.common.util.TimeUtils;\n-import org.apache.doris.common.util.Util;\n-import org.apache.doris.consistency.ConsistencyChecker;\n-import org.apache.doris.deploy.DeployManager;\n-import org.apache.doris.deploy.impl.AmbariDeployManager;\n-import org.apache.doris.deploy.impl.K8sDeployManager;\n-import org.apache.doris.deploy.impl.LocalFileDeployManager;\n-import org.apache.doris.external.elasticsearch.EsRepository;\n-import org.apache.doris.ha.BDBHA;\n-import org.apache.doris.ha.FrontendNodeType;\n-import org.apache.doris.ha.HAProtocol;\n-import org.apache.doris.ha.MasterInfo;\n-import org.apache.doris.http.meta.MetaBaseAction;\n-import org.apache.doris.journal.JournalCursor;\n-import org.apache.doris.journal.JournalEntity;\n-import org.apache.doris.journal.bdbje.Timestamp;\n-import org.apache.doris.load.DeleteHandler;\n-import org.apache.doris.load.DeleteInfo;\n-import org.apache.doris.load.ExportChecker;\n-import org.apache.doris.load.ExportJob;\n-import org.apache.doris.load.ExportMgr;\n-import org.apache.doris.load.Load;\n-import org.apache.doris.load.LoadChecker;\n-import org.apache.doris.load.LoadErrorHub;\n-import org.apache.doris.load.LoadJob;\n-import org.apache.doris.load.LoadJob.JobState;\n-import org.apache.doris.load.loadv2.LoadEtlChecker;\n-import org.apache.doris.load.loadv2.LoadJobScheduler;\n-import org.apache.doris.load.loadv2.LoadLoadingChecker;\n-import org.apache.doris.load.loadv2.LoadManager;\n-import org.apache.doris.load.loadv2.LoadTimeoutChecker;\n-import org.apache.doris.load.routineload.RoutineLoadManager;\n-import org.apache.doris.load.routineload.RoutineLoadScheduler;\n-import org.apache.doris.load.routineload.RoutineLoadTaskScheduler;\n-import org.apache.doris.master.Checkpoint;\n-import org.apache.doris.master.MetaHelper;\n-import org.apache.doris.meta.MetaContext;\n-import org.apache.doris.metric.MetricRepo;\n-import org.apache.doris.mysql.privilege.PaloAuth;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.persist.BackendIdsUpdateInfo;\n-import org.apache.doris.persist.BackendTabletsInfo;\n-import org.apache.doris.persist.ClusterInfo;\n-import org.apache.doris.persist.ColocatePersistInfo;\n-import org.apache.doris.persist.DatabaseInfo;\n-import org.apache.doris.persist.DropInfo;\n-import org.apache.doris.persist.DropLinkDbAndUpdateDbInfo;\n-import org.apache.doris.persist.DropPartitionInfo;\n-import org.apache.doris.persist.EditLog;\n-import org.apache.doris.persist.ModifyPartitionInfo;\n-import org.apache.doris.persist.ModifyTablePropertyOperationLog;\n-import org.apache.doris.persist.OperationType;\n-import org.apache.doris.persist.PartitionPersistInfo;\n-import org.apache.doris.persist.RecoverInfo;\n-import org.apache.doris.persist.ReplacePartitionOperationLog;\n-import org.apache.doris.persist.ReplicaPersistInfo;\n-import org.apache.doris.persist.SetReplicaStatusOperationLog;\n-import org.apache.doris.persist.Storage;\n-import org.apache.doris.persist.StorageInfo;\n-import org.apache.doris.persist.TableInfo;\n-import org.apache.doris.persist.TablePropertyInfo;\n-import org.apache.doris.persist.TruncateTableInfo;\n-import org.apache.doris.plugin.PluginInfo;\n-import org.apache.doris.plugin.PluginMgr;\n-import org.apache.doris.qe.AuditEventProcessor;\n-import org.apache.doris.qe.ConnectContext;\n-import org.apache.doris.qe.JournalObservable;\n-import org.apache.doris.qe.SessionVariable;\n-import org.apache.doris.qe.VariableMgr;\n-import org.apache.doris.service.FrontendOptions;\n-import org.apache.doris.system.Backend;\n-import org.apache.doris.system.Backend.BackendState;\n-import org.apache.doris.system.Frontend;\n-import org.apache.doris.system.HeartbeatMgr;\n-import org.apache.doris.system.SystemInfoService;\n-import org.apache.doris.task.AgentBatchTask;\n-import org.apache.doris.task.AgentTaskExecutor;\n-import org.apache.doris.task.AgentTaskQueue;\n-import org.apache.doris.task.CreateReplicaTask;\n-import org.apache.doris.task.MasterTaskExecutor;\n-import org.apache.doris.task.PullLoadJobMgr;\n-import org.apache.doris.thrift.TStorageFormat;\n-import org.apache.doris.thrift.TStorageMedium;\n-import org.apache.doris.thrift.TStorageType;\n-import org.apache.doris.thrift.TTabletType;\n-import org.apache.doris.thrift.TTaskType;\n-import org.apache.doris.transaction.GlobalTransactionMgr;\n-import org.apache.doris.transaction.PublishVersionDaemon;\n-\n-import com.google.common.base.Joiner;\n-import com.google.common.base.Joiner.MapJoiner;\n-import com.google.common.base.Preconditions;\n-import com.google.common.base.Strings;\n-import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import com.google.common.collect.Multimap;\n-import com.google.common.collect.Queues;\n-import com.google.common.collect.Range;\n-import com.google.common.collect.Sets;\n-import com.sleepycat.je.rep.InsufficientLogException;\n-import com.sleepycat.je.rep.NetworkRestore;\n-import com.sleepycat.je.rep.NetworkRestoreConfig;\n-\n-import org.apache.commons.collections.CollectionUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.codehaus.jackson.map.ObjectMapper;\n-\n-import java.io.BufferedInputStream;\n-import java.io.DataInputStream;\n-import java.io.DataOutputStream;\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.net.HttpURLConnection;\n-import java.net.URL;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-import java.util.Set;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicLong;\n-\n-\n-public class Catalog {\n-    private static final Logger LOG = LogManager.getLogger(Catalog.class);\n-    // 0 ~ 9999 used for qe\n-    public static final long NEXT_ID_INIT_VALUE = 10000;\n-    private static final int HTTP_TIMEOUT_SECOND = 5;\n-    private static final int STATE_CHANGE_CHECK_INTERVAL_MS = 100;\n-    private static final int REPLAY_INTERVAL_MS = 1;\n-    private static final String BDB_DIR = \"/bdb\";\n-    private static final String IMAGE_DIR = \"/image\";\n-\n-    private String metaDir;\n-    private String bdbDir;\n-    private String imageDir;\n-\n-    private MetaContext metaContext;\n-    private long epoch = 0;\n-\n-    // Lock to perform atomic modification on map like 'idToDb' and 'fullNameToDb'.\n-    // These maps are all thread safe, we only use lock to perform atomic operations.\n-    // Operations like Get or Put do not need lock.\n-    // We use fair ReentrantLock to avoid starvation. Do not use this lock in critical code pass\n-    // because fair lock has poor performance.\n-    // Using QueryableReentrantLock to print owner thread in debug mode.\n-    private QueryableReentrantLock lock;\n-\n-    private ConcurrentHashMap<Long, Database> idToDb;\n-    private ConcurrentHashMap<String, Database> fullNameToDb;\n-\n-    private ConcurrentHashMap<Long, Cluster> idToCluster;\n-    private ConcurrentHashMap<String, Cluster> nameToCluster;\n-\n-    private Load load;\n-    private LoadManager loadManager;\n-    private RoutineLoadManager routineLoadManager;\n-    private ExportMgr exportMgr;\n-    private Alter alter;\n-    private ConsistencyChecker consistencyChecker;\n-    private BackupHandler backupHandler;\n-    private PublishVersionDaemon publishVersionDaemon;\n-    private DeleteHandler deleteHandler;\n-\n-    private MasterDaemon labelCleaner; // To clean old LabelInfo, ExportJobInfos\n-    private MasterDaemon txnCleaner; // To clean aborted or timeout txns\n-    private Daemon replayer;\n-    private Daemon timePrinter;\n-    private Daemon listener;\n-    private EsRepository esRepository;  // it is a daemon, so add it here\n-\n-    private boolean isFirstTimeStartUp = false;\n-    private boolean isElectable;\n-    // set to true after finished replay all meta and ready to serve\n-    // set to false when catalog is not ready.\n-    private AtomicBoolean isReady = new AtomicBoolean(false);\n-    // set to true if FE can offer READ service.\n-    // canRead can be true even if isReady is false.\n-    // for example: OBSERVER transfer to UNKNOWN, then isReady will be set to false, but canRead can still be true\n-    private AtomicBoolean canRead = new AtomicBoolean(false);\n-    private BlockingQueue<FrontendNodeType> typeTransferQueue;\n-\n-    // false if default_cluster is not created.\n-    private boolean isDefaultClusterCreated = false;\n-\n-    // node name is used for bdbje NodeName.\n-    private String nodeName;\n-    private FrontendNodeType role;\n-    private FrontendNodeType feType;\n-    // replica and observer use this value to decide provide read service or not\n-    private long synchronizedTimeMs;\n-    private int masterRpcPort;\n-    private int masterHttpPort;\n-    private String masterIp;\n-\n-    private CatalogIdGenerator idGenerator = new CatalogIdGenerator(NEXT_ID_INIT_VALUE);\n-\n-    private EditLog editLog;\n-    private int clusterId;\n-    private String token;\n-    // For checkpoint and observer memory replayed marker\n-    private AtomicLong replayedJournalId;\n-\n-    private static Catalog CHECKPOINT = null;\n-    private static long checkpointThreadId = -1;\n-    private Checkpoint checkpointer;\n-    private List<Pair<String, Integer>> helperNodes = Lists.newArrayList();\n-    private Pair<String, Integer> selfNode = null;\n-\n-    // node name -> Frontend\n-    private ConcurrentHashMap<String, Frontend> frontends;\n-    // removed frontends' name. used for checking if name is duplicated in bdbje\n-    private ConcurrentLinkedQueue<String> removedFrontends;\n-\n-    private HAProtocol haProtocol = null;\n-\n-    private JournalObservable journalObservable;\n-\n-    private SystemInfoService systemInfo;\n-    private HeartbeatMgr heartbeatMgr;\n-    private TabletInvertedIndex tabletInvertedIndex;\n-    private ColocateTableIndex colocateTableIndex;\n-\n-    private CatalogRecycleBin recycleBin;\n-    private FunctionSet functionSet;\n-\n-    private MetaReplayState metaReplayState;\n-\n-    private PullLoadJobMgr pullLoadJobMgr;\n-    private BrokerMgr brokerMgr;\n-    private ResourceMgr resourceMgr;\n-\n-    private GlobalTransactionMgr globalTransactionMgr;\n-\n-    private DeployManager deployManager;\n-\n-    private TabletStatMgr tabletStatMgr;\n-\n-    private PaloAuth auth;\n-\n-    private DomainResolver domainResolver;\n-\n-    private TabletSchedulerStat stat;\n-\n-    private TabletScheduler tabletScheduler;\n-\n-    private TabletChecker tabletChecker;\n-\n-    private MasterTaskExecutor loadTaskScheduler;\n-\n-    private LoadJobScheduler loadJobScheduler;\n-\n-    private LoadTimeoutChecker loadTimeoutChecker;\n-    private LoadEtlChecker loadEtlChecker;\n-    private LoadLoadingChecker loadLoadingChecker;\n-\n-    private RoutineLoadScheduler routineLoadScheduler;\n-\n-    private RoutineLoadTaskScheduler routineLoadTaskScheduler;\n-\n-    private SmallFileMgr smallFileMgr;\n-\n-    private DynamicPartitionScheduler dynamicPartitionScheduler;\n-    \n-    private PluginMgr pluginMgr;\n-\n-    private AuditEventProcessor auditEventProcessor;\n-\n-    public List<Frontend> getFrontends(FrontendNodeType nodeType) {\n-        if (nodeType == null) {\n-            // get all\n-            return Lists.newArrayList(frontends.values());\n-        }\n-\n-        List<Frontend> result = Lists.newArrayList();\n-        for (Frontend frontend : frontends.values()) {\n-            if (frontend.getRole() == nodeType) {\n-                result.add(frontend);\n-            }\n-        }\n-\n-        return result;\n-    }\n-\n-    public List<String> getRemovedFrontendNames() {\n-        return Lists.newArrayList(removedFrontends);\n-    }\n-\n-    public JournalObservable getJournalObservable() {\n-        return journalObservable;\n-    }\n-\n-    private SystemInfoService getClusterInfo() {\n-        return this.systemInfo;\n-    }\n-\n-    private HeartbeatMgr getHeartbeatMgr() {\n-        return this.heartbeatMgr;\n-    }\n-\n-    public TabletInvertedIndex getTabletInvertedIndex() {\n-        return this.tabletInvertedIndex;\n-    }\n-\n-    // only for test\n-    public void setColocateTableIndex(ColocateTableIndex colocateTableIndex) {\n-        this.colocateTableIndex = colocateTableIndex;\n-    }\n-\n-    public ColocateTableIndex getColocateTableIndex() {\n-        return this.colocateTableIndex;\n-    }\n-\n-    private CatalogRecycleBin getRecycleBin() {\n-        return this.recycleBin;\n-    }\n-\n-    public MetaReplayState getMetaReplayState() {\n-        return metaReplayState;\n-    }\n-\n-    public DynamicPartitionScheduler getDynamicPartitionScheduler() {\n-        return this.dynamicPartitionScheduler;\n-    }\n-\n-    private static class SingletonHolder {\n-        private static final Catalog INSTANCE = new Catalog();\n-    }\n-\n-    private Catalog() {\n-        this.idToDb = new ConcurrentHashMap<>();\n-        this.fullNameToDb = new ConcurrentHashMap<>();\n-        this.load = new Load();\n-        this.routineLoadManager = new RoutineLoadManager();\n-        this.exportMgr = new ExportMgr();\n-        this.alter = new Alter();\n-        this.consistencyChecker = new ConsistencyChecker();\n-        this.lock = new QueryableReentrantLock(true);\n-        this.backupHandler = new BackupHandler(this);\n-        this.metaDir = Config.meta_dir;\n-        this.publishVersionDaemon = new PublishVersionDaemon();\n-        this.deleteHandler = new DeleteHandler();\n-\n-        this.replayedJournalId = new AtomicLong(0L);\n-        this.isElectable = false;\n-        this.synchronizedTimeMs = 0;\n-        this.feType = FrontendNodeType.INIT;\n-        this.typeTransferQueue = Queues.newLinkedBlockingDeque();\n-\n-        this.role = FrontendNodeType.UNKNOWN;\n-        this.frontends = new ConcurrentHashMap<>();\n-        this.removedFrontends = new ConcurrentLinkedQueue<>();\n-\n-        this.journalObservable = new JournalObservable();\n-        this.masterRpcPort = 0;\n-        this.masterHttpPort = 0;\n-        this.masterIp = \"\";\n-\n-        this.systemInfo = new SystemInfoService();\n-        this.heartbeatMgr = new HeartbeatMgr(systemInfo);\n-        this.tabletInvertedIndex = new TabletInvertedIndex();\n-        this.colocateTableIndex = new ColocateTableIndex();\n-        this.recycleBin = new CatalogRecycleBin();\n-        this.functionSet = new FunctionSet();\n-        this.functionSet.init();\n-\n-        this.metaReplayState = new MetaReplayState();\n-\n-        this.idToCluster = new ConcurrentHashMap<>();\n-        this.nameToCluster = new ConcurrentHashMap<>();\n-\n-        this.isDefaultClusterCreated = false;\n-\n-        this.pullLoadJobMgr = new PullLoadJobMgr();\n-        this.brokerMgr = new BrokerMgr();\n-        this.resourceMgr = new ResourceMgr();\n-\n-        this.globalTransactionMgr = new GlobalTransactionMgr(this);\n-        this.tabletStatMgr = new TabletStatMgr();\n-\n-        this.auth = new PaloAuth();\n-        this.domainResolver = new DomainResolver(auth);\n-\n-        this.esRepository = new EsRepository();\n-\n-        this.metaContext = new MetaContext();\n-        this.metaContext.setThreadLocalInfo();\n-        \n-        this.stat = new TabletSchedulerStat();\n-        this.tabletScheduler = new TabletScheduler(this, systemInfo, tabletInvertedIndex, stat);\n-        this.tabletChecker = new TabletChecker(this, systemInfo, tabletScheduler, stat);\n-\n-        this.loadTaskScheduler = new MasterTaskExecutor(Config.async_load_task_pool_size);\n-        this.loadJobScheduler = new LoadJobScheduler();\n-        this.loadManager = new LoadManager(loadJobScheduler);\n-        this.loadTimeoutChecker = new LoadTimeoutChecker(loadManager);\n-        this.loadEtlChecker = new LoadEtlChecker(loadManager);\n-        this.loadLoadingChecker = new LoadLoadingChecker(loadManager);\n-        this.routineLoadScheduler = new RoutineLoadScheduler(routineLoadManager);\n-        this.routineLoadTaskScheduler = new RoutineLoadTaskScheduler(routineLoadManager);\n-\n-        this.smallFileMgr = new SmallFileMgr();\n-\n-        this.dynamicPartitionScheduler = new DynamicPartitionScheduler(\"DynamicPartitionScheduler\",\n-                Config.dynamic_partition_check_interval_seconds * 1000L);\n-        \n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        this.pluginMgr = new PluginMgr();\n-        this.auditEventProcessor = new AuditEventProcessor(this.pluginMgr);\n-    }\n-\n-    public static void destroyCheckpoint() {\n-        if (CHECKPOINT != null) {\n-            CHECKPOINT = null;\n-        }\n-    }\n-\n-    public static Catalog getCurrentCatalog() {\n-        if (isCheckpointThread()) {\n-            // only checkpoint thread it self will goes here.\n-            // so no need to care about the thread safe.\n-            if (CHECKPOINT == null) {\n-                CHECKPOINT = new Catalog();\n-            }\n-            return CHECKPOINT;\n-        } else {\n-            return SingletonHolder.INSTANCE;\n-        }\n-    }\n-\n-    // NOTICE: in most case, we should use getCurrentCatalog() to get the right catalog.\n-    // but in some cases, we should get the serving catalog explicitly.\n-    public static Catalog getServingCatalog() {\n-        return SingletonHolder.INSTANCE;\n-    }\n-\n-    public PullLoadJobMgr getPullLoadJobMgr() {\n-        return pullLoadJobMgr;\n-    }\n-\n-    public BrokerMgr getBrokerMgr() {\n-        return brokerMgr;\n-    }\n-\n-    public ResourceMgr getResourceMgr() {\n-        return resourceMgr;\n-    }\n-\n-    public static GlobalTransactionMgr getCurrentGlobalTransactionMgr() {\n-        return getCurrentCatalog().globalTransactionMgr;\n-    }\n-\n-    public GlobalTransactionMgr getGlobalTransactionMgr() {\n-        return globalTransactionMgr;\n-    }\n-\n-    public PluginMgr getPluginMgr() {\n-        return pluginMgr;\n-    }\n-\n-    public PaloAuth getAuth() {\n-        return auth;\n-    }\n-\n-    public TabletScheduler getTabletScheduler() {\n-        return tabletScheduler;\n-    }\n-\n-    public TabletChecker getTabletChecker() {\n-        return tabletChecker;\n-    }\n-\n-    public ConcurrentHashMap<String, Database> getFullNameToDb() {\n-        return fullNameToDb;\n-    }\n-\n-    public AuditEventProcessor getAuditEventProcessor() {\n-        return auditEventProcessor;\n-    }\n-\n-    // use this to get correct ClusterInfoService instance\n-    public static SystemInfoService getCurrentSystemInfo() {\n-        return getCurrentCatalog().getClusterInfo();\n-    }\n-\n-    public static HeartbeatMgr getCurrentHeartbeatMgr() {\n-        return getCurrentCatalog().getHeartbeatMgr();\n-    }\n-\n-    // use this to get correct TabletInvertedIndex instance\n-    public static TabletInvertedIndex getCurrentInvertedIndex() {\n-        return getCurrentCatalog().getTabletInvertedIndex();\n-    }\n-\n-    // use this to get correct ColocateTableIndex instance\n-    public static ColocateTableIndex getCurrentColocateIndex() {\n-        return getCurrentCatalog().getColocateTableIndex();\n-    }\n-\n-    public static CatalogRecycleBin getCurrentRecycleBin() {\n-        return getCurrentCatalog().getRecycleBin();\n-    }\n-\n-    // use this to get correct Catalog's journal version\n-    public static int getCurrentCatalogJournalVersion() {\n-        return MetaContext.get().getMetaVersion();\n-    }\n-\n-    public static final boolean isCheckpointThread() {\n-        return Thread.currentThread().getId() == checkpointThreadId;\n-    }\n-\n-    public static PluginMgr getCurrentPluginMgr() {\n-        return getCurrentCatalog().getPluginMgr();\n-    }\n-\n-    public static AuditEventProcessor getCurrentAuditEventProcessor() {\n-        return getCurrentCatalog().getAuditEventProcessor();\n-    }\n-\n-    // Use tryLock to avoid potential dead lock\n-    private boolean tryLock(boolean mustLock) {\n-        while (true) {\n-            try {\n-                if (!lock.tryLock(Config.catalog_try_lock_timeout_ms, TimeUnit.MILLISECONDS)) {\n-                    if (LOG.isDebugEnabled()) {\n-                        // to see which thread held this lock for long time.\n-                        Thread owner = lock.getOwner();\n-                        if (owner != null) {\n-                            LOG.debug(\"catalog lock is held by: {}\", Util.dumpThread(owner, 10));\n-                        }\n-                    }\n-                    \n-                    if (mustLock) {\n-                        continue;\n-                    } else {\n-                        return false;\n-                    }\n-                }\n-                return true;\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception while getting catalog lock\", e);\n-                if (mustLock) {\n-                    continue;\n-                } else {\n-                    return lock.isHeldByCurrentThread();\n-                }\n-            }\n-        }\n-    }\n-\n-    private void unlock() {\n-        if (lock.isHeldByCurrentThread()) {\n-            this.lock.unlock();\n-        }\n-    }\n-\n-    public String getBdbDir() {\n-        return bdbDir;\n-    }\n-\n-    public String getImageDir() {\n-        return imageDir;\n-    }\n-\n-    public void initialize(String[] args) throws Exception {\n-        // set meta dir first.\n-        // we already set these variables in constructor. but Catalog is a singleton class.\n-        // so they may be set before Config is initialized.\n-        // set them here again to make sure these variables use values in fe.conf.\n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        // 0. get local node and helper node info\n-        getSelfHostPort();\n-        getHelperNodes(args);\n-\n-        // 1. check and create dirs and files\n-        File meta = new File(metaDir);\n-        if (!meta.exists()) {\n-            LOG.error(\"{} does not exist, will exit\", meta.getAbsolutePath());\n-            System.exit(-1);\n-        }\n-\n-        if (Config.edit_log_type.equalsIgnoreCase(\"bdb\")) {\n-            File bdbDir = new File(this.bdbDir);\n-            if (!bdbDir.exists()) {\n-                bdbDir.mkdirs();\n-            }\n-\n-            File imageDir = new File(this.imageDir);\n-            if (!imageDir.exists()) {\n-                imageDir.mkdirs();\n-            }\n-        } else {\n-            LOG.error(\"Invalid edit log type: {}\", Config.edit_log_type);\n-            System.exit(-1);\n-        }\n-\n-        // init plugin manager\n-        pluginMgr.init();\n-        auditEventProcessor.start();\n-\n-        // 2. get cluster id and role (Observer or Follower)\n-        getClusterIdAndRole();\n-\n-        // 3. Load image first and replay edits\n-        this.editLog = new EditLog(nodeName);\n-        loadImage(this.imageDir); // load image file\n-        editLog.open(); // open bdb env\n-        this.globalTransactionMgr.setEditLog(editLog);\n-        this.idGenerator.setEditLog(editLog);\n-\n-        // 4. create load and export job label cleaner thread\n-        createLabelCleaner();\n-\n-        // 5. create txn cleaner thread\n-        createTxnCleaner();\n-\n-        // 6. start state listener thread\n-        createStateListener();\n-        listener.start();\n-    }\n-\n-    // wait until FE is ready.\n-    public void waitForReady() throws InterruptedException {\n-        while (true) {\n-            if (isReady()) {\n-                LOG.info(\"catalog is ready. FE type: {}\", feType);\n-                break;\n-            }\n-\n-            Thread.sleep(2000);\n-            LOG.info(\"wait catalog to be ready. FE type: {}. is ready: {}\", feType, isReady.get());\n-        }\n-    }\n-    \n-    public boolean isReady() {\n-        return isReady.get();\n-    }\n-\n-    private void getClusterIdAndRole() throws IOException {\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-\n-        // if helper node is point to self, or there is ROLE and VERSION file in local.\n-        // get the node type from local\n-        if (isMyself() || (roleFile.exists() && versionFile.exists())) {\n-\n-            if (!isMyself()) {\n-                LOG.info(\"find ROLE and VERSION file in local, ignore helper nodes: {}\", helperNodes);\n-            }\n-\n-            // check file integrity, if has.\n-            if ((roleFile.exists() && !versionFile.exists())\n-                    || (!roleFile.exists() && versionFile.exists())) {\n-                LOG.error(\"role file and version file must both exist or both not exist. \"\n-                        + \"please specific one helper node to recover. will exit.\");\n-                System.exit(-1);\n-            }\n-\n-            // ATTN:\n-            // If the version file and role file does not exist and the helper node is itself,\n-            // this should be the very beginning startup of the cluster, so we create ROLE and VERSION file,\n-            // set isFirstTimeStartUp to true, and add itself to frontends list.\n-            // If ROLE and VERSION file is deleted for some reason, we may arbitrarily start this node as\n-            // FOLLOWER, which may cause UNDEFINED behavior.\n-            // Everything may be OK if the origin role is exactly FOLLOWER,\n-            // but if not, FE process will exit somehow.\n-            Storage storage = new Storage(this.imageDir);\n-            if (!roleFile.exists()) {\n-                // The very first time to start the first node of the cluster.\n-                // It should became a Master node (Master node's role is also FOLLOWER, which means electable)\n-\n-                // For compatibility. Because this is the very first time to start, so we arbitrarily choose\n-                // a new name for this node\n-                role = FrontendNodeType.FOLLOWER;\n-                nodeName = genFeNodeName(selfNode.first, selfNode.second, false /* new style */);\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                LOG.info(\"very first time to start this node. role: {}, node name: {}\", role.name(), nodeName);\n-            } else {\n-                role = storage.getRole();\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-\n-                nodeName = storage.getNodeName();\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // In normal case, if ROLE file exist, role and nodeName should both exist.\n-                    // But we will get a empty nodeName after upgrading.\n-                    // So for forward compatibility, we use the \"old-style\" way of naming: \"ip_port\",\n-                    // and update the ROLE file.\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true/* old style */);\n-                    storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                    LOG.info(\"forward compatibility. role: {}, node name: {}\", role.name(), nodeName);\n-                }\n-            }\n-\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            if (!versionFile.exists()) {\n-                clusterId = Config.cluster_id == -1 ? Storage.newClusterID() : Config.cluster_id;\n-                token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                        Storage.newToken() : Config.auth_token;\n-                storage = new Storage(clusterId, token, this.imageDir);\n-                storage.writeClusterIdAndToken();\n-\n-                isFirstTimeStartUp = true;\n-                Frontend self = new Frontend(role, nodeName, selfNode.first, selfNode.second);\n-                // We don't need to check if frontends already contains self.\n-                // frontends must be empty cause no image is loaded and no journal is replayed yet.\n-                // And this frontend will be persisted later after opening bdbje environment.\n-                frontends.put(nodeName, self);\n-            } else {\n-                clusterId = storage.getClusterID();\n-                if (storage.getToken() == null) {\n-                    token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                            Storage.newToken() : Config.auth_token;\n-                    LOG.info(\"new token={}\", token);\n-                    storage.setToken(token);\n-                    storage.writeClusterIdAndToken();\n-                } else {\n-                    token = storage.getToken();\n-                }\n-                isFirstTimeStartUp = false;\n-            }\n-        } else {\n-            // try to get role and node name from helper node,\n-            // this loop will not end until we get certain role type and name\n-            while (true) {\n-                if (!getFeNodeTypeAndNameFromHelpers()) {\n-                    LOG.warn(\"current node is not added to the group. please add it first. \"\n-                            + \"sleep 5 seconds and retry, current helper nodes: {}\", helperNodes);\n-                    try {\n-                        Thread.sleep(5000);\n-                        continue;\n-                    } catch (InterruptedException e) {\n-                        e.printStackTrace();\n-                        System.exit(-1);\n-                    }\n-                }\n-\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-                break;\n-            }\n-\n-            Preconditions.checkState(helperNodes.size() == 1);\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            Pair<String, Integer> rightHelperNode = helperNodes.get(0);\n-\n-            Storage storage = new Storage(this.imageDir);\n-            if (roleFile.exists() && (role != storage.getRole() || !nodeName.equals(storage.getNodeName()))\n-                    || !roleFile.exists()) {\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-            }\n-            if (!versionFile.exists()) {\n-                // If the version file doesn't exist, download it from helper node\n-                if (!getVersionFileFromHelper(rightHelperNode)) {\n-                    LOG.error(\"fail to download version file from \" + rightHelperNode.first + \" will exit.\");\n-                    System.exit(-1);\n-                }\n-\n-                // NOTE: cluster_id will be init when Storage object is constructed,\n-                //       so we new one.\n-                storage = new Storage(this.imageDir);\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                if (Strings.isNullOrEmpty(token)) {\n-                    token = Config.auth_token;\n-                }\n-            } else {\n-                // If the version file exist, read the cluster id and check the\n-                // id with helper node to make sure they are identical\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                try {\n-                    URL idURL = new URL(\"http://\" + rightHelperNode.first + \":\" + Config.http_port + \"/check\");\n-                    HttpURLConnection conn = null;\n-                    conn = (HttpURLConnection) idURL.openConnection();\n-                    conn.setConnectTimeout(2 * 1000);\n-                    conn.setReadTimeout(2 * 1000);\n-                    String clusterIdString = conn.getHeaderField(MetaBaseAction.CLUSTER_ID);\n-                    int remoteClusterId = Integer.parseInt(clusterIdString);\n-                    if (remoteClusterId != clusterId) {\n-                        LOG.error(\"cluster id is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                        System.exit(-1);\n-                    }\n-                    String remoteToken = conn.getHeaderField(MetaBaseAction.TOKEN);\n-                    if (token == null && remoteToken != null) {\n-                        LOG.info(\"get token from helper node. token={}.\", remoteToken);\n-                        token = remoteToken;\n-                        storage.writeClusterIdAndToken();\n-                        storage.reload();\n-                    }\n-                    if (Config.enable_token_check) {\n-                        Preconditions.checkNotNull(token);\n-                        Preconditions.checkNotNull(remoteToken);\n-                        if (!token.equals(remoteToken)) {\n-                            LOG.error(\"token is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                            System.exit(-1);\n-                        }\n-                    }\n-                } catch (Exception e) {\n-                    LOG.warn(\"fail to check cluster_id and token with helper node.\", e);\n-                    System.exit(-1);\n-                }\n-            }\n-\n-            getNewImage(rightHelperNode);\n-        }\n-\n-        if (Config.cluster_id != -1 && clusterId != Config.cluster_id) {\n-            LOG.error(\"cluster id is not equal with config item cluster_id. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (role.equals(FrontendNodeType.FOLLOWER)) {\n-            isElectable = true;\n-        } else {\n-            isElectable = false;\n-        }\n-\n-        Preconditions.checkState(helperNodes.size() == 1);\n-        LOG.info(\"finished to get cluster id: {}, role: {} and node name: {}\",\n-                clusterId, role.name(), nodeName);\n-    }\n-\n-    public static String genFeNodeName(String host, int port, boolean isOldStyle) {\n-        String name = host + \"_\" + port;\n-        if (isOldStyle) {\n-            return name;\n-        } else {\n-            return name + \"_\" + System.currentTimeMillis();\n-        }\n-    }\n-\n-    // Get the role info and node name from helper node.\n-    // return false if failed.\n-    private boolean getFeNodeTypeAndNameFromHelpers() {\n-        // we try to get info from helper nodes, once we get the right helper node,\n-        // other helper nodes will be ignored and removed.\n-        Pair<String, Integer> rightHelperNode = null;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            try {\n-                URL url = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/role?host=\" + selfNode.first + \"&port=\" + selfNode.second);\n-                HttpURLConnection conn = null;\n-                conn = (HttpURLConnection) url.openConnection();\n-                if (conn.getResponseCode() != 200) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}. response code: {}\",\n-                            helperNode, conn.getResponseCode());\n-                    continue;\n-                }\n-\n-                String type = conn.getHeaderField(\"role\");\n-                if (type == null) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode);\n-                    continue;\n-                }\n-                role = FrontendNodeType.valueOf(type);\n-                nodeName = conn.getHeaderField(\"name\");\n-\n-                // get role and node name before checking them, because we want to throw any exception\n-                // as early as we encounter.\n-\n-                if (role == FrontendNodeType.UNKNOWN) {\n-                    LOG.warn(\"frontend {} is not added to cluster yet. role UNKNOWN\", selfNode);\n-                    return false;\n-                }\n-\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // For forward compatibility, we use old-style name: \"ip_port\"\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true /* old style */);\n-                }\n-            } catch (Exception e) {\n-                LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode, e);\n-                continue;\n-            }\n-\n-            LOG.info(\"get fe node type {}, name {} from {}:{}\", role, nodeName, helperNode.first, Config.http_port);\n-            rightHelperNode = helperNode;\n-            break;\n-        }\n-\n-        if (rightHelperNode == null) {\n-            return false;\n-        }\n-\n-        helperNodes.clear();\n-        helperNodes.add(rightHelperNode);\n-        return true;\n-    }\n-\n-    private void getSelfHostPort() {\n-        selfNode = new Pair<String, Integer>(FrontendOptions.getLocalHostAddress(), Config.edit_log_port);\n-        LOG.debug(\"get self node: {}\", selfNode);\n-    }\n-\n-    private void getHelperNodes(String[] args) throws AnalysisException {\n-        String helpers = null;\n-        for (int i = 0; i < args.length; i++) {\n-            if (args[i].equalsIgnoreCase(\"-helper\")) {\n-                if (i + 1 >= args.length) {\n-                    System.out.println(\"-helper need parameter host:port,host:port\");\n-                    System.exit(-1);\n-                }\n-                helpers = args[i + 1];\n-                break;\n-            }\n-        }\n-\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            if (Config.enable_deploy_manager.equalsIgnoreCase(\"k8s\")) {\n-                deployManager = new K8sDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"ambari\")) {\n-                deployManager = new AmbariDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"local\")) {\n-                deployManager = new LocalFileDeployManager(this, 5000 /* 5s interval */);\n-            } else {\n-                System.err.println(\"Unknow deploy manager: \" + Config.enable_deploy_manager);\n-                System.exit(-1);\n-            }\n-\n-            getHelperNodeFromDeployManager();\n-\n-        } else {\n-            if (helpers != null) {\n-                String[] splittedHelpers = helpers.split(\",\");\n-                for (String helper : splittedHelpers) {\n-                    Pair<String, Integer> helperHostPort = SystemInfoService.validateHostAndPort(helper);\n-                    if (helperHostPort.equals(selfNode)) {\n-                        /**\n-                         * If user specified the helper node to this FE itself,\n-                         * we will stop the starting FE process and report an error.\n-                         * First, it is meaningless to point the helper to itself.\n-                         * Secondly, when some users add FE for the first time, they will mistakenly\n-                         * point the helper that should have pointed to the Master to themselves.\n-                         * In this case, some errors have caused users to be troubled.\n-                         * So here directly exit the program and inform the user to avoid unnecessary trouble.\n-                         */\n-                        throw new AnalysisException(\n-                                \"Do not specify the helper node to FE itself. \"\n-                                        + \"Please specify it to the existing running Master or Follower FE\");\n-                    }\n-                    helperNodes.add(helperHostPort);\n-                }\n-            } else {\n-                // If helper node is not designated, use local node as helper node.\n-                helperNodes.add(Pair.create(selfNode.first, Config.edit_log_port));\n-            }\n-        }\n-\n-        LOG.info(\"get helper nodes: {}\", helperNodes);\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private void getHelperNodeFromDeployManager() {\n-        Preconditions.checkNotNull(deployManager);\n-\n-        // 1. check if this is the first time to start up\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-        if ((roleFile.exists() && !versionFile.exists())\n-                || (!roleFile.exists() && versionFile.exists())) {\n-            LOG.error(\"role file and version file must both exist or both not exist. \"\n-                    + \"please specific one helper node to recover. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (roleFile.exists()) {\n-            // This is not the first time this node start up.\n-            // It should already added to FE group, just set helper node as it self.\n-            LOG.info(\"role file exist. this is not the first time to start up\");\n-            helperNodes = Lists.newArrayList(Pair.create(selfNode.first, Config.edit_log_port));\n-            return;\n-        }\n-\n-        // This is the first time this node start up.\n-        // Get helper node from deploy manager.\n-        helperNodes = deployManager.getHelperNodes();\n-        if (helperNodes == null || helperNodes.isEmpty()) {\n-            LOG.error(\"failed to get helper node from deploy manager. exit\");\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private void transferToMaster() {\n-        // stop replayer\n-        if (replayer != null) {\n-            replayer.exit();\n-            try {\n-                replayer.join();\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception when stopping the replayer thread\", e);\n-            }\n-            replayer = null;\n-        }\n-\n-        // set this after replay thread stopped. to avoid replay thread modify them.\n-        isReady.set(false);\n-        canRead.set(false);\n-\n-        editLog.open();\n-\n-        if (!haProtocol.fencing()) {\n-            LOG.error(\"fencing failed. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        long replayStartTime = System.currentTimeMillis();\n-        // replay journals. -1 means replay all the journals larger than current journal id.\n-        replayJournal(-1);\n-        long replayEndTime = System.currentTimeMillis();\n-        LOG.info(\"finish replay in \" + (replayEndTime - replayStartTime) + \" msec\");\n-\n-        checkCurrentNodeExist();\n-\n-        editLog.rollEditLog();\n-\n-        // Log meta_version\n-        long journalVersion = MetaContext.get().getMetaVersion();\n-        if (journalVersion < FeConstants.meta_version) {\n-            editLog.logMetaVersion(FeConstants.meta_version);\n-            MetaContext.get().setMetaVersion(FeConstants.meta_version);\n-        }\n-\n-        // Log the first frontend\n-        if (isFirstTimeStartUp) {\n-            // if isFirstTimeStartUp is true, frontends must contains this Node.\n-            Frontend self = frontends.get(nodeName);\n-            Preconditions.checkNotNull(self);\n-            // OP_ADD_FIRST_FRONTEND is emitted, so it can write to BDBJE even if canWrite is false\n-            editLog.logAddFirstFrontend(self);\n-        }\n-\n-        if (!isDefaultClusterCreated) {\n-            initDefaultCluster();\n-        }\n-\n-        // MUST set master ip before starting checkpoint thread.\n-        // because checkpoint thread need this info to select non-master FE to push image\n-        this.masterIp = FrontendOptions.getLocalHostAddress();\n-        this.masterRpcPort = Config.rpc_port;\n-        this.masterHttpPort = Config.http_port;\n-        MasterInfo info = new MasterInfo(this.masterIp, this.masterHttpPort, this.masterRpcPort);\n-        editLog.logMasterInfo(info);\n-\n-        // for master, the 'isReady' is set behind.\n-        // but we are sure that all metadata is replayed if we get here.\n-        // so no need to check 'isReady' flag in this method\n-        fixBugAfterMetadataReplayed(false);\n-\n-        // start all daemon threads that only running on MASTER FE\n-        startMasterOnlyDaemonThreads();\n-        // start other daemon threads that should running on all FE\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-\n-        canRead.set(true);\n-        isReady.set(true);\n-\n-        String msg = \"master finished to replay journal, can write now.\";\n-        Util.stdoutWithTime(msg);\n-        LOG.info(msg);\n-    }\n-\n-    /*\n-     * Add anything necessary here if there is meta data need to be fixed.\n-     */\n-    public void fixBugAfterMetadataReplayed(boolean waitCatalogReady) {\n-        if (waitCatalogReady) {\n-            while (!isReady()) {\n-                try {\n-                    Thread.sleep(10 * 1000);\n-                } catch (InterruptedException e) {\n-                    e.printStackTrace();\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"start to fix meta data bug\");\n-        loadManager.fixLoadJobMetaBugs(globalTransactionMgr);\n-    }\n-\n-    // start all daemon threads only running on Master\n-    private void startMasterOnlyDaemonThreads() {\n-        // start checkpoint thread\n-        checkpointer = new Checkpoint(editLog);\n-        checkpointer.setMetaContext(metaContext);\n-        // set \"checkpointThreadId\" before the checkpoint thread start, because the thread\n-        // need to check the \"checkpointThreadId\" when running.\n-        checkpointThreadId = checkpointer.getId();\n-\n-        checkpointer.start();\n-        LOG.info(\"checkpointer thread started. thread id is {}\", checkpointThreadId);\n-\n-        // heartbeat mgr\n-        heartbeatMgr.setMaster(clusterId, token, epoch);\n-        heartbeatMgr.start();\n-        // Load checker\n-        LoadChecker.init(Config.load_checker_interval_second * 1000L);\n-        LoadChecker.startAll();\n-        // New load scheduler\n-        loadManager.prepareJobs();\n-        loadJobScheduler.start();\n-        loadTimeoutChecker.start();\n-        loadEtlChecker.start();\n-        loadLoadingChecker.start();\n-        // Export checker\n-        ExportChecker.init(Config.export_checker_interval_second * 1000L);\n-        ExportChecker.startAll();\n-        // Tablet checker and scheduler\n-        tabletChecker.start();\n-        tabletScheduler.start();\n-        // Colocate tables balancer\n-        if (!Config.disable_colocate_join) {\n-            ColocateTableBalancer.getInstance().start();\n-        }\n-        // Publish Version Daemon\n-        publishVersionDaemon.start();\n-        // Start txn cleaner\n-        txnCleaner.start();\n-        // Alter\n-        getAlterInstance().start();\n-        // Consistency checker\n-        getConsistencyChecker().start();\n-        // Backup handler\n-        getBackupHandler().start();\n-        // catalog recycle bin\n-        getRecycleBin().start();\n-        // time printer\n-        createTimePrinter();\n-        timePrinter.start();\n-        // deploy manager\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            LOG.info(\"deploy manager {} start\", deployManager.getName());\n-            deployManager.start();\n-        }\n-        // start routine load scheduler\n-        routineLoadScheduler.start();\n-        routineLoadTaskScheduler.start();\n-        // start dynamic partition task\n-        dynamicPartitionScheduler.start();\n-    }\n-\n-    // start threads that should running on all FE\n-    private void startNonMasterDaemonThreads() {\n-        tabletStatMgr.start();\n-        // load and export job label cleaner thread\n-        labelCleaner.start();\n-        // ES state store\n-        esRepository.start();\n-        // domain resolver\n-        domainResolver.start();\n-    }\n-\n-    private void transferToNonMaster(FrontendNodeType newType) {\n-        isReady.set(false);\n-\n-        if (feType == FrontendNodeType.OBSERVER || feType == FrontendNodeType.FOLLOWER) {\n-            Preconditions.checkState(newType == FrontendNodeType.UNKNOWN);\n-            LOG.warn(\"{} to UNKNOWN, still offer read service\", feType.name());\n-            // not set canRead here, leave canRead as what is was.\n-            // if meta out of date, canRead will be set to false in replayer thread.\n-            metaReplayState.setTransferToUnknown();\n-            return;\n-        }\n-\n-        // transfer from INIT/UNKNOWN to OBSERVER/FOLLOWER\n-\n-        // add helper sockets\n-        if (Config.edit_log_type.equalsIgnoreCase(\"BDB\")) {\n-            for (Frontend fe : frontends.values()) {\n-                if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                    ((BDBHA) getHaProtocol()).addHelperSocket(fe.getHost(), fe.getEditLogPort());\n-                }\n-            }\n-        }\n-\n-        if (replayer == null) {\n-            createReplayer();\n-            replayer.start();\n-        }\n-\n-        // 'isReady' will be set to true in 'setCanRead()' method\n-        fixBugAfterMetadataReplayed(true);\n-\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-    }\n-\n-    /*\n-     * If the current node is not in the frontend list, then exit. This may\n-     * happen when this node is removed from frontend list, and the drop\n-     * frontend log is deleted because of checkpoint.\n-     */\n-    private void checkCurrentNodeExist() {\n-        if (Config.metadata_failure_recovery.equals(\"true\")) {\n-            return;\n-        }\n-\n-        Frontend fe = checkFeExist(selfNode.first, selfNode.second);\n-        if (fe == null) {\n-            LOG.error(\"current node is not added to the cluster, will exit\");\n-            System.exit(-1);\n-        } else if (fe.getRole() != role) {\n-            LOG.error(\"current node role is {} not match with frontend recorded role {}. will exit\", role,\n-                    fe.getRole());\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private boolean getVersionFileFromHelper(Pair<String, Integer> helperNode) throws IOException {\n-        try {\n-            String url = \"http://\" + helperNode.first + \":\" + Config.http_port + \"/version\";\n-            File dir = new File(this.imageDir);\n-            MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000,\n-                    MetaHelper.getOutputStream(Storage.VERSION_FILE, dir));\n-            MetaHelper.complete(Storage.VERSION_FILE, dir);\n-            return true;\n-        } catch (Exception e) {\n-            LOG.warn(e);\n-        }\n-\n-        return false;\n-    }\n-\n-    private void getNewImage(Pair<String, Integer> helperNode) throws IOException {\n-        long localImageVersion = 0;\n-        Storage storage = new Storage(this.imageDir);\n-        localImageVersion = storage.getImageSeq();\n-\n-        try {\n-            URL infoUrl = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port + \"/info\");\n-            StorageInfo info = getStorageInfo(infoUrl);\n-            long version = info.getImageSeq();\n-            if (version > localImageVersion) {\n-                String url = \"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/image?version=\" + version;\n-                String filename = Storage.IMAGE + \".\" + version;\n-                File dir = new File(this.imageDir);\n-                MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000, MetaHelper.getOutputStream(filename, dir));\n-                MetaHelper.complete(filename, dir);\n-            }\n-        } catch (Exception e) {\n-            return;\n-        }\n-    }\n-\n-    private boolean isMyself() {\n-        Preconditions.checkNotNull(selfNode);\n-        Preconditions.checkNotNull(helperNodes);\n-        LOG.debug(\"self: {}. helpers: {}\", selfNode, helperNodes);\n-        // if helper nodes contain it self, remove other helpers\n-        boolean containSelf = false;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            if (selfNode.equals(helperNode)) {\n-                containSelf = true;\n-            }\n-        }\n-        if (containSelf) {\n-            helperNodes.clear();\n-            helperNodes.add(selfNode);\n-        }\n-\n-        return containSelf;\n-    }\n-\n-    private StorageInfo getStorageInfo(URL url) throws IOException {\n-        ObjectMapper mapper = new ObjectMapper();\n-\n-        HttpURLConnection connection = null;\n-        try {\n-            connection = (HttpURLConnection) url.openConnection();\n-            connection.setConnectTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            connection.setReadTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            return mapper.readValue(connection.getInputStream(), StorageInfo.class);\n-        } finally {\n-            if (connection != null) {\n-                connection.disconnect();\n-            }\n-        }\n-    }\n-\n-    public boolean hasReplayer() {\n-        return replayer != null;\n-    }\n-\n-    public void loadImage(String imageDir) throws IOException, DdlException {\n-        Storage storage = new Storage(imageDir);\n-        clusterId = storage.getClusterID();\n-        File curFile = storage.getCurrentImageFile();\n-        if (!curFile.exists()) {\n-            // image.0 may not exist\n-            LOG.info(\"image does not exist: {}\", curFile.getAbsolutePath());\n-            return;\n-        }\n-        replayedJournalId.set(storage.getImageSeq());\n-        LOG.info(\"start load image from {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-        long loadImageStartTime = System.currentTimeMillis();\n-        DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(curFile)));\n-\n-        long checksum = 0;\n-        try {\n-            checksum = loadHeader(dis, checksum);\n-            checksum = loadMasterInfo(dis, checksum);\n-            checksum = loadFrontends(dis, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().loadBackends(dis, checksum);\n-            checksum = loadDb(dis, checksum);\n-            // ATTN: this should be done after load Db, and before loadAlterJob\n-            recreateTabletInvertIndex();\n-            // rebuild es state state\n-            esRepository.loadTableFromCatalog();\n-\n-            checksum = loadLoadJob(dis, checksum);\n-            checksum = loadAlterJob(dis, checksum);\n-            checksum = loadRecycleBin(dis, checksum);\n-            checksum = loadGlobalVariable(dis, checksum);\n-            checksum = loadCluster(dis, checksum);\n-            checksum = loadBrokers(dis, checksum);\n-            checksum = loadResources(dis, checksum);\n-            checksum = loadExportJob(dis, checksum);\n-            checksum = loadBackupHandler(dis, checksum);\n-            checksum = loadPaloAuth(dis, checksum);\n-            // global transaction must be replayed before load jobs v2\n-            checksum = loadTransactionState(dis, checksum);\n-            checksum = loadColocateTableIndex(dis, checksum);\n-            checksum = loadRoutineLoadJobs(dis, checksum);\n-            checksum = loadLoadJobsV2(dis, checksum);\n-            checksum = loadSmallFiles(dis, checksum);\n-            checksum = loadPlugins(dis, checksum);\n-            checksum = loadDeleteHandler(dis, checksum);\n-\n-            long remoteChecksum = dis.readLong();\n-            Preconditions.checkState(remoteChecksum == checksum, remoteChecksum + \" vs. \" + checksum);\n-        } finally {\n-            dis.close();\n-        }\n-\n-        long loadImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished to load image in \" + (loadImageEndTime - loadImageStartTime) + \" ms\");\n-    }\n-\n-    private void recreateTabletInvertIndex() {\n-        if (isCheckpointThread()) {\n-            return;\n-        }\n-\n-        // create inverted index\n-        TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-        for (Database db : this.fullNameToDb.values()) {\n-            long dbId = db.getId();\n-            for (Table table : db.getTables()) {\n-                if (table.getType() != TableType.OLAP) {\n-                    continue;\n-                }\n-\n-                OlapTable olapTable = (OlapTable) table;\n-                long tableId = olapTable.getId();\n-                Collection<Partition> allPartitions = olapTable.getAllPartitions();\n-                for (Partition partition : allPartitions) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = index.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : index.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                                if (MetaContext.get().getMetaVersion() < FeMetaVersion.VERSION_48) {\n-                                    // set replica's schema hash\n-                                    replica.setSchemaHash(schemaHash);\n-                                }\n-                            }\n-                        }\n-                    } // end for indices\n-                } // end for partitions\n-            } // end for tables\n-        } // end for dbs\n-    }\n-\n-    public long loadHeader(DataInputStream dis, long checksum) throws IOException {\n-        int journalVersion = dis.readInt();\n-        long newChecksum = checksum ^ journalVersion;\n-        MetaContext.get().setMetaVersion(journalVersion);\n-\n-        long replayedJournalId = dis.readLong();\n-        newChecksum ^= replayedJournalId;\n-\n-        long catalogId = dis.readLong();\n-        newChecksum ^= catalogId;\n-        idGenerator.setId(catalogId);\n-\n-        if (journalVersion >= FeMetaVersion.VERSION_32) {\n-            isDefaultClusterCreated = dis.readBoolean();\n-        }\n-\n-        LOG.info(\"finished replay header from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadMasterInfo(DataInputStream dis, long checksum) throws IOException {\n-        masterIp = Text.readString(dis);\n-        masterRpcPort = dis.readInt();\n-        long newChecksum = checksum ^ masterRpcPort;\n-        masterHttpPort = dis.readInt();\n-        newChecksum ^= masterHttpPort;\n-\n-        LOG.info(\"finished replay masterInfo from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadFrontends(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; i++) {\n-                Frontend fe = Frontend.read(dis);\n-                replayAddFrontend(fe);\n-            }\n-            \n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_41) {\n-                    Frontend fe = Frontend.read(dis);\n-                    removedFrontends.add(fe.getNodeName());\n-                } else {\n-                    removedFrontends.add(Text.readString(dis));\n-                }\n-            }\n-            return newChecksum;\n-        }\n-        LOG.info(\"finished replay frontends from image\");\n-        return checksum;\n-    }\n-\n-    public long loadDb(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        int dbCount = dis.readInt();\n-        long newChecksum = checksum ^ dbCount;\n-        for (long i = 0; i < dbCount; ++i) {\n-            Database db = new Database();\n-            db.readFields(dis);\n-            newChecksum ^= db.getId();\n-            idToDb.put(db.getId(), db);\n-            fullNameToDb.put(db.getFullName(), db);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.put(db.getAttachDb(), db);\n-            }\n-            globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-        }\n-        LOG.info(\"finished replay databases from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadLoadJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        // load jobs\n-        int jobSize = dis.readInt();\n-        long newChecksum = checksum ^ jobSize;\n-        for (int i = 0; i < jobSize; i++) {\n-            long dbId = dis.readLong();\n-            newChecksum ^= dbId;\n-\n-            int loadJobCount = dis.readInt();\n-            newChecksum ^= loadJobCount;\n-            for (int j = 0; j < loadJobCount; j++) {\n-                LoadJob job = new LoadJob();\n-                job.readFields(dis);\n-                long currentTimeMs = System.currentTimeMillis();\n-\n-                // Delete the history load jobs that are older than\n-                // LABEL_KEEP_MAX_MS\n-                // This job must be FINISHED or CANCELLED\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                        || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                    load.unprotectAddLoadJob(job, true /* replay */);\n-                }\n-            }\n-        }\n-\n-        // delete jobs\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_11) {\n-            jobSize = dis.readInt();\n-            newChecksum ^= jobSize;\n-            for (int i = 0; i < jobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteCount = dis.readInt();\n-                newChecksum ^= deleteCount;\n-                for (int j = 0; j < deleteCount; j++) {\n-                    DeleteInfo deleteInfo = new DeleteInfo();\n-                    deleteInfo.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history delete jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    if ((currentTimeMs - deleteInfo.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second) {\n-                        load.unprotectAddDeleteInfo(deleteInfo);\n-                    }\n-                }\n-            }\n-        }\n-\n-        // load error hub info\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_24) {\n-            LoadErrorHub.Param param = new LoadErrorHub.Param();\n-            param.readFields(dis);\n-            load.setLoadErrorHubInfo(param);\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            // 4. load delete jobs\n-            int deleteJobSize = dis.readInt();\n-            newChecksum ^= deleteJobSize;\n-            for (int i = 0; i < deleteJobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteJobCount = dis.readInt();\n-                newChecksum ^= deleteJobCount;\n-                for (int j = 0; j < deleteJobCount; j++) {\n-                    LoadJob job = new LoadJob();\n-                    job.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history load jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    // This job must be FINISHED or CANCELLED\n-                    if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                            || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                        load.unprotectAddLoadJob(job, true /* replay */);\n-                    }\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"finished replay loadJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadExportJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        long newChecksum = checksum;\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_32) {\n-            int size = dis.readInt();\n-            newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; ++i) {\n-                long jobId = dis.readLong();\n-                newChecksum ^= jobId;\n-                ExportJob job = new ExportJob();\n-                job.readFields(dis);\n-                exportMgr.unprotectAddJob(job);\n-            }\n-        }\n-        LOG.info(\"finished replay exportJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum) throws IOException {\n-        long newChecksum = checksum;\n-        for (JobType type : JobType.values()) {\n-            if (type == JobType.DECOMMISSION_BACKEND) {\n-                if (Catalog.getCurrentCatalogJournalVersion() >= 5) {\n-                    newChecksum = loadAlterJob(dis, newChecksum, type);\n-                }\n-            } else {\n-                newChecksum = loadAlterJob(dis, newChecksum, type);\n-            }\n-        }\n-        LOG.info(\"finished replay alterJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = dis.readInt();\n-        long newChecksum = checksum ^ size;\n-        for (int i = 0; i < size; i++) {\n-            long tableId = dis.readLong();\n-            newChecksum ^= tableId;\n-            AlterJob job = AlterJob.read(dis);\n-            alterJobs.put(tableId, job);\n-\n-            // init job\n-            Database db = getDb(job.getDbId());\n-            // should check job state here because the job is finished but not removed from alter jobs list\n-            if (db != null && (job.getState() == org.apache.doris.alter.AlterJob.JobState.PENDING\n-                    || job.getState() == org.apache.doris.alter.AlterJob.JobState.RUNNING)) {\n-                job.replayInitJob(db);\n-            }\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= 2) {\n-            // finished or cancelled jobs\n-            long currentTimeMs = System.currentTimeMillis();\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                long tableId = dis.readLong();\n-                newChecksum ^= tableId;\n-                AlterJob job = AlterJob.read(dis);\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.history_job_keep_max_second) {\n-                    // delete history jobs\n-                    finishedOrCancelledAlterJobs.add(job);\n-                }\n-            }\n-        }\n-\n-        // alter job v2\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_61) {\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                AlterJobV2 alterJobV2 = AlterJobV2.read(dis);\n-                if (type == JobType.ROLLUP || type == JobType.SCHEMA_CHANGE) {\n-                    if (type == JobType.ROLLUP) {\n-                        this.getRollupHandler().addAlterJobV2(alterJobV2);\n-                    } else {\n-                        alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                    }\n-                    // ATTN : we just want to add tablet into TabletInvertedIndex when only PendingJob is checkpointed\n-                    // to prevent TabletInvertedIndex data loss,\n-                    // So just use AlterJob.replay() instead of AlterHandler.replay().\n-                    if (alterJobV2.getJobState() == AlterJobV2.JobState.PENDING) {\n-                        alterJobV2.replay(alterJobV2);\n-                        LOG.info(\"replay pending alter job when load alter job {} \", alterJobV2.getJobId());\n-                    }\n-                } else {\n-                    alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                }\n-            }\n-        }\n-\n-        return newChecksum;\n-    }\n-\n-    public long loadBackupHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_42) {\n-            getBackupHandler().readFields(dis);\n-        }\n-        getBackupHandler().setCatalog(this);\n-        LOG.info(\"finished replay backupHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveBackupHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getBackupHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadDeleteHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_82) {\n-            this.deleteHandler = DeleteHandler.read(dis);\n-        }\n-        LOG.info(\"finished replay deleteHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveDeleteHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getDeleteHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPaloAuth(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_43) {\n-            // CAN NOT use PaloAuth.read(), cause this auth instance is already passed to DomainResolver\n-            auth.readFields(dis);\n-        }\n-        LOG.info(\"finished replay paloAuth from image\");\n-        return checksum;\n-    }\n-\n-    public long loadTransactionState(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            globalTransactionMgr.readFields(dis);\n-            LOG.info(\"finished replay transactionState from image\");\n-            return newChecksum;\n-        }\n-        return checksum;\n-    }\n-\n-    public long loadRecycleBin(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_10) {\n-            recycleBin.readFields(dis);\n-            if (!isCheckpointThread()) {\n-                // add tablet in Recycle bin to TabletInvertedIndex\n-                recycleBin.addTabletToInvertedIndex();\n-            }\n-            // create DatabaseTransactionMgr for db in recycle bin.\n-            // these dbs do not exist in `idToDb` of the catalog.\n-            for (Long dbId : recycleBin.getAllDbIds()) {\n-                globalTransactionMgr.addDatabaseTransactionMgr(dbId);\n-            }\n-        }\n-        LOG.info(\"finished replay recycleBin from image\");\n-        return checksum;\n-    }\n-\n-    public long loadColocateTableIndex(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_46) {\n-            Catalog.getCurrentColocateIndex().readFields(dis);\n-        }\n-        LOG.info(\"finished replay colocateTableIndex from image\");\n-        return checksum;\n-    }\n-\n-    public long loadRoutineLoadJobs(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_49) {\n-            Catalog.getCurrentCatalog().getRoutineLoadManager().readFields(dis);\n-        }\n-        LOG.info(\"finished replay routineLoadJobs from image\");\n-        return checksum;\n-    }\n-\n-    public long loadLoadJobsV2(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_50) {\n-            loadManager.readFields(in);\n-        }\n-        LOG.info(\"finished replay loadJobsV2 from image\");\n-        return checksum;\n-    }\n-\n-    public long loadResources(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_87) {\n-            resourceMgr = ResourceMgr.read(in);\n-        }\n-        LOG.info(\"finished replay resources from image\");\n-        return checksum;\n-    }\n-\n-    public long loadSmallFiles(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_52) {\n-            smallFileMgr.readFields(in);\n-        }\n-        LOG.info(\"finished replay smallFiles from image\");\n-        return checksum;\n-    }\n-\n-    // Only called by checkpoint thread\n-    public void saveImage() throws IOException {\n-        // Write image.ckpt\n-        Storage storage = new Storage(this.imageDir);\n-        File curFile = storage.getImageFile(replayedJournalId.get());\n-        File ckpt = new File(this.imageDir, Storage.IMAGE_NEW);\n-        saveImage(ckpt, replayedJournalId.get());\n-\n-        // Move image.ckpt to image.dataVersion\n-        LOG.info(\"Move \" + ckpt.getAbsolutePath() + \" to \" + curFile.getAbsolutePath());\n-        if (!ckpt.renameTo(curFile)) {\n-            curFile.delete();\n-            throw new IOException();\n-        }\n-    }\n-\n-    public void saveImage(File curFile, long replayedJournalId) throws IOException {\n-        if (!curFile.exists()) {\n-            curFile.createNewFile();\n-        }\n-\n-        // save image does not need any lock. because only checkpoint thread will call this method.\n-        LOG.info(\"start save image to {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-\n-        long checksum = 0;\n-        long saveImageStartTime = System.currentTimeMillis();\n-        try (DataOutputStream dos = new DataOutputStream(new FileOutputStream(curFile))) {\n-            checksum = saveHeader(dos, replayedJournalId, checksum);\n-            checksum = saveMasterInfo(dos, checksum);\n-            checksum = saveFrontends(dos, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().saveBackends(dos, checksum);\n-            checksum = saveDb(dos, checksum);\n-            checksum = saveLoadJob(dos, checksum);\n-            checksum = saveAlterJob(dos, checksum);\n-            checksum = saveRecycleBin(dos, checksum);\n-            checksum = saveGlobalVariable(dos, checksum);\n-            checksum = saveCluster(dos, checksum);\n-            checksum = saveBrokers(dos, checksum);\n-            checksum = saveResources(dos, checksum);\n-            checksum = saveExportJob(dos, checksum);\n-            checksum = saveBackupHandler(dos, checksum);\n-            checksum = savePaloAuth(dos, checksum);\n-            checksum = saveTransactionState(dos, checksum);\n-            checksum = saveColocateTableIndex(dos, checksum);\n-            checksum = saveRoutineLoadJobs(dos, checksum);\n-            checksum = saveLoadJobsV2(dos, checksum);\n-            checksum = saveSmallFiles(dos, checksum);\n-            checksum = savePlugins(dos, checksum);\n-            checksum = saveDeleteHandler(dos, checksum);\n-            dos.writeLong(checksum);\n-        }\n-\n-        long saveImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished save image {} in {} ms. checksum is {}\",\n-                curFile.getAbsolutePath(), (saveImageEndTime - saveImageStartTime), checksum);\n-    }\n-\n-    public long saveHeader(DataOutputStream dos, long replayedJournalId, long checksum) throws IOException {\n-        // Write meta version\n-        checksum ^= FeConstants.meta_version;\n-        dos.writeInt(FeConstants.meta_version);\n-\n-        // Write replayed journal id\n-        checksum ^= replayedJournalId;\n-        dos.writeLong(replayedJournalId);\n-\n-        // Write id\n-        long id = idGenerator.getBatchEndId();\n-        checksum ^= id;\n-        dos.writeLong(id);\n-\n-        dos.writeBoolean(isDefaultClusterCreated);\n-\n-        return checksum;\n-    }\n-\n-    public long saveMasterInfo(DataOutputStream dos, long checksum) throws IOException {\n-        Text.writeString(dos, masterIp);\n-\n-        checksum ^= masterRpcPort;\n-        dos.writeInt(masterRpcPort);\n-\n-        checksum ^= masterHttpPort;\n-        dos.writeInt(masterHttpPort);\n-\n-        return checksum;\n-    }\n-\n-    public long saveFrontends(DataOutputStream dos, long checksum) throws IOException {\n-        int size = frontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (Frontend fe : frontends.values()) {\n-            fe.write(dos);\n-        }\n-\n-        size = removedFrontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (String feName : removedFrontends) {\n-            Text.writeString(dos, feName);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveDb(DataOutputStream dos, long checksum) throws IOException {\n-        int dbCount = idToDb.size() - nameToCluster.keySet().size();\n-        checksum ^= dbCount;\n-        dos.writeInt(dbCount);\n-        for (Map.Entry<Long, Database> entry : idToDb.entrySet()) {\n-            Database db = entry.getValue();\n-            String dbName = db.getFullName();\n-            // Don't write information_schema db meta\n-            if (!InfoSchemaDb.isInfoSchemaDb(dbName)) {\n-                checksum ^= entry.getKey();\n-                db.readLock();\n-                try {\n-                    db.write(dos);\n-                } finally {\n-                    db.readUnlock();\n-                }\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveLoadJob(DataOutputStream dos, long checksum) throws IOException {\n-        // 1. save load.dbToLoadJob\n-        Map<Long, List<LoadJob>> dbToLoadJob = load.getDbToLoadJobs();\n-        int jobSize = dbToLoadJob.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToLoadJob.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> loadJobs = entry.getValue();\n-            int loadJobCount = loadJobs.size();\n-            checksum ^= loadJobCount;\n-            dos.writeInt(loadJobCount);\n-            for (LoadJob job : loadJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        // 2. save delete jobs\n-        Map<Long, List<DeleteInfo>> dbToDeleteInfos = load.getDbToDeleteInfos();\n-        jobSize = dbToDeleteInfos.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<DeleteInfo>> entry : dbToDeleteInfos.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<DeleteInfo> deleteInfos = entry.getValue();\n-            int deletInfoCount = deleteInfos.size();\n-            checksum ^= deletInfoCount;\n-            dos.writeInt(deletInfoCount);\n-            for (DeleteInfo deleteInfo : deleteInfos) {\n-                deleteInfo.write(dos);\n-            }\n-        }\n-\n-        // 3. load error hub info\n-        LoadErrorHub.Param param = load.getLoadErrorHubInfo();\n-        param.write(dos);\n-\n-        // 4. save delete load job info\n-        Map<Long, List<LoadJob>> dbToDeleteJobs = load.getDbToDeleteJobs();\n-        int deleteJobSize = dbToDeleteJobs.size();\n-        checksum ^= deleteJobSize;\n-        dos.writeInt(deleteJobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToDeleteJobs.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> deleteJobs = entry.getValue();\n-            int deleteJobCount = deleteJobs.size();\n-            checksum ^= deleteJobCount;\n-            dos.writeInt(deleteJobCount);\n-            for (LoadJob job : deleteJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveExportJob(DataOutputStream dos, long checksum) throws IOException {\n-        Map<Long, ExportJob> idToJob = exportMgr.getIdToJob();\n-        int size = idToJob.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (ExportJob job : idToJob.values()) {\n-            long jobId = job.getId();\n-            checksum ^= jobId;\n-            dos.writeLong(jobId);\n-            job.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum) throws IOException {\n-        for (JobType type : JobType.values()) {\n-            checksum = saveAlterJob(dos, checksum, type);\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getRollupHandler().getAlterJobsV2();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = alterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (Entry<Long, AlterJob> entry : alterJobs.entrySet()) {\n-            long tableId = entry.getKey();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            entry.getValue().write(dos);\n-        }\n-\n-        // finished or cancelled jobs\n-        size = finishedOrCancelledAlterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJob alterJob : finishedOrCancelledAlterJobs) {\n-            long tableId = alterJob.getTableId();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            alterJob.write(dos);\n-        }\n-\n-        // alter job v2\n-        size = alterJobsV2.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJobV2 alterJobV2 : alterJobsV2.values()) {\n-            alterJobV2.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long savePaloAuth(DataOutputStream dos, long checksum) throws IOException {\n-        auth.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveTransactionState(DataOutputStream dos, long checksum) throws IOException {\n-        int size = globalTransactionMgr.getTransactionNum();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        globalTransactionMgr.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRecycleBin(DataOutputStream dos, long checksum) throws IOException {\n-        CatalogRecycleBin recycleBin = Catalog.getCurrentRecycleBin();\n-        recycleBin.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveColocateTableIndex(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentColocateIndex().write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRoutineLoadJobs(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getRoutineLoadManager().write(dos);\n-        return checksum;\n-    }\n-\n-    // global variable persistence\n-    public long loadGlobalVariable(DataInputStream in, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            VariableMgr.read(in);\n-        }\n-        LOG.info(\"finished replay globalVariable from image\");\n-        return checksum;\n-    }\n-\n-    public long saveGlobalVariable(DataOutputStream out, long checksum) throws IOException {\n-        VariableMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void replayGlobalVariable(SessionVariable variable) throws IOException, DdlException {\n-        VariableMgr.replayGlobalVariable(variable);\n-    }\n-\n-    public long saveLoadJobsV2(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getLoadManager().write(out);\n-        return checksum;\n-    }\n-\n-\tpublic long saveResources(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getResourceMgr().write(out);\n-        return checksum;\n-    }\n-\n-    private long saveSmallFiles(DataOutputStream out, long checksum) throws IOException {\n-        smallFileMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void createLabelCleaner() {\n-        labelCleaner = new MasterDaemon(\"LoadLabelCleaner\", Config.label_clean_interval_second * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                load.removeOldLoadJobs();\n-                load.removeOldDeleteJobs();\n-                loadManager.removeOldLoadJob();\n-                exportMgr.removeOldExportJobs();\n-            }\n-        };\n-    }\n-\n-    public void createTxnCleaner() {\n-        txnCleaner = new MasterDaemon(\"txnCleaner\", Config.transaction_clean_interval_second) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                globalTransactionMgr.removeExpiredAndTimeoutTxns();\n-            }\n-        };\n-    }\n-\n-    public void createReplayer() {\n-        replayer = new Daemon(\"replayer\", REPLAY_INTERVAL_MS) {\n-            protected void runOneCycle() {\n-                boolean err = false;\n-                boolean hasLog = false;\n-                try {\n-                    hasLog = replayJournal(-1);\n-                    metaReplayState.setOk();\n-                } catch (InsufficientLogException insufficientLogEx) {\n-                    // Copy the missing log files from a member of the\n-                    // replication group who owns the files\n-                    LOG.error(\"catch insufficient log exception. please restart.\", insufficientLogEx);\n-                    NetworkRestore restore = new NetworkRestore();\n-                    NetworkRestoreConfig config = new NetworkRestoreConfig();\n-                    config.setRetainLogFiles(false);\n-                    restore.execute(insufficientLogEx, config);\n-                    System.exit(-1);\n-                } catch (Throwable e) {\n-                    LOG.error(\"replayer thread catch an exception when replay journal.\", e);\n-                    metaReplayState.setException(e);\n-                    try {\n-                        Thread.sleep(5000);\n-                    } catch (InterruptedException e1) {\n-                        LOG.error(\"sleep got exception. \", e);\n-                    }\n-                    err = true;\n-                }\n-\n-                setCanRead(hasLog, err);\n-            }\n-        };\n-        replayer.setMetaContext(metaContext);\n-    }\n-\n-    private void setCanRead(boolean hasLog, boolean err) {\n-        if (err) {\n-            canRead.set(false);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        if (Config.ignore_meta_check) {\n-            // can still offer read, but is not ready\n-            canRead.set(true);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        long currentTimeMs = System.currentTimeMillis();\n-        if (currentTimeMs - synchronizedTimeMs > Config.meta_delay_toleration_second * 1000) {\n-            // we still need this log to observe this situation\n-            // but service may be continued when there is no log being replayed.\n-            LOG.warn(\"meta out of date. current time: {}, synchronized time: {}, has log: {}, fe type: {}\",\n-                    currentTimeMs, synchronizedTimeMs, hasLog, feType);\n-            if (hasLog || feType == FrontendNodeType.UNKNOWN) {\n-                // 1. if we read log from BDB, which means master is still alive.\n-                // So we need to set meta out of date.\n-                // 2. if we didn't read any log from BDB and feType is UNKNOWN,\n-                // which means this non-master node is disconnected with master.\n-                // So we need to set meta out of date either.\n-                metaReplayState.setOutOfDate(currentTimeMs, synchronizedTimeMs);\n-                canRead.set(false);\n-                isReady.set(false);\n-            }\n-\n-            // sleep 5s to avoid numerous 'meta out of date' log\n-            try {\n-                Thread.sleep(5000L);\n-            } catch (InterruptedException e) {\n-                LOG.error(\"unhandled exception when sleep\", e);\n-            }\n-\n-        } else {\n-            canRead.set(true);\n-            isReady.set(true);\n-        }\n-    }\n-\n-    public void notifyNewFETypeTransfer(FrontendNodeType newType) {\n-        try {\n-            String msg = \"notify new FE type transfer: \" + newType;\n-            LOG.warn(msg);\n-            Util.stdoutWithTime(msg);\n-            this.typeTransferQueue.put(newType);\n-        } catch (InterruptedException e) {\n-            LOG.error(\"failed to put new FE type: {}\", newType, e);\n-        }\n-    }\n-\n-    public void createStateListener() {\n-        listener = new Daemon(\"stateListener\", STATE_CHANGE_CHECK_INTERVAL_MS) {\n-            @Override\n-            protected synchronized void runOneCycle() {\n-\n-                while (true) {\n-                    FrontendNodeType newType = null;\n-                    try {\n-                        newType = typeTransferQueue.take();\n-                    } catch (InterruptedException e) {\n-                        LOG.error(\"got exception when take FE type from queue\", e);\n-                        Util.stdoutWithTime(\"got exception when take FE type from queue. \" + e.getMessage());\n-                        System.exit(-1);\n-                    }\n-                    Preconditions.checkNotNull(newType);\n-                    LOG.info(\"begin to transfer FE type from {} to {}\", feType, newType);\n-                    if (feType == newType) {\n-                        return;\n-                    }\n-\n-                    /*\n-                     * INIT -> MASTER: transferToMaster\n-                     * INIT -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * UNKNOWN -> MASTER: transferToMaster\n-                     * UNKNOWN -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * FOLLOWER -> MASTER: transferToMaster\n-                     * FOLLOWER/OBSERVER -> INIT/UNKNOWN: set isReady to false\n-                     */\n-                    switch (feType) {\n-                        case INIT: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                case UNKNOWN:\n-                                    break;\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case UNKNOWN: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case FOLLOWER: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case OBSERVER: {\n-                            switch (newType) {\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case MASTER: {\n-                            // exit if master changed to any other type\n-                            String msg = \"transfer FE type from MASTER to \" + newType.name() + \". exit\";\n-                            LOG.error(msg);\n-                            Util.stdoutWithTime(msg);\n-                            System.exit(-1);\n-                        }\n-                        default:\n-                            break;\n-                    } // end switch formerFeType\n-\n-                    feType = newType;\n-                    LOG.info(\"finished to transfer FE type to {}\", feType);\n-                }\n-            } // end runOneCycle\n-        };\n-\n-        listener.setMetaContext(metaContext);\n-    }\n-\n-    public synchronized boolean replayJournal(long toJournalId) {\n-        long newToJournalId = toJournalId;\n-        if (newToJournalId == -1) {\n-            newToJournalId = getMaxJournalId();\n-        }\n-        if (newToJournalId <= replayedJournalId.get()) {\n-            return false;\n-        }\n-\n-        LOG.info(\"replayed journal id is {}, replay to journal id is {}\", replayedJournalId, newToJournalId);\n-        JournalCursor cursor = editLog.read(replayedJournalId.get() + 1, newToJournalId);\n-        if (cursor == null) {\n-            LOG.warn(\"failed to get cursor from {} to {}\", replayedJournalId.get() + 1, newToJournalId);\n-            return false;\n-        }\n-\n-        long startTime = System.currentTimeMillis();\n-        boolean hasLog = false;\n-        while (true) {\n-            JournalEntity entity = cursor.next();\n-            if (entity == null) {\n-                break;\n-            }\n-            hasLog = true;\n-            EditLog.loadJournal(this, entity);\n-            replayedJournalId.incrementAndGet();\n-            LOG.debug(\"journal {} replayed.\", replayedJournalId);\n-            if (feType != FrontendNodeType.MASTER) {\n-                journalObservable.notifyObservers(replayedJournalId.get());\n-            }\n-            if (MetricRepo.isInit.get()) {\n-                // Metric repo may not init after this replay thread start\n-                MetricRepo.COUNTER_EDIT_LOG_READ.increase(1L);\n-            }\n-        }\n-        long cost = System.currentTimeMillis() - startTime;\n-        if (cost >= 1000) {\n-            LOG.warn(\"replay journal cost too much time: {} replayedJournalId: {}\", cost, replayedJournalId);\n-        }\n-\n-        return hasLog;\n-    }\n-\n-    public void createTimePrinter() {\n-        // time printer will write timestamp edit log every 10 seconds\n-        timePrinter = new MasterDaemon(\"timePrinter\", 10 * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                Timestamp stamp = new Timestamp();\n-                editLog.logTimestamp(stamp);\n-            }\n-        };\n-    }\n-\n-    public void addFrontend(FrontendNodeType role, String host, int editLogPort) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, editLogPort);\n-            if (fe != null) {\n-                throw new DdlException(\"frontend already exists \" + fe);\n-            }\n-\n-            String nodeName = genFeNodeName(host, editLogPort, false /* new name style */);\n-\n-            if (removedFrontends.contains(nodeName)) {\n-                throw new DdlException(\"frontend name already exists \" + nodeName + \". Try again\");\n-            }\n-\n-            fe = new Frontend(role, nodeName, host, editLogPort);\n-            frontends.put(nodeName, fe);\n-            if (role == FrontendNodeType.FOLLOWER || role == FrontendNodeType.REPLICA) {\n-                ((BDBHA) getHaProtocol()).addHelperSocket(host, editLogPort);\n-                helperNodes.add(Pair.create(host, editLogPort));\n-            }\n-            editLog.logAddFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropFrontend(FrontendNodeType role, String host, int port) throws DdlException {\n-        if (host.equals(selfNode.first) && port == selfNode.second && feType == FrontendNodeType.MASTER) {\n-            throw new DdlException(\"can not drop current master node.\");\n-        }\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, port);\n-            if (fe == null) {\n-                throw new DdlException(\"frontend does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            if (fe.getRole() != role) {\n-                throw new DdlException(role.toString() + \" does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            frontends.remove(fe.getNodeName());\n-            removedFrontends.add(fe.getNodeName());\n-\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                haProtocol.removeElectableNode(fe.getNodeName());\n-                helperNodes.remove(Pair.create(host, port));\n-            }\n-            editLog.logRemoveFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Frontend checkFeExist(String host, int port) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host) && fe.getEditLogPort() == port) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByHost(String host) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByName(String name) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getNodeName().equals(name)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-\n-    // The interface which DdlExecutor needs.\n-    public void createDb(CreateDbStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        String fullDbName = stmt.getFullDbName();\n-        long id = 0L;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_SELECT_CLUSTER, clusterName);\n-            }\n-            if (fullNameToDb.containsKey(fullDbName)) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create database[{}] which already exists\", fullDbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, fullDbName);\n-                }\n-            } else {\n-                id = getNextId();\n-                Database db = new Database(id, fullDbName);\n-                db.setClusterName(clusterName);\n-                unprotectCreateDb(db);\n-                editLog.logCreateDb(db);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-        LOG.info(\"createDb dbName = \" + fullDbName + \", id = \" + id);\n-    }\n-\n-    // For replay edit log, need't lock metadata\n-    public void unprotectCreateDb(Database db) {\n-        idToDb.put(db.getId(), db);\n-        fullNameToDb.put(db.getFullName(), db);\n-        final Cluster cluster = nameToCluster.get(db.getClusterName());\n-        cluster.addDb(db.getFullName(), db.getId());\n-        globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-    }\n-\n-    // for test\n-    public void addCluster(Cluster cluster) {\n-        nameToCluster.put(cluster.getName(), cluster);\n-        idToCluster.put(cluster.getId(), cluster);\n-    }\n-\n-    public void replayCreateDb(Database db) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateDb(db);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropDb(DropDbStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-\n-        // 1. check if database exists\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!fullNameToDb.containsKey(dbName)) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop database[{}] which does not exist\", dbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_DROP_EXISTS, dbName);\n-                }\n-            }\n-\n-            // 2. drop tables in db\n-            Database db = this.fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                if (stmt.isNeedCheckCommitedTxns()) {\n-                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), null, null)) {\n-                        throw new DdlException(\"There are still some commited txns cannot be aborted in db [\"\n-                                + dbName + \"], please wait for GlobalTransactionMgr to finish publish tasks.\" +\n-                                \" If you don't need to recover db, use DROPP DATABASE stmt (double P).\");\n-                    }\n-                }\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getAttachDb())) {\n-                    // We try to drop a hard link.\n-                    final DropLinkDbAndUpdateDbInfo info = new DropLinkDbAndUpdateDbInfo();\n-                    fullNameToDb.remove(db.getAttachDb());\n-                    db.setDbState(DbState.NORMAL);\n-                    info.setUpdateDbState(DbState.NORMAL);\n-                    final Cluster cluster = nameToCluster\n-                            .get(ClusterNamespace.getClusterNameFromFullName(db.getAttachDb()));\n-                    final BaseParam param = new BaseParam();\n-                    param.addStringParam(db.getAttachDb());\n-                    param.addLongParam(db.getId());\n-                    cluster.removeLinkDb(param);\n-                    info.setDropDbCluster(cluster.getName());\n-                    info.setDropDbId(db.getId());\n-                    info.setDropDbName(db.getAttachDb());\n-                    editLog.logDropLinkDb(info);\n-                    return;\n-                }\n-\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getFullName())) {\n-                    // We try to drop a db which other dbs attach to it,\n-                    // which is not allowed.\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                if (dbName.equals(db.getAttachDb()) && db.getDbState() == DbState.MOVE) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                // save table names for recycling\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            // 3. remove db from catalog\n-            idToDb.remove(db.getId());\n-            fullNameToDb.remove(db.getFullName());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-            editLog.logDropDb(dbName);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"finish drop database[{}]\", dbName);\n-    }\n-\n-    public void unprotectDropDb(Database db) {\n-        for (Table table : db.getTables()) {\n-            unprotectDropTable(db, table.getId());\n-        }\n-    }\n-\n-    public void replayDropLinkDb(DropLinkDbAndUpdateDbInfo info) {\n-        tryLock(true);\n-        try {\n-            final Database db = this.fullNameToDb.remove(info.getDropDbName());\n-            db.setDbState(info.getUpdateDbState());\n-            final Cluster cluster = nameToCluster\n-                    .get(info.getDropDbCluster());\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(db.getAttachDb());\n-            param.addLongParam(db.getId());\n-            cluster.removeLinkDb(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropDb(String dbName) throws DdlException {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            fullNameToDb.remove(dbName);\n-            idToDb.remove(db.getId());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void recoverDatabase(RecoverDbStmt recoverStmt) throws DdlException {\n-        // check is new db with same name already exist\n-        if (getDb(recoverStmt.getDbName()) != null) {\n-            throw new DdlException(\"Database[\" + recoverStmt.getDbName() + \"] already exist.\");\n-        }\n-\n-        Database db = Catalog.getCurrentRecycleBin().recoverDatabase(recoverStmt.getDbName());\n-\n-        // add db to catalog\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (fullNameToDb.containsKey(db.getFullName())) {\n-                throw new DdlException(\"Database[\" + db.getFullName() + \"] already exist.\");\n-                // it's ok that we do not put db back to CatalogRecycleBin\n-                // cause this db cannot recover any more\n-            }\n-\n-            fullNameToDb.put(db.getFullName(), db);\n-            idToDb.put(db.getId(), db);\n-\n-            // log\n-            RecoverInfo recoverInfo = new RecoverInfo(db.getId(), -1L, -1L);\n-            editLog.logRecoverDb(recoverInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"recover database[{}]\", db.getId());\n-    }\n-\n-    public void recoverTable(RecoverTableStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table != null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-            }\n-\n-            if (!Catalog.getCurrentRecycleBin().recoverTable(db, tableName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void recoverPartition(RecoverPartitionStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"table[\" + tableName + \"] is not OLAP table\");\n-            }\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            String partitionName = recoverStmt.getPartitionName();\n-            if (olapTable.getPartition(partitionName) != null) {\n-                throw new DdlException(\"partition[\" + partitionName + \"] already exist in table[\" + tableName + \"]\");\n-            }\n-\n-            Catalog.getCurrentRecycleBin().recoverPartition(db.getId(), olapTable, partitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseDatabase(long dbId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseDatabase(dbId);\n-    }\n-\n-    public void replayRecoverDatabase(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = Catalog.getCurrentRecycleBin().replayRecoverDatabase(dbId);\n-\n-        // add db to catalog\n-        replayCreateDb(db);\n-\n-        LOG.info(\"replay recover db[{}]\", dbId);\n-    }\n-\n-    public void alterDatabaseQuota(AlterDatabaseQuotaStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        QuotaType quotaType = stmt.getQuotaType();\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(stmt.getQuota());\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(stmt.getQuota());\n-        }\n-        long quota = stmt.getQuota();\n-        DatabaseInfo dbInfo = new DatabaseInfo(dbName, \"\", quota, quotaType);\n-        editLog.logAlterDb(dbInfo);\n-    }\n-\n-    public void replayAlterDatabaseQuota(String dbName, long quota, QuotaType quotaType) {\n-        Database db = getDb(dbName);\n-        Preconditions.checkNotNull(db);\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(quota);\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(quota);\n-        }\n-    }\n-\n-    public void renameDatabase(AlterDatabaseRename stmt) throws DdlException {\n-        String fullDbName = stmt.getDbName();\n-        String newFullDbName = stmt.getNewDbName();\n-        String clusterName = stmt.getClusterName();\n-\n-        if (fullDbName.equals(newFullDbName)) {\n-            throw new DdlException(\"Same database name\");\n-        }\n-\n-        Database db = null;\n-        Cluster cluster = null;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            // check if db exists\n-            db = fullNameToDb.get(fullDbName);\n-            if (db == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, fullDbName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK || db.getDbState() == DbState.MOVE) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_RENAME_DB_ERR, fullDbName);\n-            }\n-            // check if name is already used\n-            if (fullNameToDb.get(newFullDbName) != null) {\n-                throw new DdlException(\"Database name[\" + newFullDbName + \"] is already used\");\n-            }\n-\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            cluster.addDb(newFullDbName, db.getId());\n-            // 1. rename db\n-            db.setNameWithLock(newFullDbName);\n-\n-            // 2. add to meta. check again\n-            fullNameToDb.remove(fullDbName);\n-            fullNameToDb.put(newFullDbName, db);\n-\n-            DatabaseInfo dbInfo = new DatabaseInfo(fullDbName, newFullDbName, -1L, QuotaType.NONE);\n-            editLog.logDatabaseRename(dbInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"rename database[{}] to [{}]\", fullDbName, newFullDbName);\n-    }\n-\n-    public void replayRenameDatabase(String dbName, String newDbName) {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            db.setName(newDbName);\n-            cluster.addDb(newDbName, db.getId());\n-            fullNameToDb.remove(dbName);\n-            fullNameToDb.put(newDbName, db);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"replay rename database {} to {}\", dbName, newDbName);\n-    }\n-\n-    public void createTable(CreateTableStmt stmt) throws DdlException {\n-        String engineName = stmt.getEngineName();\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check if db exists\n-        Database db = getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // only internal table should check quota and cluster capacity\n-        if (!stmt.isExternal()) {\n-            // check cluster capacity\n-            Catalog.getCurrentSystemInfo().checkClusterCapacity(stmt.getClusterName());\n-            // check db quota\n-            db.checkQuota();\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create table[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        if (engineName.equals(\"olap\")) {\n-            createOlapTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"mysql\")) {\n-            createMysqlTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"broker\")) {\n-            createBrokerTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"elasticsearch\") || engineName.equalsIgnoreCase(\"es\")) {\n-            createEsTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"hive\")) {\n-            createHiveTable(db, stmt);\n-            return;\n-        } else {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_UNKNOWN_STORAGE_ENGINE, engineName);\n-        }\n-        Preconditions.checkState(false);\n-        return;\n-    }\n-\n-    public void addPartition(Database db, String tableName, AddPartitionClause addPartitionClause) throws DdlException {\n-        SingleRangePartitionDesc singlePartitionDesc = addPartitionClause.getSingeRangePartitionDesc();\n-        DistributionDesc distributionDesc = addPartitionClause.getDistributionDesc();\n-        boolean isTempPartition = addPartitionClause.isTempPartition();\n-\n-        DistributionInfo distributionInfo = null;\n-        OlapTable olapTable = null;\n-\n-        Map<Long, MaterializedIndexMeta> indexIdToMeta;\n-        Set<String> bfColumns = null;\n-\n-        String partitionName = singlePartitionDesc.getPartitionName();\n-\n-        // check\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            // check state\n-            olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-            }\n-\n-            // check partition type\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (partitionInfo.getType() != PartitionType.RANGE) {\n-                throw new DdlException(\"Only support adding partition to range partitioned table\");\n-            }\n-\n-            // check partition name\n-            if (olapTable.checkPartitionNameExist(partitionName)) {\n-                if (singlePartitionDesc.isSetIfNotExists()) {\n-                    LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                }\n-            }\n-\n-            Map<String, String> properties = singlePartitionDesc.getProperties();\n-            // partition properties should inherit table properties\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM, replicationNum.toString());\n-            }\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_INMEMORY)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_INMEMORY, olapTable.isInMemory().toString());\n-            }\n-\n-            RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-            singlePartitionDesc.analyze(rangePartitionInfo.getPartitionColumns().size(), properties);\n-            rangePartitionInfo.checkAndCreateRange(singlePartitionDesc, isTempPartition);\n-\n-            // get distributionInfo\n-            List<Column> baseSchema = olapTable.getBaseSchema();\n-            DistributionInfo defaultDistributionInfo = olapTable.getDefaultDistributionInfo();\n-            if (distributionDesc != null) {\n-                distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-                // for now. we only support modify distribution's bucket num\n-                if (distributionInfo.getType() != defaultDistributionInfo.getType()) {\n-                    throw new DdlException(\"Cannot assign different distribution type. default is: \"\n-                            + defaultDistributionInfo.getType());\n-                }\n-\n-                if (distributionInfo.getType() == DistributionInfoType.HASH) {\n-                    HashDistributionInfo hashDistributionInfo = (HashDistributionInfo) distributionInfo;\n-                    List<Column> newDistriCols = hashDistributionInfo.getDistributionColumns();\n-                    List<Column> defaultDistriCols = ((HashDistributionInfo) defaultDistributionInfo)\n-                            .getDistributionColumns();\n-                    if (!newDistriCols.equals(defaultDistriCols)) {\n-                        throw new DdlException(\"Cannot assign hash distribution with different distribution cols. \"\n-                                + \"default is: \" + defaultDistriCols);\n-                    }\n-                    if (hashDistributionInfo.getBucketNum() <= 0) {\n-                        throw new DdlException(\"Cannot assign hash distribution buckets less than 1\");\n-                    }\n-                }\n-            } else {\n-                distributionInfo = defaultDistributionInfo;\n-            }\n-\n-            // check colocation\n-            if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-                String fullGroupName = db.getId() + \"_\" + olapTable.getColocateGroup();\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                Preconditions.checkNotNull(groupSchema);\n-                groupSchema.checkDistribution(distributionInfo);\n-                groupSchema.checkReplicationNum(singlePartitionDesc.getReplicationNum());\n-            }\n-\n-            indexIdToMeta = olapTable.getCopiedIndexIdToMeta();\n-            bfColumns = olapTable.getCopiedBfColumns();\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        Preconditions.checkNotNull(distributionInfo);\n-        Preconditions.checkNotNull(olapTable);\n-        Preconditions.checkNotNull(indexIdToMeta);\n-\n-        // create partition outside db lock\n-        DataProperty dataProperty = singlePartitionDesc.getPartitionDataProperty();\n-        Preconditions.checkNotNull(dataProperty);\n-\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-        try {\n-            long partitionId = getNextId();\n-            Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                    olapTable.getId(),\n-                    olapTable.getBaseIndexId(),\n-                    partitionId, partitionName,\n-                    indexIdToMeta,\n-                    olapTable.getKeysType(),\n-                    distributionInfo,\n-                    dataProperty.getStorageMedium(),\n-                    singlePartitionDesc.getReplicationNum(),\n-                    singlePartitionDesc.getVersionInfo(),\n-                    bfColumns, olapTable.getBfFpp(),\n-                    tabletIdSet, olapTable.getCopiedIndexes(),\n-                    singlePartitionDesc.isInMemory(),\n-                    olapTable.getStorageFormat(),\n-                    singlePartitionDesc.getTabletType()\n-                    );\n-\n-            // check again\n-            db.writeLock();\n-            try {\n-                Table table = db.getTable(tableName);\n-                if (table == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-\n-                if (table.getType() != TableType.OLAP) {\n-                    throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-                }\n-\n-                olapTable = (OlapTable) table;\n-                if (olapTable.getState() != OlapTableState.NORMAL) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-                }\n-\n-                // check partition name\n-                if (olapTable.checkPartitionNameExist(partitionName)) {\n-                    if (singlePartitionDesc.isSetIfNotExists()) {\n-                        LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                        return;\n-                    } else {\n-                        ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                    }\n-                }\n-\n-                // check if meta changed\n-                // rollup index may be added or dropped during add partition operation.\n-                // schema may be changed during add partition operation.\n-                boolean metaChanged = false;\n-                if (olapTable.getIndexNameToId().size() != indexIdToMeta.size()) {\n-                    metaChanged = true;\n-                } else {\n-                    // compare schemaHash\n-                    for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                        long indexId = entry.getKey();\n-                        if (!indexIdToMeta.containsKey(indexId)) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                        if (indexIdToMeta.get(indexId).getSchemaHash() != entry.getValue().getSchemaHash()) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                    }\n-                }\n-\n-                if (metaChanged) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s meta has been changed. try again.\");\n-                }\n-\n-                // check partition type\n-                PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                if (partitionInfo.getType() != PartitionType.RANGE) {\n-                    throw new DdlException(\"Only support adding partition to range partitioned table\");\n-                }\n-\n-                // update partition info\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                rangePartitionInfo.handleNewSinglePartitionDesc(singlePartitionDesc, partitionId, isTempPartition);\n-\n-                if (isTempPartition) {\n-                    olapTable.addTempPartition(partition);\n-                } else {\n-                    olapTable.addPartition(partition);\n-                }\n-\n-                // log\n-                PartitionPersistInfo info = new PartitionPersistInfo(db.getId(), olapTable.getId(), partition,\n-                        rangePartitionInfo.getRange(partitionId), dataProperty,\n-                        rangePartitionInfo.getReplicationNum(partitionId),\n-                        rangePartitionInfo.getIsInMemory(partitionId),\n-                        isTempPartition);\n-                editLog.logAddPartition(info);\n-\n-                LOG.info(\"succeed in creating partition[{}], temp: {}\", partitionId, isTempPartition);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-    }\n-\n-    public void replayAddPartition(PartitionPersistInfo info) throws DdlException {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            Partition partition = info.getPartition();\n-\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.isTempPartition()) {\n-                olapTable.addTempPartition(partition);\n-            } else {\n-                olapTable.addPartition(partition);\n-            }\n-\n-            ((RangePartitionInfo) partitionInfo).unprotectHandleNewSinglePartitionDesc(partition.getId(),\n-                    info.isTempPartition(), info.getRange(), info.getDataProperty(), info.getReplicationNum(),\n-                    info.isInMemory());\n-\n-            if (!isCheckpointThread()) {\n-                // add to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                    long indexId = index.getId();\n-                    int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                    TabletMeta tabletMeta = new TabletMeta(info.getDbId(), info.getTableId(), partition.getId(),\n-                            index.getId(), schemaHash, info.getDataProperty().getStorageMedium());\n-                    for (Tablet tablet : index.getTablets()) {\n-                        long tabletId = tablet.getId();\n-                        invertedIndex.addTablet(tabletId, tabletMeta);\n-                        for (Replica replica : tablet.getReplicas()) {\n-                            invertedIndex.addReplica(tabletId, replica);\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void dropPartition(Database db, OlapTable olapTable, DropPartitionClause clause) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-\n-        String partitionName = clause.getPartitionName();\n-        boolean isTempPartition = clause.isTempPartition();\n-\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        if (!olapTable.checkPartitionNameExist(partitionName, isTempPartition)) {\n-            if (clause.isSetIfExists()) {\n-                LOG.info(\"drop partition[{}] which does not exist\", partitionName);\n-                return;\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DROP_PARTITION_NON_EXISTENT, partitionName);\n-            }\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-        if (partitionInfo.getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Alter table [\" + olapTable.getName() + \"] failed. Not a partitioned table\");\n-        }\n-\n-        // drop\n-        if (isTempPartition) {\n-            olapTable.dropTempPartition(partitionName, true);\n-        } else {\n-            if (clause.isNeedCheckCommitedTxns()) {\n-                if (clause.isNeedCheckCommitedTxns()) {\n-                    Partition partition = olapTable.getPartition(partitionName);\n-                    if (partition != null) {\n-                        if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), olapTable.getId(), partition.getId())) {\n-                            throw new DdlException(\"There are still some commited txns cannot be aborted in table [\"\n-                                    + olapTable.getName() + \"] partition [\" + partitionName + \"], please wait for GlobalTransactionMgr to\" +\n-                                    \" finish publish tasks. If you don't need to recover partition, use DROPP PARTITION stmt (double P).\");\n-                        }\n-                    }\n-\n-                }\n-\n-            }\n-            olapTable.dropPartition(db.getId(), partitionName);\n-        }\n-\n-        // log\n-        DropPartitionInfo info = new DropPartitionInfo(db.getId(), olapTable.getId(), partitionName, isTempPartition);\n-        editLog.logDropPartition(info);\n-\n-        LOG.info(\"succeed in droping partition[{}]\", partitionName);\n-    }\n-\n-    public void replayDropPartition(DropPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            if (info.isTempPartition()) {\n-                olapTable.dropTempPartition(info.getPartitionName(), true);\n-            } else {\n-                olapTable.dropPartition(info.getDbId(), info.getPartitionName());\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayErasePartition(long partitionId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayErasePartition(partitionId);\n-    }\n-\n-    public void replayRecoverPartition(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(info.getTableId());\n-            Catalog.getCurrentRecycleBin().replayRecoverPartition((OlapTable) table, info.getPartitionId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void modifyPartitionProperty(Database db, OlapTable olapTable, String partitionName, Map<String, String> properties)\n-            throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        Partition partition = olapTable.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\n-                    \"Partition[\" + partitionName + \"] does not exist in table[\" + olapTable.getName() + \"]\");\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-        // 1. data property\n-        DataProperty oldDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        DataProperty newDataProperty = null;\n-        try {\n-            newDataProperty = PropertyAnalyzer.analyzeDataProperty(properties, oldDataProperty);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(newDataProperty);\n-\n-        if (newDataProperty.equals(oldDataProperty)) {\n-            newDataProperty = null;\n-        }\n-\n-        // 2. replication num\n-        short oldReplicationNum = partitionInfo.getReplicationNum(partition.getId());\n-        short newReplicationNum = (short) -1;\n-        try {\n-            newReplicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, oldReplicationNum);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (newReplicationNum == oldReplicationNum) {\n-            newReplicationNum = (short) -1;\n-        } else if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_HAS_SAME_REPLICATION_NUM, oldReplicationNum);\n-        }\n-\n-        // 3. in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties,\n-                PropertyAnalyzer.PROPERTIES_INMEMORY, partitionInfo.getIsInMemory(partition.getId()));\n-\n-        // 4. tablet type\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // check if has other undefined properties\n-        if (properties != null && !properties.isEmpty()) {\n-            MapJoiner mapJoiner = Joiner.on(\", \").withKeyValueSeparator(\" = \");\n-            throw new DdlException(\"Unknown properties: \" + mapJoiner.join(properties));\n-        }\n-\n-        // modify meta here\n-        // date property\n-        if (newDataProperty != null) {\n-            partitionInfo.setDataProperty(partition.getId(), newDataProperty);\n-            LOG.debug(\"modify partition[{}-{}-{}] data property to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newDataProperty.toString());\n-        }\n-\n-        // replication num\n-        if (newReplicationNum != (short) -1) {\n-            partitionInfo.setReplicationNum(partition.getId(), newReplicationNum);\n-            LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newReplicationNum);\n-        }\n-\n-        // in memory\n-        if (isInMemory != partitionInfo.getIsInMemory(partition.getId())) {\n-            partitionInfo.setIsInMemory(partition.getId(), isInMemory);\n-            LOG.debug(\"modify partition[{}-{}-{}] in memory to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    isInMemory);\n-        }\n-\n-        // tablet type\n-        // TODO: serialize to edit log\n-        if (tabletType != partitionInfo.getTabletType(partition.getId())) {\n-            partitionInfo.setTabletType(partition.getId(), tabletType);\n-            LOG.debug(\"modify partition[{}-{}-{}] tablet type to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    tabletType);\n-        }\n-\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), olapTable.getId(), partition.getId(),\n-                newDataProperty, newReplicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-    }\n-\n-    public void replayModifyPartition(ModifyPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.getDataProperty() != null) {\n-                partitionInfo.setDataProperty(info.getPartitionId(), info.getDataProperty());\n-            }\n-            if (info.getReplicationNum() != (short) -1) {\n-                partitionInfo.setReplicationNum(info.getPartitionId(), info.getReplicationNum());\n-            }\n-            partitionInfo.setIsInMemory(info.getPartitionId(), info.isInMemory());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private Partition createPartitionWithIndices(String clusterName, long dbId, long tableId,\n-                                                 long baseIndexId, long partitionId, String partitionName,\n-                                                 Map<Long, MaterializedIndexMeta> indexIdToMeta,\n-                                                 KeysType keysType,\n-                                                 DistributionInfo distributionInfo,\n-                                                 TStorageMedium storageMedium,\n-                                                 short replicationNum,\n-                                                 Pair<Long, Long> versionInfo,\n-                                                 Set<String> bfColumns,\n-                                                 double bfFpp,\n-                                                 Set<Long> tabletIdSet,\n-                                                 List<Index> indexes,\n-                                                 boolean isInMemory,\n-                                                 TStorageFormat storageFormat,\n-                                                 TTabletType tabletType) throws DdlException {\n-        // create base index first.\n-        Preconditions.checkArgument(baseIndexId != -1);\n-        MaterializedIndex baseIndex = new MaterializedIndex(baseIndexId, IndexState.NORMAL);\n-\n-        // create partition with base index\n-        Partition partition = new Partition(partitionId, partitionName, baseIndex, distributionInfo);\n-\n-        // add to index map\n-        Map<Long, MaterializedIndex> indexMap = new HashMap<Long, MaterializedIndex>();\n-        indexMap.put(baseIndexId, baseIndex);\n-\n-        // create rollup index if has\n-        for (long indexId : indexIdToMeta.keySet()) {\n-            if (indexId == baseIndexId) {\n-                continue;\n-            }\n-\n-            MaterializedIndex rollup = new MaterializedIndex(indexId, IndexState.NORMAL);\n-            indexMap.put(indexId, rollup);\n-        }\n-\n-        // version and version hash\n-        if (versionInfo != null) {\n-            partition.updateVisibleVersionAndVersionHash(versionInfo.first, versionInfo.second);\n-        }\n-        long version = partition.getVisibleVersion();\n-        long versionHash = partition.getVisibleVersionHash();\n-\n-        for (Map.Entry<Long, MaterializedIndex> entry : indexMap.entrySet()) {\n-            long indexId = entry.getKey();\n-            MaterializedIndex index = entry.getValue();\n-            MaterializedIndexMeta indexMeta = indexIdToMeta.get(indexId);\n-\n-            // create tablets\n-            int schemaHash = indexMeta.getSchemaHash();\n-            TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, storageMedium);\n-            createTablets(clusterName, index, ReplicaState.NORMAL, distributionInfo, version, versionHash,\n-                    replicationNum, tabletMeta, tabletIdSet);\n-\n-            boolean ok = false;\n-            String errMsg = null;\n-\n-            // add create replica task for olap\n-            short shortKeyColumnCount = indexMeta.getShortKeyColumnCount();\n-            TStorageType storageType = indexMeta.getStorageType();\n-            List<Column> schema = indexMeta.getSchema();\n-            int totalTaskNum = index.getTablets().size() * replicationNum;\n-            MarkedCountDownLatch<Long, Long> countDownLatch = new MarkedCountDownLatch<Long, Long>(totalTaskNum);\n-            AgentBatchTask batchTask = new AgentBatchTask();\n-            for (Tablet tablet : index.getTablets()) {\n-                long tabletId = tablet.getId();\n-                for (Replica replica : tablet.getReplicas()) {\n-                    long backendId = replica.getBackendId();\n-                    countDownLatch.addMark(backendId, tabletId);\n-                    CreateReplicaTask task = new CreateReplicaTask(backendId, dbId, tableId,\n-                            partitionId, indexId, tabletId,\n-                            shortKeyColumnCount, schemaHash,\n-                            version, versionHash,\n-                            keysType,\n-                            storageType, storageMedium,\n-                            schema, bfColumns, bfFpp,\n-                            countDownLatch,\n-                            indexes,\n-                            isInMemory,\n-                            tabletType);\n-                    task.setStorageFormat(storageFormat);\n-                    batchTask.addTask(task);\n-                    // add to AgentTaskQueue for handling finish report.\n-                    // not for resending task\n-                    AgentTaskQueue.addTask(task);\n-                }\n-            }\n-            AgentTaskExecutor.submit(batchTask);\n-\n-            // estimate timeout\n-            long timeout = Config.tablet_create_timeout_second * 1000L * totalTaskNum;\n-            timeout = Math.min(timeout, Config.max_create_table_timeout_second * 1000);\n-            try {\n-                ok = countDownLatch.await(timeout, TimeUnit.MILLISECONDS);\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"InterruptedException: \", e);\n-                ok = false;\n-            }\n-\n-            if (!ok || !countDownLatch.getStatus().ok()) {\n-                errMsg = \"Failed to create partition[\" + partitionName + \"]. Timeout.\";\n-                // clear tasks\n-                AgentTaskQueue.removeBatchTask(batchTask, TTaskType.CREATE);\n-\n-                if (!countDownLatch.getStatus().ok()) {\n-                    errMsg += \" Error: \" + countDownLatch.getStatus().getErrorMsg();\n-                } else {\n-                    List<Entry<Long, Long>> unfinishedMarks = countDownLatch.getLeftMarks();\n-                    // only show at most 3 results\n-                    List<Entry<Long, Long>> subList = unfinishedMarks.subList(0, Math.min(unfinishedMarks.size(), 3));\n-                    if (!subList.isEmpty()) {\n-                        errMsg += \" Unfinished mark: \" + Joiner.on(\", \").join(subList);\n-                    }\n-                }\n-                LOG.warn(errMsg);\n-                throw new DdlException(errMsg);\n-            }\n-\n-            if (index.getId() != baseIndexId) {\n-                // add rollup index to partition\n-                partition.createRollupIndex(index);\n-            }\n-        } // end for indexMap\n-        return partition;\n-    }\n-\n-    // Create olap table and related base index synchronously.\n-    private void createOlapTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        LOG.debug(\"begin create olap table: {}\", tableName);\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            // gen partition id first\n-            if (partitionDesc instanceof RangePartitionDesc) {\n-                RangePartitionDesc rangeDesc = (RangePartitionDesc) partitionDesc;\n-                for (SingleRangePartitionDesc desc : rangeDesc.getSingleRangePartitionDescs()) {\n-                    long partitionId = getNextId();\n-                    partitionNameToId.put(desc.getPartitionName(), partitionId);\n-                }\n-            }\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            if (DynamicPartitionUtil.checkDynamicPartitionPropertiesExist(stmt.getProperties())) {\n-                throw new DdlException(\"Only support dynamic partition properties on range partition table\");\n-            }\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        // get keys type\n-        KeysDesc keysDesc = stmt.getKeysDesc();\n-        Preconditions.checkNotNull(keysDesc);\n-        KeysType keysType = keysDesc.getKeysType();\n-\n-        // create distribution info\n-        DistributionDesc distributionDesc = stmt.getDistributionDesc();\n-        Preconditions.checkNotNull(distributionDesc);\n-        DistributionInfo distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-\n-        // calc short key column count\n-        short shortKeyColumnCount = Catalog.calcShortKeyColumnCount(baseSchema, stmt.getProperties());\n-        LOG.debug(\"create table[{}] short key column count: {}\", tableName, shortKeyColumnCount);\n-\n-        // indexes\n-        TableIndexes indexes = new TableIndexes(stmt.getIndexes());\n-\n-        // create table\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        OlapTable olapTable = new OlapTable(tableId, tableName, baseSchema, keysType, partitionInfo,\n-                distributionInfo, indexes);\n-        olapTable.setComment(stmt.getComment());\n-\n-        // set base index id\n-        long baseIndexId = getNextId();\n-        olapTable.setBaseIndexId(baseIndexId);\n-\n-        // set base index info to table\n-        // this should be done before create partition.\n-        Map<String, String> properties = stmt.getProperties();\n-\n-        // analyze bloom filter columns\n-        Set<String> bfColumns = null;\n-        double bfFpp = 0;\n-        try {\n-            bfColumns = PropertyAnalyzer.analyzeBloomFilterColumns(properties, baseSchema);\n-            if (bfColumns != null && bfColumns.isEmpty()) {\n-                bfColumns = null;\n-            }\n-\n-            bfFpp = PropertyAnalyzer.analyzeBloomFilterFpp(properties);\n-            if (bfColumns != null && bfFpp == 0) {\n-                bfFpp = FeConstants.default_bloom_filter_fpp;\n-            } else if (bfColumns == null) {\n-                bfFpp = 0;\n-            }\n-\n-            olapTable.setBloomFilterInfo(bfColumns, bfFpp);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // analyze replication_num\n-        short replicationNum = FeConstants.default_replication_num;\n-        try {\n-            boolean isReplicationNumSet = properties != null && properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM);\n-            replicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, replicationNum);\n-            if (isReplicationNumSet) {\n-                olapTable.setReplicationNum(replicationNum);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // set in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties, PropertyAnalyzer.PROPERTIES_INMEMORY, false);\n-        olapTable.setIsInMemory(isInMemory);\n-\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-            // if this is an unpartitioned table, we should analyze data property and replication num here.\n-            // if this is a partitioned table, there properties are already analyzed in RangePartitionDesc analyze phase.\n-\n-            // use table name as this single partition name\n-            long partitionId = partitionNameToId.get(tableName);\n-            DataProperty dataProperty = null;\n-            try {\n-                dataProperty = PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(),\n-                        DataProperty.DEFAULT_DATA_PROPERTY);\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(dataProperty);\n-            partitionInfo.setDataProperty(partitionId, dataProperty);\n-            partitionInfo.setReplicationNum(partitionId, replicationNum);\n-            partitionInfo.setIsInMemory(partitionId, isInMemory);\n-            partitionInfo.setTabletType(partitionId, tabletType);\n-        }\n-\n-        // check colocation properties\n-        try {\n-            String colocateGroup = PropertyAnalyzer.analyzeColocate(properties);\n-            if (colocateGroup != null) {\n-                if (Config.disable_colocate_join) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_FEATURE_DISABLED);\n-                }\n-                String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                if (groupSchema != null) {\n-                    // group already exist, check if this table can be added to this group\n-                    groupSchema.checkColocateSchema(olapTable);\n-                }\n-                // add table to this group, if group does not exist, create a new one\n-                getColocateTableIndex().addTableToGroup(db.getId(), olapTable, colocateGroup,\n-                        null /* generate group id inside */);\n-                olapTable.setColocateGroup(colocateGroup);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // get base index storage type. default is COLUMN\n-        TStorageType baseIndexStorageType = null;\n-        try {\n-            baseIndexStorageType = PropertyAnalyzer.analyzeStorageType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(baseIndexStorageType);\n-        // set base index meta\n-        int schemaVersion = 0;\n-        try {\n-            schemaVersion = PropertyAnalyzer.analyzeSchemaVersion(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        int schemaHash = Util.schemaHash(schemaVersion, baseSchema, bfColumns, bfFpp);\n-        olapTable.setIndexMeta(baseIndexId, tableName, baseSchema, schemaVersion, schemaHash,\n-                shortKeyColumnCount, baseIndexStorageType, keysType);\n-\n-        for (AlterClause alterClause : stmt.getRollupAlterClauseList()) {\n-            AddRollupClause addRollupClause = (AddRollupClause)alterClause;\n-\n-            Long baseRollupIndex = olapTable.getIndexIdByName(tableName);\n-\n-            // get storage type for rollup index\n-            TStorageType rollupIndexStorageType = null;\n-            try {\n-                rollupIndexStorageType = PropertyAnalyzer.analyzeStorageType(addRollupClause.getProperties());\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(rollupIndexStorageType);\n-            // set rollup index meta to olap table\n-            List<Column> rollupColumns = getRollupHandler().checkAndPrepareMaterializedView(addRollupClause,\n-                    olapTable, baseRollupIndex, false);\n-            short rollupShortKeyColumnCount = Catalog.calcShortKeyColumnCount(rollupColumns, alterClause.getProperties());\n-            int rollupSchemaHash = Util.schemaHash(schemaVersion, rollupColumns, bfColumns, bfFpp);\n-            long rollupIndexId = getCurrentCatalog().getNextId();\n-            olapTable.setIndexMeta(rollupIndexId, addRollupClause.getRollupName(), rollupColumns, schemaVersion,\n-                    rollupSchemaHash, rollupShortKeyColumnCount, rollupIndexStorageType, keysType);\n-        }\n-\n-        // analyze version info\n-        Pair<Long, Long> versionInfo = null;\n-        try {\n-            versionInfo = PropertyAnalyzer.analyzeVersionInfo(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(versionInfo);\n-\n-        // get storage format\n-        TStorageFormat storageFormat = TStorageFormat.DEFAULT; // default means it's up to BE's config\n-        try {\n-            storageFormat = PropertyAnalyzer.analyzeStorageFormat(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        olapTable.setStorageFormat(storageFormat);\n-\n-        // a set to record every new tablet created when create table\n-        // if failed in any step, use this set to do clear things\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-\n-        // create partition\n-        try {\n-            if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-                // this is a 1-level partitioned table\n-                // use table name as partition name\n-                String partitionName = tableName;\n-                long partitionId = partitionNameToId.get(partitionName);\n-                // create partition\n-                Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                        olapTable.getId(), olapTable.getBaseIndexId(),\n-                        partitionId, partitionName,\n-                        olapTable.getIndexIdToMeta(),\n-                        keysType,\n-                        distributionInfo,\n-                        partitionInfo.getDataProperty(partitionId).getStorageMedium(),\n-                        partitionInfo.getReplicationNum(partitionId),\n-                        versionInfo, bfColumns, bfFpp,\n-                        tabletIdSet, olapTable.getCopiedIndexes(),\n-                        isInMemory, storageFormat, tabletType);\n-                olapTable.addPartition(partition);\n-            } else if (partitionInfo.getType() == PartitionType.RANGE) {\n-                try {\n-                    // just for remove entries in stmt.getProperties(),\n-                    // and then check if there still has unknown properties\n-                    PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(), DataProperty.DEFAULT_DATA_PROPERTY);\n-                    DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(olapTable, properties);\n-\n-                    if (properties != null && !properties.isEmpty()) {\n-                        // here, all properties should be checked\n-                        throw new DdlException(\"Unknown properties: \" + properties);\n-                    }\n-                } catch (AnalysisException e) {\n-                    throw new DdlException(e.getMessage());\n-                }\n-\n-                // this is a 2-level partitioned tables\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Map.Entry<String, Long> entry : partitionNameToId.entrySet()) {\n-                    DataProperty dataProperty = rangePartitionInfo.getDataProperty(entry.getValue());\n-                    Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(), olapTable.getId(),\n-                            olapTable.getBaseIndexId(), entry.getValue(), entry.getKey(),\n-                            olapTable.getIndexIdToMeta(),\n-                            keysType, distributionInfo,\n-                            dataProperty.getStorageMedium(),\n-                            partitionInfo.getReplicationNum(entry.getValue()),\n-                            versionInfo, bfColumns, bfFpp,\n-                            tabletIdSet, olapTable.getCopiedIndexes(),\n-                            isInMemory, storageFormat,\n-                            rangePartitionInfo.getTabletType(entry.getValue()));\n-                    olapTable.addPartition(partition);\n-                }\n-            } else {\n-                throw new DdlException(\"Unsupport partition method: \" + partitionInfo.getType().name());\n-            }\n-\n-            if (!db.createTableWithLock(olapTable, false, stmt.isSetIfNotExists())) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exists\");\n-            }\n-\n-            // we have added these index to memory, only need to persist here\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                GroupId groupId = getColocateTableIndex().getGroup(tableId);\n-                List<List<Long>> backendsPerBucketSeq = getColocateTableIndex().getBackendsPerBucketSeq(groupId);\n-                ColocatePersistInfo info = ColocatePersistInfo.createForAddTable(groupId, tableId, backendsPerBucketSeq);\n-                editLog.logColocateAddTable(info);\n-            }\n-            LOG.info(\"successfully create table[{};{}]\", tableName, tableId);\n-            // register or remove table from DynamicPartition after table created\n-            DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), olapTable);\n-            dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                    tableName, DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-\n-            // only remove from memory, because we have not persist it\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                getColocateTableIndex().removeTable(tableId);\n-            }\n-\n-            throw e;\n-        }\n-        return;\n-    }\n-\n-    private void createMysqlTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        MysqlTable mysqlTable = new MysqlTable(tableId, tableName, columns, stmt.getProperties());\n-        mysqlTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(mysqlTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-        return;\n-    }\n-\n-    private Table createEsTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        EsTable esTable = new EsTable(tableId, tableName, baseSchema, stmt.getProperties(), partitionInfo);\n-        esTable.setComment(stmt.getComment());\n-\n-        if (!db.createTableWithLock(esTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table{} with id {}\", tableName, tableId);\n-        return esTable;\n-    }\n-\n-    private void createBrokerTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        BrokerTable brokerTable = new BrokerTable(tableId, tableName, columns, stmt.getProperties());\n-        brokerTable.setComment(stmt.getComment());\n-        brokerTable.setBrokerProperties(stmt.getExtProperties());\n-\n-        if (!db.createTableWithLock(brokerTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-\n-        return;\n-    }\n-\n-    private void createHiveTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        List<Column> columns = stmt.getColumns();\n-        long tableId = getNextId();\n-        HiveTable hiveTable = new HiveTable(tableId, tableName, columns, stmt.getProperties());\n-        hiveTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(hiveTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-    }\n-\n-    public static void getDdlStmt(Table table, List<String> createTableStmt, List<String> addPartitionStmt,\n-                                  List<String> createRollupStmt, boolean separatePartition, boolean hidePassword) {\n-        StringBuilder sb = new StringBuilder();\n-\n-        // 1. create table\n-        // 1.1 view\n-        if (table.getType() == TableType.VIEW) {\n-            View view = (View) table;\n-            sb.append(\"CREATE VIEW `\").append(table.getName()).append(\"` AS \").append(view.getInlineViewDef());\n-            sb.append(\";\");\n-            createTableStmt.add(sb.toString());\n-            return;\n-        }\n-\n-        // 1.2 other table type\n-        sb.append(\"CREATE \");\n-        if (table.getType() == TableType.MYSQL || table.getType() == TableType.ELASTICSEARCH\n-                || table.getType() == TableType.BROKER || table.getType() == TableType.HIVE) {\n-            sb.append(\"EXTERNAL \");\n-        }\n-        sb.append(\"TABLE \");\n-        sb.append(\"`\").append(table.getName()).append(\"` (\\n\");\n-        int idx = 0;\n-        for (Column column : table.getBaseSchema()) {\n-            if (idx++ != 0) {\n-                sb.append(\",\\n\");\n-            }\n-            // There MUST BE 2 space in front of each column description line\n-            // sqlalchemy requires this to parse SHOW CREATE TAEBL stmt.\n-            sb.append(\"  \").append(column.toSql());\n-        }\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-            if (CollectionUtils.isNotEmpty(olapTable.getIndexes())) {\n-                for (Index index : olapTable.getIndexes()) {\n-                    sb.append(\",\\n\");\n-                    sb.append(\"  \").append(index.toSql());\n-                }\n-            }\n-        }\n-        sb.append(\"\\n) ENGINE=\");\n-        sb.append(table.getType().name());\n-\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            // keys\n-            sb.append(\"\\n\").append(olapTable.getKeysType().toSql()).append(\"(\");\n-            List<String> keysColumnNames = Lists.newArrayList();\n-            for (Column column : olapTable.getBaseSchema()) {\n-                if (column.isKey()) {\n-                    keysColumnNames.add(\"`\" + column.getName() + \"`\");\n-                }\n-            }\n-            sb.append(Joiner.on(\", \").join(keysColumnNames)).append(\")\");\n-\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            List<Long> partitionId = null;\n-            if (separatePartition) {\n-                partitionId = Lists.newArrayList();\n-            }\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\").append(partitionInfo.toSql(olapTable, partitionId));\n-            }\n-\n-            // distribution\n-            DistributionInfo distributionInfo = olapTable.getDefaultDistributionInfo();\n-            sb.append(\"\\n\").append(distributionInfo.toSql());\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-\n-            // replicationNum\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            sb.append(\"\\\"\").append(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM).append(\"\\\" = \\\"\");\n-            sb.append(replicationNum).append(\"\\\"\");\n-\n-            // bloom filter\n-            Set<String> bfColumnNames = olapTable.getCopiedBfColumns();\n-            if (bfColumnNames != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_BF_COLUMNS).append(\"\\\" = \\\"\");\n-                sb.append(Joiner.on(\", \").join(olapTable.getCopiedBfColumns())).append(\"\\\"\");\n-            }\n-\n-            if (separatePartition) {\n-                // version info\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_VERSION_INFO).append(\"\\\" = \\\"\");\n-                Partition partition = null;\n-                if (olapTable.getPartitionInfo().getType() == PartitionType.UNPARTITIONED) {\n-                    partition = olapTable.getPartition(olapTable.getName());\n-                } else {\n-                    Preconditions.checkState(partitionId.size() == 1);\n-                    partition = olapTable.getPartition(partitionId.get(0));\n-                }\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-            }\n-\n-            // colocateTable\n-            String colocateTable = olapTable.getColocateGroup();\n-            if (colocateTable != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH).append(\"\\\" = \\\"\");\n-                sb.append(colocateTable).append(\"\\\"\");\n-            }\n-\n-            // dynamic partition\n-            if (olapTable.dynamicPartitionExists()) {\n-                sb.append(olapTable.getTableProperty().getDynamicPartitionProperty().toString());\n-            }\n-\n-            // in memory\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_INMEMORY).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.isInMemory()).append(\"\\\"\");\n-\n-            // storage type\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_STORAGE_FORMAT).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.getStorageFormat()).append(\"\\\"\");\n-\n-            sb.append(\"\\n)\");\n-        } else if (table.getType() == TableType.MYSQL) {\n-            MysqlTable mysqlTable = (MysqlTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"host\\\" = \\\"\").append(mysqlTable.getHost()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"port\\\" = \\\"\").append(mysqlTable.getPort()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(mysqlTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : mysqlTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(mysqlTable.getMysqlDatabaseName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(mysqlTable.getMysqlTableName()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.BROKER) {\n-            BrokerTable brokerTable = (BrokerTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"broker_name\\\" = \\\"\").append(brokerTable.getBrokerName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"path\\\" = \\\"\").append(Joiner.on(\",\").join(brokerTable.getEncodedPaths())).append(\"\\\",\\n\");\n-            sb.append(\"\\\"column_separator\\\" = \\\"\").append(brokerTable.getReadableColumnSeparator()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"line_delimiter\\\" = \\\"\").append(brokerTable.getReadableLineDelimiter()).append(\"\\\",\\n\");\n-            sb.append(\")\");\n-            if (!brokerTable.getBrokerProperties().isEmpty()) {\n-                sb.append(\"\\nBROKER PROPERTIES (\\n\");\n-                sb.append(new PrintableMap<>(brokerTable.getBrokerProperties(), \" = \", true, true,\n-                        hidePassword).toString());\n-                sb.append(\"\\n)\");\n-            }\n-        } else if (table.getType() == TableType.ELASTICSEARCH) {\n-            EsTable esTable = (EsTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = esTable.getPartitionInfo();\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\");\n-                sb.append(\"PARTITION BY RANGE(\");\n-                idx = 0;\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Column column : rangePartitionInfo.getPartitionColumns()) {\n-                    if (idx != 0) {\n-                        sb.append(\", \");\n-                    }\n-                    sb.append(\"`\").append(column.getName()).append(\"`\");\n-                }\n-                sb.append(\")\\n()\");\n-            }\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"hosts\\\" = \\\"\").append(esTable.getHosts()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(esTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : esTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"index\\\" = \\\"\").append(esTable.getIndexName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"type\\\" = \\\"\").append(esTable.getMappingType()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"transport\\\" = \\\"\").append(esTable.getTransport()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_docvalue_scan\\\" = \\\"\").append(esTable.isDocValueScanEnable()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_keyword_sniff\\\" = \\\"\").append(esTable.isKeywordSniffEnable()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.HIVE) {\n-            HiveTable hiveTable = (HiveTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(hiveTable.getHiveDb()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(hiveTable.getHiveTable()).append(\"\\\",\\n\");\n-            sb.append(new PrintableMap<>(hiveTable.getHiveProperties(), \" = \", true, true, false).toString());\n-            sb.append(\"\\n)\");\n-        }\n-        sb.append(\";\");\n-\n-        createTableStmt.add(sb.toString());\n-\n-        // 2. add partition\n-        if (separatePartition && (table instanceof OlapTable)\n-                && ((OlapTable) table).getPartitionInfo().getType() == PartitionType.RANGE\n-                && ((OlapTable) table).getPartitions().size() > 1) {\n-            OlapTable olapTable = (OlapTable) table;\n-            RangePartitionInfo partitionInfo = (RangePartitionInfo) olapTable.getPartitionInfo();\n-            boolean first = true;\n-            for (Map.Entry<Long, Range<PartitionKey>> entry : partitionInfo.getSortedRangeMap(false)) {\n-                if (first) {\n-                    first = false;\n-                    continue;\n-                }\n-                sb = new StringBuilder();\n-                Partition partition = olapTable.getPartition(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName());\n-                sb.append(\" ADD PARTITION \").append(partition.getName()).append(\" VALUES [\");\n-                sb.append(entry.getValue().lowerEndpoint().toSql());\n-                sb.append(\", \").append(entry.getValue().upperEndpoint().toSql()).append(\")\");\n-                sb.append(\"(\\\"version_info\\\" = \\\"\");\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-                sb.append(\");\");\n-                addPartitionStmt.add(sb.toString());\n-            }\n-        }\n-\n-        // 3. rollup\n-        if (createRollupStmt != null && (table instanceof OlapTable)) {\n-            OlapTable olapTable = (OlapTable) table;\n-            for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                if (entry.getKey() == olapTable.getBaseIndexId()) {\n-                    continue;\n-                }\n-                MaterializedIndexMeta materializedIndexMeta = entry.getValue();\n-                sb = new StringBuilder();\n-                String indexName = olapTable.getIndexNameById(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName()).append(\" ADD ROLLUP \").append(indexName);\n-                sb.append(\"(\");\n-\n-                List<Column> indexSchema = materializedIndexMeta.getSchema();\n-                for (int i = 0; i < indexSchema.size(); i++) {\n-                    Column column = indexSchema.get(i);\n-                    sb.append(column.getName());\n-                    if (i != indexSchema.size() - 1) {\n-                        sb.append(\", \");\n-                    }\n-                }\n-                sb.append(\");\");\n-                createRollupStmt.add(sb.toString());\n-            }\n-        }\n-    }\n-\n-    public void replayCreateTable(String dbName, Table table) {\n-        Database db = this.fullNameToDb.get(dbName);\n-        db.createTableWithLock(table, true, false);\n-\n-        if (!isCheckpointThread()) {\n-            // add to inverted index\n-            if (table.getType() == TableType.OLAP) {\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                OlapTable olapTable = (OlapTable) table;\n-                long dbId = db.getId();\n-                long tableId = table.getId();\n-                for (Partition partition : olapTable.getAllPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                } // end for partitions\n-                DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(dbId, olapTable);\n-            }\n-        }\n-    }\n-\n-    private void createTablets(String clusterName, MaterializedIndex index, ReplicaState replicaState,\n-                               DistributionInfo distributionInfo, long version, long versionHash, short replicationNum,\n-                               TabletMeta tabletMeta, Set<Long> tabletIdSet) throws DdlException {\n-        Preconditions.checkArgument(replicationNum > 0);\n-\n-        DistributionInfoType distributionInfoType = distributionInfo.getType();\n-        if (distributionInfoType == DistributionInfoType.HASH) {\n-            ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();\n-            List<List<Long>> backendsPerBucketSeq = null;\n-            GroupId groupId = null;\n-            if (colocateIndex.isColocateTable(tabletMeta.getTableId())) {\n-                // if this is a colocate table, try to get backend seqs from colocation index.\n-                Database db = Catalog.getCurrentCatalog().getDb(tabletMeta.getDbId());\n-                groupId = colocateIndex.getGroup(tabletMeta.getTableId());\n-                // Use db write lock here to make sure the backendsPerBucketSeq is consistent when the backendsPerBucketSeq is updating.\n-                // This lock will release very fast.\n-                db.writeLock();\n-                try {\n-                    backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-            }\n-\n-            // chooseBackendsArbitrary is true, means this may be the first table of colocation group,\n-            // or this is just a normal table, and we can choose backends arbitrary.\n-            // otherwise, backends should be chosen from backendsPerBucketSeq;\n-            boolean chooseBackendsArbitrary = backendsPerBucketSeq == null || backendsPerBucketSeq.isEmpty();\n-            if (chooseBackendsArbitrary) {\n-                backendsPerBucketSeq = Lists.newArrayList();\n-            }\n-            for (int i = 0; i < distributionInfo.getBucketNum(); ++i) {\n-                // create a new tablet with random chosen backends\n-                Tablet tablet = new Tablet(getNextId());\n-\n-                // add tablet to inverted index first\n-                index.addTablet(tablet, tabletMeta);\n-                tabletIdSet.add(tablet.getId());\n-\n-                // get BackendIds\n-                List<Long> chosenBackendIds;\n-                if (chooseBackendsArbitrary) {\n-                    // This is the first colocate table in the group, or just a normal table,\n-                    // randomly choose backends\n-                    if (Config.enable_strict_storage_medium_check) {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName, tabletMeta.getStorageMedium());\n-                    } else {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName);\n-                    }\n-                    backendsPerBucketSeq.add(chosenBackendIds);\n-                } else {\n-                    // get backends from existing backend sequence\n-                    chosenBackendIds = backendsPerBucketSeq.get(i);\n-                }\n-                \n-                // create replicas\n-                for (long backendId : chosenBackendIds) {\n-                    long replicaId = getNextId();\n-                    Replica replica = new Replica(replicaId, backendId, replicaState, version, versionHash,\n-                            tabletMeta.getOldSchemaHash());\n-                    tablet.addReplica(replica);\n-                }\n-                Preconditions.checkState(chosenBackendIds.size() == replicationNum, chosenBackendIds.size() + \" vs. \"+ replicationNum);\n-            }\n-\n-            if (groupId != null) {\n-                colocateIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-        } else {\n-            throw new DdlException(\"Unknown distribution type: \" + distributionInfoType);\n-        }\n-    }\n-\n-    // create replicas for tablet with random chosen backends\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName, TStorageMedium storageMedium) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIdsByStorageMedium(replicationNum,\n-                true, true, clusterName, storageMedium);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host with storage medium is \" + storageMedium + \" in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIds(replicationNum, true, true, clusterName);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    // Drop table\n-    public void dropTable(DropTableStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check database\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        Table table = null;\n-        db.writeLock();\n-        try {\n-            table = db.getTable(tableName);\n-            if (table == null) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop table[{}] which does not exist\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-            }\n-\n-            // Check if a view\n-            if (stmt.isView()) {\n-                if (!(table instanceof View)) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"VIEW\");\n-                }\n-            } else {\n-                if (table instanceof View) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"TABLE\");\n-                }\n-            }\n-\n-            if (stmt.isNeedCheckCommitedTxns()) {\n-                if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), table.getId(), null)) {\n-                    throw new DdlException(\"There are still some commited txns cannot be aborted in table [\"\n-                            + table.getName() + \"], please wait for GlobalTransactionMgr to finish publish tasks.\" +\n-                            \" If you don't need to recover table, use DROPP TABLE stmt (double P).\");\n-                }\n-            }\n-\n-            unprotectDropTable(db, table.getId());\n-\n-            DropInfo info = new DropInfo(db.getId(), table.getId(), -1L);\n-            editLog.logDropTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-\n-        LOG.info(\"finished dropping table: {} from db: {}\", tableName, dbName);\n-    }\n-\n-    public boolean unprotectDropTable(Database db, long tableId) {\n-        Table table = db.getTable(tableId);\n-        // delete from db meta\n-        if (table == null) {\n-            return false;\n-        }\n-\n-        if (table.getType() == TableType.ELASTICSEARCH) {\n-            esRepository.deRegisterTable(tableId);\n-        } else if (table.getType() == TableType.OLAP) {\n-            // drop all temp partitions of this table, so that there is no temp partitions in recycle bin,\n-            // which make things easier.\n-            ((OlapTable) table).dropAllTempPartitions();\n-        }\n-\n-        db.dropTable(table.getName());\n-\n-        Catalog.getCurrentRecycleBin().recycleTable(db.getId(), table);\n-\n-        LOG.info(\"finished dropping table[{}] in db[{}]\", table.getName(), db.getFullName());\n-        return true;\n-    }\n-\n-    public void replayDropTable(Database db, long tableId) {\n-        db.writeLock();\n-        try {\n-            unprotectDropTable(db, tableId);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseTable(long tableId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseTable(tableId);\n-    }\n-\n-    public void replayRecoverTable(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Catalog.getCurrentRecycleBin().replayRecoverTable(db, info.getTableId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private void unprotectAddReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay add a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-\n-        // for compatibility\n-        int schemaHash = info.getSchemaHash();\n-        if (schemaHash == -1) {\n-            schemaHash = olapTable.getSchemaHashByIndexId(info.getIndexId());\n-        }\n-\n-        Replica replica = new Replica(info.getReplicaId(), info.getBackendId(), info.getVersion(),\n-                info.getVersionHash(), schemaHash, info.getDataSize(), info.getRowCount(),\n-                ReplicaState.NORMAL,\n-                info.getLastFailedVersion(),\n-                info.getLastFailedVersionHash(),\n-                info.getLastSuccessVersion(),\n-                info.getLastSuccessVersionHash());\n-        tablet.addReplica(replica);\n-    }\n-\n-    private void unprotectUpdateReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay update a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        Replica replica = tablet.getReplicaByBackendId(info.getBackendId());\n-        Preconditions.checkNotNull(replica, info);\n-        replica.updateVersionInfo(info.getVersion(), info.getVersionHash(), info.getDataSize(), info.getRowCount());\n-        replica.setBad(false);\n-    }\n-\n-    public void replayAddReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectAddReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayUpdateReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectUpdateReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void unprotectDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        tablet.deleteReplicaByBackendId(info.getBackendId());\n-    }\n-\n-    public void replayDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectDeleteReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayAddFrontend(Frontend fe) {\n-        tryLock(true);\n-        try {\n-            Frontend existFe = checkFeExist(fe.getHost(), fe.getEditLogPort());\n-            if (existFe != null) {\n-                LOG.warn(\"fe {} already exist.\", existFe);\n-                if (existFe.getRole() != fe.getRole()) {\n-                    /*\n-                     * This may happen if:\n-                     * 1. first, add a FE as OBSERVER.\n-                     * 2. This OBSERVER is restarted with ROLE and VERSION file being DELETED.\n-                     *    In this case, this OBSERVER will be started as a FOLLOWER, and add itself to the frontends.\n-                     * 3. this \"FOLLOWER\" begin to load image or replay journal,\n-                     *    then find the origin OBSERVER in image or journal.\n-                     * This will cause UNDEFINED behavior, so it is better to exit and fix it manually.\n-                     */\n-                    System.err.println(\"Try to add an already exist FE with different role\" + fe.getRole());\n-                    System.exit(-1);\n-                }\n-                return;\n-            }\n-            frontends.put(fe.getNodeName(), fe);\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                // DO NOT add helper sockets here, cause BDBHA is not instantiated yet.\n-                // helper sockets will be added after start BDBHA\n-                // But add to helperNodes, just for show\n-                helperNodes.add(Pair.create(fe.getHost(), fe.getEditLogPort()));\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropFrontend(Frontend frontend) {\n-        tryLock(true);\n-        try {\n-            Frontend removedFe = frontends.remove(frontend.getNodeName());\n-            if (removedFe == null) {\n-                LOG.error(frontend.toString() + \" does not exist.\");\n-                return;\n-            }\n-            if (removedFe.getRole() == FrontendNodeType.FOLLOWER\n-                    || removedFe.getRole() == FrontendNodeType.REPLICA) {\n-                helperNodes.remove(Pair.create(removedFe.getHost(), removedFe.getEditLogPort()));\n-            }\n-\n-            removedFrontends.add(removedFe.getNodeName());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public int getClusterId() {\n-        return this.clusterId;\n-    }\n-\n-    public String getToken() {\n-        return token;\n-    }\n-\n-    public Database getDb(String name) {\n-        if (fullNameToDb.containsKey(name)) {\n-            return fullNameToDb.get(name);\n-        } else {\n-            // This maybe a information_schema db request, and information_schema db name is case insensitive.\n-            // So, we first extract db name to check if it is information_schema.\n-            // Then we reassemble the origin cluster name with lower case db name,\n-            // and finally get information_schema db from the name map.\n-            String dbName = ClusterNamespace.getNameFromFullName(name);\n-            if (dbName.equalsIgnoreCase(InfoSchemaDb.DATABASE_NAME)) {\n-                String clusterName = ClusterNamespace.getClusterNameFromFullName(name);\n-                return fullNameToDb.get(ClusterNamespace.getFullName(clusterName, dbName.toLowerCase()));\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Database getDb(long dbId) {\n-        return idToDb.get(dbId);\n-    }\n-\n-    public EditLog getEditLog() {\n-        return editLog;\n-    }\n-\n-    // Get the next available, need't lock because of nextId is atomic.\n-    public long getNextId() {\n-        long id = idGenerator.getNextId();\n-        return id;\n-    }\n-\n-    public List<String> getDbNames() {\n-        return Lists.newArrayList(fullNameToDb.keySet());\n-    }\n-\n-    public List<String> getClusterDbNames(String clusterName) throws AnalysisException {\n-        final Cluster cluster = nameToCluster.get(clusterName);\n-        if (cluster == null) {\n-            throw new AnalysisException(\"No cluster selected\");\n-        }\n-        return Lists.newArrayList(cluster.getDbNames());\n-    }\n-\n-    public List<Long> getDbIds() {\n-        return Lists.newArrayList(idToDb.keySet());\n-    }\n-\n-    public HashMap<Long, TStorageMedium> getPartitionIdToStorageMediumMap() {\n-        HashMap<Long, TStorageMedium> storageMediumMap = new HashMap<Long, TStorageMedium>();\n-\n-        // record partition which need to change storage medium\n-        // dbId -> (tableId -> partitionId)\n-        HashMap<Long, Multimap<Long, Long>> changedPartitionsMap = new HashMap<Long, Multimap<Long, Long>>();\n-        long currentTimeMs = System.currentTimeMillis();\n-        List<Long> dbIds = getDbIds();\n-\n-        for (long dbId : dbIds) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while doing backend report\", dbId);\n-                continue;\n-            }\n-\n-            db.readLock();\n-            try {\n-                for (Table table : db.getTables()) {\n-                    if (table.getType() != TableType.OLAP) {\n-                        continue;\n-                    }\n-\n-                    long tableId = table.getId();\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                    for (Partition partition : olapTable.getAllPartitions()) {\n-                        long partitionId = partition.getId();\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        Preconditions.checkNotNull(dataProperty, partition.getName() + \", pId:\" + partitionId + \", db: \" + dbId + \", tbl: \" + tableId);\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            // record and change when holding write lock\n-                            Multimap<Long, Long> multimap = changedPartitionsMap.get(dbId);\n-                            if (multimap == null) {\n-                                multimap = HashMultimap.create();\n-                                changedPartitionsMap.put(dbId, multimap);\n-                            }\n-                            multimap.put(tableId, partitionId);\n-                        } else {\n-                            storageMediumMap.put(partitionId, dataProperty.getStorageMedium());\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.readUnlock();\n-            }\n-        } // end for dbs\n-\n-        // handle data property changed\n-        for (Long dbId : changedPartitionsMap.keySet()) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while checking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Multimap<Long, Long> tableIdToPartitionIds = changedPartitionsMap.get(dbId);\n-\n-            // use try lock to avoid blocking a long time.\n-            // if block too long, backend report rpc will timeout.\n-            if (!db.tryWriteLock(Database.TRY_LOCK_TIMEOUT_MS, TimeUnit.MILLISECONDS)) {\n-                LOG.warn(\"try get db {} writelock but failed when hecking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Preconditions.checkState(db.isWriteLockHeldByCurrentThread());\n-            try {\n-                for (Long tableId : tableIdToPartitionIds.keySet()) {\n-                    Table table = db.getTable(tableId);\n-                    if (table == null) {\n-                        continue;\n-                    }\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-                    Collection<Long> partitionIds = tableIdToPartitionIds.get(tableId);\n-                    for (Long partitionId : partitionIds) {\n-                        Partition partition = olapTable.getPartition(partitionId);\n-                        if (partition == null) {\n-                            continue;\n-                        }\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            partitionInfo.setDataProperty(partition.getId(), new DataProperty(TStorageMedium.HDD));\n-                            storageMediumMap.put(partitionId, TStorageMedium.HDD);\n-                            LOG.debug(\"partition[{}-{}-{}] storage medium changed from SSD to HDD\",\n-                                    dbId, tableId, partitionId);\n-\n-                            // log\n-                            ModifyPartitionInfo info =\n-                                    new ModifyPartitionInfo(db.getId(), olapTable.getId(),\n-                                            partition.getId(),\n-                                            DataProperty.DEFAULT_DATA_PROPERTY,\n-                                            (short) -1,\n-                                            partitionInfo.getIsInMemory(partition.getId()));\n-                            editLog.logModifyPartition(info);\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } // end for dbs\n-        return storageMediumMap;\n-    }\n-\n-    public ConsistencyChecker getConsistencyChecker() {\n-        return this.consistencyChecker;\n-    }\n-\n-    public Alter getAlterInstance() {\n-        return this.alter;\n-    }\n-\n-    public SchemaChangeHandler getSchemaChangeHandler() {\n-        return (SchemaChangeHandler) this.alter.getSchemaChangeHandler();\n-    }\n-\n-    public MaterializedViewHandler getRollupHandler() {\n-        return (MaterializedViewHandler) this.alter.getMaterializedViewHandler();\n-    }\n-\n-    public SystemHandler getClusterHandler() {\n-        return (SystemHandler) this.alter.getClusterHandler();\n-    }\n-\n-    public BackupHandler getBackupHandler() {\n-        return this.backupHandler;\n-    }\n-\n-    public DeleteHandler getDeleteHandler() {\n-        return this.deleteHandler;\n-    }\n-\n-    public Load getLoadInstance() {\n-        return this.load;\n-    }\n-\n-    public LoadManager getLoadManager() {\n-        return loadManager;\n-    }\n-\n-    public MasterTaskExecutor getLoadTaskScheduler() {\n-        return loadTaskScheduler;\n-    }\n-\n-    public RoutineLoadManager getRoutineLoadManager() {\n-        return routineLoadManager;\n-    }\n-\n-    public RoutineLoadTaskScheduler getRoutineLoadTaskScheduler(){\n-        return routineLoadTaskScheduler;\n-    }\n-\n-    public ExportMgr getExportMgr() {\n-        return this.exportMgr;\n-    }\n-\n-    public SmallFileMgr getSmallFileMgr() {\n-        return this.smallFileMgr;\n-    }\n-\n-    public long getReplayedJournalId() {\n-        return this.replayedJournalId.get();\n-    }\n-\n-    public HAProtocol getHaProtocol() {\n-        return this.haProtocol;\n-    }\n-\n-    public Long getMaxJournalId() {\n-        return this.editLog.getMaxJournalId();\n-    }\n-\n-    public long getEpoch() {\n-        return this.epoch;\n-    }\n-\n-    public void setEpoch(long epoch) {\n-        this.epoch = epoch;\n-    }\n-\n-    public FrontendNodeType getRole() {\n-        return this.role;\n-    }\n-\n-    public Pair<String, Integer> getHelperNode() {\n-        Preconditions.checkState(helperNodes.size() >= 1);\n-        return this.helperNodes.get(0);\n-    }\n-\n-    public List<Pair<String, Integer>> getHelperNodes() {\n-        return Lists.newArrayList(helperNodes);\n-    }\n-\n-    public Pair<String, Integer> getSelfNode() {\n-        return this.selfNode;\n-    }\n-\n-    public String getNodeName() {\n-        return this.nodeName;\n-    }\n-\n-    public FrontendNodeType getFeType() {\n-        return this.feType;\n-    }\n-\n-    public int getMasterRpcPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterRpcPort;\n-    }\n-\n-    public int getMasterHttpPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterHttpPort;\n-    }\n-\n-    public String getMasterIp() {\n-        if (!isReady()) {\n-            return \"\";\n-        }\n-        return this.masterIp;\n-    }\n-\n-    public EsRepository getEsRepository() {\n-        return this.esRepository;\n-    }\n-\n-    public void setMaster(MasterInfo info) {\n-        this.masterIp = info.getIp();\n-        this.masterHttpPort = info.getHttpPort();\n-        this.masterRpcPort = info.getRpcPort();\n-    }\n-\n-    public boolean canRead() {\n-        return this.canRead.get();\n-    }\n-\n-    public boolean isElectable() {\n-        return this.isElectable;\n-    }\n-\n-    public boolean isMaster() {\n-        return feType == FrontendNodeType.MASTER;\n-    }\n-\n-    public void setSynchronizedTime(long time) {\n-        this.synchronizedTimeMs = time;\n-    }\n-\n-    public void setEditLog(EditLog editLog) {\n-        this.editLog = editLog;\n-    }\n-\n-    public void setNextId(long id) {\n-        idGenerator.setId(id);\n-    }\n-\n-    public void setHaProtocol(HAProtocol protocol) {\n-        this.haProtocol = protocol;\n-    }\n-\n-    public static short calcShortKeyColumnCount(List<Column> columns, Map<String, String> properties)\n-            throws DdlException {\n-        List<Column> indexColumns = new ArrayList<Column>();\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                indexColumns.add(column);\n-            }\n-        }\n-        LOG.debug(\"index column size: {}\", indexColumns.size());\n-        Preconditions.checkArgument(indexColumns.size() > 0);\n-\n-        // figure out shortKeyColumnCount\n-        short shortKeyColumnCount = (short) -1;\n-        try {\n-            shortKeyColumnCount = PropertyAnalyzer.analyzeShortKeyColumnCount(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        if (shortKeyColumnCount != (short) -1) {\n-            // use user specified short key column count\n-            if (shortKeyColumnCount <= 0) {\n-                throw new DdlException(\"Invalid short key: \" + shortKeyColumnCount);\n-            }\n-\n-            if (shortKeyColumnCount > indexColumns.size()) {\n-                throw new DdlException(\"Short key is too large. should less than: \" + indexColumns.size());\n-            }\n-\n-            for (int pos = 0; pos < shortKeyColumnCount; pos++) {\n-                if (indexColumns.get(pos).getDataType() == PrimitiveType.VARCHAR && pos != shortKeyColumnCount - 1) {\n-                    throw new DdlException(\"Varchar should not in the middle of short keys.\");\n-                }\n-            }\n-        } else {\n-            /*\n-             * Calc short key column count. NOTE: short key column count is\n-             * calculated as follow: 1. All index column are taking into\n-             * account. 2. Max short key column count is Min(Num of\n-             * indexColumns, META_MAX_SHORT_KEY_NUM). 3. Short key list can\n-             * contains at most one VARCHAR column. And if contains, it should\n-             * be at the last position of the short key list.\n-             */\n-            shortKeyColumnCount = 0;\n-            int shortKeySizeByte = 0;\n-            int maxShortKeyColumnCount = Math.min(indexColumns.size(), FeConstants.shortkey_max_column_count);\n-            for (int i = 0; i < maxShortKeyColumnCount; i++) {\n-                Column column = indexColumns.get(i);\n-                shortKeySizeByte += column.getOlapColumnIndexSize();\n-                if (shortKeySizeByte > FeConstants.shortkey_maxsize_bytes) {\n-                    if (column.getDataType().isCharFamily()) {\n-                        ++shortKeyColumnCount;\n-                    }\n-                    break;\n-                }\n-                if (column.getType().isFloatingPointType()) {\n-                    break;\n-                }\n-                if (column.getDataType() == PrimitiveType.VARCHAR) {\n-                    ++shortKeyColumnCount;\n-                    break;\n-                }\n-                ++shortKeyColumnCount;\n-            }\n-            if (shortKeyColumnCount == 0) {\n-                throw new DdlException(\"The first column could not be float or double type, use decimal instead\");\n-            }\n-\n-        } // end calc shortKeyColumnCount\n-\n-        return shortKeyColumnCount;\n-    }\n-\n-    /*\n-     * used for handling AlterTableStmt (for client is the ALTER TABLE command).\n-     * including SchemaChangeHandler and RollupHandler\n-     */\n-    public void alterTable(AlterTableStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterTable(stmt);\n-    }\n-\n-    /**\n-     * used for handling AlterViewStmt (the ALTER VIEW command).\n-     */\n-    public void alterView(AlterViewStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterView(stmt, ConnectContext.get());\n-    }\n-\n-    public void createMaterializedView(CreateMaterializedViewStmt stmt)\n-            throws AnalysisException, DdlException {\n-        this.alter.processCreateMaterializedView(stmt);\n-    }\n-\n-    public void dropMaterializedView(DropMaterializedViewStmt stmt) throws DdlException, MetaNotFoundException {\n-        this.alter.processDropMaterializedView(stmt);\n-    }\n-\n-    /*\n-     * used for handling CacnelAlterStmt (for client is the CANCEL ALTER\n-     * command). including SchemaChangeHandler and RollupHandler\n-     */\n-    public void cancelAlter(CancelAlterTableStmt stmt) throws DdlException {\n-        if (stmt.getAlterType() == AlterType.ROLLUP) {\n-            this.getRollupHandler().cancel(stmt);\n-        } else if (stmt.getAlterType() == AlterType.COLUMN) {\n-            this.getSchemaChangeHandler().cancel(stmt);\n-        } else {\n-            throw new DdlException(\"Cancel \" + stmt.getAlterType() + \" does not implement yet\");\n-        }\n-    }\n-\n-    /*\n-     * used for handling backup opt\n-     */\n-    public void backup(BackupStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void restore(RestoreStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void cancelBackup(CancelBackupStmt stmt) throws DdlException {\n-        getBackupHandler().cancel(stmt);\n-    }\n-\n-    public void renameTable(Database db, OlapTable table, TableRenameClause tableRenameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String tableName = table.getName();\n-        String newTableName = tableRenameClause.getNewTableName();\n-        if (tableName.equals(newTableName)) {\n-            throw new DdlException(\"Same table name\");\n-        }\n-\n-        // check if name is already used\n-        if (db.getTable(newTableName) != null) {\n-            throw new DdlException(\"Table name[\" + newTableName + \"] is already used\");\n-        }\n-\n-        // check if rollup has same name\n-        for (String idxName : table.getIndexNameToId().keySet()) {\n-            if (idxName.equals(newTableName)) {\n-                throw new DdlException(\"New name conflicts with rollup index name: \" + idxName);\n-            }\n-        }\n-\n-        table.setName(newTableName);\n-\n-        db.dropTable(tableName);\n-        db.createTable(table);\n-\n-        TableInfo tableInfo = TableInfo.createForTableRename(db.getId(), table.getId(), newTableName);\n-        editLog.logTableRename(tableInfo);\n-        LOG.info(\"rename table[{}] to {}\", tableName, newTableName);\n-    }\n-\n-    public void replayRenameTable(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        String newTableName = tableInfo.getNewTableName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String tableName = table.getName();\n-            db.dropTable(tableName);\n-            table.setName(newTableName);\n-            db.createTable(table);\n-\n-            LOG.info(\"replay rename table[{}] to {}\", tableName, newTableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    // the invoker should keep db write lock\n-    public void modifyTableColocate(Database db, OlapTable table, String colocateGroup, boolean isReplay,\n-            GroupId assignedGroupId)\n-            throws DdlException {\n-\n-        String oldGroup = table.getColocateGroup();\n-        GroupId groupId = null;\n-        if (!Strings.isNullOrEmpty(colocateGroup)) {\n-            String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-            ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-            if (groupSchema == null) {\n-                // user set a new colocate group,\n-                // check if all partitions all this table has same buckets num and same replication number\n-                PartitionInfo partitionInfo = table.getPartitionInfo();\n-                if (partitionInfo.getType() == PartitionType.RANGE) {\n-                    int bucketsNum = -1;\n-                    short replicationNum = -1;\n-                    for (Partition partition : table.getPartitions()) {\n-                        if (bucketsNum == -1) {\n-                            bucketsNum = partition.getDistributionInfo().getBucketNum();\n-                        } else if (bucketsNum != partition.getDistributionInfo().getBucketNum()) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different buckets number\");\n-                        }\n-                        \n-                        if (replicationNum == -1) {\n-                            replicationNum = partitionInfo.getReplicationNum(partition.getId());\n-                        } else if (replicationNum != partitionInfo.getReplicationNum(partition.getId())) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different replication number\");\n-                        }\n-                    }\n-                }\n-            } else {\n-                // set to an already exist colocate group, check if this table can be added to this group.\n-                groupSchema.checkColocateSchema(table);\n-            }\n-            \n-            List<List<Long>> backendsPerBucketSeq = null;\n-            if (groupSchema == null) {\n-                // assign to a newly created group, set backends sequence.\n-                // we arbitrarily choose a tablet backends sequence from this table,\n-                // let the colocation balancer do the work.\n-                backendsPerBucketSeq = table.getArbitraryTabletBucketsSeq();\n-            }\n-            // change group after getting backends sequence(if has), in case 'getArbitraryTabletBucketsSeq' failed\n-            groupId = colocateTableIndex.changeGroup(db.getId(), table, oldGroup, colocateGroup, assignedGroupId);\n-\n-            if (groupSchema == null) {\n-                Preconditions.checkNotNull(backendsPerBucketSeq);\n-                colocateTableIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-            // set this group as unstable\n-            colocateTableIndex.markGroupUnstable(groupId, false /* edit log is along with modify table log */);\n-            table.setColocateGroup(colocateGroup);\n-        } else {\n-            // unset colocation group\n-            if (Strings.isNullOrEmpty(oldGroup)) {\n-                // this table is not a colocate table, do nothing\n-                return;\n-            }\n-\n-            // when replayModifyTableColocate, we need the groupId info\n-            String fullGroupName = db.getId() + \"_\" + oldGroup;\n-            groupId = colocateTableIndex.getGroupSchema(fullGroupName).getGroupId();\n-\n-            colocateTableIndex.removeTable(table.getId());\n-            table.setColocateGroup(null);\n-        }\n-\n-        if (!isReplay) {\n-            Map<String, String> properties = Maps.newHashMapWithExpectedSize(1);\n-            properties.put(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH, colocateGroup);\n-            TablePropertyInfo info = new TablePropertyInfo(table.getId(), groupId, properties);\n-            editLog.logModifyTableColocate(info);\n-        }\n-        LOG.info(\"finished modify table's colocation property. table: {}, is replay: {}\",\n-                table.getName(), isReplay);\n-    }\n-\n-    public void replayModifyTableColocate(TablePropertyInfo info) {\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getPropertyMap();\n-\n-        Database db = getDb(info.getGroupId().dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            modifyTableColocate(db, table, properties.get(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH), true,\n-                    info.getGroupId());\n-        } catch (DdlException e) {\n-            // should not happen\n-            LOG.warn(\"failed to replay modify table colocate\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameRollup(Database db, OlapTable table, RollupRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String rollupName = renameClause.getRollupName();\n-        // check if it is base table name\n-        if (rollupName.equals(table.getName())) {\n-            throw new DdlException(\"Using ALTER TABLE RENAME to change table name\");\n-        }\n-\n-        String newRollupName = renameClause.getNewRollupName();\n-        if (rollupName.equals(newRollupName)) {\n-            throw new DdlException(\"Same rollup name\");\n-        }\n-\n-        Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-        if (indexNameToIdMap.get(rollupName) == null) {\n-            throw new DdlException(\"Rollup index[\" + rollupName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (indexNameToIdMap.get(newRollupName) != null) {\n-            throw new DdlException(\"Rollup name[\" + newRollupName + \"] is already used\");\n-        }\n-\n-        long indexId = indexNameToIdMap.remove(rollupName);\n-        indexNameToIdMap.put(newRollupName, indexId);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForRollupRename(db.getId(), table.getId(), indexId, newRollupName);\n-        editLog.logRollupRename(tableInfo);\n-        LOG.info(\"rename rollup[{}] to {}\", rollupName, newRollupName);\n-    }\n-\n-    public void replayRenameRollup(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long indexId = tableInfo.getIndexId();\n-        String newRollupName = tableInfo.getNewRollupName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String rollupName = table.getIndexNameById(indexId);\n-            Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-            indexNameToIdMap.remove(rollupName);\n-            indexNameToIdMap.put(newRollupName, indexId);\n-\n-            LOG.info(\"replay rename rollup[{}] to {}\", rollupName, newRollupName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renamePartition(Database db, OlapTable table, PartitionRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        if (table.getPartitionInfo().getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is single partitioned. \"\n-                    + \"no need to rename partition name.\");\n-        }\n-\n-        String partitionName = renameClause.getPartitionName();\n-        String newPartitionName = renameClause.getNewPartitionName();\n-        if (partitionName.equalsIgnoreCase(newPartitionName)) {\n-            throw new DdlException(\"Same partition name\");\n-        }\n-\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition[\" + partitionName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (table.checkPartitionNameExist(newPartitionName)) {\n-            throw new DdlException(\"Partition name[\" + newPartitionName + \"] is already used\");\n-        }\n-\n-        table.renamePartition(partitionName, newPartitionName);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForPartitionRename(db.getId(), table.getId(), partition.getId(),\n-                newPartitionName);\n-        editLog.logPartitionRename(tableInfo);\n-        LOG.info(\"rename partition[{}] to {}\", partitionName, newPartitionName);\n-    }\n-\n-    public void replayRenamePartition(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long partitionId = tableInfo.getPartitionId();\n-        String newPartitionName = tableInfo.getNewPartitionName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            Partition partition = table.getPartition(partitionId);\n-            table.renamePartition(partition.getName(), newPartitionName);\n-\n-            LOG.info(\"replay rename partition[{}] to {}\", partition.getName(), newPartitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameColumn(Database db, OlapTable table, ColumnRenameClause renameClause) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void replayRenameColumn(TableInfo tableInfo) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void modifyTableDynamicPartition(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Map<String, String> logProperties = new HashMap<>(properties);\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(table, properties);\n-        } else {\n-            Map<String, String> analyzedDynamicPartition = DynamicPartitionUtil.analyzeDynamicPartition(properties);\n-            tableProperty.modifyTableProperties(analyzedDynamicPartition);\n-            tableProperty.buildDynamicProperty();\n-        }\n-\n-        DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), table);\n-        dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                table.getName(), DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), logProperties);\n-        editLog.logDynamicPartition(info);\n-    }\n-\n-    /**\n-     * Set replication number for unpartitioned table.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     * @throws DdlException\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableReplicationNum(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        String defaultReplicationNumName = \"default.\"+ PropertyAnalyzer.PROPERTIES_REPLICATION_NUM;\n-        PartitionInfo partitionInfo = table.getPartitionInfo();\n-        if (partitionInfo.getType() == PartitionType.RANGE) {\n-            throw new DdlException(\"This is a range partitioned table, you should specify partitions with MODIFY PARTITION clause.\" +\n-                    \" If you want to set default replication number, please use '\" + defaultReplicationNumName +\n-                    \"' instead of '\" + PropertyAnalyzer.PROPERTIES_REPLICATION_NUM + \"' to escape misleading.\");\n-        }\n-        String partitionName = table.getName();\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition does not exist. name: \" + partitionName);\n-        }\n-\n-        short replicationNum = Short.valueOf(properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-        boolean isInMemory = partitionInfo.getIsInMemory(partition.getId());\n-        DataProperty newDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        partitionInfo.setReplicationNum(partition.getId(), replicationNum);\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), table.getId(), partition.getId(),\n-                newDataProperty, replicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-        LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), table.getId(), partition.getName(),\n-                replicationNum);\n-    }\n-\n-    /**\n-     * Set default replication number for a specified table.\n-     * You can see the default replication number by Show Create Table stmt.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableDefaultReplicationNum(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildReplicationNum();\n-        // log\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyReplicationNum(info);\n-        LOG.debug(\"modify table[{}] replication num to {}\", table.getName(),\n-                properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-    }\n-\n-    // The caller need to hold the db write lock\n-    public void modifyTableInMemoryMeta(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildInMemory();\n-\n-        // need to update partition info meta\n-        for(Partition partition: table.getPartitions()) {\n-            table.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-        }\n-\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyInMemory(info);\n-    }\n-\n-    public void replayModifyTableProperty(short opCode, ModifyTablePropertyOperationLog info) {\n-        long dbId = info.getDbId();\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getProperties();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(tableId);\n-            TableProperty tableProperty = olapTable.getTableProperty();\n-            if (tableProperty == null) {\n-                olapTable.setTableProperty(new TableProperty(properties).buildProperty(opCode));\n-            } else {\n-                tableProperty.modifyTableProperties(properties);\n-                tableProperty.buildProperty(opCode);\n-            }\n-\n-            // need to replay partition info meta\n-            if (opCode == OperationType.OP_MODIFY_IN_MEMORY) {\n-                for(Partition partition: olapTable.getPartitions()) {\n-                    olapTable.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * used for handling AlterClusterStmt\n-     * (for client is the ALTER CLUSTER command).\n-     */\n-    public void alterCluster(AlterSystemStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterCluster(stmt);\n-    }\n-\n-    public void cancelAlterCluster(CancelAlterSystemStmt stmt) throws DdlException {\n-        this.alter.getClusterHandler().cancel(stmt);\n-    }\n-\n-    /*\n-     * generate and check columns' order and key's existence\n-     */\n-    private void validateColumns(List<Column> columns) throws DdlException {\n-        if (columns.isEmpty()) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_COLUMNS);\n-        }\n-\n-        boolean encounterValue = false;\n-        boolean hasKey = false;\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                if (encounterValue) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_OLAP_KEY_MUST_BEFORE_VALUE);\n-                }\n-                hasKey = true;\n-            } else {\n-                encounterValue = true;\n-            }\n-        }\n-\n-        if (!hasKey) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_KEYS);\n-        }\n-    }\n-\n-    // Change current database of this session.\n-    public void changeDb(ConnectContext ctx, String qualifiedDb) throws DdlException {\n-        if (!auth.checkDbPriv(ctx, qualifiedDb, PrivPredicate.SHOW)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_DB_ACCESS_DENIED, ctx.getQualifiedUser(), qualifiedDb);\n-        }\n-\n-        if (getDb(qualifiedDb) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, qualifiedDb);\n-        }\n-\n-        ctx.setDatabase(qualifiedDb);\n-    }\n-\n-    // for test only\n-    public void clear() {\n-        if (SingletonHolder.INSTANCE.idToDb != null) {\n-            SingletonHolder.INSTANCE.idToDb.clear();\n-        }\n-        if (SingletonHolder.INSTANCE.fullNameToDb != null) {\n-            SingletonHolder.INSTANCE.fullNameToDb.clear();\n-        }\n-        if (load.getIdToLoadJob() != null) {\n-            load.getIdToLoadJob().clear();\n-            // load = null;\n-        }\n-\n-        SingletonHolder.INSTANCE.getRollupHandler().unprotectedGetAlterJobs().clear();\n-        SingletonHolder.INSTANCE.getSchemaChangeHandler().unprotectedGetAlterJobs().clear();\n-        System.gc();\n-    }\n-\n-    public void createView(CreateViewStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTable();\n-\n-        // check if db exists\n-        Database db = this.getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create view[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        View newView = new View(tableId, tableName, columns);\n-        newView.setComment(stmt.getComment());\n-        newView.setInlineViewDefWithSqlMode(stmt.getInlineViewDef(),\n-                ConnectContext.get().getSessionVariable().getSqlMode());\n-        // init here in case the stmt string from view.toSql() has some syntax error.\n-        try {\n-            newView.init();\n-        } catch (UserException e) {\n-            throw new DdlException(\"failed to init view stmt\", e);\n-        }\n-      \n-        if (!db.createTableWithLock(newView, false, stmt.isSetIfNotExists())) {\n-            throw new DdlException(\"Failed to create view[\" + tableName + \"].\");\n-        }\n-        LOG.info(\"successfully create view[\" + tableName + \"-\" + newView.getId() + \"]\");\n-    }\n-\n-    /**\n-     * Returns the function that best matches 'desc' that is registered with the\n-     * catalog using 'mode' to check for matching. If desc matches multiple\n-     * functions in the catalog, it will return the function with the strictest\n-     * matching mode. If multiple functions match at the same matching mode,\n-     * ties are broken by comparing argument types in lexical order. Argument\n-     * types are ordered by argument precision (e.g. double is preferred over\n-     * float) and then by alphabetical order of argument type name, to guarantee\n-     * deterministic results.\n-     */\n-    public Function getFunction(Function desc, Function.CompareMode mode) {\n-        return functionSet.getFunction(desc, mode);\n-    }\n-\n-    public List<Function> getBuiltinFunctions() {\n-        return functionSet.getBulitinFunctions();\n-    }\n-\n-    public boolean isNonNullResultWithNullParamFunction(String funcName) {\n-        return functionSet.isNonNullResultWithNullParamFunctions(funcName);\n-    }\n-\n-    /**\n-     * create cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void createCluster(CreateClusterStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_HAS_EXIST, clusterName);\n-            } else {\n-                List<Long> backendList = systemInfo.createCluster(clusterName, stmt.getInstanceNum());\n-                // 1: BE returned is less than requested, throws DdlException.\n-                // 2: BE returned is more than or equal to 0, succeeds.\n-                if (backendList != null || stmt.getInstanceNum() == 0) {\n-                    final long id = getNextId();\n-                    final Cluster cluster = new Cluster(clusterName, id);\n-                    cluster.setBackendIdList(backendList);\n-                    unprotectCreateCluster(cluster);\n-                    if (clusterName.equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                        for (Database db : idToDb.values()) {\n-                            if (db.getClusterName().equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                                cluster.addDb(db.getFullName(), db.getId());\n-                            }\n-                        }\n-                    }\n-                    editLog.logCreateCluster(cluster);\n-                    LOG.info(\"finish to create cluster: {}\", clusterName);\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-            }\n-        } finally {\n-            unlock();\n-        }\n-\n-        // create super user for this cluster\n-        UserIdentity adminUser = new UserIdentity(PaloAuth.ADMIN_USER, \"%\");\n-        try {\n-            adminUser.analyze(stmt.getClusterName());\n-        } catch (AnalysisException e) {\n-            LOG.error(\"should not happen\", e);\n-        }\n-        auth.createUser(new CreateUserStmt(new UserDesc(adminUser, \"\", true)));\n-    }\n-\n-    private void unprotectCreateCluster(Cluster cluster) {\n-        final Iterator<Long> iterator = cluster.getBackendIdList().iterator();\n-        while (iterator.hasNext()) {\n-            final Long id = iterator.next();\n-            final Backend backend = systemInfo.getBackend(id);\n-            backend.setOwnerClusterName(cluster.getName());\n-            backend.setBackendState(BackendState.using);\n-        }\n-\n-        idToCluster.put(cluster.getId(), cluster);\n-        nameToCluster.put(cluster.getName(), cluster);\n-\n-        // create info schema db\n-        final InfoSchemaDb infoDb = new InfoSchemaDb(cluster.getName());\n-        infoDb.setClusterName(cluster.getName());\n-        unprotectCreateDb(infoDb);\n-\n-        // only need to create default cluster once.\n-        if (cluster.getName().equalsIgnoreCase(SystemInfoService.DEFAULT_CLUSTER)) {\n-            isDefaultClusterCreated = true;\n-        }\n-    }\n-\n-    /**\n-     * replay create cluster\n-     *\n-     * @param cluster\n-     */\n-    public void replayCreateCluster(Cluster cluster) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateCluster(cluster);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * drop cluster and cluster's db must be have deleted\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void dropCluster(DropClusterStmt stmt) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            final String clusterName = stmt.getClusterName();\n-            final Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            final List<Backend> backends = systemInfo.getClusterBackends(clusterName);\n-            for (Backend backend : backends) {\n-                if (backend.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            // check if there still have databases undropped, except for information_schema db\n-            if (cluster.getDbNames().size() > 1) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DELETE_DB_EXIST, clusterName);\n-            }\n-\n-            systemInfo.releaseBackends(clusterName, false /* is not replay */);\n-            final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId());\n-            unprotectDropCluster(info, false /* is not replay */);\n-            editLog.logDropCluster(info);\n-        } finally {\n-            unlock();\n-        }\n-\n-        // drop user of this cluster\n-        // set is replay to true, not write log\n-        auth.dropUserOfCluster(stmt.getClusterName(), true /* is replay */);\n-    }\n-\n-    private void unprotectDropCluster(ClusterInfo info, boolean isReplay) {\n-        systemInfo.releaseBackends(info.getClusterName(), isReplay);\n-        idToCluster.remove(info.getClusterId());\n-        nameToCluster.remove(info.getClusterName());\n-        final Database infoSchemaDb = fullNameToDb.get(InfoSchemaDb.getFullInfoSchemaDbName(info.getClusterName()));\n-        fullNameToDb.remove(infoSchemaDb.getFullName());\n-        idToDb.remove(infoSchemaDb.getId());\n-    }\n-\n-    public void replayDropCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            unprotectDropCluster(info, true/* is replay */);\n-        } finally {\n-            unlock();\n-        }\n-\n-        auth.dropUserOfCluster(info.getClusterName(), true /* is replay */);\n-    }\n-\n-    public void replayExpandCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            final Cluster cluster = nameToCluster.get(info.getClusterName());\n-            cluster.addBackends(info.getBackendIdList());\n-\n-            for (Long beId : info.getBackendIdList()) {\n-                Backend be = Catalog.getCurrentSystemInfo().getBackend(beId);\n-                if (be == null) {\n-                    continue;\n-                }\n-                be.setOwnerClusterName(info.getClusterName());\n-                be.setBackendState(BackendState.using);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * modify cluster: Expansion or shrink\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void processModifyCluster(AlterClusterStmt stmt) throws UserException {\n-        final String clusterName = stmt.getAlterClusterName();\n-        final int newInstanceNum = stmt.getInstanceNum();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-\n-            // check if this cluster has backend in decommission\n-            final List<Long> backendIdsInCluster = cluster.getBackendIdList();\n-            for (Long beId : backendIdsInCluster) {\n-                Backend be = systemInfo.getBackend(beId);\n-                if (be.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            final int oldInstanceNum = backendIdsInCluster.size();\n-            if (newInstanceNum > oldInstanceNum) {\n-                // expansion\n-                final List<Long> expandBackendIds = systemInfo.calculateExpansionBackends(clusterName,\n-                        newInstanceNum - oldInstanceNum);\n-                if (expandBackendIds == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-                cluster.addBackends(expandBackendIds);\n-                final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId(), expandBackendIds);\n-                editLog.logExpandCluster(info);\n-            } else if (newInstanceNum < oldInstanceNum) {\n-                // shrink\n-                final List<Long> decomBackendIds = systemInfo.calculateDecommissionBackends(clusterName,\n-                        oldInstanceNum - newInstanceNum);\n-                if (decomBackendIds == null || decomBackendIds.size() == 0) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BACKEND_ERROR);\n-                }\n-\n-                List<String> hostPortList = Lists.newArrayList();\n-                for (Long id : decomBackendIds) {\n-                    final Backend backend = systemInfo.getBackend(id);\n-                    hostPortList.add(new StringBuilder().append(backend.getHost()).append(\":\")\n-                            .append(backend.getHeartbeatPort()).toString());\n-                }\n-\n-                // here we reuse the process of decommission backends. but set backend's decommission type to\n-                // ClusterDecommission, which means this backend will not be removed from the system\n-                // after decommission is done.\n-                final DecommissionBackendClause clause = new DecommissionBackendClause(hostPortList);\n-                try {\n-                    clause.analyze(null);\n-                    clause.setType(DecommissionType.ClusterDecommission);\n-                    AlterSystemStmt alterStmt = new AlterSystemStmt(clause);\n-                    alterStmt.setClusterName(clusterName);\n-                    this.alter.processAlterCluster(alterStmt);\n-                } catch (AnalysisException e) {\n-                    Preconditions.checkState(false, \"should not happend: \" + e.getMessage());\n-                }\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_NO_CHANGE, newInstanceNum);\n-            }\n-\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * @param ctx\n-     * @param clusterName\n-     * @throws DdlException\n-     */\n-    public void changeCluster(ConnectContext ctx, String clusterName) throws DdlException {\n-        if (!Catalog.getCurrentCatalog().getAuth().checkCanEnterCluster(ConnectContext.get(), clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_AUTHORITY,\n-                    ConnectContext.get().getQualifiedUser(), \"enter\");\n-        }\n-\n-        if (!nameToCluster.containsKey(clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-        }\n-\n-        ctx.setCluster(clusterName);\n-    }\n-\n-    /**\n-     * migrate db to link dest cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void migrateDb(MigrateDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-            if (!destCluster.containLink(destDbName, srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-\n-            final Database db = fullNameToDb.get(srcDbName);\n-\n-            // if the max replication num of the src db is larger then the backends num of the dest cluster,\n-            // the migration will not be processed.\n-            final int maxReplicationNum = db.getMaxReplicationNum();\n-            if (maxReplicationNum > destCluster.getBackendIdList().size()) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_BE_NOT_ENOUGH, destClusterName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK) {\n-                final BaseParam param = new BaseParam();\n-                param.addStringParam(destDbName);\n-                param.addLongParam(db.getId());\n-                param.addStringParam(srcDbName);\n-                param.addStringParam(destClusterName);\n-                param.addStringParam(srcClusterName);\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                destCluster.removeLinkDb(param);\n-                destCluster.addDb(destDbName, db.getId());\n-                db.writeLock();\n-                try {\n-                    db.setDbState(DbState.MOVE);\n-                    // set cluster to the dest cluster.\n-                    // and Clone process will do the migration things.\n-                    db.setClusterName(destClusterName);\n-                    db.setName(destDbName);\n-                    db.setAttachDb(srcDbName);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-                editLog.logMigrateCluster(param);\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayMigrateDb(BaseParam param) {\n-        final String desDbName = param.getStringParam();\n-        final String srcDbName = param.getStringParam(1);\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcClusterName = param.getStringParam(3);\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Database db = fullNameToDb.get(srcDbName);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                desCluster.removeLinkDb(param);\n-                desCluster.addDb(param.getStringParam(), db.getId());\n-\n-                db.writeLock();\n-                db.setName(desDbName);\n-                db.setAttachDb(srcDbName);\n-                db.setDbState(DbState.MOVE);\n-                db.setClusterName(desClusterName);\n-                db.writeUnlock();\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayLinkDb(BaseParam param) {\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcDbName = param.getStringParam(1);\n-        final String desDbName = param.getStringParam();\n-\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-            srcDb.writeLock();\n-            srcDb.setDbState(DbState.LINK);\n-            srcDb.setAttachDb(desDbName);\n-            srcDb.writeUnlock();\n-            desCluster.addLinkDb(param);\n-            fullNameToDb.put(desDbName, srcDb);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * link src db to dest db. we use java's quotation Mechanism to realize db hard links\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void linkDb(LinkDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            if (fullNameToDb.containsKey(destDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, destDbName);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-\n-            if (srcDb.getDbState() != DbState.NORMAL) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                        ClusterNamespace.getNameFromFullName(srcDbName));\n-            }\n-\n-            srcDb.writeLock();\n-            try {\n-                srcDb.setDbState(DbState.LINK);\n-                srcDb.setAttachDb(destDbName);\n-            } finally {\n-                srcDb.writeUnlock();\n-            }\n-\n-            final long id = getNextId();\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(destDbName);\n-            param.addStringParam(srcDbName);\n-            param.addLongParam(id);\n-            param.addLongParam(srcDb.getId());\n-            param.addStringParam(destClusterName);\n-            param.addStringParam(srcClusterName);\n-            destCluster.addLinkDb(param);\n-            fullNameToDb.put(destDbName, srcDb);\n-            editLog.logLinkCluster(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Cluster getCluster(String clusterName) {\n-        return nameToCluster.get(clusterName);\n-    }\n-\n-    public List<String> getClusterNames() {\n-        return new ArrayList<String>(nameToCluster.keySet());\n-    }\n-\n-    /**\n-     * get migrate progress , when finish migration, next clonecheck will reset dbState\n-     *\n-     * @return\n-     */\n-    public Set<BaseParam> getMigrations() {\n-        final Set<BaseParam> infos = Sets.newHashSet();\n-        for (Database db : fullNameToDb.values()) {\n-            db.readLock();\n-            try {\n-                if (db.getDbState() == DbState.MOVE) {\n-                    int tabletTotal = 0;\n-                    int tabletQuorum = 0;\n-                    final Set<Long> beIds = Sets.newHashSet(systemInfo.getClusterBackendIds(db.getClusterName()));\n-                    final Set<String> tableNames = db.getTableNamesWithLock();\n-                    for (String tableName : tableNames) {\n-\n-                        Table table = db.getTable(tableName);\n-                        if (table == null || table.getType() != TableType.OLAP) {\n-                            continue;\n-                        }\n-\n-                        OlapTable olapTable = (OlapTable) table;\n-                        for (Partition partition : olapTable.getPartitions()) {\n-                            final short replicationNum = olapTable.getPartitionInfo()\n-                                    .getReplicationNum(partition.getId());\n-                            for (MaterializedIndex materializedIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                                if (materializedIndex.getState() != IndexState.NORMAL) {\n-                                    continue;\n-                                }\n-                                for (Tablet tablet : materializedIndex.getTablets()) {\n-                                    int replicaNum = 0;\n-                                    int quorum = replicationNum / 2 + 1;\n-                                    for (Replica replica : tablet.getReplicas()) {\n-                                        if (replica.getState() != ReplicaState.CLONE\n-                                                && beIds.contains(replica.getBackendId())) {\n-                                            replicaNum++;\n-                                        }\n-                                    }\n-                                    if (replicaNum > quorum) {\n-                                        replicaNum = quorum;\n-                                    }\n-\n-                                    tabletQuorum = tabletQuorum + replicaNum;\n-                                    tabletTotal = tabletTotal + quorum;\n-                                }\n-                            }\n-                        }\n-                    }\n-                    final BaseParam info = new BaseParam();\n-                    info.addStringParam(db.getClusterName());\n-                    info.addStringParam(db.getAttachDb());\n-                    info.addStringParam(db.getFullName());\n-                    final float percentage = tabletTotal > 0 ? (float) tabletQuorum / (float) tabletTotal : 0f;\n-                    info.addFloatParam(percentage);\n-                    infos.add(info);\n-                }\n-            } finally {\n-                db.readUnlock();\n-            }\n-        }\n-\n-        return infos;\n-    }\n-\n-    public long loadCluster(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_30) {\n-            int clusterCount = dis.readInt();\n-            checksum ^= clusterCount;\n-            for (long i = 0; i < clusterCount; ++i) {\n-                final Cluster cluster = Cluster.read(dis);\n-                checksum ^= cluster.getId();\n-\n-                List<Long> latestBackendIds = systemInfo.getClusterBackendIds(cluster.getName());\n-                if (latestBackendIds.size() != cluster.getBackendIdList().size()) {\n-                    LOG.warn(\"Cluster:\" + cluster.getName() + \", backends in Cluster is \"\n-                            + cluster.getBackendIdList().size() + \", backends in SystemInfoService is \"\n-                            + cluster.getBackendIdList().size());\n-                }\n-                // The number of BE in cluster is not same as in SystemInfoService, when perform 'ALTER\n-                // SYSTEM ADD BACKEND TO ...' or 'ALTER SYSTEM ADD BACKEND ...', because both of them are \n-                // for adding BE to some Cluster, but loadCluster is after loadBackend.\n-                cluster.setBackendIdList(latestBackendIds);\n-\n-                String dbName =  InfoSchemaDb.getFullInfoSchemaDbName(cluster.getName());\n-                InfoSchemaDb db;\n-                // Use real Catalog instance to avoid InfoSchemaDb id continuously increment\n-                // when checkpoint thread load image.\n-                if (Catalog.getCurrentCatalog().getFullNameToDb().containsKey(dbName)) {\n-                    db = (InfoSchemaDb)Catalog.getCurrentCatalog().getFullNameToDb().get(dbName);\n-                } else {\n-                    db = new InfoSchemaDb(cluster.getName());\n-                    db.setClusterName(cluster.getName());\n-                }\n-                String errMsg = \"InfoSchemaDb id shouldn't larger than 10000, please restart your FE server\";\n-                // Every time we construct the InfoSchemaDb, which id will increment.\n-                // When InfoSchemaDb id larger than 10000 and put it to idToDb,\n-                // which may be overwrite the normal db meta in idToDb,\n-                // so we ensure InfoSchemaDb id less than 10000.\n-                Preconditions.checkState(db.getId() < NEXT_ID_INIT_VALUE, errMsg);\n-                idToDb.put(db.getId(), db);\n-                fullNameToDb.put(db.getFullName(), db);\n-                cluster.addDb(dbName, db.getId());\n-                idToCluster.put(cluster.getId(), cluster);\n-                nameToCluster.put(cluster.getName(), cluster);\n-            }\n-        }\n-        LOG.info(\"finished replay cluster from image\");\n-        return checksum;\n-    }\n-\n-    public void initDefaultCluster() {\n-        final List<Long> backendList = Lists.newArrayList();\n-        final List<Backend> defaultClusterBackends = systemInfo.getClusterBackends(SystemInfoService.DEFAULT_CLUSTER);\n-        for (Backend backend : defaultClusterBackends) {\n-            backendList.add(backend.getId());\n-        }\n-\n-        final long id = getNextId();\n-        final Cluster cluster = new Cluster(SystemInfoService.DEFAULT_CLUSTER, id);\n-\n-        // make sure one host hold only one backend.\n-        Set<String> beHost = Sets.newHashSet();\n-        for (Backend be : defaultClusterBackends) {\n-            if (beHost.contains(be.getHost())) {\n-                // we can not handle this situation automatically.\n-                LOG.error(\"found more than one backends in same host: {}\", be.getHost());\n-                System.exit(-1);\n-            } else {\n-                beHost.add(be.getHost());\n-            }\n-        }\n-\n-        // we create default_cluster to meet the need for ease of use, because\n-        // most users hava no multi tenant needs.\n-        cluster.setBackendIdList(backendList);\n-        unprotectCreateCluster(cluster);\n-        for (Database db : idToDb.values()) {\n-            db.setClusterName(SystemInfoService.DEFAULT_CLUSTER);\n-            cluster.addDb(db.getFullName(), db.getId());\n-        }\n-\n-        // no matter default_cluster is created or not,\n-        // mark isDefaultClusterCreated as true\n-        isDefaultClusterCreated = true;\n-        editLog.logCreateCluster(cluster);\n-    }\n-\n-    public void replayUpdateDb(DatabaseInfo info) {\n-        final Database db = fullNameToDb.get(info.getDbName());\n-        db.setClusterName(info.getClusterName());\n-        db.setDbState(info.getDbState());\n-    }\n-\n-    public long saveCluster(DataOutputStream dos, long checksum) throws IOException {\n-        final int clusterCount = idToCluster.size();\n-        checksum ^= clusterCount;\n-        dos.writeInt(clusterCount);\n-        for (Map.Entry<Long, Cluster> entry : idToCluster.entrySet()) {\n-            long clusterId = entry.getKey();\n-            if (clusterId >= NEXT_ID_INIT_VALUE) {\n-                checksum ^= clusterId;\n-                final Cluster cluster = entry.getValue();\n-                cluster.write(dos);\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveBrokers(DataOutputStream dos, long checksum) throws IOException {\n-        Map<String, List<FsBroker>> addressListMap = brokerMgr.getBrokerListMap();\n-        int size = addressListMap.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-\n-        for (Map.Entry<String, List<FsBroker>> entry : addressListMap.entrySet()) {\n-            Text.writeString(dos, entry.getKey());\n-            final List<FsBroker> addrs = entry.getValue();\n-            size = addrs.size();\n-            checksum ^= size;\n-            dos.writeInt(size);\n-            for (FsBroker addr : addrs) {\n-                addr.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long loadBrokers(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (MetaContext.get().getMetaVersion() >= FeMetaVersion.VERSION_31) {\n-            int count = dis.readInt();\n-            checksum ^= count;\n-            for (long i = 0; i < count; ++i) {\n-                String brokerName = Text.readString(dis);\n-                int size = dis.readInt();\n-                checksum ^= size;\n-                List<FsBroker> addrs = Lists.newArrayList();\n-                for (int j = 0; j < size; j++) {\n-                    FsBroker addr = FsBroker.readIn(dis);\n-                    addrs.add(addr);\n-                }\n-                brokerMgr.replayAddBrokers(brokerName, addrs);\n-            }\n-            LOG.info(\"finished replay brokerMgr from image\");\n-        }\n-        return checksum;\n-    }\n-\n-    public void replayUpdateClusterAndBackends(BackendIdsUpdateInfo info) {\n-        for (long id : info.getBackendList()) {\n-            final Backend backend = systemInfo.getBackend(id);\n-            final Cluster cluster = nameToCluster.get(backend.getOwnerClusterName());\n-            cluster.removeBackend(id);\n-            backend.setDecommissioned(false);\n-            backend.clearClusterName();\n-            backend.setBackendState(BackendState.free);\n-        }\n-    }\n-\n-    public String dumpImage() {\n-        LOG.info(\"begin to dump meta data\");\n-        String dumpFilePath;\n-        Map<Long, Database> lockedDbMap = Maps.newTreeMap();\n-        tryLock(true);\n-        try {\n-            // sort all dbs\n-            for (long dbId : getDbIds()) {\n-                Database db = getDb(dbId);\n-                Preconditions.checkNotNull(db);\n-                lockedDbMap.put(dbId, db);\n-            }\n-\n-            // lock all dbs\n-            for (Database db : lockedDbMap.values()) {\n-                db.readLock();\n-            }\n-            LOG.info(\"acquired all the dbs' read lock.\");\n-\n-            load.readLock();\n-\n-            LOG.info(\"acquired all jobs' read lock.\");\n-            long journalId = getMaxJournalId();\n-            File dumpFile = new File(Config.meta_dir, \"image.\" + journalId);\n-            dumpFilePath = dumpFile.getAbsolutePath();\n-            try {\n-                LOG.info(\"begin to dump {}\", dumpFilePath);\n-                saveImage(dumpFile, journalId);\n-            } catch (IOException e) {\n-                LOG.error(\"failed to dump image to {}\", dumpFilePath, e);\n-            }\n-        } finally {\n-            // unlock all\n-            load.readUnlock();\n-            for (Database db : lockedDbMap.values()) {\n-                db.readUnlock();\n-            }\n-\n-            unlock();\n-        }\n-\n-        LOG.info(\"finished dumpping image to {}\", dumpFilePath);\n-        return dumpFilePath;\n-    }\n-\n-    /*\n-     * Truncate specified table or partitions.\n-     * The main idea is:\n-     * \n-     * 1. using the same schema to create new table(partitions)\n-     * 2. use the new created table(partitions) to replace the old ones.\n-     * \n-     * if no partition specified, it will truncate all partitions of this table, including all temp partitions,\n-     * otherwise, it will only truncate those specified partitions.\n-     * \n-     */\n-    public void truncateTable(TruncateTableStmt truncateTableStmt) throws DdlException {\n-        TableRef tblRef = truncateTableStmt.getTblRef();\n-        TableName dbTbl = tblRef.getName();\n-\n-        // check, and save some info which need to be checked again later\n-        Map<String, Long> origPartitions = Maps.newHashMap();\n-        OlapTable copiedTbl = null;\n-        Database db = getDb(dbTbl.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbTbl.getDb());\n-        }\n-\n-        boolean truncateEntireTable = tblRef.getPartitionNames() == null;\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(dbTbl.getTbl());\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, dbTbl.getTbl());\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Only support truncate OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-            \n-            if (!truncateEntireTable) {\n-                for (String partName : tblRef.getPartitionNames().getPartitionNames()) {\n-                    Partition partition = olapTable.getPartition(partName);\n-                    if (partition == null) {\n-                        throw new DdlException(\"Partition \" + partName + \" does not exist\");\n-                    }\n-                    \n-                    origPartitions.put(partName, partition.getId());\n-                }\n-            } else {\n-                for (Partition partition : olapTable.getPartitions()) {\n-                    origPartitions.put(partition.getName(), partition.getId());\n-                }\n-            }\n-            \n-            copiedTbl = olapTable.selectiveCopy(origPartitions.keySet(), true, IndexExtState.VISIBLE);\n-        } finally {\n-            db.readUnlock();\n-        }\n-        \n-        // 2. use the copied table to create partitions\n-        List<Partition> newPartitions = Lists.newArrayList();\n-        // tabletIdSet to save all newly created tablet ids.\n-        Set<Long> tabletIdSet = Sets.newHashSet();\n-        try {\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                // the new partition must use new id\n-                // If we still use the old partition id, the behavior of current load jobs on this partition\n-                // will be undefined.\n-                // By using a new id, load job will be aborted(just like partition is dropped),\n-                // which is the right behavior.\n-                long oldPartitionId = entry.getValue();\n-                long newPartitionId = getNextId();\n-                Partition newPartition = createPartitionWithIndices(db.getClusterName(),\n-                        db.getId(), copiedTbl.getId(), copiedTbl.getBaseIndexId(),\n-                        newPartitionId, entry.getKey(),\n-                        copiedTbl.getIndexIdToMeta(),\n-                        copiedTbl.getKeysType(),\n-                        copiedTbl.getDefaultDistributionInfo(),\n-                        copiedTbl.getPartitionInfo().getDataProperty(oldPartitionId).getStorageMedium(),\n-                        copiedTbl.getPartitionInfo().getReplicationNum(oldPartitionId),\n-                        null /* version info */,\n-                        copiedTbl.getCopiedBfColumns(),\n-                        copiedTbl.getBfFpp(),\n-                        tabletIdSet,\n-                        copiedTbl.getCopiedIndexes(),\n-                        copiedTbl.isInMemory(),\n-                        copiedTbl.getStorageFormat(),\n-                        copiedTbl.getPartitionInfo().getTabletType(oldPartitionId));\n-                newPartitions.add(newPartition);\n-            }\n-        } catch (DdlException e) {\n-            // create partition failed, remove all newly created tablets\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-        Preconditions.checkState(origPartitions.size() == newPartitions.size());\n-\n-        // all partitions are created successfully, try to replace the old partitions.\n-        // before replacing, we need to check again.\n-        // Things may be changed outside the database lock.\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(copiedTbl.getId());\n-            if (olapTable == null) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"] is dropped\");\n-            }\n-            \n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-\n-            // check partitions\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                Partition partition = copiedTbl.getPartition(entry.getValue());\n-                if (partition == null || !partition.getName().equals(entry.getKey())) {\n-                    throw new DdlException(\"Partition [\" + entry.getKey() + \"] is changed\");\n-                }\n-            }\n-\n-            // check if meta changed\n-            // rollup index may be added or dropped, and schema may be changed during creating partition operation.\n-            boolean metaChanged = false;\n-            if (olapTable.getIndexNameToId().size() != copiedTbl.getIndexNameToId().size()) {\n-                metaChanged = true;\n-            } else {\n-                // compare schemaHash\n-                Map<Long, Integer> copiedIndexIdToSchemaHash = copiedTbl.getIndexIdToSchemaHash();\n-                for (Map.Entry<Long, Integer> entry : olapTable.getIndexIdToSchemaHash().entrySet()) {\n-                    long indexId = entry.getKey();\n-                    if (!copiedIndexIdToSchemaHash.containsKey(indexId)) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                    if (!copiedIndexIdToSchemaHash.get(indexId).equals(entry.getValue())) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            if (metaChanged) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"]'s meta has been changed. try again.\");\n-            }\n-\n-            // replace\n-            truncateTableInternal(olapTable, newPartitions, truncateEntireTable);\n-\n-            // write edit log\n-            TruncateTableInfo info = new TruncateTableInfo(db.getId(), olapTable.getId(), newPartitions,\n-                    truncateEntireTable);\n-            editLog.logTruncateTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-        \n-        LOG.info(\"finished to truncate table {}, partitions: {}\",\n-                tblRef.getName().toSql(), tblRef.getPartitionNames());\n-    }\n-\n-    private void truncateTableInternal(OlapTable olapTable, List<Partition> newPartitions, boolean isEntireTable) {\n-        // use new partitions to replace the old ones.\n-        Set<Long> oldTabletIds = Sets.newHashSet();\n-        for (Partition newPartition : newPartitions) {\n-            Partition oldPartition = olapTable.replacePartition(newPartition);\n-            // save old tablets to be removed\n-            for (MaterializedIndex index : oldPartition.getMaterializedIndices(IndexExtState.ALL)) {\n-                index.getTablets().stream().forEach(t -> {\n-                    oldTabletIds.add(t.getId());\n-                });\n-            }\n-        }\n-\n-        if (isEntireTable) {\n-            // drop all temp partitions\n-            olapTable.dropAllTempPartitions();\n-        }\n-\n-        // remove the tablets in old partitions\n-        for (Long tabletId : oldTabletIds) {\n-            Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-        }\n-    }\n-\n-    public void replayTruncateTable(TruncateTableInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTblId());\n-            truncateTableInternal(olapTable, info.getPartitions(), info.isEntireTable());\n-\n-            if (!Catalog.isCheckpointThread()) {\n-                // add tablet to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (Partition partition : info.getPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(db.getId(), olapTable.getId(),\n-                                partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void createFunction(CreateFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.addFunction(stmt.getFunction());\n-    }\n-\n-    public void replayCreateFunction(Function function) {\n-        String dbName = function.getFunctionName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayAddFunction(function);\n-    }\n-\n-    public void dropFunction(DropFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.dropFunction(stmt.getFunction());\n-    }\n-\n-    public void replayDropFunction(FunctionSearchDesc functionSearchDesc) {\n-        String dbName = functionSearchDesc.getName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayDropFunction(functionSearchDesc);\n-    }\n-\n-    public void setConfig(AdminSetConfigStmt stmt) throws DdlException {\n-        Map<String, String> configs = stmt.getConfigs();\n-        Preconditions.checkState(configs.size() == 1);\n-\n-        for (Map.Entry<String, String> entry : configs.entrySet()) {\n-            ConfigBase.setMutableConfig(entry.getKey(), entry.getValue());\n-        }\n-    }\n-\n-    public void replayBackendTabletsInfo(BackendTabletsInfo backendTabletsInfo) {\n-        List<Pair<Long, Integer>> tabletsWithSchemaHash = backendTabletsInfo.getTabletSchemaHash();\n-        for (Pair<Long, Integer> tabletInfo : tabletsWithSchemaHash) {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletInfo.first,\n-                    backendTabletsInfo.getBackendId());\n-            if (replica == null) {\n-                LOG.warn(\"replica does not found when replay. tablet {}, backend {}\",\n-                        tabletInfo.first, backendTabletsInfo.getBackendId());\n-                continue;\n-            }\n-\n-            if (replica.getSchemaHash() != tabletInfo.second) {\n-                continue;\n-            }\n-\n-            replica.setBad(backendTabletsInfo.isBad());\n-        }\n-    }\n-\n-    // Convert table's distribution type from random to hash.\n-    // random distribution is no longer supported.\n-    public void convertDistributionType(Database db, OlapTable tbl) throws DdlException {\n-        db.writeLock();\n-        try {\n-            if (!tbl.convertRandomDistributionToHashDistribution()) {\n-                throw new DdlException(\"Table \" + tbl.getName() + \" is not random distributed\");\n-            }\n-            TableInfo tableInfo = TableInfo.createForModifyDistribution(db.getId(), tbl.getId());\n-            editLog.logModifyDistributionType(tableInfo);\n-            LOG.info(\"finished to modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayConvertDistributionType(TableInfo tableInfo) {\n-        Database db = getDb(tableInfo.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable tbl = (OlapTable) db.getTable(tableInfo.getTableId());\n-            tbl.convertRandomDistributionToHashDistribution();\n-            LOG.info(\"replay modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * The entry of replacing partitions with temp partitions.\n-     */\n-    public void replaceTempPartition(Database db, String tableName, ReplacePartitionClause clause) throws DdlException {\n-        List<String> partitionNames = clause.getPartitionNames();\n-        List<String> tempPartitonNames = clause.getTempPartitionNames();\n-        boolean isStrictRange = clause.isStrictRange();\n-        boolean useTempPartitionName = clause.useTempPartitionName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            // check partition exist\n-            for (String partName : partitionNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, false)) {\n-                    throw new DdlException(\"Partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-            for (String partName : tempPartitonNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, true)) {\n-                    throw new DdlException(\"Temp partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-\n-            olapTable.replaceTempPartitions(partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-\n-            // write log\n-            ReplacePartitionOperationLog info = new ReplacePartitionOperationLog(db.getId(), olapTable.getId(),\n-                    partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-            editLog.logReplaceTempPartition(info);\n-            LOG.info(\"finished to replace partitions {} with temp partitions {} from table: {}\",\n-                    clause.getPartitionNames(), clause.getTempPartitionNames(), tableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayReplaceTempPartition(ReplacePartitionOperationLog replaceTempPartitionLog) {\n-        Database db = getDb(replaceTempPartitionLog.getDbId());\n-        if (db == null) {\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(replaceTempPartitionLog.getTblId());\n-            if (olapTable == null) {\n-                return;\n-            }\n-            olapTable.replaceTempPartitions(replaceTempPartitionLog.getPartitions(),\n-                    replaceTempPartitionLog.getTempPartitions(),\n-                    replaceTempPartitionLog.isStrictRange(),\n-                    replaceTempPartitionLog.useTempPartitionName());\n-        } catch (DdlException e) {\n-            LOG.warn(\"should not happen. {}\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void installPlugin(InstallPluginStmt stmt) throws UserException, IOException {\n-        pluginMgr.installPlugin(stmt);\n-    }\n-\n-    public long savePlugins(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentPluginMgr().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPlugins(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_78) {\n-            Catalog.getCurrentPluginMgr().readFields(dis);\n-        }\n-        LOG.info(\"finished replay plugins from image\");\n-        return checksum;\n-    }\n-\n-    public void replayInstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.replayLoadDynamicPlugin(pluginInfo);\n-        } catch (Exception e) {\n-            LOG.warn(\"replay install plugin failed.\", e);\n-        }\n-    }\n-\n-    public void uninstallPlugin(UninstallPluginStmt stmt) throws IOException, UserException {\n-        PluginInfo info = pluginMgr.uninstallPlugin(stmt.getPluginName());\n-        if (null != info) {\n-            editLog.logUninstallPlugin(info);\n-        }\n-        LOG.info(\"uninstall plugin = \" + stmt.getPluginName());\n-    }\n-\n-    public void replayUninstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.uninstallPlugin(pluginInfo.getName());\n-        } catch (Exception e) {\n-            LOG.warn(\"replay uninstall plugin failed.\", e);\n-        }\n-    }\n-\n-    // entry of checking tablets operation\n-    public void checkTablets(AdminCheckTabletsStmt stmt) {\n-        CheckType type = stmt.getType();\n-        switch (type) {\n-            case CONSISTENCY:\n-                consistencyChecker.addTabletsToCheck(stmt.getTabletIds());\n-                break;\n-            default:\n-                break;\n-        }\n-    }\n-\n-    // Set specified replica's status. If replica does not exist, just ignore it.\n-    public void setReplicaStatus(AdminSetReplicaStatusStmt stmt) {\n-        long tabletId = stmt.getTabletId();\n-        long backendId = stmt.getBackendId();\n-        ReplicaStatus status = stmt.getStatus();\n-        setReplicaStatusInternal(tabletId, backendId, status, false);\n-    }\n-\n-    public void replaySetReplicaStatus(SetReplicaStatusOperationLog log) {\n-        setReplicaStatusInternal(log.getTabletId(), log.getBackendId(), log.getReplicaStatus(), true);\n-    }\n-\n-    private void setReplicaStatusInternal(long tabletId, long backendId, ReplicaStatus status, boolean isReplay) {\n-        TabletMeta meta = tabletInvertedIndex.getTabletMeta(tabletId);\n-        if (meta == null) {\n-            LOG.info(\"tablet {} does not exist\", tabletId);\n-            return;\n-        }\n-        long dbId = meta.getDbId();\n-        Database db = getDb(dbId);\n-        if (db == null) {\n-            LOG.info(\"database {} of tablet {} does not exist\", dbId, tabletId);\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletId, backendId);\n-            if (replica == null) {\n-                LOG.info(\"replica of tablet {} does not exist\", tabletId);\n-                return;\n-            }\n-            if (status == ReplicaStatus.BAD || status == ReplicaStatus.OK) {\n-                if (replica.setBad(status == ReplicaStatus.BAD)) {\n-                    if (!isReplay) {\n-                        SetReplicaStatusOperationLog log = new SetReplicaStatusOperationLog(backendId, tabletId, status);\n-                        getEditLog().logSetReplicaStatus(log);\n-                    }\n-                    LOG.info(\"set replica {} of tablet {} on backend {} as {}. is replay: {}\",\n-                            replica.getId(), tabletId, backendId, status, isReplay);\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-}\n-\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTMzOA==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450275338", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public boolean isNeedCheckCommitedTxns() {\n          \n          \n            \n                public boolean isNeedCheckCommittedTxns() {", "author": "morningman", "createdAt": "2020-07-06T14:49:44Z", "path": "fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java", "diffHunk": "@@ -61,6 +64,10 @@ public boolean isView() {\n         return isView;\n     }\n \n+    public boolean isNeedCheckCommitedTxns() {", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java b/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\ndeleted file mode 100644\nindex 0104f0c4e..000000000\n--- a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\n+++ /dev/null\n\n@@ -1,96 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.analysis;\n-\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.qe.ConnectContext;\n-\n-import com.google.common.base.Strings;\n-\n-// DROP TABLE\n-public class DropTableStmt extends DdlStmt {\n-    private boolean ifExists;\n-    private final TableName tableName;\n-    private final boolean isView;\n-    private boolean needCheckCommitedTxns;\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = false;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean isView, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = isView;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public boolean isSetIfExists() {\n-        return ifExists;\n-    }\n-\n-    public String getDbName() {\n-        return tableName.getDb();\n-    }\n-\n-    public String getTableName() {\n-        return tableName.getTbl();\n-    }\n-\n-    public boolean isView() {\n-        return isView;\n-    }\n-\n-    public boolean isNeedCheckCommitedTxns() {\n-        return this.needCheckCommitedTxns;\n-    }\n-\n-    @Override\n-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {\n-        if (Strings.isNullOrEmpty(tableName.getDb())) {\n-            tableName.setDb(analyzer.getDefaultDb());\n-        }\n-        tableName.analyze(analyzer);\n-\n-        // check access\n-        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), tableName.getDb(),\n-                                                                tableName.getTbl(), PrivPredicate.DROP)) {\n-            ErrorReport.reportAnalysisException(ErrorCode.ERR_SPECIFIC_ACCESS_DENIED_ERROR, \"DROP\");\n-        }\n-    }\n-\n-    @Override\n-    public String toSql() {\n-        StringBuilder stringBuilder = new StringBuilder();\n-        stringBuilder.append(\"DROP TABLE \").append(tableName.toSql());\n-        return stringBuilder.toString();\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return toSql();\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTQyMw==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450275423", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n          \n          \n            \n                public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommittedTxns) {", "author": "morningman", "createdAt": "2020-07-06T14:49:50Z", "path": "fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java", "diffHunk": "@@ -32,17 +32,20 @@\n     private boolean ifExists;\n     private final TableName tableName;\n     private final boolean isView;\n+    private boolean needCheckCommitedTxns;\n \n-    public DropTableStmt(boolean ifExists, TableName tableName) {\n+    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java b/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\ndeleted file mode 100644\nindex 0104f0c4e..000000000\n--- a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\n+++ /dev/null\n\n@@ -1,96 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.analysis;\n-\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.qe.ConnectContext;\n-\n-import com.google.common.base.Strings;\n-\n-// DROP TABLE\n-public class DropTableStmt extends DdlStmt {\n-    private boolean ifExists;\n-    private final TableName tableName;\n-    private final boolean isView;\n-    private boolean needCheckCommitedTxns;\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = false;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean isView, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = isView;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public boolean isSetIfExists() {\n-        return ifExists;\n-    }\n-\n-    public String getDbName() {\n-        return tableName.getDb();\n-    }\n-\n-    public String getTableName() {\n-        return tableName.getTbl();\n-    }\n-\n-    public boolean isView() {\n-        return isView;\n-    }\n-\n-    public boolean isNeedCheckCommitedTxns() {\n-        return this.needCheckCommitedTxns;\n-    }\n-\n-    @Override\n-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {\n-        if (Strings.isNullOrEmpty(tableName.getDb())) {\n-            tableName.setDb(analyzer.getDefaultDb());\n-        }\n-        tableName.analyze(analyzer);\n-\n-        // check access\n-        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), tableName.getDb(),\n-                                                                tableName.getTbl(), PrivPredicate.DROP)) {\n-            ErrorReport.reportAnalysisException(ErrorCode.ERR_SPECIFIC_ACCESS_DENIED_ERROR, \"DROP\");\n-        }\n-    }\n-\n-    @Override\n-    public String toSql() {\n-        StringBuilder stringBuilder = new StringBuilder();\n-        stringBuilder.append(\"DROP TABLE \").append(tableName.toSql());\n-        return stringBuilder.toString();\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return toSql();\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTQ5OQ==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450275499", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private boolean needCheckCommitedTxns;\n          \n          \n            \n                private boolean needCheckCommittedTxns;", "author": "morningman", "createdAt": "2020-07-06T14:49:57Z", "path": "fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java", "diffHunk": "@@ -32,17 +32,20 @@\n     private boolean ifExists;\n     private final TableName tableName;\n     private final boolean isView;\n+    private boolean needCheckCommitedTxns;", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java b/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\ndeleted file mode 100644\nindex 0104f0c4e..000000000\n--- a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\n+++ /dev/null\n\n@@ -1,96 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.analysis;\n-\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.qe.ConnectContext;\n-\n-import com.google.common.base.Strings;\n-\n-// DROP TABLE\n-public class DropTableStmt extends DdlStmt {\n-    private boolean ifExists;\n-    private final TableName tableName;\n-    private final boolean isView;\n-    private boolean needCheckCommitedTxns;\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = false;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean isView, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = isView;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public boolean isSetIfExists() {\n-        return ifExists;\n-    }\n-\n-    public String getDbName() {\n-        return tableName.getDb();\n-    }\n-\n-    public String getTableName() {\n-        return tableName.getTbl();\n-    }\n-\n-    public boolean isView() {\n-        return isView;\n-    }\n-\n-    public boolean isNeedCheckCommitedTxns() {\n-        return this.needCheckCommitedTxns;\n-    }\n-\n-    @Override\n-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {\n-        if (Strings.isNullOrEmpty(tableName.getDb())) {\n-            tableName.setDb(analyzer.getDefaultDb());\n-        }\n-        tableName.analyze(analyzer);\n-\n-        // check access\n-        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), tableName.getDb(),\n-                                                                tableName.getTbl(), PrivPredicate.DROP)) {\n-            ErrorReport.reportAnalysisException(ErrorCode.ERR_SPECIFIC_ACCESS_DENIED_ERROR, \"DROP\");\n-        }\n-    }\n-\n-    @Override\n-    public String toSql() {\n-        StringBuilder stringBuilder = new StringBuilder();\n-        stringBuilder.append(\"DROP TABLE \").append(tableName.toSql());\n-        return stringBuilder.toString();\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return toSql();\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTY4OQ==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450275689", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    this.needCheckCommitedTxns = needCheckCommitedTxns;\n          \n          \n            \n                    this.needCheckCommittedTxns = needCheckCommittedTxns;", "author": "morningman", "createdAt": "2020-07-06T14:50:14Z", "path": "fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java", "diffHunk": "@@ -32,17 +32,20 @@\n     private boolean ifExists;\n     private final TableName tableName;\n     private final boolean isView;\n+    private boolean needCheckCommitedTxns;\n \n-    public DropTableStmt(boolean ifExists, TableName tableName) {\n+    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n         this.ifExists = ifExists;\n         this.tableName = tableName;\n         this.isView = false;\n+        this.needCheckCommitedTxns = needCheckCommitedTxns;", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java b/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\ndeleted file mode 100644\nindex 0104f0c4e..000000000\n--- a/fe/src/main/java/org/apache/doris/analysis/DropTableStmt.java\n+++ /dev/null\n\n@@ -1,96 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.analysis;\n-\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.qe.ConnectContext;\n-\n-import com.google.common.base.Strings;\n-\n-// DROP TABLE\n-public class DropTableStmt extends DdlStmt {\n-    private boolean ifExists;\n-    private final TableName tableName;\n-    private final boolean isView;\n-    private boolean needCheckCommitedTxns;\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = false;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public DropTableStmt(boolean ifExists, TableName tableName, boolean isView, boolean needCheckCommitedTxns) {\n-        this.ifExists = ifExists;\n-        this.tableName = tableName;\n-        this.isView = isView;\n-        this.needCheckCommitedTxns = needCheckCommitedTxns;\n-    }\n-\n-    public boolean isSetIfExists() {\n-        return ifExists;\n-    }\n-\n-    public String getDbName() {\n-        return tableName.getDb();\n-    }\n-\n-    public String getTableName() {\n-        return tableName.getTbl();\n-    }\n-\n-    public boolean isView() {\n-        return isView;\n-    }\n-\n-    public boolean isNeedCheckCommitedTxns() {\n-        return this.needCheckCommitedTxns;\n-    }\n-\n-    @Override\n-    public void analyze(Analyzer analyzer) throws AnalysisException, UserException {\n-        if (Strings.isNullOrEmpty(tableName.getDb())) {\n-            tableName.setDb(analyzer.getDefaultDb());\n-        }\n-        tableName.analyze(analyzer);\n-\n-        // check access\n-        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), tableName.getDb(),\n-                                                                tableName.getTbl(), PrivPredicate.DROP)) {\n-            ErrorReport.reportAnalysisException(ErrorCode.ERR_SPECIFIC_ACCESS_DENIED_ERROR, \"DROP\");\n-        }\n-    }\n-\n-    @Override\n-    public String toSql() {\n-        StringBuilder stringBuilder = new StringBuilder();\n-        stringBuilder.append(\"DROP TABLE \").append(tableName.toSql());\n-        return stringBuilder.toString();\n-    }\n-\n-    @Override\n-    public String toString() {\n-        return toSql();\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTg5MQ==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450275891", "bodyText": "Duplicated if clause?", "author": "morningman", "createdAt": "2020-07-06T14:50:32Z", "path": "fe/src/main/java/org/apache/doris/catalog/Catalog.java", "diffHunk": "@@ -3308,6 +3315,20 @@ public void dropPartition(Database db, OlapTable olapTable, DropPartitionClause\n         if (isTempPartition) {\n             olapTable.dropTempPartition(partitionName, true);\n         } else {\n+            if (clause.isNeedCheckCommitedTxns()) {", "originalCommit": "032bbaf691e529f103ed8bd9acbc13b559087f83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMzODkzMQ==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r450338931", "bodyText": "my bad, I will fix it.", "author": "caiconghui", "createdAt": "2020-07-06T16:26:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDI3NTg5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/catalog/Catalog.java b/fe/src/main/java/org/apache/doris/catalog/Catalog.java\ndeleted file mode 100755\nindex 5370a6543..000000000\n--- a/fe/src/main/java/org/apache/doris/catalog/Catalog.java\n+++ /dev/null\n\n@@ -1,6784 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.catalog;\n-\n-import org.apache.doris.alter.Alter;\n-import org.apache.doris.alter.AlterJob;\n-import org.apache.doris.alter.AlterJob.JobType;\n-import org.apache.doris.alter.AlterJobV2;\n-import org.apache.doris.alter.DecommissionBackendJob.DecommissionType;\n-import org.apache.doris.alter.MaterializedViewHandler;\n-import org.apache.doris.alter.SchemaChangeHandler;\n-import org.apache.doris.alter.SystemHandler;\n-import org.apache.doris.analysis.AddPartitionClause;\n-import org.apache.doris.analysis.AddRollupClause;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt.CheckType;\n-import org.apache.doris.analysis.AdminSetConfigStmt;\n-import org.apache.doris.analysis.AdminSetReplicaStatusStmt;\n-import org.apache.doris.analysis.AlterClause;\n-import org.apache.doris.analysis.AlterClusterStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt.QuotaType;\n-import org.apache.doris.analysis.AlterDatabaseRename;\n-import org.apache.doris.analysis.AlterSystemStmt;\n-import org.apache.doris.analysis.AlterTableStmt;\n-import org.apache.doris.analysis.AlterViewStmt;\n-import org.apache.doris.analysis.BackupStmt;\n-import org.apache.doris.analysis.CancelAlterSystemStmt;\n-import org.apache.doris.analysis.CancelAlterTableStmt;\n-import org.apache.doris.analysis.CancelBackupStmt;\n-import org.apache.doris.analysis.ColumnRenameClause;\n-import org.apache.doris.analysis.CreateClusterStmt;\n-import org.apache.doris.analysis.CreateDbStmt;\n-import org.apache.doris.analysis.CreateFunctionStmt;\n-import org.apache.doris.analysis.CreateMaterializedViewStmt;\n-import org.apache.doris.analysis.CreateTableStmt;\n-import org.apache.doris.analysis.CreateUserStmt;\n-import org.apache.doris.analysis.CreateViewStmt;\n-import org.apache.doris.analysis.DecommissionBackendClause;\n-import org.apache.doris.analysis.DistributionDesc;\n-import org.apache.doris.analysis.DropClusterStmt;\n-import org.apache.doris.analysis.DropDbStmt;\n-import org.apache.doris.analysis.DropFunctionStmt;\n-import org.apache.doris.analysis.DropMaterializedViewStmt;\n-import org.apache.doris.analysis.DropPartitionClause;\n-import org.apache.doris.analysis.DropTableStmt;\n-import org.apache.doris.analysis.FunctionName;\n-import org.apache.doris.analysis.InstallPluginStmt;\n-import org.apache.doris.analysis.KeysDesc;\n-import org.apache.doris.analysis.LinkDbStmt;\n-import org.apache.doris.analysis.MigrateDbStmt;\n-import org.apache.doris.analysis.PartitionDesc;\n-import org.apache.doris.analysis.PartitionRenameClause;\n-import org.apache.doris.analysis.RangePartitionDesc;\n-import org.apache.doris.analysis.RecoverDbStmt;\n-import org.apache.doris.analysis.RecoverPartitionStmt;\n-import org.apache.doris.analysis.RecoverTableStmt;\n-import org.apache.doris.analysis.ReplacePartitionClause;\n-import org.apache.doris.analysis.RestoreStmt;\n-import org.apache.doris.analysis.RollupRenameClause;\n-import org.apache.doris.analysis.ShowAlterStmt.AlterType;\n-import org.apache.doris.analysis.SingleRangePartitionDesc;\n-import org.apache.doris.analysis.TableName;\n-import org.apache.doris.analysis.TableRef;\n-import org.apache.doris.analysis.TableRenameClause;\n-import org.apache.doris.analysis.TruncateTableStmt;\n-import org.apache.doris.analysis.UninstallPluginStmt;\n-import org.apache.doris.analysis.UserDesc;\n-import org.apache.doris.analysis.UserIdentity;\n-import org.apache.doris.backup.BackupHandler;\n-import org.apache.doris.catalog.ColocateTableIndex.GroupId;\n-import org.apache.doris.catalog.Database.DbState;\n-import org.apache.doris.catalog.DistributionInfo.DistributionInfoType;\n-import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n-import org.apache.doris.catalog.MaterializedIndex.IndexState;\n-import org.apache.doris.catalog.OlapTable.OlapTableState;\n-import org.apache.doris.catalog.Replica.ReplicaState;\n-import org.apache.doris.catalog.Replica.ReplicaStatus;\n-import org.apache.doris.catalog.Table.TableType;\n-import org.apache.doris.clone.ColocateTableBalancer;\n-import org.apache.doris.clone.DynamicPartitionScheduler;\n-import org.apache.doris.clone.TabletChecker;\n-import org.apache.doris.clone.TabletScheduler;\n-import org.apache.doris.clone.TabletSchedulerStat;\n-import org.apache.doris.cluster.BaseParam;\n-import org.apache.doris.cluster.Cluster;\n-import org.apache.doris.cluster.ClusterNamespace;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.Config;\n-import org.apache.doris.common.ConfigBase;\n-import org.apache.doris.common.DdlException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.FeConstants;\n-import org.apache.doris.common.FeMetaVersion;\n-import org.apache.doris.common.MarkedCountDownLatch;\n-import org.apache.doris.common.MetaNotFoundException;\n-import org.apache.doris.common.Pair;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.common.io.Text;\n-import org.apache.doris.common.util.Daemon;\n-import org.apache.doris.common.util.DynamicPartitionUtil;\n-import org.apache.doris.common.util.MasterDaemon;\n-import org.apache.doris.common.util.PrintableMap;\n-import org.apache.doris.common.util.PropertyAnalyzer;\n-import org.apache.doris.common.util.QueryableReentrantLock;\n-import org.apache.doris.common.util.SmallFileMgr;\n-import org.apache.doris.common.util.TimeUtils;\n-import org.apache.doris.common.util.Util;\n-import org.apache.doris.consistency.ConsistencyChecker;\n-import org.apache.doris.deploy.DeployManager;\n-import org.apache.doris.deploy.impl.AmbariDeployManager;\n-import org.apache.doris.deploy.impl.K8sDeployManager;\n-import org.apache.doris.deploy.impl.LocalFileDeployManager;\n-import org.apache.doris.external.elasticsearch.EsRepository;\n-import org.apache.doris.ha.BDBHA;\n-import org.apache.doris.ha.FrontendNodeType;\n-import org.apache.doris.ha.HAProtocol;\n-import org.apache.doris.ha.MasterInfo;\n-import org.apache.doris.http.meta.MetaBaseAction;\n-import org.apache.doris.journal.JournalCursor;\n-import org.apache.doris.journal.JournalEntity;\n-import org.apache.doris.journal.bdbje.Timestamp;\n-import org.apache.doris.load.DeleteHandler;\n-import org.apache.doris.load.DeleteInfo;\n-import org.apache.doris.load.ExportChecker;\n-import org.apache.doris.load.ExportJob;\n-import org.apache.doris.load.ExportMgr;\n-import org.apache.doris.load.Load;\n-import org.apache.doris.load.LoadChecker;\n-import org.apache.doris.load.LoadErrorHub;\n-import org.apache.doris.load.LoadJob;\n-import org.apache.doris.load.LoadJob.JobState;\n-import org.apache.doris.load.loadv2.LoadEtlChecker;\n-import org.apache.doris.load.loadv2.LoadJobScheduler;\n-import org.apache.doris.load.loadv2.LoadLoadingChecker;\n-import org.apache.doris.load.loadv2.LoadManager;\n-import org.apache.doris.load.loadv2.LoadTimeoutChecker;\n-import org.apache.doris.load.routineload.RoutineLoadManager;\n-import org.apache.doris.load.routineload.RoutineLoadScheduler;\n-import org.apache.doris.load.routineload.RoutineLoadTaskScheduler;\n-import org.apache.doris.master.Checkpoint;\n-import org.apache.doris.master.MetaHelper;\n-import org.apache.doris.meta.MetaContext;\n-import org.apache.doris.metric.MetricRepo;\n-import org.apache.doris.mysql.privilege.PaloAuth;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.persist.BackendIdsUpdateInfo;\n-import org.apache.doris.persist.BackendTabletsInfo;\n-import org.apache.doris.persist.ClusterInfo;\n-import org.apache.doris.persist.ColocatePersistInfo;\n-import org.apache.doris.persist.DatabaseInfo;\n-import org.apache.doris.persist.DropInfo;\n-import org.apache.doris.persist.DropLinkDbAndUpdateDbInfo;\n-import org.apache.doris.persist.DropPartitionInfo;\n-import org.apache.doris.persist.EditLog;\n-import org.apache.doris.persist.ModifyPartitionInfo;\n-import org.apache.doris.persist.ModifyTablePropertyOperationLog;\n-import org.apache.doris.persist.OperationType;\n-import org.apache.doris.persist.PartitionPersistInfo;\n-import org.apache.doris.persist.RecoverInfo;\n-import org.apache.doris.persist.ReplacePartitionOperationLog;\n-import org.apache.doris.persist.ReplicaPersistInfo;\n-import org.apache.doris.persist.SetReplicaStatusOperationLog;\n-import org.apache.doris.persist.Storage;\n-import org.apache.doris.persist.StorageInfo;\n-import org.apache.doris.persist.TableInfo;\n-import org.apache.doris.persist.TablePropertyInfo;\n-import org.apache.doris.persist.TruncateTableInfo;\n-import org.apache.doris.plugin.PluginInfo;\n-import org.apache.doris.plugin.PluginMgr;\n-import org.apache.doris.qe.AuditEventProcessor;\n-import org.apache.doris.qe.ConnectContext;\n-import org.apache.doris.qe.JournalObservable;\n-import org.apache.doris.qe.SessionVariable;\n-import org.apache.doris.qe.VariableMgr;\n-import org.apache.doris.service.FrontendOptions;\n-import org.apache.doris.system.Backend;\n-import org.apache.doris.system.Backend.BackendState;\n-import org.apache.doris.system.Frontend;\n-import org.apache.doris.system.HeartbeatMgr;\n-import org.apache.doris.system.SystemInfoService;\n-import org.apache.doris.task.AgentBatchTask;\n-import org.apache.doris.task.AgentTaskExecutor;\n-import org.apache.doris.task.AgentTaskQueue;\n-import org.apache.doris.task.CreateReplicaTask;\n-import org.apache.doris.task.MasterTaskExecutor;\n-import org.apache.doris.task.PullLoadJobMgr;\n-import org.apache.doris.thrift.TStorageFormat;\n-import org.apache.doris.thrift.TStorageMedium;\n-import org.apache.doris.thrift.TStorageType;\n-import org.apache.doris.thrift.TTabletType;\n-import org.apache.doris.thrift.TTaskType;\n-import org.apache.doris.transaction.GlobalTransactionMgr;\n-import org.apache.doris.transaction.PublishVersionDaemon;\n-\n-import com.google.common.base.Joiner;\n-import com.google.common.base.Joiner.MapJoiner;\n-import com.google.common.base.Preconditions;\n-import com.google.common.base.Strings;\n-import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import com.google.common.collect.Multimap;\n-import com.google.common.collect.Queues;\n-import com.google.common.collect.Range;\n-import com.google.common.collect.Sets;\n-import com.sleepycat.je.rep.InsufficientLogException;\n-import com.sleepycat.je.rep.NetworkRestore;\n-import com.sleepycat.je.rep.NetworkRestoreConfig;\n-\n-import org.apache.commons.collections.CollectionUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.codehaus.jackson.map.ObjectMapper;\n-\n-import java.io.BufferedInputStream;\n-import java.io.DataInputStream;\n-import java.io.DataOutputStream;\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.net.HttpURLConnection;\n-import java.net.URL;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-import java.util.Set;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicLong;\n-\n-\n-public class Catalog {\n-    private static final Logger LOG = LogManager.getLogger(Catalog.class);\n-    // 0 ~ 9999 used for qe\n-    public static final long NEXT_ID_INIT_VALUE = 10000;\n-    private static final int HTTP_TIMEOUT_SECOND = 5;\n-    private static final int STATE_CHANGE_CHECK_INTERVAL_MS = 100;\n-    private static final int REPLAY_INTERVAL_MS = 1;\n-    private static final String BDB_DIR = \"/bdb\";\n-    private static final String IMAGE_DIR = \"/image\";\n-\n-    private String metaDir;\n-    private String bdbDir;\n-    private String imageDir;\n-\n-    private MetaContext metaContext;\n-    private long epoch = 0;\n-\n-    // Lock to perform atomic modification on map like 'idToDb' and 'fullNameToDb'.\n-    // These maps are all thread safe, we only use lock to perform atomic operations.\n-    // Operations like Get or Put do not need lock.\n-    // We use fair ReentrantLock to avoid starvation. Do not use this lock in critical code pass\n-    // because fair lock has poor performance.\n-    // Using QueryableReentrantLock to print owner thread in debug mode.\n-    private QueryableReentrantLock lock;\n-\n-    private ConcurrentHashMap<Long, Database> idToDb;\n-    private ConcurrentHashMap<String, Database> fullNameToDb;\n-\n-    private ConcurrentHashMap<Long, Cluster> idToCluster;\n-    private ConcurrentHashMap<String, Cluster> nameToCluster;\n-\n-    private Load load;\n-    private LoadManager loadManager;\n-    private RoutineLoadManager routineLoadManager;\n-    private ExportMgr exportMgr;\n-    private Alter alter;\n-    private ConsistencyChecker consistencyChecker;\n-    private BackupHandler backupHandler;\n-    private PublishVersionDaemon publishVersionDaemon;\n-    private DeleteHandler deleteHandler;\n-\n-    private MasterDaemon labelCleaner; // To clean old LabelInfo, ExportJobInfos\n-    private MasterDaemon txnCleaner; // To clean aborted or timeout txns\n-    private Daemon replayer;\n-    private Daemon timePrinter;\n-    private Daemon listener;\n-    private EsRepository esRepository;  // it is a daemon, so add it here\n-\n-    private boolean isFirstTimeStartUp = false;\n-    private boolean isElectable;\n-    // set to true after finished replay all meta and ready to serve\n-    // set to false when catalog is not ready.\n-    private AtomicBoolean isReady = new AtomicBoolean(false);\n-    // set to true if FE can offer READ service.\n-    // canRead can be true even if isReady is false.\n-    // for example: OBSERVER transfer to UNKNOWN, then isReady will be set to false, but canRead can still be true\n-    private AtomicBoolean canRead = new AtomicBoolean(false);\n-    private BlockingQueue<FrontendNodeType> typeTransferQueue;\n-\n-    // false if default_cluster is not created.\n-    private boolean isDefaultClusterCreated = false;\n-\n-    // node name is used for bdbje NodeName.\n-    private String nodeName;\n-    private FrontendNodeType role;\n-    private FrontendNodeType feType;\n-    // replica and observer use this value to decide provide read service or not\n-    private long synchronizedTimeMs;\n-    private int masterRpcPort;\n-    private int masterHttpPort;\n-    private String masterIp;\n-\n-    private CatalogIdGenerator idGenerator = new CatalogIdGenerator(NEXT_ID_INIT_VALUE);\n-\n-    private EditLog editLog;\n-    private int clusterId;\n-    private String token;\n-    // For checkpoint and observer memory replayed marker\n-    private AtomicLong replayedJournalId;\n-\n-    private static Catalog CHECKPOINT = null;\n-    private static long checkpointThreadId = -1;\n-    private Checkpoint checkpointer;\n-    private List<Pair<String, Integer>> helperNodes = Lists.newArrayList();\n-    private Pair<String, Integer> selfNode = null;\n-\n-    // node name -> Frontend\n-    private ConcurrentHashMap<String, Frontend> frontends;\n-    // removed frontends' name. used for checking if name is duplicated in bdbje\n-    private ConcurrentLinkedQueue<String> removedFrontends;\n-\n-    private HAProtocol haProtocol = null;\n-\n-    private JournalObservable journalObservable;\n-\n-    private SystemInfoService systemInfo;\n-    private HeartbeatMgr heartbeatMgr;\n-    private TabletInvertedIndex tabletInvertedIndex;\n-    private ColocateTableIndex colocateTableIndex;\n-\n-    private CatalogRecycleBin recycleBin;\n-    private FunctionSet functionSet;\n-\n-    private MetaReplayState metaReplayState;\n-\n-    private PullLoadJobMgr pullLoadJobMgr;\n-    private BrokerMgr brokerMgr;\n-    private ResourceMgr resourceMgr;\n-\n-    private GlobalTransactionMgr globalTransactionMgr;\n-\n-    private DeployManager deployManager;\n-\n-    private TabletStatMgr tabletStatMgr;\n-\n-    private PaloAuth auth;\n-\n-    private DomainResolver domainResolver;\n-\n-    private TabletSchedulerStat stat;\n-\n-    private TabletScheduler tabletScheduler;\n-\n-    private TabletChecker tabletChecker;\n-\n-    private MasterTaskExecutor loadTaskScheduler;\n-\n-    private LoadJobScheduler loadJobScheduler;\n-\n-    private LoadTimeoutChecker loadTimeoutChecker;\n-    private LoadEtlChecker loadEtlChecker;\n-    private LoadLoadingChecker loadLoadingChecker;\n-\n-    private RoutineLoadScheduler routineLoadScheduler;\n-\n-    private RoutineLoadTaskScheduler routineLoadTaskScheduler;\n-\n-    private SmallFileMgr smallFileMgr;\n-\n-    private DynamicPartitionScheduler dynamicPartitionScheduler;\n-    \n-    private PluginMgr pluginMgr;\n-\n-    private AuditEventProcessor auditEventProcessor;\n-\n-    public List<Frontend> getFrontends(FrontendNodeType nodeType) {\n-        if (nodeType == null) {\n-            // get all\n-            return Lists.newArrayList(frontends.values());\n-        }\n-\n-        List<Frontend> result = Lists.newArrayList();\n-        for (Frontend frontend : frontends.values()) {\n-            if (frontend.getRole() == nodeType) {\n-                result.add(frontend);\n-            }\n-        }\n-\n-        return result;\n-    }\n-\n-    public List<String> getRemovedFrontendNames() {\n-        return Lists.newArrayList(removedFrontends);\n-    }\n-\n-    public JournalObservable getJournalObservable() {\n-        return journalObservable;\n-    }\n-\n-    private SystemInfoService getClusterInfo() {\n-        return this.systemInfo;\n-    }\n-\n-    private HeartbeatMgr getHeartbeatMgr() {\n-        return this.heartbeatMgr;\n-    }\n-\n-    public TabletInvertedIndex getTabletInvertedIndex() {\n-        return this.tabletInvertedIndex;\n-    }\n-\n-    // only for test\n-    public void setColocateTableIndex(ColocateTableIndex colocateTableIndex) {\n-        this.colocateTableIndex = colocateTableIndex;\n-    }\n-\n-    public ColocateTableIndex getColocateTableIndex() {\n-        return this.colocateTableIndex;\n-    }\n-\n-    private CatalogRecycleBin getRecycleBin() {\n-        return this.recycleBin;\n-    }\n-\n-    public MetaReplayState getMetaReplayState() {\n-        return metaReplayState;\n-    }\n-\n-    public DynamicPartitionScheduler getDynamicPartitionScheduler() {\n-        return this.dynamicPartitionScheduler;\n-    }\n-\n-    private static class SingletonHolder {\n-        private static final Catalog INSTANCE = new Catalog();\n-    }\n-\n-    private Catalog() {\n-        this.idToDb = new ConcurrentHashMap<>();\n-        this.fullNameToDb = new ConcurrentHashMap<>();\n-        this.load = new Load();\n-        this.routineLoadManager = new RoutineLoadManager();\n-        this.exportMgr = new ExportMgr();\n-        this.alter = new Alter();\n-        this.consistencyChecker = new ConsistencyChecker();\n-        this.lock = new QueryableReentrantLock(true);\n-        this.backupHandler = new BackupHandler(this);\n-        this.metaDir = Config.meta_dir;\n-        this.publishVersionDaemon = new PublishVersionDaemon();\n-        this.deleteHandler = new DeleteHandler();\n-\n-        this.replayedJournalId = new AtomicLong(0L);\n-        this.isElectable = false;\n-        this.synchronizedTimeMs = 0;\n-        this.feType = FrontendNodeType.INIT;\n-        this.typeTransferQueue = Queues.newLinkedBlockingDeque();\n-\n-        this.role = FrontendNodeType.UNKNOWN;\n-        this.frontends = new ConcurrentHashMap<>();\n-        this.removedFrontends = new ConcurrentLinkedQueue<>();\n-\n-        this.journalObservable = new JournalObservable();\n-        this.masterRpcPort = 0;\n-        this.masterHttpPort = 0;\n-        this.masterIp = \"\";\n-\n-        this.systemInfo = new SystemInfoService();\n-        this.heartbeatMgr = new HeartbeatMgr(systemInfo);\n-        this.tabletInvertedIndex = new TabletInvertedIndex();\n-        this.colocateTableIndex = new ColocateTableIndex();\n-        this.recycleBin = new CatalogRecycleBin();\n-        this.functionSet = new FunctionSet();\n-        this.functionSet.init();\n-\n-        this.metaReplayState = new MetaReplayState();\n-\n-        this.idToCluster = new ConcurrentHashMap<>();\n-        this.nameToCluster = new ConcurrentHashMap<>();\n-\n-        this.isDefaultClusterCreated = false;\n-\n-        this.pullLoadJobMgr = new PullLoadJobMgr();\n-        this.brokerMgr = new BrokerMgr();\n-        this.resourceMgr = new ResourceMgr();\n-\n-        this.globalTransactionMgr = new GlobalTransactionMgr(this);\n-        this.tabletStatMgr = new TabletStatMgr();\n-\n-        this.auth = new PaloAuth();\n-        this.domainResolver = new DomainResolver(auth);\n-\n-        this.esRepository = new EsRepository();\n-\n-        this.metaContext = new MetaContext();\n-        this.metaContext.setThreadLocalInfo();\n-        \n-        this.stat = new TabletSchedulerStat();\n-        this.tabletScheduler = new TabletScheduler(this, systemInfo, tabletInvertedIndex, stat);\n-        this.tabletChecker = new TabletChecker(this, systemInfo, tabletScheduler, stat);\n-\n-        this.loadTaskScheduler = new MasterTaskExecutor(Config.async_load_task_pool_size);\n-        this.loadJobScheduler = new LoadJobScheduler();\n-        this.loadManager = new LoadManager(loadJobScheduler);\n-        this.loadTimeoutChecker = new LoadTimeoutChecker(loadManager);\n-        this.loadEtlChecker = new LoadEtlChecker(loadManager);\n-        this.loadLoadingChecker = new LoadLoadingChecker(loadManager);\n-        this.routineLoadScheduler = new RoutineLoadScheduler(routineLoadManager);\n-        this.routineLoadTaskScheduler = new RoutineLoadTaskScheduler(routineLoadManager);\n-\n-        this.smallFileMgr = new SmallFileMgr();\n-\n-        this.dynamicPartitionScheduler = new DynamicPartitionScheduler(\"DynamicPartitionScheduler\",\n-                Config.dynamic_partition_check_interval_seconds * 1000L);\n-        \n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        this.pluginMgr = new PluginMgr();\n-        this.auditEventProcessor = new AuditEventProcessor(this.pluginMgr);\n-    }\n-\n-    public static void destroyCheckpoint() {\n-        if (CHECKPOINT != null) {\n-            CHECKPOINT = null;\n-        }\n-    }\n-\n-    public static Catalog getCurrentCatalog() {\n-        if (isCheckpointThread()) {\n-            // only checkpoint thread it self will goes here.\n-            // so no need to care about the thread safe.\n-            if (CHECKPOINT == null) {\n-                CHECKPOINT = new Catalog();\n-            }\n-            return CHECKPOINT;\n-        } else {\n-            return SingletonHolder.INSTANCE;\n-        }\n-    }\n-\n-    // NOTICE: in most case, we should use getCurrentCatalog() to get the right catalog.\n-    // but in some cases, we should get the serving catalog explicitly.\n-    public static Catalog getServingCatalog() {\n-        return SingletonHolder.INSTANCE;\n-    }\n-\n-    public PullLoadJobMgr getPullLoadJobMgr() {\n-        return pullLoadJobMgr;\n-    }\n-\n-    public BrokerMgr getBrokerMgr() {\n-        return brokerMgr;\n-    }\n-\n-    public ResourceMgr getResourceMgr() {\n-        return resourceMgr;\n-    }\n-\n-    public static GlobalTransactionMgr getCurrentGlobalTransactionMgr() {\n-        return getCurrentCatalog().globalTransactionMgr;\n-    }\n-\n-    public GlobalTransactionMgr getGlobalTransactionMgr() {\n-        return globalTransactionMgr;\n-    }\n-\n-    public PluginMgr getPluginMgr() {\n-        return pluginMgr;\n-    }\n-\n-    public PaloAuth getAuth() {\n-        return auth;\n-    }\n-\n-    public TabletScheduler getTabletScheduler() {\n-        return tabletScheduler;\n-    }\n-\n-    public TabletChecker getTabletChecker() {\n-        return tabletChecker;\n-    }\n-\n-    public ConcurrentHashMap<String, Database> getFullNameToDb() {\n-        return fullNameToDb;\n-    }\n-\n-    public AuditEventProcessor getAuditEventProcessor() {\n-        return auditEventProcessor;\n-    }\n-\n-    // use this to get correct ClusterInfoService instance\n-    public static SystemInfoService getCurrentSystemInfo() {\n-        return getCurrentCatalog().getClusterInfo();\n-    }\n-\n-    public static HeartbeatMgr getCurrentHeartbeatMgr() {\n-        return getCurrentCatalog().getHeartbeatMgr();\n-    }\n-\n-    // use this to get correct TabletInvertedIndex instance\n-    public static TabletInvertedIndex getCurrentInvertedIndex() {\n-        return getCurrentCatalog().getTabletInvertedIndex();\n-    }\n-\n-    // use this to get correct ColocateTableIndex instance\n-    public static ColocateTableIndex getCurrentColocateIndex() {\n-        return getCurrentCatalog().getColocateTableIndex();\n-    }\n-\n-    public static CatalogRecycleBin getCurrentRecycleBin() {\n-        return getCurrentCatalog().getRecycleBin();\n-    }\n-\n-    // use this to get correct Catalog's journal version\n-    public static int getCurrentCatalogJournalVersion() {\n-        return MetaContext.get().getMetaVersion();\n-    }\n-\n-    public static final boolean isCheckpointThread() {\n-        return Thread.currentThread().getId() == checkpointThreadId;\n-    }\n-\n-    public static PluginMgr getCurrentPluginMgr() {\n-        return getCurrentCatalog().getPluginMgr();\n-    }\n-\n-    public static AuditEventProcessor getCurrentAuditEventProcessor() {\n-        return getCurrentCatalog().getAuditEventProcessor();\n-    }\n-\n-    // Use tryLock to avoid potential dead lock\n-    private boolean tryLock(boolean mustLock) {\n-        while (true) {\n-            try {\n-                if (!lock.tryLock(Config.catalog_try_lock_timeout_ms, TimeUnit.MILLISECONDS)) {\n-                    if (LOG.isDebugEnabled()) {\n-                        // to see which thread held this lock for long time.\n-                        Thread owner = lock.getOwner();\n-                        if (owner != null) {\n-                            LOG.debug(\"catalog lock is held by: {}\", Util.dumpThread(owner, 10));\n-                        }\n-                    }\n-                    \n-                    if (mustLock) {\n-                        continue;\n-                    } else {\n-                        return false;\n-                    }\n-                }\n-                return true;\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception while getting catalog lock\", e);\n-                if (mustLock) {\n-                    continue;\n-                } else {\n-                    return lock.isHeldByCurrentThread();\n-                }\n-            }\n-        }\n-    }\n-\n-    private void unlock() {\n-        if (lock.isHeldByCurrentThread()) {\n-            this.lock.unlock();\n-        }\n-    }\n-\n-    public String getBdbDir() {\n-        return bdbDir;\n-    }\n-\n-    public String getImageDir() {\n-        return imageDir;\n-    }\n-\n-    public void initialize(String[] args) throws Exception {\n-        // set meta dir first.\n-        // we already set these variables in constructor. but Catalog is a singleton class.\n-        // so they may be set before Config is initialized.\n-        // set them here again to make sure these variables use values in fe.conf.\n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        // 0. get local node and helper node info\n-        getSelfHostPort();\n-        getHelperNodes(args);\n-\n-        // 1. check and create dirs and files\n-        File meta = new File(metaDir);\n-        if (!meta.exists()) {\n-            LOG.error(\"{} does not exist, will exit\", meta.getAbsolutePath());\n-            System.exit(-1);\n-        }\n-\n-        if (Config.edit_log_type.equalsIgnoreCase(\"bdb\")) {\n-            File bdbDir = new File(this.bdbDir);\n-            if (!bdbDir.exists()) {\n-                bdbDir.mkdirs();\n-            }\n-\n-            File imageDir = new File(this.imageDir);\n-            if (!imageDir.exists()) {\n-                imageDir.mkdirs();\n-            }\n-        } else {\n-            LOG.error(\"Invalid edit log type: {}\", Config.edit_log_type);\n-            System.exit(-1);\n-        }\n-\n-        // init plugin manager\n-        pluginMgr.init();\n-        auditEventProcessor.start();\n-\n-        // 2. get cluster id and role (Observer or Follower)\n-        getClusterIdAndRole();\n-\n-        // 3. Load image first and replay edits\n-        this.editLog = new EditLog(nodeName);\n-        loadImage(this.imageDir); // load image file\n-        editLog.open(); // open bdb env\n-        this.globalTransactionMgr.setEditLog(editLog);\n-        this.idGenerator.setEditLog(editLog);\n-\n-        // 4. create load and export job label cleaner thread\n-        createLabelCleaner();\n-\n-        // 5. create txn cleaner thread\n-        createTxnCleaner();\n-\n-        // 6. start state listener thread\n-        createStateListener();\n-        listener.start();\n-    }\n-\n-    // wait until FE is ready.\n-    public void waitForReady() throws InterruptedException {\n-        while (true) {\n-            if (isReady()) {\n-                LOG.info(\"catalog is ready. FE type: {}\", feType);\n-                break;\n-            }\n-\n-            Thread.sleep(2000);\n-            LOG.info(\"wait catalog to be ready. FE type: {}. is ready: {}\", feType, isReady.get());\n-        }\n-    }\n-    \n-    public boolean isReady() {\n-        return isReady.get();\n-    }\n-\n-    private void getClusterIdAndRole() throws IOException {\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-\n-        // if helper node is point to self, or there is ROLE and VERSION file in local.\n-        // get the node type from local\n-        if (isMyself() || (roleFile.exists() && versionFile.exists())) {\n-\n-            if (!isMyself()) {\n-                LOG.info(\"find ROLE and VERSION file in local, ignore helper nodes: {}\", helperNodes);\n-            }\n-\n-            // check file integrity, if has.\n-            if ((roleFile.exists() && !versionFile.exists())\n-                    || (!roleFile.exists() && versionFile.exists())) {\n-                LOG.error(\"role file and version file must both exist or both not exist. \"\n-                        + \"please specific one helper node to recover. will exit.\");\n-                System.exit(-1);\n-            }\n-\n-            // ATTN:\n-            // If the version file and role file does not exist and the helper node is itself,\n-            // this should be the very beginning startup of the cluster, so we create ROLE and VERSION file,\n-            // set isFirstTimeStartUp to true, and add itself to frontends list.\n-            // If ROLE and VERSION file is deleted for some reason, we may arbitrarily start this node as\n-            // FOLLOWER, which may cause UNDEFINED behavior.\n-            // Everything may be OK if the origin role is exactly FOLLOWER,\n-            // but if not, FE process will exit somehow.\n-            Storage storage = new Storage(this.imageDir);\n-            if (!roleFile.exists()) {\n-                // The very first time to start the first node of the cluster.\n-                // It should became a Master node (Master node's role is also FOLLOWER, which means electable)\n-\n-                // For compatibility. Because this is the very first time to start, so we arbitrarily choose\n-                // a new name for this node\n-                role = FrontendNodeType.FOLLOWER;\n-                nodeName = genFeNodeName(selfNode.first, selfNode.second, false /* new style */);\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                LOG.info(\"very first time to start this node. role: {}, node name: {}\", role.name(), nodeName);\n-            } else {\n-                role = storage.getRole();\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-\n-                nodeName = storage.getNodeName();\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // In normal case, if ROLE file exist, role and nodeName should both exist.\n-                    // But we will get a empty nodeName after upgrading.\n-                    // So for forward compatibility, we use the \"old-style\" way of naming: \"ip_port\",\n-                    // and update the ROLE file.\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true/* old style */);\n-                    storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                    LOG.info(\"forward compatibility. role: {}, node name: {}\", role.name(), nodeName);\n-                }\n-            }\n-\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            if (!versionFile.exists()) {\n-                clusterId = Config.cluster_id == -1 ? Storage.newClusterID() : Config.cluster_id;\n-                token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                        Storage.newToken() : Config.auth_token;\n-                storage = new Storage(clusterId, token, this.imageDir);\n-                storage.writeClusterIdAndToken();\n-\n-                isFirstTimeStartUp = true;\n-                Frontend self = new Frontend(role, nodeName, selfNode.first, selfNode.second);\n-                // We don't need to check if frontends already contains self.\n-                // frontends must be empty cause no image is loaded and no journal is replayed yet.\n-                // And this frontend will be persisted later after opening bdbje environment.\n-                frontends.put(nodeName, self);\n-            } else {\n-                clusterId = storage.getClusterID();\n-                if (storage.getToken() == null) {\n-                    token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                            Storage.newToken() : Config.auth_token;\n-                    LOG.info(\"new token={}\", token);\n-                    storage.setToken(token);\n-                    storage.writeClusterIdAndToken();\n-                } else {\n-                    token = storage.getToken();\n-                }\n-                isFirstTimeStartUp = false;\n-            }\n-        } else {\n-            // try to get role and node name from helper node,\n-            // this loop will not end until we get certain role type and name\n-            while (true) {\n-                if (!getFeNodeTypeAndNameFromHelpers()) {\n-                    LOG.warn(\"current node is not added to the group. please add it first. \"\n-                            + \"sleep 5 seconds and retry, current helper nodes: {}\", helperNodes);\n-                    try {\n-                        Thread.sleep(5000);\n-                        continue;\n-                    } catch (InterruptedException e) {\n-                        e.printStackTrace();\n-                        System.exit(-1);\n-                    }\n-                }\n-\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-                break;\n-            }\n-\n-            Preconditions.checkState(helperNodes.size() == 1);\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            Pair<String, Integer> rightHelperNode = helperNodes.get(0);\n-\n-            Storage storage = new Storage(this.imageDir);\n-            if (roleFile.exists() && (role != storage.getRole() || !nodeName.equals(storage.getNodeName()))\n-                    || !roleFile.exists()) {\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-            }\n-            if (!versionFile.exists()) {\n-                // If the version file doesn't exist, download it from helper node\n-                if (!getVersionFileFromHelper(rightHelperNode)) {\n-                    LOG.error(\"fail to download version file from \" + rightHelperNode.first + \" will exit.\");\n-                    System.exit(-1);\n-                }\n-\n-                // NOTE: cluster_id will be init when Storage object is constructed,\n-                //       so we new one.\n-                storage = new Storage(this.imageDir);\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                if (Strings.isNullOrEmpty(token)) {\n-                    token = Config.auth_token;\n-                }\n-            } else {\n-                // If the version file exist, read the cluster id and check the\n-                // id with helper node to make sure they are identical\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                try {\n-                    URL idURL = new URL(\"http://\" + rightHelperNode.first + \":\" + Config.http_port + \"/check\");\n-                    HttpURLConnection conn = null;\n-                    conn = (HttpURLConnection) idURL.openConnection();\n-                    conn.setConnectTimeout(2 * 1000);\n-                    conn.setReadTimeout(2 * 1000);\n-                    String clusterIdString = conn.getHeaderField(MetaBaseAction.CLUSTER_ID);\n-                    int remoteClusterId = Integer.parseInt(clusterIdString);\n-                    if (remoteClusterId != clusterId) {\n-                        LOG.error(\"cluster id is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                        System.exit(-1);\n-                    }\n-                    String remoteToken = conn.getHeaderField(MetaBaseAction.TOKEN);\n-                    if (token == null && remoteToken != null) {\n-                        LOG.info(\"get token from helper node. token={}.\", remoteToken);\n-                        token = remoteToken;\n-                        storage.writeClusterIdAndToken();\n-                        storage.reload();\n-                    }\n-                    if (Config.enable_token_check) {\n-                        Preconditions.checkNotNull(token);\n-                        Preconditions.checkNotNull(remoteToken);\n-                        if (!token.equals(remoteToken)) {\n-                            LOG.error(\"token is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                            System.exit(-1);\n-                        }\n-                    }\n-                } catch (Exception e) {\n-                    LOG.warn(\"fail to check cluster_id and token with helper node.\", e);\n-                    System.exit(-1);\n-                }\n-            }\n-\n-            getNewImage(rightHelperNode);\n-        }\n-\n-        if (Config.cluster_id != -1 && clusterId != Config.cluster_id) {\n-            LOG.error(\"cluster id is not equal with config item cluster_id. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (role.equals(FrontendNodeType.FOLLOWER)) {\n-            isElectable = true;\n-        } else {\n-            isElectable = false;\n-        }\n-\n-        Preconditions.checkState(helperNodes.size() == 1);\n-        LOG.info(\"finished to get cluster id: {}, role: {} and node name: {}\",\n-                clusterId, role.name(), nodeName);\n-    }\n-\n-    public static String genFeNodeName(String host, int port, boolean isOldStyle) {\n-        String name = host + \"_\" + port;\n-        if (isOldStyle) {\n-            return name;\n-        } else {\n-            return name + \"_\" + System.currentTimeMillis();\n-        }\n-    }\n-\n-    // Get the role info and node name from helper node.\n-    // return false if failed.\n-    private boolean getFeNodeTypeAndNameFromHelpers() {\n-        // we try to get info from helper nodes, once we get the right helper node,\n-        // other helper nodes will be ignored and removed.\n-        Pair<String, Integer> rightHelperNode = null;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            try {\n-                URL url = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/role?host=\" + selfNode.first + \"&port=\" + selfNode.second);\n-                HttpURLConnection conn = null;\n-                conn = (HttpURLConnection) url.openConnection();\n-                if (conn.getResponseCode() != 200) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}. response code: {}\",\n-                            helperNode, conn.getResponseCode());\n-                    continue;\n-                }\n-\n-                String type = conn.getHeaderField(\"role\");\n-                if (type == null) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode);\n-                    continue;\n-                }\n-                role = FrontendNodeType.valueOf(type);\n-                nodeName = conn.getHeaderField(\"name\");\n-\n-                // get role and node name before checking them, because we want to throw any exception\n-                // as early as we encounter.\n-\n-                if (role == FrontendNodeType.UNKNOWN) {\n-                    LOG.warn(\"frontend {} is not added to cluster yet. role UNKNOWN\", selfNode);\n-                    return false;\n-                }\n-\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // For forward compatibility, we use old-style name: \"ip_port\"\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true /* old style */);\n-                }\n-            } catch (Exception e) {\n-                LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode, e);\n-                continue;\n-            }\n-\n-            LOG.info(\"get fe node type {}, name {} from {}:{}\", role, nodeName, helperNode.first, Config.http_port);\n-            rightHelperNode = helperNode;\n-            break;\n-        }\n-\n-        if (rightHelperNode == null) {\n-            return false;\n-        }\n-\n-        helperNodes.clear();\n-        helperNodes.add(rightHelperNode);\n-        return true;\n-    }\n-\n-    private void getSelfHostPort() {\n-        selfNode = new Pair<String, Integer>(FrontendOptions.getLocalHostAddress(), Config.edit_log_port);\n-        LOG.debug(\"get self node: {}\", selfNode);\n-    }\n-\n-    private void getHelperNodes(String[] args) throws AnalysisException {\n-        String helpers = null;\n-        for (int i = 0; i < args.length; i++) {\n-            if (args[i].equalsIgnoreCase(\"-helper\")) {\n-                if (i + 1 >= args.length) {\n-                    System.out.println(\"-helper need parameter host:port,host:port\");\n-                    System.exit(-1);\n-                }\n-                helpers = args[i + 1];\n-                break;\n-            }\n-        }\n-\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            if (Config.enable_deploy_manager.equalsIgnoreCase(\"k8s\")) {\n-                deployManager = new K8sDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"ambari\")) {\n-                deployManager = new AmbariDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"local\")) {\n-                deployManager = new LocalFileDeployManager(this, 5000 /* 5s interval */);\n-            } else {\n-                System.err.println(\"Unknow deploy manager: \" + Config.enable_deploy_manager);\n-                System.exit(-1);\n-            }\n-\n-            getHelperNodeFromDeployManager();\n-\n-        } else {\n-            if (helpers != null) {\n-                String[] splittedHelpers = helpers.split(\",\");\n-                for (String helper : splittedHelpers) {\n-                    Pair<String, Integer> helperHostPort = SystemInfoService.validateHostAndPort(helper);\n-                    if (helperHostPort.equals(selfNode)) {\n-                        /**\n-                         * If user specified the helper node to this FE itself,\n-                         * we will stop the starting FE process and report an error.\n-                         * First, it is meaningless to point the helper to itself.\n-                         * Secondly, when some users add FE for the first time, they will mistakenly\n-                         * point the helper that should have pointed to the Master to themselves.\n-                         * In this case, some errors have caused users to be troubled.\n-                         * So here directly exit the program and inform the user to avoid unnecessary trouble.\n-                         */\n-                        throw new AnalysisException(\n-                                \"Do not specify the helper node to FE itself. \"\n-                                        + \"Please specify it to the existing running Master or Follower FE\");\n-                    }\n-                    helperNodes.add(helperHostPort);\n-                }\n-            } else {\n-                // If helper node is not designated, use local node as helper node.\n-                helperNodes.add(Pair.create(selfNode.first, Config.edit_log_port));\n-            }\n-        }\n-\n-        LOG.info(\"get helper nodes: {}\", helperNodes);\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private void getHelperNodeFromDeployManager() {\n-        Preconditions.checkNotNull(deployManager);\n-\n-        // 1. check if this is the first time to start up\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-        if ((roleFile.exists() && !versionFile.exists())\n-                || (!roleFile.exists() && versionFile.exists())) {\n-            LOG.error(\"role file and version file must both exist or both not exist. \"\n-                    + \"please specific one helper node to recover. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (roleFile.exists()) {\n-            // This is not the first time this node start up.\n-            // It should already added to FE group, just set helper node as it self.\n-            LOG.info(\"role file exist. this is not the first time to start up\");\n-            helperNodes = Lists.newArrayList(Pair.create(selfNode.first, Config.edit_log_port));\n-            return;\n-        }\n-\n-        // This is the first time this node start up.\n-        // Get helper node from deploy manager.\n-        helperNodes = deployManager.getHelperNodes();\n-        if (helperNodes == null || helperNodes.isEmpty()) {\n-            LOG.error(\"failed to get helper node from deploy manager. exit\");\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private void transferToMaster() {\n-        // stop replayer\n-        if (replayer != null) {\n-            replayer.exit();\n-            try {\n-                replayer.join();\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception when stopping the replayer thread\", e);\n-            }\n-            replayer = null;\n-        }\n-\n-        // set this after replay thread stopped. to avoid replay thread modify them.\n-        isReady.set(false);\n-        canRead.set(false);\n-\n-        editLog.open();\n-\n-        if (!haProtocol.fencing()) {\n-            LOG.error(\"fencing failed. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        long replayStartTime = System.currentTimeMillis();\n-        // replay journals. -1 means replay all the journals larger than current journal id.\n-        replayJournal(-1);\n-        long replayEndTime = System.currentTimeMillis();\n-        LOG.info(\"finish replay in \" + (replayEndTime - replayStartTime) + \" msec\");\n-\n-        checkCurrentNodeExist();\n-\n-        editLog.rollEditLog();\n-\n-        // Log meta_version\n-        long journalVersion = MetaContext.get().getMetaVersion();\n-        if (journalVersion < FeConstants.meta_version) {\n-            editLog.logMetaVersion(FeConstants.meta_version);\n-            MetaContext.get().setMetaVersion(FeConstants.meta_version);\n-        }\n-\n-        // Log the first frontend\n-        if (isFirstTimeStartUp) {\n-            // if isFirstTimeStartUp is true, frontends must contains this Node.\n-            Frontend self = frontends.get(nodeName);\n-            Preconditions.checkNotNull(self);\n-            // OP_ADD_FIRST_FRONTEND is emitted, so it can write to BDBJE even if canWrite is false\n-            editLog.logAddFirstFrontend(self);\n-        }\n-\n-        if (!isDefaultClusterCreated) {\n-            initDefaultCluster();\n-        }\n-\n-        // MUST set master ip before starting checkpoint thread.\n-        // because checkpoint thread need this info to select non-master FE to push image\n-        this.masterIp = FrontendOptions.getLocalHostAddress();\n-        this.masterRpcPort = Config.rpc_port;\n-        this.masterHttpPort = Config.http_port;\n-        MasterInfo info = new MasterInfo(this.masterIp, this.masterHttpPort, this.masterRpcPort);\n-        editLog.logMasterInfo(info);\n-\n-        // for master, the 'isReady' is set behind.\n-        // but we are sure that all metadata is replayed if we get here.\n-        // so no need to check 'isReady' flag in this method\n-        fixBugAfterMetadataReplayed(false);\n-\n-        // start all daemon threads that only running on MASTER FE\n-        startMasterOnlyDaemonThreads();\n-        // start other daemon threads that should running on all FE\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-\n-        canRead.set(true);\n-        isReady.set(true);\n-\n-        String msg = \"master finished to replay journal, can write now.\";\n-        Util.stdoutWithTime(msg);\n-        LOG.info(msg);\n-    }\n-\n-    /*\n-     * Add anything necessary here if there is meta data need to be fixed.\n-     */\n-    public void fixBugAfterMetadataReplayed(boolean waitCatalogReady) {\n-        if (waitCatalogReady) {\n-            while (!isReady()) {\n-                try {\n-                    Thread.sleep(10 * 1000);\n-                } catch (InterruptedException e) {\n-                    e.printStackTrace();\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"start to fix meta data bug\");\n-        loadManager.fixLoadJobMetaBugs(globalTransactionMgr);\n-    }\n-\n-    // start all daemon threads only running on Master\n-    private void startMasterOnlyDaemonThreads() {\n-        // start checkpoint thread\n-        checkpointer = new Checkpoint(editLog);\n-        checkpointer.setMetaContext(metaContext);\n-        // set \"checkpointThreadId\" before the checkpoint thread start, because the thread\n-        // need to check the \"checkpointThreadId\" when running.\n-        checkpointThreadId = checkpointer.getId();\n-\n-        checkpointer.start();\n-        LOG.info(\"checkpointer thread started. thread id is {}\", checkpointThreadId);\n-\n-        // heartbeat mgr\n-        heartbeatMgr.setMaster(clusterId, token, epoch);\n-        heartbeatMgr.start();\n-        // Load checker\n-        LoadChecker.init(Config.load_checker_interval_second * 1000L);\n-        LoadChecker.startAll();\n-        // New load scheduler\n-        loadManager.prepareJobs();\n-        loadJobScheduler.start();\n-        loadTimeoutChecker.start();\n-        loadEtlChecker.start();\n-        loadLoadingChecker.start();\n-        // Export checker\n-        ExportChecker.init(Config.export_checker_interval_second * 1000L);\n-        ExportChecker.startAll();\n-        // Tablet checker and scheduler\n-        tabletChecker.start();\n-        tabletScheduler.start();\n-        // Colocate tables balancer\n-        if (!Config.disable_colocate_join) {\n-            ColocateTableBalancer.getInstance().start();\n-        }\n-        // Publish Version Daemon\n-        publishVersionDaemon.start();\n-        // Start txn cleaner\n-        txnCleaner.start();\n-        // Alter\n-        getAlterInstance().start();\n-        // Consistency checker\n-        getConsistencyChecker().start();\n-        // Backup handler\n-        getBackupHandler().start();\n-        // catalog recycle bin\n-        getRecycleBin().start();\n-        // time printer\n-        createTimePrinter();\n-        timePrinter.start();\n-        // deploy manager\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            LOG.info(\"deploy manager {} start\", deployManager.getName());\n-            deployManager.start();\n-        }\n-        // start routine load scheduler\n-        routineLoadScheduler.start();\n-        routineLoadTaskScheduler.start();\n-        // start dynamic partition task\n-        dynamicPartitionScheduler.start();\n-    }\n-\n-    // start threads that should running on all FE\n-    private void startNonMasterDaemonThreads() {\n-        tabletStatMgr.start();\n-        // load and export job label cleaner thread\n-        labelCleaner.start();\n-        // ES state store\n-        esRepository.start();\n-        // domain resolver\n-        domainResolver.start();\n-    }\n-\n-    private void transferToNonMaster(FrontendNodeType newType) {\n-        isReady.set(false);\n-\n-        if (feType == FrontendNodeType.OBSERVER || feType == FrontendNodeType.FOLLOWER) {\n-            Preconditions.checkState(newType == FrontendNodeType.UNKNOWN);\n-            LOG.warn(\"{} to UNKNOWN, still offer read service\", feType.name());\n-            // not set canRead here, leave canRead as what is was.\n-            // if meta out of date, canRead will be set to false in replayer thread.\n-            metaReplayState.setTransferToUnknown();\n-            return;\n-        }\n-\n-        // transfer from INIT/UNKNOWN to OBSERVER/FOLLOWER\n-\n-        // add helper sockets\n-        if (Config.edit_log_type.equalsIgnoreCase(\"BDB\")) {\n-            for (Frontend fe : frontends.values()) {\n-                if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                    ((BDBHA) getHaProtocol()).addHelperSocket(fe.getHost(), fe.getEditLogPort());\n-                }\n-            }\n-        }\n-\n-        if (replayer == null) {\n-            createReplayer();\n-            replayer.start();\n-        }\n-\n-        // 'isReady' will be set to true in 'setCanRead()' method\n-        fixBugAfterMetadataReplayed(true);\n-\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-    }\n-\n-    /*\n-     * If the current node is not in the frontend list, then exit. This may\n-     * happen when this node is removed from frontend list, and the drop\n-     * frontend log is deleted because of checkpoint.\n-     */\n-    private void checkCurrentNodeExist() {\n-        if (Config.metadata_failure_recovery.equals(\"true\")) {\n-            return;\n-        }\n-\n-        Frontend fe = checkFeExist(selfNode.first, selfNode.second);\n-        if (fe == null) {\n-            LOG.error(\"current node is not added to the cluster, will exit\");\n-            System.exit(-1);\n-        } else if (fe.getRole() != role) {\n-            LOG.error(\"current node role is {} not match with frontend recorded role {}. will exit\", role,\n-                    fe.getRole());\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private boolean getVersionFileFromHelper(Pair<String, Integer> helperNode) throws IOException {\n-        try {\n-            String url = \"http://\" + helperNode.first + \":\" + Config.http_port + \"/version\";\n-            File dir = new File(this.imageDir);\n-            MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000,\n-                    MetaHelper.getOutputStream(Storage.VERSION_FILE, dir));\n-            MetaHelper.complete(Storage.VERSION_FILE, dir);\n-            return true;\n-        } catch (Exception e) {\n-            LOG.warn(e);\n-        }\n-\n-        return false;\n-    }\n-\n-    private void getNewImage(Pair<String, Integer> helperNode) throws IOException {\n-        long localImageVersion = 0;\n-        Storage storage = new Storage(this.imageDir);\n-        localImageVersion = storage.getImageSeq();\n-\n-        try {\n-            URL infoUrl = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port + \"/info\");\n-            StorageInfo info = getStorageInfo(infoUrl);\n-            long version = info.getImageSeq();\n-            if (version > localImageVersion) {\n-                String url = \"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/image?version=\" + version;\n-                String filename = Storage.IMAGE + \".\" + version;\n-                File dir = new File(this.imageDir);\n-                MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000, MetaHelper.getOutputStream(filename, dir));\n-                MetaHelper.complete(filename, dir);\n-            }\n-        } catch (Exception e) {\n-            return;\n-        }\n-    }\n-\n-    private boolean isMyself() {\n-        Preconditions.checkNotNull(selfNode);\n-        Preconditions.checkNotNull(helperNodes);\n-        LOG.debug(\"self: {}. helpers: {}\", selfNode, helperNodes);\n-        // if helper nodes contain it self, remove other helpers\n-        boolean containSelf = false;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            if (selfNode.equals(helperNode)) {\n-                containSelf = true;\n-            }\n-        }\n-        if (containSelf) {\n-            helperNodes.clear();\n-            helperNodes.add(selfNode);\n-        }\n-\n-        return containSelf;\n-    }\n-\n-    private StorageInfo getStorageInfo(URL url) throws IOException {\n-        ObjectMapper mapper = new ObjectMapper();\n-\n-        HttpURLConnection connection = null;\n-        try {\n-            connection = (HttpURLConnection) url.openConnection();\n-            connection.setConnectTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            connection.setReadTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            return mapper.readValue(connection.getInputStream(), StorageInfo.class);\n-        } finally {\n-            if (connection != null) {\n-                connection.disconnect();\n-            }\n-        }\n-    }\n-\n-    public boolean hasReplayer() {\n-        return replayer != null;\n-    }\n-\n-    public void loadImage(String imageDir) throws IOException, DdlException {\n-        Storage storage = new Storage(imageDir);\n-        clusterId = storage.getClusterID();\n-        File curFile = storage.getCurrentImageFile();\n-        if (!curFile.exists()) {\n-            // image.0 may not exist\n-            LOG.info(\"image does not exist: {}\", curFile.getAbsolutePath());\n-            return;\n-        }\n-        replayedJournalId.set(storage.getImageSeq());\n-        LOG.info(\"start load image from {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-        long loadImageStartTime = System.currentTimeMillis();\n-        DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(curFile)));\n-\n-        long checksum = 0;\n-        try {\n-            checksum = loadHeader(dis, checksum);\n-            checksum = loadMasterInfo(dis, checksum);\n-            checksum = loadFrontends(dis, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().loadBackends(dis, checksum);\n-            checksum = loadDb(dis, checksum);\n-            // ATTN: this should be done after load Db, and before loadAlterJob\n-            recreateTabletInvertIndex();\n-            // rebuild es state state\n-            esRepository.loadTableFromCatalog();\n-\n-            checksum = loadLoadJob(dis, checksum);\n-            checksum = loadAlterJob(dis, checksum);\n-            checksum = loadRecycleBin(dis, checksum);\n-            checksum = loadGlobalVariable(dis, checksum);\n-            checksum = loadCluster(dis, checksum);\n-            checksum = loadBrokers(dis, checksum);\n-            checksum = loadResources(dis, checksum);\n-            checksum = loadExportJob(dis, checksum);\n-            checksum = loadBackupHandler(dis, checksum);\n-            checksum = loadPaloAuth(dis, checksum);\n-            // global transaction must be replayed before load jobs v2\n-            checksum = loadTransactionState(dis, checksum);\n-            checksum = loadColocateTableIndex(dis, checksum);\n-            checksum = loadRoutineLoadJobs(dis, checksum);\n-            checksum = loadLoadJobsV2(dis, checksum);\n-            checksum = loadSmallFiles(dis, checksum);\n-            checksum = loadPlugins(dis, checksum);\n-            checksum = loadDeleteHandler(dis, checksum);\n-\n-            long remoteChecksum = dis.readLong();\n-            Preconditions.checkState(remoteChecksum == checksum, remoteChecksum + \" vs. \" + checksum);\n-        } finally {\n-            dis.close();\n-        }\n-\n-        long loadImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished to load image in \" + (loadImageEndTime - loadImageStartTime) + \" ms\");\n-    }\n-\n-    private void recreateTabletInvertIndex() {\n-        if (isCheckpointThread()) {\n-            return;\n-        }\n-\n-        // create inverted index\n-        TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-        for (Database db : this.fullNameToDb.values()) {\n-            long dbId = db.getId();\n-            for (Table table : db.getTables()) {\n-                if (table.getType() != TableType.OLAP) {\n-                    continue;\n-                }\n-\n-                OlapTable olapTable = (OlapTable) table;\n-                long tableId = olapTable.getId();\n-                Collection<Partition> allPartitions = olapTable.getAllPartitions();\n-                for (Partition partition : allPartitions) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = index.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : index.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                                if (MetaContext.get().getMetaVersion() < FeMetaVersion.VERSION_48) {\n-                                    // set replica's schema hash\n-                                    replica.setSchemaHash(schemaHash);\n-                                }\n-                            }\n-                        }\n-                    } // end for indices\n-                } // end for partitions\n-            } // end for tables\n-        } // end for dbs\n-    }\n-\n-    public long loadHeader(DataInputStream dis, long checksum) throws IOException {\n-        int journalVersion = dis.readInt();\n-        long newChecksum = checksum ^ journalVersion;\n-        MetaContext.get().setMetaVersion(journalVersion);\n-\n-        long replayedJournalId = dis.readLong();\n-        newChecksum ^= replayedJournalId;\n-\n-        long catalogId = dis.readLong();\n-        newChecksum ^= catalogId;\n-        idGenerator.setId(catalogId);\n-\n-        if (journalVersion >= FeMetaVersion.VERSION_32) {\n-            isDefaultClusterCreated = dis.readBoolean();\n-        }\n-\n-        LOG.info(\"finished replay header from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadMasterInfo(DataInputStream dis, long checksum) throws IOException {\n-        masterIp = Text.readString(dis);\n-        masterRpcPort = dis.readInt();\n-        long newChecksum = checksum ^ masterRpcPort;\n-        masterHttpPort = dis.readInt();\n-        newChecksum ^= masterHttpPort;\n-\n-        LOG.info(\"finished replay masterInfo from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadFrontends(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; i++) {\n-                Frontend fe = Frontend.read(dis);\n-                replayAddFrontend(fe);\n-            }\n-            \n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_41) {\n-                    Frontend fe = Frontend.read(dis);\n-                    removedFrontends.add(fe.getNodeName());\n-                } else {\n-                    removedFrontends.add(Text.readString(dis));\n-                }\n-            }\n-            return newChecksum;\n-        }\n-        LOG.info(\"finished replay frontends from image\");\n-        return checksum;\n-    }\n-\n-    public long loadDb(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        int dbCount = dis.readInt();\n-        long newChecksum = checksum ^ dbCount;\n-        for (long i = 0; i < dbCount; ++i) {\n-            Database db = new Database();\n-            db.readFields(dis);\n-            newChecksum ^= db.getId();\n-            idToDb.put(db.getId(), db);\n-            fullNameToDb.put(db.getFullName(), db);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.put(db.getAttachDb(), db);\n-            }\n-            globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-        }\n-        LOG.info(\"finished replay databases from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadLoadJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        // load jobs\n-        int jobSize = dis.readInt();\n-        long newChecksum = checksum ^ jobSize;\n-        for (int i = 0; i < jobSize; i++) {\n-            long dbId = dis.readLong();\n-            newChecksum ^= dbId;\n-\n-            int loadJobCount = dis.readInt();\n-            newChecksum ^= loadJobCount;\n-            for (int j = 0; j < loadJobCount; j++) {\n-                LoadJob job = new LoadJob();\n-                job.readFields(dis);\n-                long currentTimeMs = System.currentTimeMillis();\n-\n-                // Delete the history load jobs that are older than\n-                // LABEL_KEEP_MAX_MS\n-                // This job must be FINISHED or CANCELLED\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                        || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                    load.unprotectAddLoadJob(job, true /* replay */);\n-                }\n-            }\n-        }\n-\n-        // delete jobs\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_11) {\n-            jobSize = dis.readInt();\n-            newChecksum ^= jobSize;\n-            for (int i = 0; i < jobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteCount = dis.readInt();\n-                newChecksum ^= deleteCount;\n-                for (int j = 0; j < deleteCount; j++) {\n-                    DeleteInfo deleteInfo = new DeleteInfo();\n-                    deleteInfo.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history delete jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    if ((currentTimeMs - deleteInfo.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second) {\n-                        load.unprotectAddDeleteInfo(deleteInfo);\n-                    }\n-                }\n-            }\n-        }\n-\n-        // load error hub info\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_24) {\n-            LoadErrorHub.Param param = new LoadErrorHub.Param();\n-            param.readFields(dis);\n-            load.setLoadErrorHubInfo(param);\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            // 4. load delete jobs\n-            int deleteJobSize = dis.readInt();\n-            newChecksum ^= deleteJobSize;\n-            for (int i = 0; i < deleteJobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteJobCount = dis.readInt();\n-                newChecksum ^= deleteJobCount;\n-                for (int j = 0; j < deleteJobCount; j++) {\n-                    LoadJob job = new LoadJob();\n-                    job.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history load jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    // This job must be FINISHED or CANCELLED\n-                    if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                            || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                        load.unprotectAddLoadJob(job, true /* replay */);\n-                    }\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"finished replay loadJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadExportJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        long newChecksum = checksum;\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_32) {\n-            int size = dis.readInt();\n-            newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; ++i) {\n-                long jobId = dis.readLong();\n-                newChecksum ^= jobId;\n-                ExportJob job = new ExportJob();\n-                job.readFields(dis);\n-                exportMgr.unprotectAddJob(job);\n-            }\n-        }\n-        LOG.info(\"finished replay exportJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum) throws IOException {\n-        long newChecksum = checksum;\n-        for (JobType type : JobType.values()) {\n-            if (type == JobType.DECOMMISSION_BACKEND) {\n-                if (Catalog.getCurrentCatalogJournalVersion() >= 5) {\n-                    newChecksum = loadAlterJob(dis, newChecksum, type);\n-                }\n-            } else {\n-                newChecksum = loadAlterJob(dis, newChecksum, type);\n-            }\n-        }\n-        LOG.info(\"finished replay alterJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = dis.readInt();\n-        long newChecksum = checksum ^ size;\n-        for (int i = 0; i < size; i++) {\n-            long tableId = dis.readLong();\n-            newChecksum ^= tableId;\n-            AlterJob job = AlterJob.read(dis);\n-            alterJobs.put(tableId, job);\n-\n-            // init job\n-            Database db = getDb(job.getDbId());\n-            // should check job state here because the job is finished but not removed from alter jobs list\n-            if (db != null && (job.getState() == org.apache.doris.alter.AlterJob.JobState.PENDING\n-                    || job.getState() == org.apache.doris.alter.AlterJob.JobState.RUNNING)) {\n-                job.replayInitJob(db);\n-            }\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= 2) {\n-            // finished or cancelled jobs\n-            long currentTimeMs = System.currentTimeMillis();\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                long tableId = dis.readLong();\n-                newChecksum ^= tableId;\n-                AlterJob job = AlterJob.read(dis);\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.history_job_keep_max_second) {\n-                    // delete history jobs\n-                    finishedOrCancelledAlterJobs.add(job);\n-                }\n-            }\n-        }\n-\n-        // alter job v2\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_61) {\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                AlterJobV2 alterJobV2 = AlterJobV2.read(dis);\n-                if (type == JobType.ROLLUP || type == JobType.SCHEMA_CHANGE) {\n-                    if (type == JobType.ROLLUP) {\n-                        this.getRollupHandler().addAlterJobV2(alterJobV2);\n-                    } else {\n-                        alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                    }\n-                    // ATTN : we just want to add tablet into TabletInvertedIndex when only PendingJob is checkpointed\n-                    // to prevent TabletInvertedIndex data loss,\n-                    // So just use AlterJob.replay() instead of AlterHandler.replay().\n-                    if (alterJobV2.getJobState() == AlterJobV2.JobState.PENDING) {\n-                        alterJobV2.replay(alterJobV2);\n-                        LOG.info(\"replay pending alter job when load alter job {} \", alterJobV2.getJobId());\n-                    }\n-                } else {\n-                    alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                }\n-            }\n-        }\n-\n-        return newChecksum;\n-    }\n-\n-    public long loadBackupHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_42) {\n-            getBackupHandler().readFields(dis);\n-        }\n-        getBackupHandler().setCatalog(this);\n-        LOG.info(\"finished replay backupHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveBackupHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getBackupHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadDeleteHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_82) {\n-            this.deleteHandler = DeleteHandler.read(dis);\n-        }\n-        LOG.info(\"finished replay deleteHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveDeleteHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getDeleteHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPaloAuth(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_43) {\n-            // CAN NOT use PaloAuth.read(), cause this auth instance is already passed to DomainResolver\n-            auth.readFields(dis);\n-        }\n-        LOG.info(\"finished replay paloAuth from image\");\n-        return checksum;\n-    }\n-\n-    public long loadTransactionState(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            globalTransactionMgr.readFields(dis);\n-            LOG.info(\"finished replay transactionState from image\");\n-            return newChecksum;\n-        }\n-        return checksum;\n-    }\n-\n-    public long loadRecycleBin(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_10) {\n-            recycleBin.readFields(dis);\n-            if (!isCheckpointThread()) {\n-                // add tablet in Recycle bin to TabletInvertedIndex\n-                recycleBin.addTabletToInvertedIndex();\n-            }\n-            // create DatabaseTransactionMgr for db in recycle bin.\n-            // these dbs do not exist in `idToDb` of the catalog.\n-            for (Long dbId : recycleBin.getAllDbIds()) {\n-                globalTransactionMgr.addDatabaseTransactionMgr(dbId);\n-            }\n-        }\n-        LOG.info(\"finished replay recycleBin from image\");\n-        return checksum;\n-    }\n-\n-    public long loadColocateTableIndex(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_46) {\n-            Catalog.getCurrentColocateIndex().readFields(dis);\n-        }\n-        LOG.info(\"finished replay colocateTableIndex from image\");\n-        return checksum;\n-    }\n-\n-    public long loadRoutineLoadJobs(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_49) {\n-            Catalog.getCurrentCatalog().getRoutineLoadManager().readFields(dis);\n-        }\n-        LOG.info(\"finished replay routineLoadJobs from image\");\n-        return checksum;\n-    }\n-\n-    public long loadLoadJobsV2(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_50) {\n-            loadManager.readFields(in);\n-        }\n-        LOG.info(\"finished replay loadJobsV2 from image\");\n-        return checksum;\n-    }\n-\n-    public long loadResources(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_87) {\n-            resourceMgr = ResourceMgr.read(in);\n-        }\n-        LOG.info(\"finished replay resources from image\");\n-        return checksum;\n-    }\n-\n-    public long loadSmallFiles(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_52) {\n-            smallFileMgr.readFields(in);\n-        }\n-        LOG.info(\"finished replay smallFiles from image\");\n-        return checksum;\n-    }\n-\n-    // Only called by checkpoint thread\n-    public void saveImage() throws IOException {\n-        // Write image.ckpt\n-        Storage storage = new Storage(this.imageDir);\n-        File curFile = storage.getImageFile(replayedJournalId.get());\n-        File ckpt = new File(this.imageDir, Storage.IMAGE_NEW);\n-        saveImage(ckpt, replayedJournalId.get());\n-\n-        // Move image.ckpt to image.dataVersion\n-        LOG.info(\"Move \" + ckpt.getAbsolutePath() + \" to \" + curFile.getAbsolutePath());\n-        if (!ckpt.renameTo(curFile)) {\n-            curFile.delete();\n-            throw new IOException();\n-        }\n-    }\n-\n-    public void saveImage(File curFile, long replayedJournalId) throws IOException {\n-        if (!curFile.exists()) {\n-            curFile.createNewFile();\n-        }\n-\n-        // save image does not need any lock. because only checkpoint thread will call this method.\n-        LOG.info(\"start save image to {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-\n-        long checksum = 0;\n-        long saveImageStartTime = System.currentTimeMillis();\n-        try (DataOutputStream dos = new DataOutputStream(new FileOutputStream(curFile))) {\n-            checksum = saveHeader(dos, replayedJournalId, checksum);\n-            checksum = saveMasterInfo(dos, checksum);\n-            checksum = saveFrontends(dos, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().saveBackends(dos, checksum);\n-            checksum = saveDb(dos, checksum);\n-            checksum = saveLoadJob(dos, checksum);\n-            checksum = saveAlterJob(dos, checksum);\n-            checksum = saveRecycleBin(dos, checksum);\n-            checksum = saveGlobalVariable(dos, checksum);\n-            checksum = saveCluster(dos, checksum);\n-            checksum = saveBrokers(dos, checksum);\n-            checksum = saveResources(dos, checksum);\n-            checksum = saveExportJob(dos, checksum);\n-            checksum = saveBackupHandler(dos, checksum);\n-            checksum = savePaloAuth(dos, checksum);\n-            checksum = saveTransactionState(dos, checksum);\n-            checksum = saveColocateTableIndex(dos, checksum);\n-            checksum = saveRoutineLoadJobs(dos, checksum);\n-            checksum = saveLoadJobsV2(dos, checksum);\n-            checksum = saveSmallFiles(dos, checksum);\n-            checksum = savePlugins(dos, checksum);\n-            checksum = saveDeleteHandler(dos, checksum);\n-            dos.writeLong(checksum);\n-        }\n-\n-        long saveImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished save image {} in {} ms. checksum is {}\",\n-                curFile.getAbsolutePath(), (saveImageEndTime - saveImageStartTime), checksum);\n-    }\n-\n-    public long saveHeader(DataOutputStream dos, long replayedJournalId, long checksum) throws IOException {\n-        // Write meta version\n-        checksum ^= FeConstants.meta_version;\n-        dos.writeInt(FeConstants.meta_version);\n-\n-        // Write replayed journal id\n-        checksum ^= replayedJournalId;\n-        dos.writeLong(replayedJournalId);\n-\n-        // Write id\n-        long id = idGenerator.getBatchEndId();\n-        checksum ^= id;\n-        dos.writeLong(id);\n-\n-        dos.writeBoolean(isDefaultClusterCreated);\n-\n-        return checksum;\n-    }\n-\n-    public long saveMasterInfo(DataOutputStream dos, long checksum) throws IOException {\n-        Text.writeString(dos, masterIp);\n-\n-        checksum ^= masterRpcPort;\n-        dos.writeInt(masterRpcPort);\n-\n-        checksum ^= masterHttpPort;\n-        dos.writeInt(masterHttpPort);\n-\n-        return checksum;\n-    }\n-\n-    public long saveFrontends(DataOutputStream dos, long checksum) throws IOException {\n-        int size = frontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (Frontend fe : frontends.values()) {\n-            fe.write(dos);\n-        }\n-\n-        size = removedFrontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (String feName : removedFrontends) {\n-            Text.writeString(dos, feName);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveDb(DataOutputStream dos, long checksum) throws IOException {\n-        int dbCount = idToDb.size() - nameToCluster.keySet().size();\n-        checksum ^= dbCount;\n-        dos.writeInt(dbCount);\n-        for (Map.Entry<Long, Database> entry : idToDb.entrySet()) {\n-            Database db = entry.getValue();\n-            String dbName = db.getFullName();\n-            // Don't write information_schema db meta\n-            if (!InfoSchemaDb.isInfoSchemaDb(dbName)) {\n-                checksum ^= entry.getKey();\n-                db.readLock();\n-                try {\n-                    db.write(dos);\n-                } finally {\n-                    db.readUnlock();\n-                }\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveLoadJob(DataOutputStream dos, long checksum) throws IOException {\n-        // 1. save load.dbToLoadJob\n-        Map<Long, List<LoadJob>> dbToLoadJob = load.getDbToLoadJobs();\n-        int jobSize = dbToLoadJob.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToLoadJob.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> loadJobs = entry.getValue();\n-            int loadJobCount = loadJobs.size();\n-            checksum ^= loadJobCount;\n-            dos.writeInt(loadJobCount);\n-            for (LoadJob job : loadJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        // 2. save delete jobs\n-        Map<Long, List<DeleteInfo>> dbToDeleteInfos = load.getDbToDeleteInfos();\n-        jobSize = dbToDeleteInfos.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<DeleteInfo>> entry : dbToDeleteInfos.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<DeleteInfo> deleteInfos = entry.getValue();\n-            int deletInfoCount = deleteInfos.size();\n-            checksum ^= deletInfoCount;\n-            dos.writeInt(deletInfoCount);\n-            for (DeleteInfo deleteInfo : deleteInfos) {\n-                deleteInfo.write(dos);\n-            }\n-        }\n-\n-        // 3. load error hub info\n-        LoadErrorHub.Param param = load.getLoadErrorHubInfo();\n-        param.write(dos);\n-\n-        // 4. save delete load job info\n-        Map<Long, List<LoadJob>> dbToDeleteJobs = load.getDbToDeleteJobs();\n-        int deleteJobSize = dbToDeleteJobs.size();\n-        checksum ^= deleteJobSize;\n-        dos.writeInt(deleteJobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToDeleteJobs.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> deleteJobs = entry.getValue();\n-            int deleteJobCount = deleteJobs.size();\n-            checksum ^= deleteJobCount;\n-            dos.writeInt(deleteJobCount);\n-            for (LoadJob job : deleteJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveExportJob(DataOutputStream dos, long checksum) throws IOException {\n-        Map<Long, ExportJob> idToJob = exportMgr.getIdToJob();\n-        int size = idToJob.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (ExportJob job : idToJob.values()) {\n-            long jobId = job.getId();\n-            checksum ^= jobId;\n-            dos.writeLong(jobId);\n-            job.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum) throws IOException {\n-        for (JobType type : JobType.values()) {\n-            checksum = saveAlterJob(dos, checksum, type);\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getRollupHandler().getAlterJobsV2();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = alterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (Entry<Long, AlterJob> entry : alterJobs.entrySet()) {\n-            long tableId = entry.getKey();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            entry.getValue().write(dos);\n-        }\n-\n-        // finished or cancelled jobs\n-        size = finishedOrCancelledAlterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJob alterJob : finishedOrCancelledAlterJobs) {\n-            long tableId = alterJob.getTableId();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            alterJob.write(dos);\n-        }\n-\n-        // alter job v2\n-        size = alterJobsV2.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJobV2 alterJobV2 : alterJobsV2.values()) {\n-            alterJobV2.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long savePaloAuth(DataOutputStream dos, long checksum) throws IOException {\n-        auth.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveTransactionState(DataOutputStream dos, long checksum) throws IOException {\n-        int size = globalTransactionMgr.getTransactionNum();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        globalTransactionMgr.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRecycleBin(DataOutputStream dos, long checksum) throws IOException {\n-        CatalogRecycleBin recycleBin = Catalog.getCurrentRecycleBin();\n-        recycleBin.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveColocateTableIndex(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentColocateIndex().write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRoutineLoadJobs(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getRoutineLoadManager().write(dos);\n-        return checksum;\n-    }\n-\n-    // global variable persistence\n-    public long loadGlobalVariable(DataInputStream in, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            VariableMgr.read(in);\n-        }\n-        LOG.info(\"finished replay globalVariable from image\");\n-        return checksum;\n-    }\n-\n-    public long saveGlobalVariable(DataOutputStream out, long checksum) throws IOException {\n-        VariableMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void replayGlobalVariable(SessionVariable variable) throws IOException, DdlException {\n-        VariableMgr.replayGlobalVariable(variable);\n-    }\n-\n-    public long saveLoadJobsV2(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getLoadManager().write(out);\n-        return checksum;\n-    }\n-\n-\tpublic long saveResources(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getResourceMgr().write(out);\n-        return checksum;\n-    }\n-\n-    private long saveSmallFiles(DataOutputStream out, long checksum) throws IOException {\n-        smallFileMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void createLabelCleaner() {\n-        labelCleaner = new MasterDaemon(\"LoadLabelCleaner\", Config.label_clean_interval_second * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                load.removeOldLoadJobs();\n-                load.removeOldDeleteJobs();\n-                loadManager.removeOldLoadJob();\n-                exportMgr.removeOldExportJobs();\n-            }\n-        };\n-    }\n-\n-    public void createTxnCleaner() {\n-        txnCleaner = new MasterDaemon(\"txnCleaner\", Config.transaction_clean_interval_second) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                globalTransactionMgr.removeExpiredAndTimeoutTxns();\n-            }\n-        };\n-    }\n-\n-    public void createReplayer() {\n-        replayer = new Daemon(\"replayer\", REPLAY_INTERVAL_MS) {\n-            protected void runOneCycle() {\n-                boolean err = false;\n-                boolean hasLog = false;\n-                try {\n-                    hasLog = replayJournal(-1);\n-                    metaReplayState.setOk();\n-                } catch (InsufficientLogException insufficientLogEx) {\n-                    // Copy the missing log files from a member of the\n-                    // replication group who owns the files\n-                    LOG.error(\"catch insufficient log exception. please restart.\", insufficientLogEx);\n-                    NetworkRestore restore = new NetworkRestore();\n-                    NetworkRestoreConfig config = new NetworkRestoreConfig();\n-                    config.setRetainLogFiles(false);\n-                    restore.execute(insufficientLogEx, config);\n-                    System.exit(-1);\n-                } catch (Throwable e) {\n-                    LOG.error(\"replayer thread catch an exception when replay journal.\", e);\n-                    metaReplayState.setException(e);\n-                    try {\n-                        Thread.sleep(5000);\n-                    } catch (InterruptedException e1) {\n-                        LOG.error(\"sleep got exception. \", e);\n-                    }\n-                    err = true;\n-                }\n-\n-                setCanRead(hasLog, err);\n-            }\n-        };\n-        replayer.setMetaContext(metaContext);\n-    }\n-\n-    private void setCanRead(boolean hasLog, boolean err) {\n-        if (err) {\n-            canRead.set(false);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        if (Config.ignore_meta_check) {\n-            // can still offer read, but is not ready\n-            canRead.set(true);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        long currentTimeMs = System.currentTimeMillis();\n-        if (currentTimeMs - synchronizedTimeMs > Config.meta_delay_toleration_second * 1000) {\n-            // we still need this log to observe this situation\n-            // but service may be continued when there is no log being replayed.\n-            LOG.warn(\"meta out of date. current time: {}, synchronized time: {}, has log: {}, fe type: {}\",\n-                    currentTimeMs, synchronizedTimeMs, hasLog, feType);\n-            if (hasLog || feType == FrontendNodeType.UNKNOWN) {\n-                // 1. if we read log from BDB, which means master is still alive.\n-                // So we need to set meta out of date.\n-                // 2. if we didn't read any log from BDB and feType is UNKNOWN,\n-                // which means this non-master node is disconnected with master.\n-                // So we need to set meta out of date either.\n-                metaReplayState.setOutOfDate(currentTimeMs, synchronizedTimeMs);\n-                canRead.set(false);\n-                isReady.set(false);\n-            }\n-\n-            // sleep 5s to avoid numerous 'meta out of date' log\n-            try {\n-                Thread.sleep(5000L);\n-            } catch (InterruptedException e) {\n-                LOG.error(\"unhandled exception when sleep\", e);\n-            }\n-\n-        } else {\n-            canRead.set(true);\n-            isReady.set(true);\n-        }\n-    }\n-\n-    public void notifyNewFETypeTransfer(FrontendNodeType newType) {\n-        try {\n-            String msg = \"notify new FE type transfer: \" + newType;\n-            LOG.warn(msg);\n-            Util.stdoutWithTime(msg);\n-            this.typeTransferQueue.put(newType);\n-        } catch (InterruptedException e) {\n-            LOG.error(\"failed to put new FE type: {}\", newType, e);\n-        }\n-    }\n-\n-    public void createStateListener() {\n-        listener = new Daemon(\"stateListener\", STATE_CHANGE_CHECK_INTERVAL_MS) {\n-            @Override\n-            protected synchronized void runOneCycle() {\n-\n-                while (true) {\n-                    FrontendNodeType newType = null;\n-                    try {\n-                        newType = typeTransferQueue.take();\n-                    } catch (InterruptedException e) {\n-                        LOG.error(\"got exception when take FE type from queue\", e);\n-                        Util.stdoutWithTime(\"got exception when take FE type from queue. \" + e.getMessage());\n-                        System.exit(-1);\n-                    }\n-                    Preconditions.checkNotNull(newType);\n-                    LOG.info(\"begin to transfer FE type from {} to {}\", feType, newType);\n-                    if (feType == newType) {\n-                        return;\n-                    }\n-\n-                    /*\n-                     * INIT -> MASTER: transferToMaster\n-                     * INIT -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * UNKNOWN -> MASTER: transferToMaster\n-                     * UNKNOWN -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * FOLLOWER -> MASTER: transferToMaster\n-                     * FOLLOWER/OBSERVER -> INIT/UNKNOWN: set isReady to false\n-                     */\n-                    switch (feType) {\n-                        case INIT: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                case UNKNOWN:\n-                                    break;\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case UNKNOWN: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case FOLLOWER: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case OBSERVER: {\n-                            switch (newType) {\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case MASTER: {\n-                            // exit if master changed to any other type\n-                            String msg = \"transfer FE type from MASTER to \" + newType.name() + \". exit\";\n-                            LOG.error(msg);\n-                            Util.stdoutWithTime(msg);\n-                            System.exit(-1);\n-                        }\n-                        default:\n-                            break;\n-                    } // end switch formerFeType\n-\n-                    feType = newType;\n-                    LOG.info(\"finished to transfer FE type to {}\", feType);\n-                }\n-            } // end runOneCycle\n-        };\n-\n-        listener.setMetaContext(metaContext);\n-    }\n-\n-    public synchronized boolean replayJournal(long toJournalId) {\n-        long newToJournalId = toJournalId;\n-        if (newToJournalId == -1) {\n-            newToJournalId = getMaxJournalId();\n-        }\n-        if (newToJournalId <= replayedJournalId.get()) {\n-            return false;\n-        }\n-\n-        LOG.info(\"replayed journal id is {}, replay to journal id is {}\", replayedJournalId, newToJournalId);\n-        JournalCursor cursor = editLog.read(replayedJournalId.get() + 1, newToJournalId);\n-        if (cursor == null) {\n-            LOG.warn(\"failed to get cursor from {} to {}\", replayedJournalId.get() + 1, newToJournalId);\n-            return false;\n-        }\n-\n-        long startTime = System.currentTimeMillis();\n-        boolean hasLog = false;\n-        while (true) {\n-            JournalEntity entity = cursor.next();\n-            if (entity == null) {\n-                break;\n-            }\n-            hasLog = true;\n-            EditLog.loadJournal(this, entity);\n-            replayedJournalId.incrementAndGet();\n-            LOG.debug(\"journal {} replayed.\", replayedJournalId);\n-            if (feType != FrontendNodeType.MASTER) {\n-                journalObservable.notifyObservers(replayedJournalId.get());\n-            }\n-            if (MetricRepo.isInit.get()) {\n-                // Metric repo may not init after this replay thread start\n-                MetricRepo.COUNTER_EDIT_LOG_READ.increase(1L);\n-            }\n-        }\n-        long cost = System.currentTimeMillis() - startTime;\n-        if (cost >= 1000) {\n-            LOG.warn(\"replay journal cost too much time: {} replayedJournalId: {}\", cost, replayedJournalId);\n-        }\n-\n-        return hasLog;\n-    }\n-\n-    public void createTimePrinter() {\n-        // time printer will write timestamp edit log every 10 seconds\n-        timePrinter = new MasterDaemon(\"timePrinter\", 10 * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                Timestamp stamp = new Timestamp();\n-                editLog.logTimestamp(stamp);\n-            }\n-        };\n-    }\n-\n-    public void addFrontend(FrontendNodeType role, String host, int editLogPort) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, editLogPort);\n-            if (fe != null) {\n-                throw new DdlException(\"frontend already exists \" + fe);\n-            }\n-\n-            String nodeName = genFeNodeName(host, editLogPort, false /* new name style */);\n-\n-            if (removedFrontends.contains(nodeName)) {\n-                throw new DdlException(\"frontend name already exists \" + nodeName + \". Try again\");\n-            }\n-\n-            fe = new Frontend(role, nodeName, host, editLogPort);\n-            frontends.put(nodeName, fe);\n-            if (role == FrontendNodeType.FOLLOWER || role == FrontendNodeType.REPLICA) {\n-                ((BDBHA) getHaProtocol()).addHelperSocket(host, editLogPort);\n-                helperNodes.add(Pair.create(host, editLogPort));\n-            }\n-            editLog.logAddFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropFrontend(FrontendNodeType role, String host, int port) throws DdlException {\n-        if (host.equals(selfNode.first) && port == selfNode.second && feType == FrontendNodeType.MASTER) {\n-            throw new DdlException(\"can not drop current master node.\");\n-        }\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, port);\n-            if (fe == null) {\n-                throw new DdlException(\"frontend does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            if (fe.getRole() != role) {\n-                throw new DdlException(role.toString() + \" does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            frontends.remove(fe.getNodeName());\n-            removedFrontends.add(fe.getNodeName());\n-\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                haProtocol.removeElectableNode(fe.getNodeName());\n-                helperNodes.remove(Pair.create(host, port));\n-            }\n-            editLog.logRemoveFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Frontend checkFeExist(String host, int port) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host) && fe.getEditLogPort() == port) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByHost(String host) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByName(String name) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getNodeName().equals(name)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-\n-    // The interface which DdlExecutor needs.\n-    public void createDb(CreateDbStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        String fullDbName = stmt.getFullDbName();\n-        long id = 0L;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_SELECT_CLUSTER, clusterName);\n-            }\n-            if (fullNameToDb.containsKey(fullDbName)) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create database[{}] which already exists\", fullDbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, fullDbName);\n-                }\n-            } else {\n-                id = getNextId();\n-                Database db = new Database(id, fullDbName);\n-                db.setClusterName(clusterName);\n-                unprotectCreateDb(db);\n-                editLog.logCreateDb(db);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-        LOG.info(\"createDb dbName = \" + fullDbName + \", id = \" + id);\n-    }\n-\n-    // For replay edit log, need't lock metadata\n-    public void unprotectCreateDb(Database db) {\n-        idToDb.put(db.getId(), db);\n-        fullNameToDb.put(db.getFullName(), db);\n-        final Cluster cluster = nameToCluster.get(db.getClusterName());\n-        cluster.addDb(db.getFullName(), db.getId());\n-        globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-    }\n-\n-    // for test\n-    public void addCluster(Cluster cluster) {\n-        nameToCluster.put(cluster.getName(), cluster);\n-        idToCluster.put(cluster.getId(), cluster);\n-    }\n-\n-    public void replayCreateDb(Database db) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateDb(db);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropDb(DropDbStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-\n-        // 1. check if database exists\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!fullNameToDb.containsKey(dbName)) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop database[{}] which does not exist\", dbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_DROP_EXISTS, dbName);\n-                }\n-            }\n-\n-            // 2. drop tables in db\n-            Database db = this.fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                if (stmt.isNeedCheckCommitedTxns()) {\n-                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), null, null)) {\n-                        throw new DdlException(\"There are still some commited txns cannot be aborted in db [\"\n-                                + dbName + \"], please wait for GlobalTransactionMgr to finish publish tasks.\" +\n-                                \" If you don't need to recover db, use DROPP DATABASE stmt (double P).\");\n-                    }\n-                }\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getAttachDb())) {\n-                    // We try to drop a hard link.\n-                    final DropLinkDbAndUpdateDbInfo info = new DropLinkDbAndUpdateDbInfo();\n-                    fullNameToDb.remove(db.getAttachDb());\n-                    db.setDbState(DbState.NORMAL);\n-                    info.setUpdateDbState(DbState.NORMAL);\n-                    final Cluster cluster = nameToCluster\n-                            .get(ClusterNamespace.getClusterNameFromFullName(db.getAttachDb()));\n-                    final BaseParam param = new BaseParam();\n-                    param.addStringParam(db.getAttachDb());\n-                    param.addLongParam(db.getId());\n-                    cluster.removeLinkDb(param);\n-                    info.setDropDbCluster(cluster.getName());\n-                    info.setDropDbId(db.getId());\n-                    info.setDropDbName(db.getAttachDb());\n-                    editLog.logDropLinkDb(info);\n-                    return;\n-                }\n-\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getFullName())) {\n-                    // We try to drop a db which other dbs attach to it,\n-                    // which is not allowed.\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                if (dbName.equals(db.getAttachDb()) && db.getDbState() == DbState.MOVE) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                // save table names for recycling\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            // 3. remove db from catalog\n-            idToDb.remove(db.getId());\n-            fullNameToDb.remove(db.getFullName());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-            editLog.logDropDb(dbName);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"finish drop database[{}]\", dbName);\n-    }\n-\n-    public void unprotectDropDb(Database db) {\n-        for (Table table : db.getTables()) {\n-            unprotectDropTable(db, table.getId());\n-        }\n-    }\n-\n-    public void replayDropLinkDb(DropLinkDbAndUpdateDbInfo info) {\n-        tryLock(true);\n-        try {\n-            final Database db = this.fullNameToDb.remove(info.getDropDbName());\n-            db.setDbState(info.getUpdateDbState());\n-            final Cluster cluster = nameToCluster\n-                    .get(info.getDropDbCluster());\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(db.getAttachDb());\n-            param.addLongParam(db.getId());\n-            cluster.removeLinkDb(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropDb(String dbName) throws DdlException {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            fullNameToDb.remove(dbName);\n-            idToDb.remove(db.getId());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void recoverDatabase(RecoverDbStmt recoverStmt) throws DdlException {\n-        // check is new db with same name already exist\n-        if (getDb(recoverStmt.getDbName()) != null) {\n-            throw new DdlException(\"Database[\" + recoverStmt.getDbName() + \"] already exist.\");\n-        }\n-\n-        Database db = Catalog.getCurrentRecycleBin().recoverDatabase(recoverStmt.getDbName());\n-\n-        // add db to catalog\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (fullNameToDb.containsKey(db.getFullName())) {\n-                throw new DdlException(\"Database[\" + db.getFullName() + \"] already exist.\");\n-                // it's ok that we do not put db back to CatalogRecycleBin\n-                // cause this db cannot recover any more\n-            }\n-\n-            fullNameToDb.put(db.getFullName(), db);\n-            idToDb.put(db.getId(), db);\n-\n-            // log\n-            RecoverInfo recoverInfo = new RecoverInfo(db.getId(), -1L, -1L);\n-            editLog.logRecoverDb(recoverInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"recover database[{}]\", db.getId());\n-    }\n-\n-    public void recoverTable(RecoverTableStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table != null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-            }\n-\n-            if (!Catalog.getCurrentRecycleBin().recoverTable(db, tableName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void recoverPartition(RecoverPartitionStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"table[\" + tableName + \"] is not OLAP table\");\n-            }\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            String partitionName = recoverStmt.getPartitionName();\n-            if (olapTable.getPartition(partitionName) != null) {\n-                throw new DdlException(\"partition[\" + partitionName + \"] already exist in table[\" + tableName + \"]\");\n-            }\n-\n-            Catalog.getCurrentRecycleBin().recoverPartition(db.getId(), olapTable, partitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseDatabase(long dbId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseDatabase(dbId);\n-    }\n-\n-    public void replayRecoverDatabase(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = Catalog.getCurrentRecycleBin().replayRecoverDatabase(dbId);\n-\n-        // add db to catalog\n-        replayCreateDb(db);\n-\n-        LOG.info(\"replay recover db[{}]\", dbId);\n-    }\n-\n-    public void alterDatabaseQuota(AlterDatabaseQuotaStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        QuotaType quotaType = stmt.getQuotaType();\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(stmt.getQuota());\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(stmt.getQuota());\n-        }\n-        long quota = stmt.getQuota();\n-        DatabaseInfo dbInfo = new DatabaseInfo(dbName, \"\", quota, quotaType);\n-        editLog.logAlterDb(dbInfo);\n-    }\n-\n-    public void replayAlterDatabaseQuota(String dbName, long quota, QuotaType quotaType) {\n-        Database db = getDb(dbName);\n-        Preconditions.checkNotNull(db);\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(quota);\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(quota);\n-        }\n-    }\n-\n-    public void renameDatabase(AlterDatabaseRename stmt) throws DdlException {\n-        String fullDbName = stmt.getDbName();\n-        String newFullDbName = stmt.getNewDbName();\n-        String clusterName = stmt.getClusterName();\n-\n-        if (fullDbName.equals(newFullDbName)) {\n-            throw new DdlException(\"Same database name\");\n-        }\n-\n-        Database db = null;\n-        Cluster cluster = null;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            // check if db exists\n-            db = fullNameToDb.get(fullDbName);\n-            if (db == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, fullDbName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK || db.getDbState() == DbState.MOVE) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_RENAME_DB_ERR, fullDbName);\n-            }\n-            // check if name is already used\n-            if (fullNameToDb.get(newFullDbName) != null) {\n-                throw new DdlException(\"Database name[\" + newFullDbName + \"] is already used\");\n-            }\n-\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            cluster.addDb(newFullDbName, db.getId());\n-            // 1. rename db\n-            db.setNameWithLock(newFullDbName);\n-\n-            // 2. add to meta. check again\n-            fullNameToDb.remove(fullDbName);\n-            fullNameToDb.put(newFullDbName, db);\n-\n-            DatabaseInfo dbInfo = new DatabaseInfo(fullDbName, newFullDbName, -1L, QuotaType.NONE);\n-            editLog.logDatabaseRename(dbInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"rename database[{}] to [{}]\", fullDbName, newFullDbName);\n-    }\n-\n-    public void replayRenameDatabase(String dbName, String newDbName) {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            db.setName(newDbName);\n-            cluster.addDb(newDbName, db.getId());\n-            fullNameToDb.remove(dbName);\n-            fullNameToDb.put(newDbName, db);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"replay rename database {} to {}\", dbName, newDbName);\n-    }\n-\n-    public void createTable(CreateTableStmt stmt) throws DdlException {\n-        String engineName = stmt.getEngineName();\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check if db exists\n-        Database db = getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // only internal table should check quota and cluster capacity\n-        if (!stmt.isExternal()) {\n-            // check cluster capacity\n-            Catalog.getCurrentSystemInfo().checkClusterCapacity(stmt.getClusterName());\n-            // check db quota\n-            db.checkQuota();\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create table[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        if (engineName.equals(\"olap\")) {\n-            createOlapTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"mysql\")) {\n-            createMysqlTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"broker\")) {\n-            createBrokerTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"elasticsearch\") || engineName.equalsIgnoreCase(\"es\")) {\n-            createEsTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"hive\")) {\n-            createHiveTable(db, stmt);\n-            return;\n-        } else {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_UNKNOWN_STORAGE_ENGINE, engineName);\n-        }\n-        Preconditions.checkState(false);\n-        return;\n-    }\n-\n-    public void addPartition(Database db, String tableName, AddPartitionClause addPartitionClause) throws DdlException {\n-        SingleRangePartitionDesc singlePartitionDesc = addPartitionClause.getSingeRangePartitionDesc();\n-        DistributionDesc distributionDesc = addPartitionClause.getDistributionDesc();\n-        boolean isTempPartition = addPartitionClause.isTempPartition();\n-\n-        DistributionInfo distributionInfo = null;\n-        OlapTable olapTable = null;\n-\n-        Map<Long, MaterializedIndexMeta> indexIdToMeta;\n-        Set<String> bfColumns = null;\n-\n-        String partitionName = singlePartitionDesc.getPartitionName();\n-\n-        // check\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            // check state\n-            olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-            }\n-\n-            // check partition type\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (partitionInfo.getType() != PartitionType.RANGE) {\n-                throw new DdlException(\"Only support adding partition to range partitioned table\");\n-            }\n-\n-            // check partition name\n-            if (olapTable.checkPartitionNameExist(partitionName)) {\n-                if (singlePartitionDesc.isSetIfNotExists()) {\n-                    LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                }\n-            }\n-\n-            Map<String, String> properties = singlePartitionDesc.getProperties();\n-            // partition properties should inherit table properties\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM, replicationNum.toString());\n-            }\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_INMEMORY)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_INMEMORY, olapTable.isInMemory().toString());\n-            }\n-\n-            RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-            singlePartitionDesc.analyze(rangePartitionInfo.getPartitionColumns().size(), properties);\n-            rangePartitionInfo.checkAndCreateRange(singlePartitionDesc, isTempPartition);\n-\n-            // get distributionInfo\n-            List<Column> baseSchema = olapTable.getBaseSchema();\n-            DistributionInfo defaultDistributionInfo = olapTable.getDefaultDistributionInfo();\n-            if (distributionDesc != null) {\n-                distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-                // for now. we only support modify distribution's bucket num\n-                if (distributionInfo.getType() != defaultDistributionInfo.getType()) {\n-                    throw new DdlException(\"Cannot assign different distribution type. default is: \"\n-                            + defaultDistributionInfo.getType());\n-                }\n-\n-                if (distributionInfo.getType() == DistributionInfoType.HASH) {\n-                    HashDistributionInfo hashDistributionInfo = (HashDistributionInfo) distributionInfo;\n-                    List<Column> newDistriCols = hashDistributionInfo.getDistributionColumns();\n-                    List<Column> defaultDistriCols = ((HashDistributionInfo) defaultDistributionInfo)\n-                            .getDistributionColumns();\n-                    if (!newDistriCols.equals(defaultDistriCols)) {\n-                        throw new DdlException(\"Cannot assign hash distribution with different distribution cols. \"\n-                                + \"default is: \" + defaultDistriCols);\n-                    }\n-                    if (hashDistributionInfo.getBucketNum() <= 0) {\n-                        throw new DdlException(\"Cannot assign hash distribution buckets less than 1\");\n-                    }\n-                }\n-            } else {\n-                distributionInfo = defaultDistributionInfo;\n-            }\n-\n-            // check colocation\n-            if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-                String fullGroupName = db.getId() + \"_\" + olapTable.getColocateGroup();\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                Preconditions.checkNotNull(groupSchema);\n-                groupSchema.checkDistribution(distributionInfo);\n-                groupSchema.checkReplicationNum(singlePartitionDesc.getReplicationNum());\n-            }\n-\n-            indexIdToMeta = olapTable.getCopiedIndexIdToMeta();\n-            bfColumns = olapTable.getCopiedBfColumns();\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        Preconditions.checkNotNull(distributionInfo);\n-        Preconditions.checkNotNull(olapTable);\n-        Preconditions.checkNotNull(indexIdToMeta);\n-\n-        // create partition outside db lock\n-        DataProperty dataProperty = singlePartitionDesc.getPartitionDataProperty();\n-        Preconditions.checkNotNull(dataProperty);\n-\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-        try {\n-            long partitionId = getNextId();\n-            Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                    olapTable.getId(),\n-                    olapTable.getBaseIndexId(),\n-                    partitionId, partitionName,\n-                    indexIdToMeta,\n-                    olapTable.getKeysType(),\n-                    distributionInfo,\n-                    dataProperty.getStorageMedium(),\n-                    singlePartitionDesc.getReplicationNum(),\n-                    singlePartitionDesc.getVersionInfo(),\n-                    bfColumns, olapTable.getBfFpp(),\n-                    tabletIdSet, olapTable.getCopiedIndexes(),\n-                    singlePartitionDesc.isInMemory(),\n-                    olapTable.getStorageFormat(),\n-                    singlePartitionDesc.getTabletType()\n-                    );\n-\n-            // check again\n-            db.writeLock();\n-            try {\n-                Table table = db.getTable(tableName);\n-                if (table == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-\n-                if (table.getType() != TableType.OLAP) {\n-                    throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-                }\n-\n-                olapTable = (OlapTable) table;\n-                if (olapTable.getState() != OlapTableState.NORMAL) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-                }\n-\n-                // check partition name\n-                if (olapTable.checkPartitionNameExist(partitionName)) {\n-                    if (singlePartitionDesc.isSetIfNotExists()) {\n-                        LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                        return;\n-                    } else {\n-                        ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                    }\n-                }\n-\n-                // check if meta changed\n-                // rollup index may be added or dropped during add partition operation.\n-                // schema may be changed during add partition operation.\n-                boolean metaChanged = false;\n-                if (olapTable.getIndexNameToId().size() != indexIdToMeta.size()) {\n-                    metaChanged = true;\n-                } else {\n-                    // compare schemaHash\n-                    for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                        long indexId = entry.getKey();\n-                        if (!indexIdToMeta.containsKey(indexId)) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                        if (indexIdToMeta.get(indexId).getSchemaHash() != entry.getValue().getSchemaHash()) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                    }\n-                }\n-\n-                if (metaChanged) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s meta has been changed. try again.\");\n-                }\n-\n-                // check partition type\n-                PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                if (partitionInfo.getType() != PartitionType.RANGE) {\n-                    throw new DdlException(\"Only support adding partition to range partitioned table\");\n-                }\n-\n-                // update partition info\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                rangePartitionInfo.handleNewSinglePartitionDesc(singlePartitionDesc, partitionId, isTempPartition);\n-\n-                if (isTempPartition) {\n-                    olapTable.addTempPartition(partition);\n-                } else {\n-                    olapTable.addPartition(partition);\n-                }\n-\n-                // log\n-                PartitionPersistInfo info = new PartitionPersistInfo(db.getId(), olapTable.getId(), partition,\n-                        rangePartitionInfo.getRange(partitionId), dataProperty,\n-                        rangePartitionInfo.getReplicationNum(partitionId),\n-                        rangePartitionInfo.getIsInMemory(partitionId),\n-                        isTempPartition);\n-                editLog.logAddPartition(info);\n-\n-                LOG.info(\"succeed in creating partition[{}], temp: {}\", partitionId, isTempPartition);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-    }\n-\n-    public void replayAddPartition(PartitionPersistInfo info) throws DdlException {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            Partition partition = info.getPartition();\n-\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.isTempPartition()) {\n-                olapTable.addTempPartition(partition);\n-            } else {\n-                olapTable.addPartition(partition);\n-            }\n-\n-            ((RangePartitionInfo) partitionInfo).unprotectHandleNewSinglePartitionDesc(partition.getId(),\n-                    info.isTempPartition(), info.getRange(), info.getDataProperty(), info.getReplicationNum(),\n-                    info.isInMemory());\n-\n-            if (!isCheckpointThread()) {\n-                // add to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                    long indexId = index.getId();\n-                    int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                    TabletMeta tabletMeta = new TabletMeta(info.getDbId(), info.getTableId(), partition.getId(),\n-                            index.getId(), schemaHash, info.getDataProperty().getStorageMedium());\n-                    for (Tablet tablet : index.getTablets()) {\n-                        long tabletId = tablet.getId();\n-                        invertedIndex.addTablet(tabletId, tabletMeta);\n-                        for (Replica replica : tablet.getReplicas()) {\n-                            invertedIndex.addReplica(tabletId, replica);\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void dropPartition(Database db, OlapTable olapTable, DropPartitionClause clause) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-\n-        String partitionName = clause.getPartitionName();\n-        boolean isTempPartition = clause.isTempPartition();\n-\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        if (!olapTable.checkPartitionNameExist(partitionName, isTempPartition)) {\n-            if (clause.isSetIfExists()) {\n-                LOG.info(\"drop partition[{}] which does not exist\", partitionName);\n-                return;\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DROP_PARTITION_NON_EXISTENT, partitionName);\n-            }\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-        if (partitionInfo.getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Alter table [\" + olapTable.getName() + \"] failed. Not a partitioned table\");\n-        }\n-\n-        // drop\n-        if (isTempPartition) {\n-            olapTable.dropTempPartition(partitionName, true);\n-        } else {\n-            if (clause.isNeedCheckCommitedTxns()) {\n-                if (clause.isNeedCheckCommitedTxns()) {\n-                    Partition partition = olapTable.getPartition(partitionName);\n-                    if (partition != null) {\n-                        if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), olapTable.getId(), partition.getId())) {\n-                            throw new DdlException(\"There are still some commited txns cannot be aborted in table [\"\n-                                    + olapTable.getName() + \"] partition [\" + partitionName + \"], please wait for GlobalTransactionMgr to\" +\n-                                    \" finish publish tasks. If you don't need to recover partition, use DROPP PARTITION stmt (double P).\");\n-                        }\n-                    }\n-\n-                }\n-\n-            }\n-            olapTable.dropPartition(db.getId(), partitionName);\n-        }\n-\n-        // log\n-        DropPartitionInfo info = new DropPartitionInfo(db.getId(), olapTable.getId(), partitionName, isTempPartition);\n-        editLog.logDropPartition(info);\n-\n-        LOG.info(\"succeed in droping partition[{}]\", partitionName);\n-    }\n-\n-    public void replayDropPartition(DropPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            if (info.isTempPartition()) {\n-                olapTable.dropTempPartition(info.getPartitionName(), true);\n-            } else {\n-                olapTable.dropPartition(info.getDbId(), info.getPartitionName());\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayErasePartition(long partitionId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayErasePartition(partitionId);\n-    }\n-\n-    public void replayRecoverPartition(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(info.getTableId());\n-            Catalog.getCurrentRecycleBin().replayRecoverPartition((OlapTable) table, info.getPartitionId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void modifyPartitionProperty(Database db, OlapTable olapTable, String partitionName, Map<String, String> properties)\n-            throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        Partition partition = olapTable.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\n-                    \"Partition[\" + partitionName + \"] does not exist in table[\" + olapTable.getName() + \"]\");\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-        // 1. data property\n-        DataProperty oldDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        DataProperty newDataProperty = null;\n-        try {\n-            newDataProperty = PropertyAnalyzer.analyzeDataProperty(properties, oldDataProperty);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(newDataProperty);\n-\n-        if (newDataProperty.equals(oldDataProperty)) {\n-            newDataProperty = null;\n-        }\n-\n-        // 2. replication num\n-        short oldReplicationNum = partitionInfo.getReplicationNum(partition.getId());\n-        short newReplicationNum = (short) -1;\n-        try {\n-            newReplicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, oldReplicationNum);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (newReplicationNum == oldReplicationNum) {\n-            newReplicationNum = (short) -1;\n-        } else if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_HAS_SAME_REPLICATION_NUM, oldReplicationNum);\n-        }\n-\n-        // 3. in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties,\n-                PropertyAnalyzer.PROPERTIES_INMEMORY, partitionInfo.getIsInMemory(partition.getId()));\n-\n-        // 4. tablet type\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // check if has other undefined properties\n-        if (properties != null && !properties.isEmpty()) {\n-            MapJoiner mapJoiner = Joiner.on(\", \").withKeyValueSeparator(\" = \");\n-            throw new DdlException(\"Unknown properties: \" + mapJoiner.join(properties));\n-        }\n-\n-        // modify meta here\n-        // date property\n-        if (newDataProperty != null) {\n-            partitionInfo.setDataProperty(partition.getId(), newDataProperty);\n-            LOG.debug(\"modify partition[{}-{}-{}] data property to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newDataProperty.toString());\n-        }\n-\n-        // replication num\n-        if (newReplicationNum != (short) -1) {\n-            partitionInfo.setReplicationNum(partition.getId(), newReplicationNum);\n-            LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newReplicationNum);\n-        }\n-\n-        // in memory\n-        if (isInMemory != partitionInfo.getIsInMemory(partition.getId())) {\n-            partitionInfo.setIsInMemory(partition.getId(), isInMemory);\n-            LOG.debug(\"modify partition[{}-{}-{}] in memory to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    isInMemory);\n-        }\n-\n-        // tablet type\n-        // TODO: serialize to edit log\n-        if (tabletType != partitionInfo.getTabletType(partition.getId())) {\n-            partitionInfo.setTabletType(partition.getId(), tabletType);\n-            LOG.debug(\"modify partition[{}-{}-{}] tablet type to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    tabletType);\n-        }\n-\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), olapTable.getId(), partition.getId(),\n-                newDataProperty, newReplicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-    }\n-\n-    public void replayModifyPartition(ModifyPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.getDataProperty() != null) {\n-                partitionInfo.setDataProperty(info.getPartitionId(), info.getDataProperty());\n-            }\n-            if (info.getReplicationNum() != (short) -1) {\n-                partitionInfo.setReplicationNum(info.getPartitionId(), info.getReplicationNum());\n-            }\n-            partitionInfo.setIsInMemory(info.getPartitionId(), info.isInMemory());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private Partition createPartitionWithIndices(String clusterName, long dbId, long tableId,\n-                                                 long baseIndexId, long partitionId, String partitionName,\n-                                                 Map<Long, MaterializedIndexMeta> indexIdToMeta,\n-                                                 KeysType keysType,\n-                                                 DistributionInfo distributionInfo,\n-                                                 TStorageMedium storageMedium,\n-                                                 short replicationNum,\n-                                                 Pair<Long, Long> versionInfo,\n-                                                 Set<String> bfColumns,\n-                                                 double bfFpp,\n-                                                 Set<Long> tabletIdSet,\n-                                                 List<Index> indexes,\n-                                                 boolean isInMemory,\n-                                                 TStorageFormat storageFormat,\n-                                                 TTabletType tabletType) throws DdlException {\n-        // create base index first.\n-        Preconditions.checkArgument(baseIndexId != -1);\n-        MaterializedIndex baseIndex = new MaterializedIndex(baseIndexId, IndexState.NORMAL);\n-\n-        // create partition with base index\n-        Partition partition = new Partition(partitionId, partitionName, baseIndex, distributionInfo);\n-\n-        // add to index map\n-        Map<Long, MaterializedIndex> indexMap = new HashMap<Long, MaterializedIndex>();\n-        indexMap.put(baseIndexId, baseIndex);\n-\n-        // create rollup index if has\n-        for (long indexId : indexIdToMeta.keySet()) {\n-            if (indexId == baseIndexId) {\n-                continue;\n-            }\n-\n-            MaterializedIndex rollup = new MaterializedIndex(indexId, IndexState.NORMAL);\n-            indexMap.put(indexId, rollup);\n-        }\n-\n-        // version and version hash\n-        if (versionInfo != null) {\n-            partition.updateVisibleVersionAndVersionHash(versionInfo.first, versionInfo.second);\n-        }\n-        long version = partition.getVisibleVersion();\n-        long versionHash = partition.getVisibleVersionHash();\n-\n-        for (Map.Entry<Long, MaterializedIndex> entry : indexMap.entrySet()) {\n-            long indexId = entry.getKey();\n-            MaterializedIndex index = entry.getValue();\n-            MaterializedIndexMeta indexMeta = indexIdToMeta.get(indexId);\n-\n-            // create tablets\n-            int schemaHash = indexMeta.getSchemaHash();\n-            TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, storageMedium);\n-            createTablets(clusterName, index, ReplicaState.NORMAL, distributionInfo, version, versionHash,\n-                    replicationNum, tabletMeta, tabletIdSet);\n-\n-            boolean ok = false;\n-            String errMsg = null;\n-\n-            // add create replica task for olap\n-            short shortKeyColumnCount = indexMeta.getShortKeyColumnCount();\n-            TStorageType storageType = indexMeta.getStorageType();\n-            List<Column> schema = indexMeta.getSchema();\n-            int totalTaskNum = index.getTablets().size() * replicationNum;\n-            MarkedCountDownLatch<Long, Long> countDownLatch = new MarkedCountDownLatch<Long, Long>(totalTaskNum);\n-            AgentBatchTask batchTask = new AgentBatchTask();\n-            for (Tablet tablet : index.getTablets()) {\n-                long tabletId = tablet.getId();\n-                for (Replica replica : tablet.getReplicas()) {\n-                    long backendId = replica.getBackendId();\n-                    countDownLatch.addMark(backendId, tabletId);\n-                    CreateReplicaTask task = new CreateReplicaTask(backendId, dbId, tableId,\n-                            partitionId, indexId, tabletId,\n-                            shortKeyColumnCount, schemaHash,\n-                            version, versionHash,\n-                            keysType,\n-                            storageType, storageMedium,\n-                            schema, bfColumns, bfFpp,\n-                            countDownLatch,\n-                            indexes,\n-                            isInMemory,\n-                            tabletType);\n-                    task.setStorageFormat(storageFormat);\n-                    batchTask.addTask(task);\n-                    // add to AgentTaskQueue for handling finish report.\n-                    // not for resending task\n-                    AgentTaskQueue.addTask(task);\n-                }\n-            }\n-            AgentTaskExecutor.submit(batchTask);\n-\n-            // estimate timeout\n-            long timeout = Config.tablet_create_timeout_second * 1000L * totalTaskNum;\n-            timeout = Math.min(timeout, Config.max_create_table_timeout_second * 1000);\n-            try {\n-                ok = countDownLatch.await(timeout, TimeUnit.MILLISECONDS);\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"InterruptedException: \", e);\n-                ok = false;\n-            }\n-\n-            if (!ok || !countDownLatch.getStatus().ok()) {\n-                errMsg = \"Failed to create partition[\" + partitionName + \"]. Timeout.\";\n-                // clear tasks\n-                AgentTaskQueue.removeBatchTask(batchTask, TTaskType.CREATE);\n-\n-                if (!countDownLatch.getStatus().ok()) {\n-                    errMsg += \" Error: \" + countDownLatch.getStatus().getErrorMsg();\n-                } else {\n-                    List<Entry<Long, Long>> unfinishedMarks = countDownLatch.getLeftMarks();\n-                    // only show at most 3 results\n-                    List<Entry<Long, Long>> subList = unfinishedMarks.subList(0, Math.min(unfinishedMarks.size(), 3));\n-                    if (!subList.isEmpty()) {\n-                        errMsg += \" Unfinished mark: \" + Joiner.on(\", \").join(subList);\n-                    }\n-                }\n-                LOG.warn(errMsg);\n-                throw new DdlException(errMsg);\n-            }\n-\n-            if (index.getId() != baseIndexId) {\n-                // add rollup index to partition\n-                partition.createRollupIndex(index);\n-            }\n-        } // end for indexMap\n-        return partition;\n-    }\n-\n-    // Create olap table and related base index synchronously.\n-    private void createOlapTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        LOG.debug(\"begin create olap table: {}\", tableName);\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            // gen partition id first\n-            if (partitionDesc instanceof RangePartitionDesc) {\n-                RangePartitionDesc rangeDesc = (RangePartitionDesc) partitionDesc;\n-                for (SingleRangePartitionDesc desc : rangeDesc.getSingleRangePartitionDescs()) {\n-                    long partitionId = getNextId();\n-                    partitionNameToId.put(desc.getPartitionName(), partitionId);\n-                }\n-            }\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            if (DynamicPartitionUtil.checkDynamicPartitionPropertiesExist(stmt.getProperties())) {\n-                throw new DdlException(\"Only support dynamic partition properties on range partition table\");\n-            }\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        // get keys type\n-        KeysDesc keysDesc = stmt.getKeysDesc();\n-        Preconditions.checkNotNull(keysDesc);\n-        KeysType keysType = keysDesc.getKeysType();\n-\n-        // create distribution info\n-        DistributionDesc distributionDesc = stmt.getDistributionDesc();\n-        Preconditions.checkNotNull(distributionDesc);\n-        DistributionInfo distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-\n-        // calc short key column count\n-        short shortKeyColumnCount = Catalog.calcShortKeyColumnCount(baseSchema, stmt.getProperties());\n-        LOG.debug(\"create table[{}] short key column count: {}\", tableName, shortKeyColumnCount);\n-\n-        // indexes\n-        TableIndexes indexes = new TableIndexes(stmt.getIndexes());\n-\n-        // create table\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        OlapTable olapTable = new OlapTable(tableId, tableName, baseSchema, keysType, partitionInfo,\n-                distributionInfo, indexes);\n-        olapTable.setComment(stmt.getComment());\n-\n-        // set base index id\n-        long baseIndexId = getNextId();\n-        olapTable.setBaseIndexId(baseIndexId);\n-\n-        // set base index info to table\n-        // this should be done before create partition.\n-        Map<String, String> properties = stmt.getProperties();\n-\n-        // analyze bloom filter columns\n-        Set<String> bfColumns = null;\n-        double bfFpp = 0;\n-        try {\n-            bfColumns = PropertyAnalyzer.analyzeBloomFilterColumns(properties, baseSchema);\n-            if (bfColumns != null && bfColumns.isEmpty()) {\n-                bfColumns = null;\n-            }\n-\n-            bfFpp = PropertyAnalyzer.analyzeBloomFilterFpp(properties);\n-            if (bfColumns != null && bfFpp == 0) {\n-                bfFpp = FeConstants.default_bloom_filter_fpp;\n-            } else if (bfColumns == null) {\n-                bfFpp = 0;\n-            }\n-\n-            olapTable.setBloomFilterInfo(bfColumns, bfFpp);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // analyze replication_num\n-        short replicationNum = FeConstants.default_replication_num;\n-        try {\n-            boolean isReplicationNumSet = properties != null && properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM);\n-            replicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, replicationNum);\n-            if (isReplicationNumSet) {\n-                olapTable.setReplicationNum(replicationNum);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // set in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties, PropertyAnalyzer.PROPERTIES_INMEMORY, false);\n-        olapTable.setIsInMemory(isInMemory);\n-\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-            // if this is an unpartitioned table, we should analyze data property and replication num here.\n-            // if this is a partitioned table, there properties are already analyzed in RangePartitionDesc analyze phase.\n-\n-            // use table name as this single partition name\n-            long partitionId = partitionNameToId.get(tableName);\n-            DataProperty dataProperty = null;\n-            try {\n-                dataProperty = PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(),\n-                        DataProperty.DEFAULT_DATA_PROPERTY);\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(dataProperty);\n-            partitionInfo.setDataProperty(partitionId, dataProperty);\n-            partitionInfo.setReplicationNum(partitionId, replicationNum);\n-            partitionInfo.setIsInMemory(partitionId, isInMemory);\n-            partitionInfo.setTabletType(partitionId, tabletType);\n-        }\n-\n-        // check colocation properties\n-        try {\n-            String colocateGroup = PropertyAnalyzer.analyzeColocate(properties);\n-            if (colocateGroup != null) {\n-                if (Config.disable_colocate_join) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_FEATURE_DISABLED);\n-                }\n-                String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                if (groupSchema != null) {\n-                    // group already exist, check if this table can be added to this group\n-                    groupSchema.checkColocateSchema(olapTable);\n-                }\n-                // add table to this group, if group does not exist, create a new one\n-                getColocateTableIndex().addTableToGroup(db.getId(), olapTable, colocateGroup,\n-                        null /* generate group id inside */);\n-                olapTable.setColocateGroup(colocateGroup);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // get base index storage type. default is COLUMN\n-        TStorageType baseIndexStorageType = null;\n-        try {\n-            baseIndexStorageType = PropertyAnalyzer.analyzeStorageType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(baseIndexStorageType);\n-        // set base index meta\n-        int schemaVersion = 0;\n-        try {\n-            schemaVersion = PropertyAnalyzer.analyzeSchemaVersion(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        int schemaHash = Util.schemaHash(schemaVersion, baseSchema, bfColumns, bfFpp);\n-        olapTable.setIndexMeta(baseIndexId, tableName, baseSchema, schemaVersion, schemaHash,\n-                shortKeyColumnCount, baseIndexStorageType, keysType);\n-\n-        for (AlterClause alterClause : stmt.getRollupAlterClauseList()) {\n-            AddRollupClause addRollupClause = (AddRollupClause)alterClause;\n-\n-            Long baseRollupIndex = olapTable.getIndexIdByName(tableName);\n-\n-            // get storage type for rollup index\n-            TStorageType rollupIndexStorageType = null;\n-            try {\n-                rollupIndexStorageType = PropertyAnalyzer.analyzeStorageType(addRollupClause.getProperties());\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(rollupIndexStorageType);\n-            // set rollup index meta to olap table\n-            List<Column> rollupColumns = getRollupHandler().checkAndPrepareMaterializedView(addRollupClause,\n-                    olapTable, baseRollupIndex, false);\n-            short rollupShortKeyColumnCount = Catalog.calcShortKeyColumnCount(rollupColumns, alterClause.getProperties());\n-            int rollupSchemaHash = Util.schemaHash(schemaVersion, rollupColumns, bfColumns, bfFpp);\n-            long rollupIndexId = getCurrentCatalog().getNextId();\n-            olapTable.setIndexMeta(rollupIndexId, addRollupClause.getRollupName(), rollupColumns, schemaVersion,\n-                    rollupSchemaHash, rollupShortKeyColumnCount, rollupIndexStorageType, keysType);\n-        }\n-\n-        // analyze version info\n-        Pair<Long, Long> versionInfo = null;\n-        try {\n-            versionInfo = PropertyAnalyzer.analyzeVersionInfo(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(versionInfo);\n-\n-        // get storage format\n-        TStorageFormat storageFormat = TStorageFormat.DEFAULT; // default means it's up to BE's config\n-        try {\n-            storageFormat = PropertyAnalyzer.analyzeStorageFormat(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        olapTable.setStorageFormat(storageFormat);\n-\n-        // a set to record every new tablet created when create table\n-        // if failed in any step, use this set to do clear things\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-\n-        // create partition\n-        try {\n-            if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-                // this is a 1-level partitioned table\n-                // use table name as partition name\n-                String partitionName = tableName;\n-                long partitionId = partitionNameToId.get(partitionName);\n-                // create partition\n-                Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                        olapTable.getId(), olapTable.getBaseIndexId(),\n-                        partitionId, partitionName,\n-                        olapTable.getIndexIdToMeta(),\n-                        keysType,\n-                        distributionInfo,\n-                        partitionInfo.getDataProperty(partitionId).getStorageMedium(),\n-                        partitionInfo.getReplicationNum(partitionId),\n-                        versionInfo, bfColumns, bfFpp,\n-                        tabletIdSet, olapTable.getCopiedIndexes(),\n-                        isInMemory, storageFormat, tabletType);\n-                olapTable.addPartition(partition);\n-            } else if (partitionInfo.getType() == PartitionType.RANGE) {\n-                try {\n-                    // just for remove entries in stmt.getProperties(),\n-                    // and then check if there still has unknown properties\n-                    PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(), DataProperty.DEFAULT_DATA_PROPERTY);\n-                    DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(olapTable, properties);\n-\n-                    if (properties != null && !properties.isEmpty()) {\n-                        // here, all properties should be checked\n-                        throw new DdlException(\"Unknown properties: \" + properties);\n-                    }\n-                } catch (AnalysisException e) {\n-                    throw new DdlException(e.getMessage());\n-                }\n-\n-                // this is a 2-level partitioned tables\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Map.Entry<String, Long> entry : partitionNameToId.entrySet()) {\n-                    DataProperty dataProperty = rangePartitionInfo.getDataProperty(entry.getValue());\n-                    Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(), olapTable.getId(),\n-                            olapTable.getBaseIndexId(), entry.getValue(), entry.getKey(),\n-                            olapTable.getIndexIdToMeta(),\n-                            keysType, distributionInfo,\n-                            dataProperty.getStorageMedium(),\n-                            partitionInfo.getReplicationNum(entry.getValue()),\n-                            versionInfo, bfColumns, bfFpp,\n-                            tabletIdSet, olapTable.getCopiedIndexes(),\n-                            isInMemory, storageFormat,\n-                            rangePartitionInfo.getTabletType(entry.getValue()));\n-                    olapTable.addPartition(partition);\n-                }\n-            } else {\n-                throw new DdlException(\"Unsupport partition method: \" + partitionInfo.getType().name());\n-            }\n-\n-            if (!db.createTableWithLock(olapTable, false, stmt.isSetIfNotExists())) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exists\");\n-            }\n-\n-            // we have added these index to memory, only need to persist here\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                GroupId groupId = getColocateTableIndex().getGroup(tableId);\n-                List<List<Long>> backendsPerBucketSeq = getColocateTableIndex().getBackendsPerBucketSeq(groupId);\n-                ColocatePersistInfo info = ColocatePersistInfo.createForAddTable(groupId, tableId, backendsPerBucketSeq);\n-                editLog.logColocateAddTable(info);\n-            }\n-            LOG.info(\"successfully create table[{};{}]\", tableName, tableId);\n-            // register or remove table from DynamicPartition after table created\n-            DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), olapTable);\n-            dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                    tableName, DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-\n-            // only remove from memory, because we have not persist it\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                getColocateTableIndex().removeTable(tableId);\n-            }\n-\n-            throw e;\n-        }\n-        return;\n-    }\n-\n-    private void createMysqlTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        MysqlTable mysqlTable = new MysqlTable(tableId, tableName, columns, stmt.getProperties());\n-        mysqlTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(mysqlTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-        return;\n-    }\n-\n-    private Table createEsTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        EsTable esTable = new EsTable(tableId, tableName, baseSchema, stmt.getProperties(), partitionInfo);\n-        esTable.setComment(stmt.getComment());\n-\n-        if (!db.createTableWithLock(esTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table{} with id {}\", tableName, tableId);\n-        return esTable;\n-    }\n-\n-    private void createBrokerTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        BrokerTable brokerTable = new BrokerTable(tableId, tableName, columns, stmt.getProperties());\n-        brokerTable.setComment(stmt.getComment());\n-        brokerTable.setBrokerProperties(stmt.getExtProperties());\n-\n-        if (!db.createTableWithLock(brokerTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-\n-        return;\n-    }\n-\n-    private void createHiveTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        List<Column> columns = stmt.getColumns();\n-        long tableId = getNextId();\n-        HiveTable hiveTable = new HiveTable(tableId, tableName, columns, stmt.getProperties());\n-        hiveTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(hiveTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-    }\n-\n-    public static void getDdlStmt(Table table, List<String> createTableStmt, List<String> addPartitionStmt,\n-                                  List<String> createRollupStmt, boolean separatePartition, boolean hidePassword) {\n-        StringBuilder sb = new StringBuilder();\n-\n-        // 1. create table\n-        // 1.1 view\n-        if (table.getType() == TableType.VIEW) {\n-            View view = (View) table;\n-            sb.append(\"CREATE VIEW `\").append(table.getName()).append(\"` AS \").append(view.getInlineViewDef());\n-            sb.append(\";\");\n-            createTableStmt.add(sb.toString());\n-            return;\n-        }\n-\n-        // 1.2 other table type\n-        sb.append(\"CREATE \");\n-        if (table.getType() == TableType.MYSQL || table.getType() == TableType.ELASTICSEARCH\n-                || table.getType() == TableType.BROKER || table.getType() == TableType.HIVE) {\n-            sb.append(\"EXTERNAL \");\n-        }\n-        sb.append(\"TABLE \");\n-        sb.append(\"`\").append(table.getName()).append(\"` (\\n\");\n-        int idx = 0;\n-        for (Column column : table.getBaseSchema()) {\n-            if (idx++ != 0) {\n-                sb.append(\",\\n\");\n-            }\n-            // There MUST BE 2 space in front of each column description line\n-            // sqlalchemy requires this to parse SHOW CREATE TAEBL stmt.\n-            sb.append(\"  \").append(column.toSql());\n-        }\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-            if (CollectionUtils.isNotEmpty(olapTable.getIndexes())) {\n-                for (Index index : olapTable.getIndexes()) {\n-                    sb.append(\",\\n\");\n-                    sb.append(\"  \").append(index.toSql());\n-                }\n-            }\n-        }\n-        sb.append(\"\\n) ENGINE=\");\n-        sb.append(table.getType().name());\n-\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            // keys\n-            sb.append(\"\\n\").append(olapTable.getKeysType().toSql()).append(\"(\");\n-            List<String> keysColumnNames = Lists.newArrayList();\n-            for (Column column : olapTable.getBaseSchema()) {\n-                if (column.isKey()) {\n-                    keysColumnNames.add(\"`\" + column.getName() + \"`\");\n-                }\n-            }\n-            sb.append(Joiner.on(\", \").join(keysColumnNames)).append(\")\");\n-\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            List<Long> partitionId = null;\n-            if (separatePartition) {\n-                partitionId = Lists.newArrayList();\n-            }\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\").append(partitionInfo.toSql(olapTable, partitionId));\n-            }\n-\n-            // distribution\n-            DistributionInfo distributionInfo = olapTable.getDefaultDistributionInfo();\n-            sb.append(\"\\n\").append(distributionInfo.toSql());\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-\n-            // replicationNum\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            sb.append(\"\\\"\").append(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM).append(\"\\\" = \\\"\");\n-            sb.append(replicationNum).append(\"\\\"\");\n-\n-            // bloom filter\n-            Set<String> bfColumnNames = olapTable.getCopiedBfColumns();\n-            if (bfColumnNames != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_BF_COLUMNS).append(\"\\\" = \\\"\");\n-                sb.append(Joiner.on(\", \").join(olapTable.getCopiedBfColumns())).append(\"\\\"\");\n-            }\n-\n-            if (separatePartition) {\n-                // version info\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_VERSION_INFO).append(\"\\\" = \\\"\");\n-                Partition partition = null;\n-                if (olapTable.getPartitionInfo().getType() == PartitionType.UNPARTITIONED) {\n-                    partition = olapTable.getPartition(olapTable.getName());\n-                } else {\n-                    Preconditions.checkState(partitionId.size() == 1);\n-                    partition = olapTable.getPartition(partitionId.get(0));\n-                }\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-            }\n-\n-            // colocateTable\n-            String colocateTable = olapTable.getColocateGroup();\n-            if (colocateTable != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH).append(\"\\\" = \\\"\");\n-                sb.append(colocateTable).append(\"\\\"\");\n-            }\n-\n-            // dynamic partition\n-            if (olapTable.dynamicPartitionExists()) {\n-                sb.append(olapTable.getTableProperty().getDynamicPartitionProperty().toString());\n-            }\n-\n-            // in memory\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_INMEMORY).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.isInMemory()).append(\"\\\"\");\n-\n-            // storage type\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_STORAGE_FORMAT).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.getStorageFormat()).append(\"\\\"\");\n-\n-            sb.append(\"\\n)\");\n-        } else if (table.getType() == TableType.MYSQL) {\n-            MysqlTable mysqlTable = (MysqlTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"host\\\" = \\\"\").append(mysqlTable.getHost()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"port\\\" = \\\"\").append(mysqlTable.getPort()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(mysqlTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : mysqlTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(mysqlTable.getMysqlDatabaseName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(mysqlTable.getMysqlTableName()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.BROKER) {\n-            BrokerTable brokerTable = (BrokerTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"broker_name\\\" = \\\"\").append(brokerTable.getBrokerName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"path\\\" = \\\"\").append(Joiner.on(\",\").join(brokerTable.getEncodedPaths())).append(\"\\\",\\n\");\n-            sb.append(\"\\\"column_separator\\\" = \\\"\").append(brokerTable.getReadableColumnSeparator()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"line_delimiter\\\" = \\\"\").append(brokerTable.getReadableLineDelimiter()).append(\"\\\",\\n\");\n-            sb.append(\")\");\n-            if (!brokerTable.getBrokerProperties().isEmpty()) {\n-                sb.append(\"\\nBROKER PROPERTIES (\\n\");\n-                sb.append(new PrintableMap<>(brokerTable.getBrokerProperties(), \" = \", true, true,\n-                        hidePassword).toString());\n-                sb.append(\"\\n)\");\n-            }\n-        } else if (table.getType() == TableType.ELASTICSEARCH) {\n-            EsTable esTable = (EsTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = esTable.getPartitionInfo();\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\");\n-                sb.append(\"PARTITION BY RANGE(\");\n-                idx = 0;\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Column column : rangePartitionInfo.getPartitionColumns()) {\n-                    if (idx != 0) {\n-                        sb.append(\", \");\n-                    }\n-                    sb.append(\"`\").append(column.getName()).append(\"`\");\n-                }\n-                sb.append(\")\\n()\");\n-            }\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"hosts\\\" = \\\"\").append(esTable.getHosts()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(esTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : esTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"index\\\" = \\\"\").append(esTable.getIndexName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"type\\\" = \\\"\").append(esTable.getMappingType()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"transport\\\" = \\\"\").append(esTable.getTransport()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_docvalue_scan\\\" = \\\"\").append(esTable.isDocValueScanEnable()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_keyword_sniff\\\" = \\\"\").append(esTable.isKeywordSniffEnable()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.HIVE) {\n-            HiveTable hiveTable = (HiveTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(hiveTable.getHiveDb()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(hiveTable.getHiveTable()).append(\"\\\",\\n\");\n-            sb.append(new PrintableMap<>(hiveTable.getHiveProperties(), \" = \", true, true, false).toString());\n-            sb.append(\"\\n)\");\n-        }\n-        sb.append(\";\");\n-\n-        createTableStmt.add(sb.toString());\n-\n-        // 2. add partition\n-        if (separatePartition && (table instanceof OlapTable)\n-                && ((OlapTable) table).getPartitionInfo().getType() == PartitionType.RANGE\n-                && ((OlapTable) table).getPartitions().size() > 1) {\n-            OlapTable olapTable = (OlapTable) table;\n-            RangePartitionInfo partitionInfo = (RangePartitionInfo) olapTable.getPartitionInfo();\n-            boolean first = true;\n-            for (Map.Entry<Long, Range<PartitionKey>> entry : partitionInfo.getSortedRangeMap(false)) {\n-                if (first) {\n-                    first = false;\n-                    continue;\n-                }\n-                sb = new StringBuilder();\n-                Partition partition = olapTable.getPartition(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName());\n-                sb.append(\" ADD PARTITION \").append(partition.getName()).append(\" VALUES [\");\n-                sb.append(entry.getValue().lowerEndpoint().toSql());\n-                sb.append(\", \").append(entry.getValue().upperEndpoint().toSql()).append(\")\");\n-                sb.append(\"(\\\"version_info\\\" = \\\"\");\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-                sb.append(\");\");\n-                addPartitionStmt.add(sb.toString());\n-            }\n-        }\n-\n-        // 3. rollup\n-        if (createRollupStmt != null && (table instanceof OlapTable)) {\n-            OlapTable olapTable = (OlapTable) table;\n-            for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                if (entry.getKey() == olapTable.getBaseIndexId()) {\n-                    continue;\n-                }\n-                MaterializedIndexMeta materializedIndexMeta = entry.getValue();\n-                sb = new StringBuilder();\n-                String indexName = olapTable.getIndexNameById(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName()).append(\" ADD ROLLUP \").append(indexName);\n-                sb.append(\"(\");\n-\n-                List<Column> indexSchema = materializedIndexMeta.getSchema();\n-                for (int i = 0; i < indexSchema.size(); i++) {\n-                    Column column = indexSchema.get(i);\n-                    sb.append(column.getName());\n-                    if (i != indexSchema.size() - 1) {\n-                        sb.append(\", \");\n-                    }\n-                }\n-                sb.append(\");\");\n-                createRollupStmt.add(sb.toString());\n-            }\n-        }\n-    }\n-\n-    public void replayCreateTable(String dbName, Table table) {\n-        Database db = this.fullNameToDb.get(dbName);\n-        db.createTableWithLock(table, true, false);\n-\n-        if (!isCheckpointThread()) {\n-            // add to inverted index\n-            if (table.getType() == TableType.OLAP) {\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                OlapTable olapTable = (OlapTable) table;\n-                long dbId = db.getId();\n-                long tableId = table.getId();\n-                for (Partition partition : olapTable.getAllPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                } // end for partitions\n-                DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(dbId, olapTable);\n-            }\n-        }\n-    }\n-\n-    private void createTablets(String clusterName, MaterializedIndex index, ReplicaState replicaState,\n-                               DistributionInfo distributionInfo, long version, long versionHash, short replicationNum,\n-                               TabletMeta tabletMeta, Set<Long> tabletIdSet) throws DdlException {\n-        Preconditions.checkArgument(replicationNum > 0);\n-\n-        DistributionInfoType distributionInfoType = distributionInfo.getType();\n-        if (distributionInfoType == DistributionInfoType.HASH) {\n-            ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();\n-            List<List<Long>> backendsPerBucketSeq = null;\n-            GroupId groupId = null;\n-            if (colocateIndex.isColocateTable(tabletMeta.getTableId())) {\n-                // if this is a colocate table, try to get backend seqs from colocation index.\n-                Database db = Catalog.getCurrentCatalog().getDb(tabletMeta.getDbId());\n-                groupId = colocateIndex.getGroup(tabletMeta.getTableId());\n-                // Use db write lock here to make sure the backendsPerBucketSeq is consistent when the backendsPerBucketSeq is updating.\n-                // This lock will release very fast.\n-                db.writeLock();\n-                try {\n-                    backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-            }\n-\n-            // chooseBackendsArbitrary is true, means this may be the first table of colocation group,\n-            // or this is just a normal table, and we can choose backends arbitrary.\n-            // otherwise, backends should be chosen from backendsPerBucketSeq;\n-            boolean chooseBackendsArbitrary = backendsPerBucketSeq == null || backendsPerBucketSeq.isEmpty();\n-            if (chooseBackendsArbitrary) {\n-                backendsPerBucketSeq = Lists.newArrayList();\n-            }\n-            for (int i = 0; i < distributionInfo.getBucketNum(); ++i) {\n-                // create a new tablet with random chosen backends\n-                Tablet tablet = new Tablet(getNextId());\n-\n-                // add tablet to inverted index first\n-                index.addTablet(tablet, tabletMeta);\n-                tabletIdSet.add(tablet.getId());\n-\n-                // get BackendIds\n-                List<Long> chosenBackendIds;\n-                if (chooseBackendsArbitrary) {\n-                    // This is the first colocate table in the group, or just a normal table,\n-                    // randomly choose backends\n-                    if (Config.enable_strict_storage_medium_check) {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName, tabletMeta.getStorageMedium());\n-                    } else {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName);\n-                    }\n-                    backendsPerBucketSeq.add(chosenBackendIds);\n-                } else {\n-                    // get backends from existing backend sequence\n-                    chosenBackendIds = backendsPerBucketSeq.get(i);\n-                }\n-                \n-                // create replicas\n-                for (long backendId : chosenBackendIds) {\n-                    long replicaId = getNextId();\n-                    Replica replica = new Replica(replicaId, backendId, replicaState, version, versionHash,\n-                            tabletMeta.getOldSchemaHash());\n-                    tablet.addReplica(replica);\n-                }\n-                Preconditions.checkState(chosenBackendIds.size() == replicationNum, chosenBackendIds.size() + \" vs. \"+ replicationNum);\n-            }\n-\n-            if (groupId != null) {\n-                colocateIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-        } else {\n-            throw new DdlException(\"Unknown distribution type: \" + distributionInfoType);\n-        }\n-    }\n-\n-    // create replicas for tablet with random chosen backends\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName, TStorageMedium storageMedium) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIdsByStorageMedium(replicationNum,\n-                true, true, clusterName, storageMedium);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host with storage medium is \" + storageMedium + \" in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIds(replicationNum, true, true, clusterName);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    // Drop table\n-    public void dropTable(DropTableStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check database\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        Table table = null;\n-        db.writeLock();\n-        try {\n-            table = db.getTable(tableName);\n-            if (table == null) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop table[{}] which does not exist\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-            }\n-\n-            // Check if a view\n-            if (stmt.isView()) {\n-                if (!(table instanceof View)) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"VIEW\");\n-                }\n-            } else {\n-                if (table instanceof View) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"TABLE\");\n-                }\n-            }\n-\n-            if (stmt.isNeedCheckCommitedTxns()) {\n-                if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), table.getId(), null)) {\n-                    throw new DdlException(\"There are still some commited txns cannot be aborted in table [\"\n-                            + table.getName() + \"], please wait for GlobalTransactionMgr to finish publish tasks.\" +\n-                            \" If you don't need to recover table, use DROPP TABLE stmt (double P).\");\n-                }\n-            }\n-\n-            unprotectDropTable(db, table.getId());\n-\n-            DropInfo info = new DropInfo(db.getId(), table.getId(), -1L);\n-            editLog.logDropTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-\n-        LOG.info(\"finished dropping table: {} from db: {}\", tableName, dbName);\n-    }\n-\n-    public boolean unprotectDropTable(Database db, long tableId) {\n-        Table table = db.getTable(tableId);\n-        // delete from db meta\n-        if (table == null) {\n-            return false;\n-        }\n-\n-        if (table.getType() == TableType.ELASTICSEARCH) {\n-            esRepository.deRegisterTable(tableId);\n-        } else if (table.getType() == TableType.OLAP) {\n-            // drop all temp partitions of this table, so that there is no temp partitions in recycle bin,\n-            // which make things easier.\n-            ((OlapTable) table).dropAllTempPartitions();\n-        }\n-\n-        db.dropTable(table.getName());\n-\n-        Catalog.getCurrentRecycleBin().recycleTable(db.getId(), table);\n-\n-        LOG.info(\"finished dropping table[{}] in db[{}]\", table.getName(), db.getFullName());\n-        return true;\n-    }\n-\n-    public void replayDropTable(Database db, long tableId) {\n-        db.writeLock();\n-        try {\n-            unprotectDropTable(db, tableId);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseTable(long tableId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseTable(tableId);\n-    }\n-\n-    public void replayRecoverTable(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Catalog.getCurrentRecycleBin().replayRecoverTable(db, info.getTableId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private void unprotectAddReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay add a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-\n-        // for compatibility\n-        int schemaHash = info.getSchemaHash();\n-        if (schemaHash == -1) {\n-            schemaHash = olapTable.getSchemaHashByIndexId(info.getIndexId());\n-        }\n-\n-        Replica replica = new Replica(info.getReplicaId(), info.getBackendId(), info.getVersion(),\n-                info.getVersionHash(), schemaHash, info.getDataSize(), info.getRowCount(),\n-                ReplicaState.NORMAL,\n-                info.getLastFailedVersion(),\n-                info.getLastFailedVersionHash(),\n-                info.getLastSuccessVersion(),\n-                info.getLastSuccessVersionHash());\n-        tablet.addReplica(replica);\n-    }\n-\n-    private void unprotectUpdateReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay update a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        Replica replica = tablet.getReplicaByBackendId(info.getBackendId());\n-        Preconditions.checkNotNull(replica, info);\n-        replica.updateVersionInfo(info.getVersion(), info.getVersionHash(), info.getDataSize(), info.getRowCount());\n-        replica.setBad(false);\n-    }\n-\n-    public void replayAddReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectAddReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayUpdateReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectUpdateReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void unprotectDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        tablet.deleteReplicaByBackendId(info.getBackendId());\n-    }\n-\n-    public void replayDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectDeleteReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayAddFrontend(Frontend fe) {\n-        tryLock(true);\n-        try {\n-            Frontend existFe = checkFeExist(fe.getHost(), fe.getEditLogPort());\n-            if (existFe != null) {\n-                LOG.warn(\"fe {} already exist.\", existFe);\n-                if (existFe.getRole() != fe.getRole()) {\n-                    /*\n-                     * This may happen if:\n-                     * 1. first, add a FE as OBSERVER.\n-                     * 2. This OBSERVER is restarted with ROLE and VERSION file being DELETED.\n-                     *    In this case, this OBSERVER will be started as a FOLLOWER, and add itself to the frontends.\n-                     * 3. this \"FOLLOWER\" begin to load image or replay journal,\n-                     *    then find the origin OBSERVER in image or journal.\n-                     * This will cause UNDEFINED behavior, so it is better to exit and fix it manually.\n-                     */\n-                    System.err.println(\"Try to add an already exist FE with different role\" + fe.getRole());\n-                    System.exit(-1);\n-                }\n-                return;\n-            }\n-            frontends.put(fe.getNodeName(), fe);\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                // DO NOT add helper sockets here, cause BDBHA is not instantiated yet.\n-                // helper sockets will be added after start BDBHA\n-                // But add to helperNodes, just for show\n-                helperNodes.add(Pair.create(fe.getHost(), fe.getEditLogPort()));\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropFrontend(Frontend frontend) {\n-        tryLock(true);\n-        try {\n-            Frontend removedFe = frontends.remove(frontend.getNodeName());\n-            if (removedFe == null) {\n-                LOG.error(frontend.toString() + \" does not exist.\");\n-                return;\n-            }\n-            if (removedFe.getRole() == FrontendNodeType.FOLLOWER\n-                    || removedFe.getRole() == FrontendNodeType.REPLICA) {\n-                helperNodes.remove(Pair.create(removedFe.getHost(), removedFe.getEditLogPort()));\n-            }\n-\n-            removedFrontends.add(removedFe.getNodeName());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public int getClusterId() {\n-        return this.clusterId;\n-    }\n-\n-    public String getToken() {\n-        return token;\n-    }\n-\n-    public Database getDb(String name) {\n-        if (fullNameToDb.containsKey(name)) {\n-            return fullNameToDb.get(name);\n-        } else {\n-            // This maybe a information_schema db request, and information_schema db name is case insensitive.\n-            // So, we first extract db name to check if it is information_schema.\n-            // Then we reassemble the origin cluster name with lower case db name,\n-            // and finally get information_schema db from the name map.\n-            String dbName = ClusterNamespace.getNameFromFullName(name);\n-            if (dbName.equalsIgnoreCase(InfoSchemaDb.DATABASE_NAME)) {\n-                String clusterName = ClusterNamespace.getClusterNameFromFullName(name);\n-                return fullNameToDb.get(ClusterNamespace.getFullName(clusterName, dbName.toLowerCase()));\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Database getDb(long dbId) {\n-        return idToDb.get(dbId);\n-    }\n-\n-    public EditLog getEditLog() {\n-        return editLog;\n-    }\n-\n-    // Get the next available, need't lock because of nextId is atomic.\n-    public long getNextId() {\n-        long id = idGenerator.getNextId();\n-        return id;\n-    }\n-\n-    public List<String> getDbNames() {\n-        return Lists.newArrayList(fullNameToDb.keySet());\n-    }\n-\n-    public List<String> getClusterDbNames(String clusterName) throws AnalysisException {\n-        final Cluster cluster = nameToCluster.get(clusterName);\n-        if (cluster == null) {\n-            throw new AnalysisException(\"No cluster selected\");\n-        }\n-        return Lists.newArrayList(cluster.getDbNames());\n-    }\n-\n-    public List<Long> getDbIds() {\n-        return Lists.newArrayList(idToDb.keySet());\n-    }\n-\n-    public HashMap<Long, TStorageMedium> getPartitionIdToStorageMediumMap() {\n-        HashMap<Long, TStorageMedium> storageMediumMap = new HashMap<Long, TStorageMedium>();\n-\n-        // record partition which need to change storage medium\n-        // dbId -> (tableId -> partitionId)\n-        HashMap<Long, Multimap<Long, Long>> changedPartitionsMap = new HashMap<Long, Multimap<Long, Long>>();\n-        long currentTimeMs = System.currentTimeMillis();\n-        List<Long> dbIds = getDbIds();\n-\n-        for (long dbId : dbIds) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while doing backend report\", dbId);\n-                continue;\n-            }\n-\n-            db.readLock();\n-            try {\n-                for (Table table : db.getTables()) {\n-                    if (table.getType() != TableType.OLAP) {\n-                        continue;\n-                    }\n-\n-                    long tableId = table.getId();\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                    for (Partition partition : olapTable.getAllPartitions()) {\n-                        long partitionId = partition.getId();\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        Preconditions.checkNotNull(dataProperty, partition.getName() + \", pId:\" + partitionId + \", db: \" + dbId + \", tbl: \" + tableId);\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            // record and change when holding write lock\n-                            Multimap<Long, Long> multimap = changedPartitionsMap.get(dbId);\n-                            if (multimap == null) {\n-                                multimap = HashMultimap.create();\n-                                changedPartitionsMap.put(dbId, multimap);\n-                            }\n-                            multimap.put(tableId, partitionId);\n-                        } else {\n-                            storageMediumMap.put(partitionId, dataProperty.getStorageMedium());\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.readUnlock();\n-            }\n-        } // end for dbs\n-\n-        // handle data property changed\n-        for (Long dbId : changedPartitionsMap.keySet()) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while checking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Multimap<Long, Long> tableIdToPartitionIds = changedPartitionsMap.get(dbId);\n-\n-            // use try lock to avoid blocking a long time.\n-            // if block too long, backend report rpc will timeout.\n-            if (!db.tryWriteLock(Database.TRY_LOCK_TIMEOUT_MS, TimeUnit.MILLISECONDS)) {\n-                LOG.warn(\"try get db {} writelock but failed when hecking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Preconditions.checkState(db.isWriteLockHeldByCurrentThread());\n-            try {\n-                for (Long tableId : tableIdToPartitionIds.keySet()) {\n-                    Table table = db.getTable(tableId);\n-                    if (table == null) {\n-                        continue;\n-                    }\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-                    Collection<Long> partitionIds = tableIdToPartitionIds.get(tableId);\n-                    for (Long partitionId : partitionIds) {\n-                        Partition partition = olapTable.getPartition(partitionId);\n-                        if (partition == null) {\n-                            continue;\n-                        }\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            partitionInfo.setDataProperty(partition.getId(), new DataProperty(TStorageMedium.HDD));\n-                            storageMediumMap.put(partitionId, TStorageMedium.HDD);\n-                            LOG.debug(\"partition[{}-{}-{}] storage medium changed from SSD to HDD\",\n-                                    dbId, tableId, partitionId);\n-\n-                            // log\n-                            ModifyPartitionInfo info =\n-                                    new ModifyPartitionInfo(db.getId(), olapTable.getId(),\n-                                            partition.getId(),\n-                                            DataProperty.DEFAULT_DATA_PROPERTY,\n-                                            (short) -1,\n-                                            partitionInfo.getIsInMemory(partition.getId()));\n-                            editLog.logModifyPartition(info);\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } // end for dbs\n-        return storageMediumMap;\n-    }\n-\n-    public ConsistencyChecker getConsistencyChecker() {\n-        return this.consistencyChecker;\n-    }\n-\n-    public Alter getAlterInstance() {\n-        return this.alter;\n-    }\n-\n-    public SchemaChangeHandler getSchemaChangeHandler() {\n-        return (SchemaChangeHandler) this.alter.getSchemaChangeHandler();\n-    }\n-\n-    public MaterializedViewHandler getRollupHandler() {\n-        return (MaterializedViewHandler) this.alter.getMaterializedViewHandler();\n-    }\n-\n-    public SystemHandler getClusterHandler() {\n-        return (SystemHandler) this.alter.getClusterHandler();\n-    }\n-\n-    public BackupHandler getBackupHandler() {\n-        return this.backupHandler;\n-    }\n-\n-    public DeleteHandler getDeleteHandler() {\n-        return this.deleteHandler;\n-    }\n-\n-    public Load getLoadInstance() {\n-        return this.load;\n-    }\n-\n-    public LoadManager getLoadManager() {\n-        return loadManager;\n-    }\n-\n-    public MasterTaskExecutor getLoadTaskScheduler() {\n-        return loadTaskScheduler;\n-    }\n-\n-    public RoutineLoadManager getRoutineLoadManager() {\n-        return routineLoadManager;\n-    }\n-\n-    public RoutineLoadTaskScheduler getRoutineLoadTaskScheduler(){\n-        return routineLoadTaskScheduler;\n-    }\n-\n-    public ExportMgr getExportMgr() {\n-        return this.exportMgr;\n-    }\n-\n-    public SmallFileMgr getSmallFileMgr() {\n-        return this.smallFileMgr;\n-    }\n-\n-    public long getReplayedJournalId() {\n-        return this.replayedJournalId.get();\n-    }\n-\n-    public HAProtocol getHaProtocol() {\n-        return this.haProtocol;\n-    }\n-\n-    public Long getMaxJournalId() {\n-        return this.editLog.getMaxJournalId();\n-    }\n-\n-    public long getEpoch() {\n-        return this.epoch;\n-    }\n-\n-    public void setEpoch(long epoch) {\n-        this.epoch = epoch;\n-    }\n-\n-    public FrontendNodeType getRole() {\n-        return this.role;\n-    }\n-\n-    public Pair<String, Integer> getHelperNode() {\n-        Preconditions.checkState(helperNodes.size() >= 1);\n-        return this.helperNodes.get(0);\n-    }\n-\n-    public List<Pair<String, Integer>> getHelperNodes() {\n-        return Lists.newArrayList(helperNodes);\n-    }\n-\n-    public Pair<String, Integer> getSelfNode() {\n-        return this.selfNode;\n-    }\n-\n-    public String getNodeName() {\n-        return this.nodeName;\n-    }\n-\n-    public FrontendNodeType getFeType() {\n-        return this.feType;\n-    }\n-\n-    public int getMasterRpcPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterRpcPort;\n-    }\n-\n-    public int getMasterHttpPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterHttpPort;\n-    }\n-\n-    public String getMasterIp() {\n-        if (!isReady()) {\n-            return \"\";\n-        }\n-        return this.masterIp;\n-    }\n-\n-    public EsRepository getEsRepository() {\n-        return this.esRepository;\n-    }\n-\n-    public void setMaster(MasterInfo info) {\n-        this.masterIp = info.getIp();\n-        this.masterHttpPort = info.getHttpPort();\n-        this.masterRpcPort = info.getRpcPort();\n-    }\n-\n-    public boolean canRead() {\n-        return this.canRead.get();\n-    }\n-\n-    public boolean isElectable() {\n-        return this.isElectable;\n-    }\n-\n-    public boolean isMaster() {\n-        return feType == FrontendNodeType.MASTER;\n-    }\n-\n-    public void setSynchronizedTime(long time) {\n-        this.synchronizedTimeMs = time;\n-    }\n-\n-    public void setEditLog(EditLog editLog) {\n-        this.editLog = editLog;\n-    }\n-\n-    public void setNextId(long id) {\n-        idGenerator.setId(id);\n-    }\n-\n-    public void setHaProtocol(HAProtocol protocol) {\n-        this.haProtocol = protocol;\n-    }\n-\n-    public static short calcShortKeyColumnCount(List<Column> columns, Map<String, String> properties)\n-            throws DdlException {\n-        List<Column> indexColumns = new ArrayList<Column>();\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                indexColumns.add(column);\n-            }\n-        }\n-        LOG.debug(\"index column size: {}\", indexColumns.size());\n-        Preconditions.checkArgument(indexColumns.size() > 0);\n-\n-        // figure out shortKeyColumnCount\n-        short shortKeyColumnCount = (short) -1;\n-        try {\n-            shortKeyColumnCount = PropertyAnalyzer.analyzeShortKeyColumnCount(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        if (shortKeyColumnCount != (short) -1) {\n-            // use user specified short key column count\n-            if (shortKeyColumnCount <= 0) {\n-                throw new DdlException(\"Invalid short key: \" + shortKeyColumnCount);\n-            }\n-\n-            if (shortKeyColumnCount > indexColumns.size()) {\n-                throw new DdlException(\"Short key is too large. should less than: \" + indexColumns.size());\n-            }\n-\n-            for (int pos = 0; pos < shortKeyColumnCount; pos++) {\n-                if (indexColumns.get(pos).getDataType() == PrimitiveType.VARCHAR && pos != shortKeyColumnCount - 1) {\n-                    throw new DdlException(\"Varchar should not in the middle of short keys.\");\n-                }\n-            }\n-        } else {\n-            /*\n-             * Calc short key column count. NOTE: short key column count is\n-             * calculated as follow: 1. All index column are taking into\n-             * account. 2. Max short key column count is Min(Num of\n-             * indexColumns, META_MAX_SHORT_KEY_NUM). 3. Short key list can\n-             * contains at most one VARCHAR column. And if contains, it should\n-             * be at the last position of the short key list.\n-             */\n-            shortKeyColumnCount = 0;\n-            int shortKeySizeByte = 0;\n-            int maxShortKeyColumnCount = Math.min(indexColumns.size(), FeConstants.shortkey_max_column_count);\n-            for (int i = 0; i < maxShortKeyColumnCount; i++) {\n-                Column column = indexColumns.get(i);\n-                shortKeySizeByte += column.getOlapColumnIndexSize();\n-                if (shortKeySizeByte > FeConstants.shortkey_maxsize_bytes) {\n-                    if (column.getDataType().isCharFamily()) {\n-                        ++shortKeyColumnCount;\n-                    }\n-                    break;\n-                }\n-                if (column.getType().isFloatingPointType()) {\n-                    break;\n-                }\n-                if (column.getDataType() == PrimitiveType.VARCHAR) {\n-                    ++shortKeyColumnCount;\n-                    break;\n-                }\n-                ++shortKeyColumnCount;\n-            }\n-            if (shortKeyColumnCount == 0) {\n-                throw new DdlException(\"The first column could not be float or double type, use decimal instead\");\n-            }\n-\n-        } // end calc shortKeyColumnCount\n-\n-        return shortKeyColumnCount;\n-    }\n-\n-    /*\n-     * used for handling AlterTableStmt (for client is the ALTER TABLE command).\n-     * including SchemaChangeHandler and RollupHandler\n-     */\n-    public void alterTable(AlterTableStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterTable(stmt);\n-    }\n-\n-    /**\n-     * used for handling AlterViewStmt (the ALTER VIEW command).\n-     */\n-    public void alterView(AlterViewStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterView(stmt, ConnectContext.get());\n-    }\n-\n-    public void createMaterializedView(CreateMaterializedViewStmt stmt)\n-            throws AnalysisException, DdlException {\n-        this.alter.processCreateMaterializedView(stmt);\n-    }\n-\n-    public void dropMaterializedView(DropMaterializedViewStmt stmt) throws DdlException, MetaNotFoundException {\n-        this.alter.processDropMaterializedView(stmt);\n-    }\n-\n-    /*\n-     * used for handling CacnelAlterStmt (for client is the CANCEL ALTER\n-     * command). including SchemaChangeHandler and RollupHandler\n-     */\n-    public void cancelAlter(CancelAlterTableStmt stmt) throws DdlException {\n-        if (stmt.getAlterType() == AlterType.ROLLUP) {\n-            this.getRollupHandler().cancel(stmt);\n-        } else if (stmt.getAlterType() == AlterType.COLUMN) {\n-            this.getSchemaChangeHandler().cancel(stmt);\n-        } else {\n-            throw new DdlException(\"Cancel \" + stmt.getAlterType() + \" does not implement yet\");\n-        }\n-    }\n-\n-    /*\n-     * used for handling backup opt\n-     */\n-    public void backup(BackupStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void restore(RestoreStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void cancelBackup(CancelBackupStmt stmt) throws DdlException {\n-        getBackupHandler().cancel(stmt);\n-    }\n-\n-    public void renameTable(Database db, OlapTable table, TableRenameClause tableRenameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String tableName = table.getName();\n-        String newTableName = tableRenameClause.getNewTableName();\n-        if (tableName.equals(newTableName)) {\n-            throw new DdlException(\"Same table name\");\n-        }\n-\n-        // check if name is already used\n-        if (db.getTable(newTableName) != null) {\n-            throw new DdlException(\"Table name[\" + newTableName + \"] is already used\");\n-        }\n-\n-        // check if rollup has same name\n-        for (String idxName : table.getIndexNameToId().keySet()) {\n-            if (idxName.equals(newTableName)) {\n-                throw new DdlException(\"New name conflicts with rollup index name: \" + idxName);\n-            }\n-        }\n-\n-        table.setName(newTableName);\n-\n-        db.dropTable(tableName);\n-        db.createTable(table);\n-\n-        TableInfo tableInfo = TableInfo.createForTableRename(db.getId(), table.getId(), newTableName);\n-        editLog.logTableRename(tableInfo);\n-        LOG.info(\"rename table[{}] to {}\", tableName, newTableName);\n-    }\n-\n-    public void replayRenameTable(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        String newTableName = tableInfo.getNewTableName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String tableName = table.getName();\n-            db.dropTable(tableName);\n-            table.setName(newTableName);\n-            db.createTable(table);\n-\n-            LOG.info(\"replay rename table[{}] to {}\", tableName, newTableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    // the invoker should keep db write lock\n-    public void modifyTableColocate(Database db, OlapTable table, String colocateGroup, boolean isReplay,\n-            GroupId assignedGroupId)\n-            throws DdlException {\n-\n-        String oldGroup = table.getColocateGroup();\n-        GroupId groupId = null;\n-        if (!Strings.isNullOrEmpty(colocateGroup)) {\n-            String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-            ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-            if (groupSchema == null) {\n-                // user set a new colocate group,\n-                // check if all partitions all this table has same buckets num and same replication number\n-                PartitionInfo partitionInfo = table.getPartitionInfo();\n-                if (partitionInfo.getType() == PartitionType.RANGE) {\n-                    int bucketsNum = -1;\n-                    short replicationNum = -1;\n-                    for (Partition partition : table.getPartitions()) {\n-                        if (bucketsNum == -1) {\n-                            bucketsNum = partition.getDistributionInfo().getBucketNum();\n-                        } else if (bucketsNum != partition.getDistributionInfo().getBucketNum()) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different buckets number\");\n-                        }\n-                        \n-                        if (replicationNum == -1) {\n-                            replicationNum = partitionInfo.getReplicationNum(partition.getId());\n-                        } else if (replicationNum != partitionInfo.getReplicationNum(partition.getId())) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different replication number\");\n-                        }\n-                    }\n-                }\n-            } else {\n-                // set to an already exist colocate group, check if this table can be added to this group.\n-                groupSchema.checkColocateSchema(table);\n-            }\n-            \n-            List<List<Long>> backendsPerBucketSeq = null;\n-            if (groupSchema == null) {\n-                // assign to a newly created group, set backends sequence.\n-                // we arbitrarily choose a tablet backends sequence from this table,\n-                // let the colocation balancer do the work.\n-                backendsPerBucketSeq = table.getArbitraryTabletBucketsSeq();\n-            }\n-            // change group after getting backends sequence(if has), in case 'getArbitraryTabletBucketsSeq' failed\n-            groupId = colocateTableIndex.changeGroup(db.getId(), table, oldGroup, colocateGroup, assignedGroupId);\n-\n-            if (groupSchema == null) {\n-                Preconditions.checkNotNull(backendsPerBucketSeq);\n-                colocateTableIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-            // set this group as unstable\n-            colocateTableIndex.markGroupUnstable(groupId, false /* edit log is along with modify table log */);\n-            table.setColocateGroup(colocateGroup);\n-        } else {\n-            // unset colocation group\n-            if (Strings.isNullOrEmpty(oldGroup)) {\n-                // this table is not a colocate table, do nothing\n-                return;\n-            }\n-\n-            // when replayModifyTableColocate, we need the groupId info\n-            String fullGroupName = db.getId() + \"_\" + oldGroup;\n-            groupId = colocateTableIndex.getGroupSchema(fullGroupName).getGroupId();\n-\n-            colocateTableIndex.removeTable(table.getId());\n-            table.setColocateGroup(null);\n-        }\n-\n-        if (!isReplay) {\n-            Map<String, String> properties = Maps.newHashMapWithExpectedSize(1);\n-            properties.put(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH, colocateGroup);\n-            TablePropertyInfo info = new TablePropertyInfo(table.getId(), groupId, properties);\n-            editLog.logModifyTableColocate(info);\n-        }\n-        LOG.info(\"finished modify table's colocation property. table: {}, is replay: {}\",\n-                table.getName(), isReplay);\n-    }\n-\n-    public void replayModifyTableColocate(TablePropertyInfo info) {\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getPropertyMap();\n-\n-        Database db = getDb(info.getGroupId().dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            modifyTableColocate(db, table, properties.get(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH), true,\n-                    info.getGroupId());\n-        } catch (DdlException e) {\n-            // should not happen\n-            LOG.warn(\"failed to replay modify table colocate\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameRollup(Database db, OlapTable table, RollupRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String rollupName = renameClause.getRollupName();\n-        // check if it is base table name\n-        if (rollupName.equals(table.getName())) {\n-            throw new DdlException(\"Using ALTER TABLE RENAME to change table name\");\n-        }\n-\n-        String newRollupName = renameClause.getNewRollupName();\n-        if (rollupName.equals(newRollupName)) {\n-            throw new DdlException(\"Same rollup name\");\n-        }\n-\n-        Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-        if (indexNameToIdMap.get(rollupName) == null) {\n-            throw new DdlException(\"Rollup index[\" + rollupName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (indexNameToIdMap.get(newRollupName) != null) {\n-            throw new DdlException(\"Rollup name[\" + newRollupName + \"] is already used\");\n-        }\n-\n-        long indexId = indexNameToIdMap.remove(rollupName);\n-        indexNameToIdMap.put(newRollupName, indexId);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForRollupRename(db.getId(), table.getId(), indexId, newRollupName);\n-        editLog.logRollupRename(tableInfo);\n-        LOG.info(\"rename rollup[{}] to {}\", rollupName, newRollupName);\n-    }\n-\n-    public void replayRenameRollup(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long indexId = tableInfo.getIndexId();\n-        String newRollupName = tableInfo.getNewRollupName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String rollupName = table.getIndexNameById(indexId);\n-            Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-            indexNameToIdMap.remove(rollupName);\n-            indexNameToIdMap.put(newRollupName, indexId);\n-\n-            LOG.info(\"replay rename rollup[{}] to {}\", rollupName, newRollupName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renamePartition(Database db, OlapTable table, PartitionRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        if (table.getPartitionInfo().getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is single partitioned. \"\n-                    + \"no need to rename partition name.\");\n-        }\n-\n-        String partitionName = renameClause.getPartitionName();\n-        String newPartitionName = renameClause.getNewPartitionName();\n-        if (partitionName.equalsIgnoreCase(newPartitionName)) {\n-            throw new DdlException(\"Same partition name\");\n-        }\n-\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition[\" + partitionName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (table.checkPartitionNameExist(newPartitionName)) {\n-            throw new DdlException(\"Partition name[\" + newPartitionName + \"] is already used\");\n-        }\n-\n-        table.renamePartition(partitionName, newPartitionName);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForPartitionRename(db.getId(), table.getId(), partition.getId(),\n-                newPartitionName);\n-        editLog.logPartitionRename(tableInfo);\n-        LOG.info(\"rename partition[{}] to {}\", partitionName, newPartitionName);\n-    }\n-\n-    public void replayRenamePartition(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long partitionId = tableInfo.getPartitionId();\n-        String newPartitionName = tableInfo.getNewPartitionName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            Partition partition = table.getPartition(partitionId);\n-            table.renamePartition(partition.getName(), newPartitionName);\n-\n-            LOG.info(\"replay rename partition[{}] to {}\", partition.getName(), newPartitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameColumn(Database db, OlapTable table, ColumnRenameClause renameClause) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void replayRenameColumn(TableInfo tableInfo) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void modifyTableDynamicPartition(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Map<String, String> logProperties = new HashMap<>(properties);\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(table, properties);\n-        } else {\n-            Map<String, String> analyzedDynamicPartition = DynamicPartitionUtil.analyzeDynamicPartition(properties);\n-            tableProperty.modifyTableProperties(analyzedDynamicPartition);\n-            tableProperty.buildDynamicProperty();\n-        }\n-\n-        DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), table);\n-        dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                table.getName(), DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), logProperties);\n-        editLog.logDynamicPartition(info);\n-    }\n-\n-    /**\n-     * Set replication number for unpartitioned table.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     * @throws DdlException\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableReplicationNum(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        String defaultReplicationNumName = \"default.\"+ PropertyAnalyzer.PROPERTIES_REPLICATION_NUM;\n-        PartitionInfo partitionInfo = table.getPartitionInfo();\n-        if (partitionInfo.getType() == PartitionType.RANGE) {\n-            throw new DdlException(\"This is a range partitioned table, you should specify partitions with MODIFY PARTITION clause.\" +\n-                    \" If you want to set default replication number, please use '\" + defaultReplicationNumName +\n-                    \"' instead of '\" + PropertyAnalyzer.PROPERTIES_REPLICATION_NUM + \"' to escape misleading.\");\n-        }\n-        String partitionName = table.getName();\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition does not exist. name: \" + partitionName);\n-        }\n-\n-        short replicationNum = Short.valueOf(properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-        boolean isInMemory = partitionInfo.getIsInMemory(partition.getId());\n-        DataProperty newDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        partitionInfo.setReplicationNum(partition.getId(), replicationNum);\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), table.getId(), partition.getId(),\n-                newDataProperty, replicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-        LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), table.getId(), partition.getName(),\n-                replicationNum);\n-    }\n-\n-    /**\n-     * Set default replication number for a specified table.\n-     * You can see the default replication number by Show Create Table stmt.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableDefaultReplicationNum(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildReplicationNum();\n-        // log\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyReplicationNum(info);\n-        LOG.debug(\"modify table[{}] replication num to {}\", table.getName(),\n-                properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-    }\n-\n-    // The caller need to hold the db write lock\n-    public void modifyTableInMemoryMeta(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildInMemory();\n-\n-        // need to update partition info meta\n-        for(Partition partition: table.getPartitions()) {\n-            table.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-        }\n-\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyInMemory(info);\n-    }\n-\n-    public void replayModifyTableProperty(short opCode, ModifyTablePropertyOperationLog info) {\n-        long dbId = info.getDbId();\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getProperties();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(tableId);\n-            TableProperty tableProperty = olapTable.getTableProperty();\n-            if (tableProperty == null) {\n-                olapTable.setTableProperty(new TableProperty(properties).buildProperty(opCode));\n-            } else {\n-                tableProperty.modifyTableProperties(properties);\n-                tableProperty.buildProperty(opCode);\n-            }\n-\n-            // need to replay partition info meta\n-            if (opCode == OperationType.OP_MODIFY_IN_MEMORY) {\n-                for(Partition partition: olapTable.getPartitions()) {\n-                    olapTable.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * used for handling AlterClusterStmt\n-     * (for client is the ALTER CLUSTER command).\n-     */\n-    public void alterCluster(AlterSystemStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterCluster(stmt);\n-    }\n-\n-    public void cancelAlterCluster(CancelAlterSystemStmt stmt) throws DdlException {\n-        this.alter.getClusterHandler().cancel(stmt);\n-    }\n-\n-    /*\n-     * generate and check columns' order and key's existence\n-     */\n-    private void validateColumns(List<Column> columns) throws DdlException {\n-        if (columns.isEmpty()) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_COLUMNS);\n-        }\n-\n-        boolean encounterValue = false;\n-        boolean hasKey = false;\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                if (encounterValue) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_OLAP_KEY_MUST_BEFORE_VALUE);\n-                }\n-                hasKey = true;\n-            } else {\n-                encounterValue = true;\n-            }\n-        }\n-\n-        if (!hasKey) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_KEYS);\n-        }\n-    }\n-\n-    // Change current database of this session.\n-    public void changeDb(ConnectContext ctx, String qualifiedDb) throws DdlException {\n-        if (!auth.checkDbPriv(ctx, qualifiedDb, PrivPredicate.SHOW)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_DB_ACCESS_DENIED, ctx.getQualifiedUser(), qualifiedDb);\n-        }\n-\n-        if (getDb(qualifiedDb) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, qualifiedDb);\n-        }\n-\n-        ctx.setDatabase(qualifiedDb);\n-    }\n-\n-    // for test only\n-    public void clear() {\n-        if (SingletonHolder.INSTANCE.idToDb != null) {\n-            SingletonHolder.INSTANCE.idToDb.clear();\n-        }\n-        if (SingletonHolder.INSTANCE.fullNameToDb != null) {\n-            SingletonHolder.INSTANCE.fullNameToDb.clear();\n-        }\n-        if (load.getIdToLoadJob() != null) {\n-            load.getIdToLoadJob().clear();\n-            // load = null;\n-        }\n-\n-        SingletonHolder.INSTANCE.getRollupHandler().unprotectedGetAlterJobs().clear();\n-        SingletonHolder.INSTANCE.getSchemaChangeHandler().unprotectedGetAlterJobs().clear();\n-        System.gc();\n-    }\n-\n-    public void createView(CreateViewStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTable();\n-\n-        // check if db exists\n-        Database db = this.getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create view[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        View newView = new View(tableId, tableName, columns);\n-        newView.setComment(stmt.getComment());\n-        newView.setInlineViewDefWithSqlMode(stmt.getInlineViewDef(),\n-                ConnectContext.get().getSessionVariable().getSqlMode());\n-        // init here in case the stmt string from view.toSql() has some syntax error.\n-        try {\n-            newView.init();\n-        } catch (UserException e) {\n-            throw new DdlException(\"failed to init view stmt\", e);\n-        }\n-      \n-        if (!db.createTableWithLock(newView, false, stmt.isSetIfNotExists())) {\n-            throw new DdlException(\"Failed to create view[\" + tableName + \"].\");\n-        }\n-        LOG.info(\"successfully create view[\" + tableName + \"-\" + newView.getId() + \"]\");\n-    }\n-\n-    /**\n-     * Returns the function that best matches 'desc' that is registered with the\n-     * catalog using 'mode' to check for matching. If desc matches multiple\n-     * functions in the catalog, it will return the function with the strictest\n-     * matching mode. If multiple functions match at the same matching mode,\n-     * ties are broken by comparing argument types in lexical order. Argument\n-     * types are ordered by argument precision (e.g. double is preferred over\n-     * float) and then by alphabetical order of argument type name, to guarantee\n-     * deterministic results.\n-     */\n-    public Function getFunction(Function desc, Function.CompareMode mode) {\n-        return functionSet.getFunction(desc, mode);\n-    }\n-\n-    public List<Function> getBuiltinFunctions() {\n-        return functionSet.getBulitinFunctions();\n-    }\n-\n-    public boolean isNonNullResultWithNullParamFunction(String funcName) {\n-        return functionSet.isNonNullResultWithNullParamFunctions(funcName);\n-    }\n-\n-    /**\n-     * create cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void createCluster(CreateClusterStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_HAS_EXIST, clusterName);\n-            } else {\n-                List<Long> backendList = systemInfo.createCluster(clusterName, stmt.getInstanceNum());\n-                // 1: BE returned is less than requested, throws DdlException.\n-                // 2: BE returned is more than or equal to 0, succeeds.\n-                if (backendList != null || stmt.getInstanceNum() == 0) {\n-                    final long id = getNextId();\n-                    final Cluster cluster = new Cluster(clusterName, id);\n-                    cluster.setBackendIdList(backendList);\n-                    unprotectCreateCluster(cluster);\n-                    if (clusterName.equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                        for (Database db : idToDb.values()) {\n-                            if (db.getClusterName().equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                                cluster.addDb(db.getFullName(), db.getId());\n-                            }\n-                        }\n-                    }\n-                    editLog.logCreateCluster(cluster);\n-                    LOG.info(\"finish to create cluster: {}\", clusterName);\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-            }\n-        } finally {\n-            unlock();\n-        }\n-\n-        // create super user for this cluster\n-        UserIdentity adminUser = new UserIdentity(PaloAuth.ADMIN_USER, \"%\");\n-        try {\n-            adminUser.analyze(stmt.getClusterName());\n-        } catch (AnalysisException e) {\n-            LOG.error(\"should not happen\", e);\n-        }\n-        auth.createUser(new CreateUserStmt(new UserDesc(adminUser, \"\", true)));\n-    }\n-\n-    private void unprotectCreateCluster(Cluster cluster) {\n-        final Iterator<Long> iterator = cluster.getBackendIdList().iterator();\n-        while (iterator.hasNext()) {\n-            final Long id = iterator.next();\n-            final Backend backend = systemInfo.getBackend(id);\n-            backend.setOwnerClusterName(cluster.getName());\n-            backend.setBackendState(BackendState.using);\n-        }\n-\n-        idToCluster.put(cluster.getId(), cluster);\n-        nameToCluster.put(cluster.getName(), cluster);\n-\n-        // create info schema db\n-        final InfoSchemaDb infoDb = new InfoSchemaDb(cluster.getName());\n-        infoDb.setClusterName(cluster.getName());\n-        unprotectCreateDb(infoDb);\n-\n-        // only need to create default cluster once.\n-        if (cluster.getName().equalsIgnoreCase(SystemInfoService.DEFAULT_CLUSTER)) {\n-            isDefaultClusterCreated = true;\n-        }\n-    }\n-\n-    /**\n-     * replay create cluster\n-     *\n-     * @param cluster\n-     */\n-    public void replayCreateCluster(Cluster cluster) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateCluster(cluster);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * drop cluster and cluster's db must be have deleted\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void dropCluster(DropClusterStmt stmt) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            final String clusterName = stmt.getClusterName();\n-            final Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            final List<Backend> backends = systemInfo.getClusterBackends(clusterName);\n-            for (Backend backend : backends) {\n-                if (backend.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            // check if there still have databases undropped, except for information_schema db\n-            if (cluster.getDbNames().size() > 1) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DELETE_DB_EXIST, clusterName);\n-            }\n-\n-            systemInfo.releaseBackends(clusterName, false /* is not replay */);\n-            final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId());\n-            unprotectDropCluster(info, false /* is not replay */);\n-            editLog.logDropCluster(info);\n-        } finally {\n-            unlock();\n-        }\n-\n-        // drop user of this cluster\n-        // set is replay to true, not write log\n-        auth.dropUserOfCluster(stmt.getClusterName(), true /* is replay */);\n-    }\n-\n-    private void unprotectDropCluster(ClusterInfo info, boolean isReplay) {\n-        systemInfo.releaseBackends(info.getClusterName(), isReplay);\n-        idToCluster.remove(info.getClusterId());\n-        nameToCluster.remove(info.getClusterName());\n-        final Database infoSchemaDb = fullNameToDb.get(InfoSchemaDb.getFullInfoSchemaDbName(info.getClusterName()));\n-        fullNameToDb.remove(infoSchemaDb.getFullName());\n-        idToDb.remove(infoSchemaDb.getId());\n-    }\n-\n-    public void replayDropCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            unprotectDropCluster(info, true/* is replay */);\n-        } finally {\n-            unlock();\n-        }\n-\n-        auth.dropUserOfCluster(info.getClusterName(), true /* is replay */);\n-    }\n-\n-    public void replayExpandCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            final Cluster cluster = nameToCluster.get(info.getClusterName());\n-            cluster.addBackends(info.getBackendIdList());\n-\n-            for (Long beId : info.getBackendIdList()) {\n-                Backend be = Catalog.getCurrentSystemInfo().getBackend(beId);\n-                if (be == null) {\n-                    continue;\n-                }\n-                be.setOwnerClusterName(info.getClusterName());\n-                be.setBackendState(BackendState.using);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * modify cluster: Expansion or shrink\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void processModifyCluster(AlterClusterStmt stmt) throws UserException {\n-        final String clusterName = stmt.getAlterClusterName();\n-        final int newInstanceNum = stmt.getInstanceNum();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-\n-            // check if this cluster has backend in decommission\n-            final List<Long> backendIdsInCluster = cluster.getBackendIdList();\n-            for (Long beId : backendIdsInCluster) {\n-                Backend be = systemInfo.getBackend(beId);\n-                if (be.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            final int oldInstanceNum = backendIdsInCluster.size();\n-            if (newInstanceNum > oldInstanceNum) {\n-                // expansion\n-                final List<Long> expandBackendIds = systemInfo.calculateExpansionBackends(clusterName,\n-                        newInstanceNum - oldInstanceNum);\n-                if (expandBackendIds == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-                cluster.addBackends(expandBackendIds);\n-                final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId(), expandBackendIds);\n-                editLog.logExpandCluster(info);\n-            } else if (newInstanceNum < oldInstanceNum) {\n-                // shrink\n-                final List<Long> decomBackendIds = systemInfo.calculateDecommissionBackends(clusterName,\n-                        oldInstanceNum - newInstanceNum);\n-                if (decomBackendIds == null || decomBackendIds.size() == 0) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BACKEND_ERROR);\n-                }\n-\n-                List<String> hostPortList = Lists.newArrayList();\n-                for (Long id : decomBackendIds) {\n-                    final Backend backend = systemInfo.getBackend(id);\n-                    hostPortList.add(new StringBuilder().append(backend.getHost()).append(\":\")\n-                            .append(backend.getHeartbeatPort()).toString());\n-                }\n-\n-                // here we reuse the process of decommission backends. but set backend's decommission type to\n-                // ClusterDecommission, which means this backend will not be removed from the system\n-                // after decommission is done.\n-                final DecommissionBackendClause clause = new DecommissionBackendClause(hostPortList);\n-                try {\n-                    clause.analyze(null);\n-                    clause.setType(DecommissionType.ClusterDecommission);\n-                    AlterSystemStmt alterStmt = new AlterSystemStmt(clause);\n-                    alterStmt.setClusterName(clusterName);\n-                    this.alter.processAlterCluster(alterStmt);\n-                } catch (AnalysisException e) {\n-                    Preconditions.checkState(false, \"should not happend: \" + e.getMessage());\n-                }\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_NO_CHANGE, newInstanceNum);\n-            }\n-\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * @param ctx\n-     * @param clusterName\n-     * @throws DdlException\n-     */\n-    public void changeCluster(ConnectContext ctx, String clusterName) throws DdlException {\n-        if (!Catalog.getCurrentCatalog().getAuth().checkCanEnterCluster(ConnectContext.get(), clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_AUTHORITY,\n-                    ConnectContext.get().getQualifiedUser(), \"enter\");\n-        }\n-\n-        if (!nameToCluster.containsKey(clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-        }\n-\n-        ctx.setCluster(clusterName);\n-    }\n-\n-    /**\n-     * migrate db to link dest cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void migrateDb(MigrateDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-            if (!destCluster.containLink(destDbName, srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-\n-            final Database db = fullNameToDb.get(srcDbName);\n-\n-            // if the max replication num of the src db is larger then the backends num of the dest cluster,\n-            // the migration will not be processed.\n-            final int maxReplicationNum = db.getMaxReplicationNum();\n-            if (maxReplicationNum > destCluster.getBackendIdList().size()) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_BE_NOT_ENOUGH, destClusterName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK) {\n-                final BaseParam param = new BaseParam();\n-                param.addStringParam(destDbName);\n-                param.addLongParam(db.getId());\n-                param.addStringParam(srcDbName);\n-                param.addStringParam(destClusterName);\n-                param.addStringParam(srcClusterName);\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                destCluster.removeLinkDb(param);\n-                destCluster.addDb(destDbName, db.getId());\n-                db.writeLock();\n-                try {\n-                    db.setDbState(DbState.MOVE);\n-                    // set cluster to the dest cluster.\n-                    // and Clone process will do the migration things.\n-                    db.setClusterName(destClusterName);\n-                    db.setName(destDbName);\n-                    db.setAttachDb(srcDbName);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-                editLog.logMigrateCluster(param);\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayMigrateDb(BaseParam param) {\n-        final String desDbName = param.getStringParam();\n-        final String srcDbName = param.getStringParam(1);\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcClusterName = param.getStringParam(3);\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Database db = fullNameToDb.get(srcDbName);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                desCluster.removeLinkDb(param);\n-                desCluster.addDb(param.getStringParam(), db.getId());\n-\n-                db.writeLock();\n-                db.setName(desDbName);\n-                db.setAttachDb(srcDbName);\n-                db.setDbState(DbState.MOVE);\n-                db.setClusterName(desClusterName);\n-                db.writeUnlock();\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayLinkDb(BaseParam param) {\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcDbName = param.getStringParam(1);\n-        final String desDbName = param.getStringParam();\n-\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-            srcDb.writeLock();\n-            srcDb.setDbState(DbState.LINK);\n-            srcDb.setAttachDb(desDbName);\n-            srcDb.writeUnlock();\n-            desCluster.addLinkDb(param);\n-            fullNameToDb.put(desDbName, srcDb);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * link src db to dest db. we use java's quotation Mechanism to realize db hard links\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void linkDb(LinkDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            if (fullNameToDb.containsKey(destDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, destDbName);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-\n-            if (srcDb.getDbState() != DbState.NORMAL) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                        ClusterNamespace.getNameFromFullName(srcDbName));\n-            }\n-\n-            srcDb.writeLock();\n-            try {\n-                srcDb.setDbState(DbState.LINK);\n-                srcDb.setAttachDb(destDbName);\n-            } finally {\n-                srcDb.writeUnlock();\n-            }\n-\n-            final long id = getNextId();\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(destDbName);\n-            param.addStringParam(srcDbName);\n-            param.addLongParam(id);\n-            param.addLongParam(srcDb.getId());\n-            param.addStringParam(destClusterName);\n-            param.addStringParam(srcClusterName);\n-            destCluster.addLinkDb(param);\n-            fullNameToDb.put(destDbName, srcDb);\n-            editLog.logLinkCluster(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Cluster getCluster(String clusterName) {\n-        return nameToCluster.get(clusterName);\n-    }\n-\n-    public List<String> getClusterNames() {\n-        return new ArrayList<String>(nameToCluster.keySet());\n-    }\n-\n-    /**\n-     * get migrate progress , when finish migration, next clonecheck will reset dbState\n-     *\n-     * @return\n-     */\n-    public Set<BaseParam> getMigrations() {\n-        final Set<BaseParam> infos = Sets.newHashSet();\n-        for (Database db : fullNameToDb.values()) {\n-            db.readLock();\n-            try {\n-                if (db.getDbState() == DbState.MOVE) {\n-                    int tabletTotal = 0;\n-                    int tabletQuorum = 0;\n-                    final Set<Long> beIds = Sets.newHashSet(systemInfo.getClusterBackendIds(db.getClusterName()));\n-                    final Set<String> tableNames = db.getTableNamesWithLock();\n-                    for (String tableName : tableNames) {\n-\n-                        Table table = db.getTable(tableName);\n-                        if (table == null || table.getType() != TableType.OLAP) {\n-                            continue;\n-                        }\n-\n-                        OlapTable olapTable = (OlapTable) table;\n-                        for (Partition partition : olapTable.getPartitions()) {\n-                            final short replicationNum = olapTable.getPartitionInfo()\n-                                    .getReplicationNum(partition.getId());\n-                            for (MaterializedIndex materializedIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                                if (materializedIndex.getState() != IndexState.NORMAL) {\n-                                    continue;\n-                                }\n-                                for (Tablet tablet : materializedIndex.getTablets()) {\n-                                    int replicaNum = 0;\n-                                    int quorum = replicationNum / 2 + 1;\n-                                    for (Replica replica : tablet.getReplicas()) {\n-                                        if (replica.getState() != ReplicaState.CLONE\n-                                                && beIds.contains(replica.getBackendId())) {\n-                                            replicaNum++;\n-                                        }\n-                                    }\n-                                    if (replicaNum > quorum) {\n-                                        replicaNum = quorum;\n-                                    }\n-\n-                                    tabletQuorum = tabletQuorum + replicaNum;\n-                                    tabletTotal = tabletTotal + quorum;\n-                                }\n-                            }\n-                        }\n-                    }\n-                    final BaseParam info = new BaseParam();\n-                    info.addStringParam(db.getClusterName());\n-                    info.addStringParam(db.getAttachDb());\n-                    info.addStringParam(db.getFullName());\n-                    final float percentage = tabletTotal > 0 ? (float) tabletQuorum / (float) tabletTotal : 0f;\n-                    info.addFloatParam(percentage);\n-                    infos.add(info);\n-                }\n-            } finally {\n-                db.readUnlock();\n-            }\n-        }\n-\n-        return infos;\n-    }\n-\n-    public long loadCluster(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_30) {\n-            int clusterCount = dis.readInt();\n-            checksum ^= clusterCount;\n-            for (long i = 0; i < clusterCount; ++i) {\n-                final Cluster cluster = Cluster.read(dis);\n-                checksum ^= cluster.getId();\n-\n-                List<Long> latestBackendIds = systemInfo.getClusterBackendIds(cluster.getName());\n-                if (latestBackendIds.size() != cluster.getBackendIdList().size()) {\n-                    LOG.warn(\"Cluster:\" + cluster.getName() + \", backends in Cluster is \"\n-                            + cluster.getBackendIdList().size() + \", backends in SystemInfoService is \"\n-                            + cluster.getBackendIdList().size());\n-                }\n-                // The number of BE in cluster is not same as in SystemInfoService, when perform 'ALTER\n-                // SYSTEM ADD BACKEND TO ...' or 'ALTER SYSTEM ADD BACKEND ...', because both of them are \n-                // for adding BE to some Cluster, but loadCluster is after loadBackend.\n-                cluster.setBackendIdList(latestBackendIds);\n-\n-                String dbName =  InfoSchemaDb.getFullInfoSchemaDbName(cluster.getName());\n-                InfoSchemaDb db;\n-                // Use real Catalog instance to avoid InfoSchemaDb id continuously increment\n-                // when checkpoint thread load image.\n-                if (Catalog.getCurrentCatalog().getFullNameToDb().containsKey(dbName)) {\n-                    db = (InfoSchemaDb)Catalog.getCurrentCatalog().getFullNameToDb().get(dbName);\n-                } else {\n-                    db = new InfoSchemaDb(cluster.getName());\n-                    db.setClusterName(cluster.getName());\n-                }\n-                String errMsg = \"InfoSchemaDb id shouldn't larger than 10000, please restart your FE server\";\n-                // Every time we construct the InfoSchemaDb, which id will increment.\n-                // When InfoSchemaDb id larger than 10000 and put it to idToDb,\n-                // which may be overwrite the normal db meta in idToDb,\n-                // so we ensure InfoSchemaDb id less than 10000.\n-                Preconditions.checkState(db.getId() < NEXT_ID_INIT_VALUE, errMsg);\n-                idToDb.put(db.getId(), db);\n-                fullNameToDb.put(db.getFullName(), db);\n-                cluster.addDb(dbName, db.getId());\n-                idToCluster.put(cluster.getId(), cluster);\n-                nameToCluster.put(cluster.getName(), cluster);\n-            }\n-        }\n-        LOG.info(\"finished replay cluster from image\");\n-        return checksum;\n-    }\n-\n-    public void initDefaultCluster() {\n-        final List<Long> backendList = Lists.newArrayList();\n-        final List<Backend> defaultClusterBackends = systemInfo.getClusterBackends(SystemInfoService.DEFAULT_CLUSTER);\n-        for (Backend backend : defaultClusterBackends) {\n-            backendList.add(backend.getId());\n-        }\n-\n-        final long id = getNextId();\n-        final Cluster cluster = new Cluster(SystemInfoService.DEFAULT_CLUSTER, id);\n-\n-        // make sure one host hold only one backend.\n-        Set<String> beHost = Sets.newHashSet();\n-        for (Backend be : defaultClusterBackends) {\n-            if (beHost.contains(be.getHost())) {\n-                // we can not handle this situation automatically.\n-                LOG.error(\"found more than one backends in same host: {}\", be.getHost());\n-                System.exit(-1);\n-            } else {\n-                beHost.add(be.getHost());\n-            }\n-        }\n-\n-        // we create default_cluster to meet the need for ease of use, because\n-        // most users hava no multi tenant needs.\n-        cluster.setBackendIdList(backendList);\n-        unprotectCreateCluster(cluster);\n-        for (Database db : idToDb.values()) {\n-            db.setClusterName(SystemInfoService.DEFAULT_CLUSTER);\n-            cluster.addDb(db.getFullName(), db.getId());\n-        }\n-\n-        // no matter default_cluster is created or not,\n-        // mark isDefaultClusterCreated as true\n-        isDefaultClusterCreated = true;\n-        editLog.logCreateCluster(cluster);\n-    }\n-\n-    public void replayUpdateDb(DatabaseInfo info) {\n-        final Database db = fullNameToDb.get(info.getDbName());\n-        db.setClusterName(info.getClusterName());\n-        db.setDbState(info.getDbState());\n-    }\n-\n-    public long saveCluster(DataOutputStream dos, long checksum) throws IOException {\n-        final int clusterCount = idToCluster.size();\n-        checksum ^= clusterCount;\n-        dos.writeInt(clusterCount);\n-        for (Map.Entry<Long, Cluster> entry : idToCluster.entrySet()) {\n-            long clusterId = entry.getKey();\n-            if (clusterId >= NEXT_ID_INIT_VALUE) {\n-                checksum ^= clusterId;\n-                final Cluster cluster = entry.getValue();\n-                cluster.write(dos);\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveBrokers(DataOutputStream dos, long checksum) throws IOException {\n-        Map<String, List<FsBroker>> addressListMap = brokerMgr.getBrokerListMap();\n-        int size = addressListMap.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-\n-        for (Map.Entry<String, List<FsBroker>> entry : addressListMap.entrySet()) {\n-            Text.writeString(dos, entry.getKey());\n-            final List<FsBroker> addrs = entry.getValue();\n-            size = addrs.size();\n-            checksum ^= size;\n-            dos.writeInt(size);\n-            for (FsBroker addr : addrs) {\n-                addr.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long loadBrokers(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (MetaContext.get().getMetaVersion() >= FeMetaVersion.VERSION_31) {\n-            int count = dis.readInt();\n-            checksum ^= count;\n-            for (long i = 0; i < count; ++i) {\n-                String brokerName = Text.readString(dis);\n-                int size = dis.readInt();\n-                checksum ^= size;\n-                List<FsBroker> addrs = Lists.newArrayList();\n-                for (int j = 0; j < size; j++) {\n-                    FsBroker addr = FsBroker.readIn(dis);\n-                    addrs.add(addr);\n-                }\n-                brokerMgr.replayAddBrokers(brokerName, addrs);\n-            }\n-            LOG.info(\"finished replay brokerMgr from image\");\n-        }\n-        return checksum;\n-    }\n-\n-    public void replayUpdateClusterAndBackends(BackendIdsUpdateInfo info) {\n-        for (long id : info.getBackendList()) {\n-            final Backend backend = systemInfo.getBackend(id);\n-            final Cluster cluster = nameToCluster.get(backend.getOwnerClusterName());\n-            cluster.removeBackend(id);\n-            backend.setDecommissioned(false);\n-            backend.clearClusterName();\n-            backend.setBackendState(BackendState.free);\n-        }\n-    }\n-\n-    public String dumpImage() {\n-        LOG.info(\"begin to dump meta data\");\n-        String dumpFilePath;\n-        Map<Long, Database> lockedDbMap = Maps.newTreeMap();\n-        tryLock(true);\n-        try {\n-            // sort all dbs\n-            for (long dbId : getDbIds()) {\n-                Database db = getDb(dbId);\n-                Preconditions.checkNotNull(db);\n-                lockedDbMap.put(dbId, db);\n-            }\n-\n-            // lock all dbs\n-            for (Database db : lockedDbMap.values()) {\n-                db.readLock();\n-            }\n-            LOG.info(\"acquired all the dbs' read lock.\");\n-\n-            load.readLock();\n-\n-            LOG.info(\"acquired all jobs' read lock.\");\n-            long journalId = getMaxJournalId();\n-            File dumpFile = new File(Config.meta_dir, \"image.\" + journalId);\n-            dumpFilePath = dumpFile.getAbsolutePath();\n-            try {\n-                LOG.info(\"begin to dump {}\", dumpFilePath);\n-                saveImage(dumpFile, journalId);\n-            } catch (IOException e) {\n-                LOG.error(\"failed to dump image to {}\", dumpFilePath, e);\n-            }\n-        } finally {\n-            // unlock all\n-            load.readUnlock();\n-            for (Database db : lockedDbMap.values()) {\n-                db.readUnlock();\n-            }\n-\n-            unlock();\n-        }\n-\n-        LOG.info(\"finished dumpping image to {}\", dumpFilePath);\n-        return dumpFilePath;\n-    }\n-\n-    /*\n-     * Truncate specified table or partitions.\n-     * The main idea is:\n-     * \n-     * 1. using the same schema to create new table(partitions)\n-     * 2. use the new created table(partitions) to replace the old ones.\n-     * \n-     * if no partition specified, it will truncate all partitions of this table, including all temp partitions,\n-     * otherwise, it will only truncate those specified partitions.\n-     * \n-     */\n-    public void truncateTable(TruncateTableStmt truncateTableStmt) throws DdlException {\n-        TableRef tblRef = truncateTableStmt.getTblRef();\n-        TableName dbTbl = tblRef.getName();\n-\n-        // check, and save some info which need to be checked again later\n-        Map<String, Long> origPartitions = Maps.newHashMap();\n-        OlapTable copiedTbl = null;\n-        Database db = getDb(dbTbl.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbTbl.getDb());\n-        }\n-\n-        boolean truncateEntireTable = tblRef.getPartitionNames() == null;\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(dbTbl.getTbl());\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, dbTbl.getTbl());\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Only support truncate OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-            \n-            if (!truncateEntireTable) {\n-                for (String partName : tblRef.getPartitionNames().getPartitionNames()) {\n-                    Partition partition = olapTable.getPartition(partName);\n-                    if (partition == null) {\n-                        throw new DdlException(\"Partition \" + partName + \" does not exist\");\n-                    }\n-                    \n-                    origPartitions.put(partName, partition.getId());\n-                }\n-            } else {\n-                for (Partition partition : olapTable.getPartitions()) {\n-                    origPartitions.put(partition.getName(), partition.getId());\n-                }\n-            }\n-            \n-            copiedTbl = olapTable.selectiveCopy(origPartitions.keySet(), true, IndexExtState.VISIBLE);\n-        } finally {\n-            db.readUnlock();\n-        }\n-        \n-        // 2. use the copied table to create partitions\n-        List<Partition> newPartitions = Lists.newArrayList();\n-        // tabletIdSet to save all newly created tablet ids.\n-        Set<Long> tabletIdSet = Sets.newHashSet();\n-        try {\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                // the new partition must use new id\n-                // If we still use the old partition id, the behavior of current load jobs on this partition\n-                // will be undefined.\n-                // By using a new id, load job will be aborted(just like partition is dropped),\n-                // which is the right behavior.\n-                long oldPartitionId = entry.getValue();\n-                long newPartitionId = getNextId();\n-                Partition newPartition = createPartitionWithIndices(db.getClusterName(),\n-                        db.getId(), copiedTbl.getId(), copiedTbl.getBaseIndexId(),\n-                        newPartitionId, entry.getKey(),\n-                        copiedTbl.getIndexIdToMeta(),\n-                        copiedTbl.getKeysType(),\n-                        copiedTbl.getDefaultDistributionInfo(),\n-                        copiedTbl.getPartitionInfo().getDataProperty(oldPartitionId).getStorageMedium(),\n-                        copiedTbl.getPartitionInfo().getReplicationNum(oldPartitionId),\n-                        null /* version info */,\n-                        copiedTbl.getCopiedBfColumns(),\n-                        copiedTbl.getBfFpp(),\n-                        tabletIdSet,\n-                        copiedTbl.getCopiedIndexes(),\n-                        copiedTbl.isInMemory(),\n-                        copiedTbl.getStorageFormat(),\n-                        copiedTbl.getPartitionInfo().getTabletType(oldPartitionId));\n-                newPartitions.add(newPartition);\n-            }\n-        } catch (DdlException e) {\n-            // create partition failed, remove all newly created tablets\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-        Preconditions.checkState(origPartitions.size() == newPartitions.size());\n-\n-        // all partitions are created successfully, try to replace the old partitions.\n-        // before replacing, we need to check again.\n-        // Things may be changed outside the database lock.\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(copiedTbl.getId());\n-            if (olapTable == null) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"] is dropped\");\n-            }\n-            \n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-\n-            // check partitions\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                Partition partition = copiedTbl.getPartition(entry.getValue());\n-                if (partition == null || !partition.getName().equals(entry.getKey())) {\n-                    throw new DdlException(\"Partition [\" + entry.getKey() + \"] is changed\");\n-                }\n-            }\n-\n-            // check if meta changed\n-            // rollup index may be added or dropped, and schema may be changed during creating partition operation.\n-            boolean metaChanged = false;\n-            if (olapTable.getIndexNameToId().size() != copiedTbl.getIndexNameToId().size()) {\n-                metaChanged = true;\n-            } else {\n-                // compare schemaHash\n-                Map<Long, Integer> copiedIndexIdToSchemaHash = copiedTbl.getIndexIdToSchemaHash();\n-                for (Map.Entry<Long, Integer> entry : olapTable.getIndexIdToSchemaHash().entrySet()) {\n-                    long indexId = entry.getKey();\n-                    if (!copiedIndexIdToSchemaHash.containsKey(indexId)) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                    if (!copiedIndexIdToSchemaHash.get(indexId).equals(entry.getValue())) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            if (metaChanged) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"]'s meta has been changed. try again.\");\n-            }\n-\n-            // replace\n-            truncateTableInternal(olapTable, newPartitions, truncateEntireTable);\n-\n-            // write edit log\n-            TruncateTableInfo info = new TruncateTableInfo(db.getId(), olapTable.getId(), newPartitions,\n-                    truncateEntireTable);\n-            editLog.logTruncateTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-        \n-        LOG.info(\"finished to truncate table {}, partitions: {}\",\n-                tblRef.getName().toSql(), tblRef.getPartitionNames());\n-    }\n-\n-    private void truncateTableInternal(OlapTable olapTable, List<Partition> newPartitions, boolean isEntireTable) {\n-        // use new partitions to replace the old ones.\n-        Set<Long> oldTabletIds = Sets.newHashSet();\n-        for (Partition newPartition : newPartitions) {\n-            Partition oldPartition = olapTable.replacePartition(newPartition);\n-            // save old tablets to be removed\n-            for (MaterializedIndex index : oldPartition.getMaterializedIndices(IndexExtState.ALL)) {\n-                index.getTablets().stream().forEach(t -> {\n-                    oldTabletIds.add(t.getId());\n-                });\n-            }\n-        }\n-\n-        if (isEntireTable) {\n-            // drop all temp partitions\n-            olapTable.dropAllTempPartitions();\n-        }\n-\n-        // remove the tablets in old partitions\n-        for (Long tabletId : oldTabletIds) {\n-            Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-        }\n-    }\n-\n-    public void replayTruncateTable(TruncateTableInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTblId());\n-            truncateTableInternal(olapTable, info.getPartitions(), info.isEntireTable());\n-\n-            if (!Catalog.isCheckpointThread()) {\n-                // add tablet to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (Partition partition : info.getPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(db.getId(), olapTable.getId(),\n-                                partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void createFunction(CreateFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.addFunction(stmt.getFunction());\n-    }\n-\n-    public void replayCreateFunction(Function function) {\n-        String dbName = function.getFunctionName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayAddFunction(function);\n-    }\n-\n-    public void dropFunction(DropFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.dropFunction(stmt.getFunction());\n-    }\n-\n-    public void replayDropFunction(FunctionSearchDesc functionSearchDesc) {\n-        String dbName = functionSearchDesc.getName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayDropFunction(functionSearchDesc);\n-    }\n-\n-    public void setConfig(AdminSetConfigStmt stmt) throws DdlException {\n-        Map<String, String> configs = stmt.getConfigs();\n-        Preconditions.checkState(configs.size() == 1);\n-\n-        for (Map.Entry<String, String> entry : configs.entrySet()) {\n-            ConfigBase.setMutableConfig(entry.getKey(), entry.getValue());\n-        }\n-    }\n-\n-    public void replayBackendTabletsInfo(BackendTabletsInfo backendTabletsInfo) {\n-        List<Pair<Long, Integer>> tabletsWithSchemaHash = backendTabletsInfo.getTabletSchemaHash();\n-        for (Pair<Long, Integer> tabletInfo : tabletsWithSchemaHash) {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletInfo.first,\n-                    backendTabletsInfo.getBackendId());\n-            if (replica == null) {\n-                LOG.warn(\"replica does not found when replay. tablet {}, backend {}\",\n-                        tabletInfo.first, backendTabletsInfo.getBackendId());\n-                continue;\n-            }\n-\n-            if (replica.getSchemaHash() != tabletInfo.second) {\n-                continue;\n-            }\n-\n-            replica.setBad(backendTabletsInfo.isBad());\n-        }\n-    }\n-\n-    // Convert table's distribution type from random to hash.\n-    // random distribution is no longer supported.\n-    public void convertDistributionType(Database db, OlapTable tbl) throws DdlException {\n-        db.writeLock();\n-        try {\n-            if (!tbl.convertRandomDistributionToHashDistribution()) {\n-                throw new DdlException(\"Table \" + tbl.getName() + \" is not random distributed\");\n-            }\n-            TableInfo tableInfo = TableInfo.createForModifyDistribution(db.getId(), tbl.getId());\n-            editLog.logModifyDistributionType(tableInfo);\n-            LOG.info(\"finished to modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayConvertDistributionType(TableInfo tableInfo) {\n-        Database db = getDb(tableInfo.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable tbl = (OlapTable) db.getTable(tableInfo.getTableId());\n-            tbl.convertRandomDistributionToHashDistribution();\n-            LOG.info(\"replay modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * The entry of replacing partitions with temp partitions.\n-     */\n-    public void replaceTempPartition(Database db, String tableName, ReplacePartitionClause clause) throws DdlException {\n-        List<String> partitionNames = clause.getPartitionNames();\n-        List<String> tempPartitonNames = clause.getTempPartitionNames();\n-        boolean isStrictRange = clause.isStrictRange();\n-        boolean useTempPartitionName = clause.useTempPartitionName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            // check partition exist\n-            for (String partName : partitionNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, false)) {\n-                    throw new DdlException(\"Partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-            for (String partName : tempPartitonNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, true)) {\n-                    throw new DdlException(\"Temp partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-\n-            olapTable.replaceTempPartitions(partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-\n-            // write log\n-            ReplacePartitionOperationLog info = new ReplacePartitionOperationLog(db.getId(), olapTable.getId(),\n-                    partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-            editLog.logReplaceTempPartition(info);\n-            LOG.info(\"finished to replace partitions {} with temp partitions {} from table: {}\",\n-                    clause.getPartitionNames(), clause.getTempPartitionNames(), tableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayReplaceTempPartition(ReplacePartitionOperationLog replaceTempPartitionLog) {\n-        Database db = getDb(replaceTempPartitionLog.getDbId());\n-        if (db == null) {\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(replaceTempPartitionLog.getTblId());\n-            if (olapTable == null) {\n-                return;\n-            }\n-            olapTable.replaceTempPartitions(replaceTempPartitionLog.getPartitions(),\n-                    replaceTempPartitionLog.getTempPartitions(),\n-                    replaceTempPartitionLog.isStrictRange(),\n-                    replaceTempPartitionLog.useTempPartitionName());\n-        } catch (DdlException e) {\n-            LOG.warn(\"should not happen. {}\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void installPlugin(InstallPluginStmt stmt) throws UserException, IOException {\n-        pluginMgr.installPlugin(stmt);\n-    }\n-\n-    public long savePlugins(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentPluginMgr().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPlugins(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_78) {\n-            Catalog.getCurrentPluginMgr().readFields(dis);\n-        }\n-        LOG.info(\"finished replay plugins from image\");\n-        return checksum;\n-    }\n-\n-    public void replayInstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.replayLoadDynamicPlugin(pluginInfo);\n-        } catch (Exception e) {\n-            LOG.warn(\"replay install plugin failed.\", e);\n-        }\n-    }\n-\n-    public void uninstallPlugin(UninstallPluginStmt stmt) throws IOException, UserException {\n-        PluginInfo info = pluginMgr.uninstallPlugin(stmt.getPluginName());\n-        if (null != info) {\n-            editLog.logUninstallPlugin(info);\n-        }\n-        LOG.info(\"uninstall plugin = \" + stmt.getPluginName());\n-    }\n-\n-    public void replayUninstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.uninstallPlugin(pluginInfo.getName());\n-        } catch (Exception e) {\n-            LOG.warn(\"replay uninstall plugin failed.\", e);\n-        }\n-    }\n-\n-    // entry of checking tablets operation\n-    public void checkTablets(AdminCheckTabletsStmt stmt) {\n-        CheckType type = stmt.getType();\n-        switch (type) {\n-            case CONSISTENCY:\n-                consistencyChecker.addTabletsToCheck(stmt.getTabletIds());\n-                break;\n-            default:\n-                break;\n-        }\n-    }\n-\n-    // Set specified replica's status. If replica does not exist, just ignore it.\n-    public void setReplicaStatus(AdminSetReplicaStatusStmt stmt) {\n-        long tabletId = stmt.getTabletId();\n-        long backendId = stmt.getBackendId();\n-        ReplicaStatus status = stmt.getStatus();\n-        setReplicaStatusInternal(tabletId, backendId, status, false);\n-    }\n-\n-    public void replaySetReplicaStatus(SetReplicaStatusOperationLog log) {\n-        setReplicaStatusInternal(log.getTabletId(), log.getBackendId(), log.getReplicaStatus(), true);\n-    }\n-\n-    private void setReplicaStatusInternal(long tabletId, long backendId, ReplicaStatus status, boolean isReplay) {\n-        TabletMeta meta = tabletInvertedIndex.getTabletMeta(tabletId);\n-        if (meta == null) {\n-            LOG.info(\"tablet {} does not exist\", tabletId);\n-            return;\n-        }\n-        long dbId = meta.getDbId();\n-        Database db = getDb(dbId);\n-        if (db == null) {\n-            LOG.info(\"database {} of tablet {} does not exist\", dbId, tabletId);\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletId, backendId);\n-            if (replica == null) {\n-                LOG.info(\"replica of tablet {} does not exist\", tabletId);\n-                return;\n-            }\n-            if (status == ReplicaStatus.BAD || status == ReplicaStatus.OK) {\n-                if (replica.setBad(status == ReplicaStatus.BAD)) {\n-                    if (!isReplay) {\n-                        SetReplicaStatusOperationLog log = new SetReplicaStatusOperationLog(backendId, tabletId, status);\n-                        getEditLog().logSetReplicaStatus(log);\n-                    }\n-                    LOG.info(\"set replica {} of tablet {} on backend {} as {}. is replay: {}\",\n-                            replica.getId(), tabletId, backendId, status, isReplay);\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-}\n-\n"}}, {"oid": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "url": "https://github.com/apache/incubator-doris/commit/4604338ca6d4f35a10aef464b51e2e4b839488aa", "message": "make force drop operation do not recycle meta", "committedDate": "2020-07-23T02:39:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkwNTc1Nw==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r456905757", "bodyText": "DROP database FORCE. Use upper case will be better.", "author": "chaoyli", "createdAt": "2020-07-19T12:56:41Z", "path": "fe/src/main/java/org/apache/doris/catalog/Catalog.java", "diffHunk": "@@ -2666,6 +2666,13 @@ public void dropDb(DropDbStmt stmt) throws DdlException {\n             Database db = this.fullNameToDb.get(dbName);\n             db.writeLock();\n             try {\n+                if (stmt.isNeedCheckCommittedTxns()) {\n+                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), null, null)) {\n+                       throw new DdlException(\"There are still some transactions in the COMMITTED state waiting to be completed. \" +\n+                               \"The database [\" + dbName +\"] cannot be dropped. If you want to forcibly drop(cannot be recovered),\" +\n+                               \" please use \\\"DROP database force\\\".\");\n+                    }", "originalCommit": "a6da313d589cef7426bfdad515ec89ea8f0c866d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY5Mjc0NA==", "url": "https://github.com/apache/incubator-doris/pull/4029#discussion_r460692744", "bodyText": "ok", "author": "caiconghui", "createdAt": "2020-07-27T07:21:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjkwNTc1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "4604338ca6d4f35a10aef464b51e2e4b839488aa", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/catalog/Catalog.java b/fe/src/main/java/org/apache/doris/catalog/Catalog.java\ndeleted file mode 100755\nindex 51ffff8b5..000000000\n--- a/fe/src/main/java/org/apache/doris/catalog/Catalog.java\n+++ /dev/null\n\n@@ -1,6780 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.catalog;\n-\n-import org.apache.doris.alter.Alter;\n-import org.apache.doris.alter.AlterJob;\n-import org.apache.doris.alter.AlterJob.JobType;\n-import org.apache.doris.alter.AlterJobV2;\n-import org.apache.doris.alter.DecommissionBackendJob.DecommissionType;\n-import org.apache.doris.alter.MaterializedViewHandler;\n-import org.apache.doris.alter.SchemaChangeHandler;\n-import org.apache.doris.alter.SystemHandler;\n-import org.apache.doris.analysis.AddPartitionClause;\n-import org.apache.doris.analysis.AddRollupClause;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt;\n-import org.apache.doris.analysis.AdminCheckTabletsStmt.CheckType;\n-import org.apache.doris.analysis.AdminSetConfigStmt;\n-import org.apache.doris.analysis.AdminSetReplicaStatusStmt;\n-import org.apache.doris.analysis.AlterClause;\n-import org.apache.doris.analysis.AlterClusterStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt;\n-import org.apache.doris.analysis.AlterDatabaseQuotaStmt.QuotaType;\n-import org.apache.doris.analysis.AlterDatabaseRename;\n-import org.apache.doris.analysis.AlterSystemStmt;\n-import org.apache.doris.analysis.AlterTableStmt;\n-import org.apache.doris.analysis.AlterViewStmt;\n-import org.apache.doris.analysis.BackupStmt;\n-import org.apache.doris.analysis.CancelAlterSystemStmt;\n-import org.apache.doris.analysis.CancelAlterTableStmt;\n-import org.apache.doris.analysis.CancelBackupStmt;\n-import org.apache.doris.analysis.ColumnRenameClause;\n-import org.apache.doris.analysis.CreateClusterStmt;\n-import org.apache.doris.analysis.CreateDbStmt;\n-import org.apache.doris.analysis.CreateFunctionStmt;\n-import org.apache.doris.analysis.CreateMaterializedViewStmt;\n-import org.apache.doris.analysis.CreateTableStmt;\n-import org.apache.doris.analysis.CreateUserStmt;\n-import org.apache.doris.analysis.CreateViewStmt;\n-import org.apache.doris.analysis.DecommissionBackendClause;\n-import org.apache.doris.analysis.DistributionDesc;\n-import org.apache.doris.analysis.DropClusterStmt;\n-import org.apache.doris.analysis.DropDbStmt;\n-import org.apache.doris.analysis.DropFunctionStmt;\n-import org.apache.doris.analysis.DropMaterializedViewStmt;\n-import org.apache.doris.analysis.DropPartitionClause;\n-import org.apache.doris.analysis.DropTableStmt;\n-import org.apache.doris.analysis.FunctionName;\n-import org.apache.doris.analysis.InstallPluginStmt;\n-import org.apache.doris.analysis.KeysDesc;\n-import org.apache.doris.analysis.LinkDbStmt;\n-import org.apache.doris.analysis.MigrateDbStmt;\n-import org.apache.doris.analysis.PartitionDesc;\n-import org.apache.doris.analysis.PartitionRenameClause;\n-import org.apache.doris.analysis.RangePartitionDesc;\n-import org.apache.doris.analysis.RecoverDbStmt;\n-import org.apache.doris.analysis.RecoverPartitionStmt;\n-import org.apache.doris.analysis.RecoverTableStmt;\n-import org.apache.doris.analysis.ReplacePartitionClause;\n-import org.apache.doris.analysis.RestoreStmt;\n-import org.apache.doris.analysis.RollupRenameClause;\n-import org.apache.doris.analysis.ShowAlterStmt.AlterType;\n-import org.apache.doris.analysis.SingleRangePartitionDesc;\n-import org.apache.doris.analysis.TableName;\n-import org.apache.doris.analysis.TableRef;\n-import org.apache.doris.analysis.TableRenameClause;\n-import org.apache.doris.analysis.TruncateTableStmt;\n-import org.apache.doris.analysis.UninstallPluginStmt;\n-import org.apache.doris.analysis.UserDesc;\n-import org.apache.doris.analysis.UserIdentity;\n-import org.apache.doris.backup.BackupHandler;\n-import org.apache.doris.catalog.ColocateTableIndex.GroupId;\n-import org.apache.doris.catalog.Database.DbState;\n-import org.apache.doris.catalog.DistributionInfo.DistributionInfoType;\n-import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n-import org.apache.doris.catalog.MaterializedIndex.IndexState;\n-import org.apache.doris.catalog.OlapTable.OlapTableState;\n-import org.apache.doris.catalog.Replica.ReplicaState;\n-import org.apache.doris.catalog.Replica.ReplicaStatus;\n-import org.apache.doris.catalog.Table.TableType;\n-import org.apache.doris.clone.ColocateTableBalancer;\n-import org.apache.doris.clone.DynamicPartitionScheduler;\n-import org.apache.doris.clone.TabletChecker;\n-import org.apache.doris.clone.TabletScheduler;\n-import org.apache.doris.clone.TabletSchedulerStat;\n-import org.apache.doris.cluster.BaseParam;\n-import org.apache.doris.cluster.Cluster;\n-import org.apache.doris.cluster.ClusterNamespace;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.Config;\n-import org.apache.doris.common.ConfigBase;\n-import org.apache.doris.common.DdlException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.FeConstants;\n-import org.apache.doris.common.FeMetaVersion;\n-import org.apache.doris.common.MarkedCountDownLatch;\n-import org.apache.doris.common.MetaNotFoundException;\n-import org.apache.doris.common.Pair;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.common.io.Text;\n-import org.apache.doris.common.util.Daemon;\n-import org.apache.doris.common.util.DynamicPartitionUtil;\n-import org.apache.doris.common.util.MasterDaemon;\n-import org.apache.doris.common.util.PrintableMap;\n-import org.apache.doris.common.util.PropertyAnalyzer;\n-import org.apache.doris.common.util.QueryableReentrantLock;\n-import org.apache.doris.common.util.SmallFileMgr;\n-import org.apache.doris.common.util.TimeUtils;\n-import org.apache.doris.common.util.Util;\n-import org.apache.doris.consistency.ConsistencyChecker;\n-import org.apache.doris.deploy.DeployManager;\n-import org.apache.doris.deploy.impl.AmbariDeployManager;\n-import org.apache.doris.deploy.impl.K8sDeployManager;\n-import org.apache.doris.deploy.impl.LocalFileDeployManager;\n-import org.apache.doris.external.elasticsearch.EsRepository;\n-import org.apache.doris.ha.BDBHA;\n-import org.apache.doris.ha.FrontendNodeType;\n-import org.apache.doris.ha.HAProtocol;\n-import org.apache.doris.ha.MasterInfo;\n-import org.apache.doris.http.meta.MetaBaseAction;\n-import org.apache.doris.journal.JournalCursor;\n-import org.apache.doris.journal.JournalEntity;\n-import org.apache.doris.journal.bdbje.Timestamp;\n-import org.apache.doris.load.DeleteHandler;\n-import org.apache.doris.load.DeleteInfo;\n-import org.apache.doris.load.ExportChecker;\n-import org.apache.doris.load.ExportJob;\n-import org.apache.doris.load.ExportMgr;\n-import org.apache.doris.load.Load;\n-import org.apache.doris.load.LoadChecker;\n-import org.apache.doris.load.LoadErrorHub;\n-import org.apache.doris.load.LoadJob;\n-import org.apache.doris.load.LoadJob.JobState;\n-import org.apache.doris.load.loadv2.LoadEtlChecker;\n-import org.apache.doris.load.loadv2.LoadJobScheduler;\n-import org.apache.doris.load.loadv2.LoadLoadingChecker;\n-import org.apache.doris.load.loadv2.LoadManager;\n-import org.apache.doris.load.loadv2.LoadTimeoutChecker;\n-import org.apache.doris.load.routineload.RoutineLoadManager;\n-import org.apache.doris.load.routineload.RoutineLoadScheduler;\n-import org.apache.doris.load.routineload.RoutineLoadTaskScheduler;\n-import org.apache.doris.master.Checkpoint;\n-import org.apache.doris.master.MetaHelper;\n-import org.apache.doris.meta.MetaContext;\n-import org.apache.doris.metric.MetricRepo;\n-import org.apache.doris.mysql.privilege.PaloAuth;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.persist.BackendIdsUpdateInfo;\n-import org.apache.doris.persist.BackendTabletsInfo;\n-import org.apache.doris.persist.ClusterInfo;\n-import org.apache.doris.persist.ColocatePersistInfo;\n-import org.apache.doris.persist.DatabaseInfo;\n-import org.apache.doris.persist.DropInfo;\n-import org.apache.doris.persist.DropLinkDbAndUpdateDbInfo;\n-import org.apache.doris.persist.DropPartitionInfo;\n-import org.apache.doris.persist.EditLog;\n-import org.apache.doris.persist.ModifyPartitionInfo;\n-import org.apache.doris.persist.ModifyTablePropertyOperationLog;\n-import org.apache.doris.persist.OperationType;\n-import org.apache.doris.persist.PartitionPersistInfo;\n-import org.apache.doris.persist.RecoverInfo;\n-import org.apache.doris.persist.ReplacePartitionOperationLog;\n-import org.apache.doris.persist.ReplicaPersistInfo;\n-import org.apache.doris.persist.SetReplicaStatusOperationLog;\n-import org.apache.doris.persist.Storage;\n-import org.apache.doris.persist.StorageInfo;\n-import org.apache.doris.persist.TableInfo;\n-import org.apache.doris.persist.TablePropertyInfo;\n-import org.apache.doris.persist.TruncateTableInfo;\n-import org.apache.doris.plugin.PluginInfo;\n-import org.apache.doris.plugin.PluginMgr;\n-import org.apache.doris.qe.AuditEventProcessor;\n-import org.apache.doris.qe.ConnectContext;\n-import org.apache.doris.qe.JournalObservable;\n-import org.apache.doris.qe.SessionVariable;\n-import org.apache.doris.qe.VariableMgr;\n-import org.apache.doris.service.FrontendOptions;\n-import org.apache.doris.system.Backend;\n-import org.apache.doris.system.Backend.BackendState;\n-import org.apache.doris.system.Frontend;\n-import org.apache.doris.system.HeartbeatMgr;\n-import org.apache.doris.system.SystemInfoService;\n-import org.apache.doris.task.AgentBatchTask;\n-import org.apache.doris.task.AgentTaskExecutor;\n-import org.apache.doris.task.AgentTaskQueue;\n-import org.apache.doris.task.CreateReplicaTask;\n-import org.apache.doris.task.MasterTaskExecutor;\n-import org.apache.doris.task.PullLoadJobMgr;\n-import org.apache.doris.thrift.TStorageFormat;\n-import org.apache.doris.thrift.TStorageMedium;\n-import org.apache.doris.thrift.TStorageType;\n-import org.apache.doris.thrift.TTabletType;\n-import org.apache.doris.thrift.TTaskType;\n-import org.apache.doris.transaction.GlobalTransactionMgr;\n-import org.apache.doris.transaction.PublishVersionDaemon;\n-\n-import com.google.common.base.Joiner;\n-import com.google.common.base.Joiner.MapJoiner;\n-import com.google.common.base.Preconditions;\n-import com.google.common.base.Strings;\n-import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import com.google.common.collect.Multimap;\n-import com.google.common.collect.Queues;\n-import com.google.common.collect.Range;\n-import com.google.common.collect.Sets;\n-import com.sleepycat.je.rep.InsufficientLogException;\n-import com.sleepycat.je.rep.NetworkRestore;\n-import com.sleepycat.je.rep.NetworkRestoreConfig;\n-\n-import org.apache.commons.collections.CollectionUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.codehaus.jackson.map.ObjectMapper;\n-\n-import java.io.BufferedInputStream;\n-import java.io.DataInputStream;\n-import java.io.DataOutputStream;\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.net.HttpURLConnection;\n-import java.net.URL;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-import java.util.Set;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicLong;\n-\n-\n-public class Catalog {\n-    private static final Logger LOG = LogManager.getLogger(Catalog.class);\n-    // 0 ~ 9999 used for qe\n-    public static final long NEXT_ID_INIT_VALUE = 10000;\n-    private static final int HTTP_TIMEOUT_SECOND = 5;\n-    private static final int STATE_CHANGE_CHECK_INTERVAL_MS = 100;\n-    private static final int REPLAY_INTERVAL_MS = 1;\n-    private static final String BDB_DIR = \"/bdb\";\n-    private static final String IMAGE_DIR = \"/image\";\n-\n-    private String metaDir;\n-    private String bdbDir;\n-    private String imageDir;\n-\n-    private MetaContext metaContext;\n-    private long epoch = 0;\n-\n-    // Lock to perform atomic modification on map like 'idToDb' and 'fullNameToDb'.\n-    // These maps are all thread safe, we only use lock to perform atomic operations.\n-    // Operations like Get or Put do not need lock.\n-    // We use fair ReentrantLock to avoid starvation. Do not use this lock in critical code pass\n-    // because fair lock has poor performance.\n-    // Using QueryableReentrantLock to print owner thread in debug mode.\n-    private QueryableReentrantLock lock;\n-\n-    private ConcurrentHashMap<Long, Database> idToDb;\n-    private ConcurrentHashMap<String, Database> fullNameToDb;\n-\n-    private ConcurrentHashMap<Long, Cluster> idToCluster;\n-    private ConcurrentHashMap<String, Cluster> nameToCluster;\n-\n-    private Load load;\n-    private LoadManager loadManager;\n-    private RoutineLoadManager routineLoadManager;\n-    private ExportMgr exportMgr;\n-    private Alter alter;\n-    private ConsistencyChecker consistencyChecker;\n-    private BackupHandler backupHandler;\n-    private PublishVersionDaemon publishVersionDaemon;\n-    private DeleteHandler deleteHandler;\n-\n-    private MasterDaemon labelCleaner; // To clean old LabelInfo, ExportJobInfos\n-    private MasterDaemon txnCleaner; // To clean aborted or timeout txns\n-    private Daemon replayer;\n-    private Daemon timePrinter;\n-    private Daemon listener;\n-    private EsRepository esRepository;  // it is a daemon, so add it here\n-\n-    private boolean isFirstTimeStartUp = false;\n-    private boolean isElectable;\n-    // set to true after finished replay all meta and ready to serve\n-    // set to false when catalog is not ready.\n-    private AtomicBoolean isReady = new AtomicBoolean(false);\n-    // set to true if FE can offer READ service.\n-    // canRead can be true even if isReady is false.\n-    // for example: OBSERVER transfer to UNKNOWN, then isReady will be set to false, but canRead can still be true\n-    private AtomicBoolean canRead = new AtomicBoolean(false);\n-    private BlockingQueue<FrontendNodeType> typeTransferQueue;\n-\n-    // false if default_cluster is not created.\n-    private boolean isDefaultClusterCreated = false;\n-\n-    // node name is used for bdbje NodeName.\n-    private String nodeName;\n-    private FrontendNodeType role;\n-    private FrontendNodeType feType;\n-    // replica and observer use this value to decide provide read service or not\n-    private long synchronizedTimeMs;\n-    private int masterRpcPort;\n-    private int masterHttpPort;\n-    private String masterIp;\n-\n-    private CatalogIdGenerator idGenerator = new CatalogIdGenerator(NEXT_ID_INIT_VALUE);\n-\n-    private EditLog editLog;\n-    private int clusterId;\n-    private String token;\n-    // For checkpoint and observer memory replayed marker\n-    private AtomicLong replayedJournalId;\n-\n-    private static Catalog CHECKPOINT = null;\n-    private static long checkpointThreadId = -1;\n-    private Checkpoint checkpointer;\n-    private List<Pair<String, Integer>> helperNodes = Lists.newArrayList();\n-    private Pair<String, Integer> selfNode = null;\n-\n-    // node name -> Frontend\n-    private ConcurrentHashMap<String, Frontend> frontends;\n-    // removed frontends' name. used for checking if name is duplicated in bdbje\n-    private ConcurrentLinkedQueue<String> removedFrontends;\n-\n-    private HAProtocol haProtocol = null;\n-\n-    private JournalObservable journalObservable;\n-\n-    private SystemInfoService systemInfo;\n-    private HeartbeatMgr heartbeatMgr;\n-    private TabletInvertedIndex tabletInvertedIndex;\n-    private ColocateTableIndex colocateTableIndex;\n-\n-    private CatalogRecycleBin recycleBin;\n-    private FunctionSet functionSet;\n-\n-    private MetaReplayState metaReplayState;\n-\n-    private PullLoadJobMgr pullLoadJobMgr;\n-    private BrokerMgr brokerMgr;\n-    private ResourceMgr resourceMgr;\n-\n-    private GlobalTransactionMgr globalTransactionMgr;\n-\n-    private DeployManager deployManager;\n-\n-    private TabletStatMgr tabletStatMgr;\n-\n-    private PaloAuth auth;\n-\n-    private DomainResolver domainResolver;\n-\n-    private TabletSchedulerStat stat;\n-\n-    private TabletScheduler tabletScheduler;\n-\n-    private TabletChecker tabletChecker;\n-\n-    private MasterTaskExecutor loadTaskScheduler;\n-\n-    private LoadJobScheduler loadJobScheduler;\n-\n-    private LoadTimeoutChecker loadTimeoutChecker;\n-    private LoadEtlChecker loadEtlChecker;\n-    private LoadLoadingChecker loadLoadingChecker;\n-\n-    private RoutineLoadScheduler routineLoadScheduler;\n-\n-    private RoutineLoadTaskScheduler routineLoadTaskScheduler;\n-\n-    private SmallFileMgr smallFileMgr;\n-\n-    private DynamicPartitionScheduler dynamicPartitionScheduler;\n-    \n-    private PluginMgr pluginMgr;\n-\n-    private AuditEventProcessor auditEventProcessor;\n-\n-    public List<Frontend> getFrontends(FrontendNodeType nodeType) {\n-        if (nodeType == null) {\n-            // get all\n-            return Lists.newArrayList(frontends.values());\n-        }\n-\n-        List<Frontend> result = Lists.newArrayList();\n-        for (Frontend frontend : frontends.values()) {\n-            if (frontend.getRole() == nodeType) {\n-                result.add(frontend);\n-            }\n-        }\n-\n-        return result;\n-    }\n-\n-    public List<String> getRemovedFrontendNames() {\n-        return Lists.newArrayList(removedFrontends);\n-    }\n-\n-    public JournalObservable getJournalObservable() {\n-        return journalObservable;\n-    }\n-\n-    private SystemInfoService getClusterInfo() {\n-        return this.systemInfo;\n-    }\n-\n-    private HeartbeatMgr getHeartbeatMgr() {\n-        return this.heartbeatMgr;\n-    }\n-\n-    public TabletInvertedIndex getTabletInvertedIndex() {\n-        return this.tabletInvertedIndex;\n-    }\n-\n-    // only for test\n-    public void setColocateTableIndex(ColocateTableIndex colocateTableIndex) {\n-        this.colocateTableIndex = colocateTableIndex;\n-    }\n-\n-    public ColocateTableIndex getColocateTableIndex() {\n-        return this.colocateTableIndex;\n-    }\n-\n-    private CatalogRecycleBin getRecycleBin() {\n-        return this.recycleBin;\n-    }\n-\n-    public MetaReplayState getMetaReplayState() {\n-        return metaReplayState;\n-    }\n-\n-    public DynamicPartitionScheduler getDynamicPartitionScheduler() {\n-        return this.dynamicPartitionScheduler;\n-    }\n-\n-    private static class SingletonHolder {\n-        private static final Catalog INSTANCE = new Catalog();\n-    }\n-\n-    private Catalog() {\n-        this.idToDb = new ConcurrentHashMap<>();\n-        this.fullNameToDb = new ConcurrentHashMap<>();\n-        this.load = new Load();\n-        this.routineLoadManager = new RoutineLoadManager();\n-        this.exportMgr = new ExportMgr();\n-        this.alter = new Alter();\n-        this.consistencyChecker = new ConsistencyChecker();\n-        this.lock = new QueryableReentrantLock(true);\n-        this.backupHandler = new BackupHandler(this);\n-        this.metaDir = Config.meta_dir;\n-        this.publishVersionDaemon = new PublishVersionDaemon();\n-        this.deleteHandler = new DeleteHandler();\n-\n-        this.replayedJournalId = new AtomicLong(0L);\n-        this.isElectable = false;\n-        this.synchronizedTimeMs = 0;\n-        this.feType = FrontendNodeType.INIT;\n-        this.typeTransferQueue = Queues.newLinkedBlockingDeque();\n-\n-        this.role = FrontendNodeType.UNKNOWN;\n-        this.frontends = new ConcurrentHashMap<>();\n-        this.removedFrontends = new ConcurrentLinkedQueue<>();\n-\n-        this.journalObservable = new JournalObservable();\n-        this.masterRpcPort = 0;\n-        this.masterHttpPort = 0;\n-        this.masterIp = \"\";\n-\n-        this.systemInfo = new SystemInfoService();\n-        this.heartbeatMgr = new HeartbeatMgr(systemInfo);\n-        this.tabletInvertedIndex = new TabletInvertedIndex();\n-        this.colocateTableIndex = new ColocateTableIndex();\n-        this.recycleBin = new CatalogRecycleBin();\n-        this.functionSet = new FunctionSet();\n-        this.functionSet.init();\n-\n-        this.metaReplayState = new MetaReplayState();\n-\n-        this.idToCluster = new ConcurrentHashMap<>();\n-        this.nameToCluster = new ConcurrentHashMap<>();\n-\n-        this.isDefaultClusterCreated = false;\n-\n-        this.pullLoadJobMgr = new PullLoadJobMgr();\n-        this.brokerMgr = new BrokerMgr();\n-        this.resourceMgr = new ResourceMgr();\n-\n-        this.globalTransactionMgr = new GlobalTransactionMgr(this);\n-        this.tabletStatMgr = new TabletStatMgr();\n-\n-        this.auth = new PaloAuth();\n-        this.domainResolver = new DomainResolver(auth);\n-\n-        this.esRepository = new EsRepository();\n-\n-        this.metaContext = new MetaContext();\n-        this.metaContext.setThreadLocalInfo();\n-        \n-        this.stat = new TabletSchedulerStat();\n-        this.tabletScheduler = new TabletScheduler(this, systemInfo, tabletInvertedIndex, stat);\n-        this.tabletChecker = new TabletChecker(this, systemInfo, tabletScheduler, stat);\n-\n-        this.loadTaskScheduler = new MasterTaskExecutor(Config.async_load_task_pool_size);\n-        this.loadJobScheduler = new LoadJobScheduler();\n-        this.loadManager = new LoadManager(loadJobScheduler);\n-        this.loadTimeoutChecker = new LoadTimeoutChecker(loadManager);\n-        this.loadEtlChecker = new LoadEtlChecker(loadManager);\n-        this.loadLoadingChecker = new LoadLoadingChecker(loadManager);\n-        this.routineLoadScheduler = new RoutineLoadScheduler(routineLoadManager);\n-        this.routineLoadTaskScheduler = new RoutineLoadTaskScheduler(routineLoadManager);\n-\n-        this.smallFileMgr = new SmallFileMgr();\n-\n-        this.dynamicPartitionScheduler = new DynamicPartitionScheduler(\"DynamicPartitionScheduler\",\n-                Config.dynamic_partition_check_interval_seconds * 1000L);\n-        \n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        this.pluginMgr = new PluginMgr();\n-        this.auditEventProcessor = new AuditEventProcessor(this.pluginMgr);\n-    }\n-\n-    public static void destroyCheckpoint() {\n-        if (CHECKPOINT != null) {\n-            CHECKPOINT = null;\n-        }\n-    }\n-\n-    public static Catalog getCurrentCatalog() {\n-        if (isCheckpointThread()) {\n-            // only checkpoint thread it self will goes here.\n-            // so no need to care about the thread safe.\n-            if (CHECKPOINT == null) {\n-                CHECKPOINT = new Catalog();\n-            }\n-            return CHECKPOINT;\n-        } else {\n-            return SingletonHolder.INSTANCE;\n-        }\n-    }\n-\n-    // NOTICE: in most case, we should use getCurrentCatalog() to get the right catalog.\n-    // but in some cases, we should get the serving catalog explicitly.\n-    public static Catalog getServingCatalog() {\n-        return SingletonHolder.INSTANCE;\n-    }\n-\n-    public PullLoadJobMgr getPullLoadJobMgr() {\n-        return pullLoadJobMgr;\n-    }\n-\n-    public BrokerMgr getBrokerMgr() {\n-        return brokerMgr;\n-    }\n-\n-    public ResourceMgr getResourceMgr() {\n-        return resourceMgr;\n-    }\n-\n-    public static GlobalTransactionMgr getCurrentGlobalTransactionMgr() {\n-        return getCurrentCatalog().globalTransactionMgr;\n-    }\n-\n-    public GlobalTransactionMgr getGlobalTransactionMgr() {\n-        return globalTransactionMgr;\n-    }\n-\n-    public PluginMgr getPluginMgr() {\n-        return pluginMgr;\n-    }\n-\n-    public PaloAuth getAuth() {\n-        return auth;\n-    }\n-\n-    public TabletScheduler getTabletScheduler() {\n-        return tabletScheduler;\n-    }\n-\n-    public TabletChecker getTabletChecker() {\n-        return tabletChecker;\n-    }\n-\n-    public ConcurrentHashMap<String, Database> getFullNameToDb() {\n-        return fullNameToDb;\n-    }\n-\n-    public AuditEventProcessor getAuditEventProcessor() {\n-        return auditEventProcessor;\n-    }\n-\n-    // use this to get correct ClusterInfoService instance\n-    public static SystemInfoService getCurrentSystemInfo() {\n-        return getCurrentCatalog().getClusterInfo();\n-    }\n-\n-    public static HeartbeatMgr getCurrentHeartbeatMgr() {\n-        return getCurrentCatalog().getHeartbeatMgr();\n-    }\n-\n-    // use this to get correct TabletInvertedIndex instance\n-    public static TabletInvertedIndex getCurrentInvertedIndex() {\n-        return getCurrentCatalog().getTabletInvertedIndex();\n-    }\n-\n-    // use this to get correct ColocateTableIndex instance\n-    public static ColocateTableIndex getCurrentColocateIndex() {\n-        return getCurrentCatalog().getColocateTableIndex();\n-    }\n-\n-    public static CatalogRecycleBin getCurrentRecycleBin() {\n-        return getCurrentCatalog().getRecycleBin();\n-    }\n-\n-    // use this to get correct Catalog's journal version\n-    public static int getCurrentCatalogJournalVersion() {\n-        return MetaContext.get().getMetaVersion();\n-    }\n-\n-    public static final boolean isCheckpointThread() {\n-        return Thread.currentThread().getId() == checkpointThreadId;\n-    }\n-\n-    public static PluginMgr getCurrentPluginMgr() {\n-        return getCurrentCatalog().getPluginMgr();\n-    }\n-\n-    public static AuditEventProcessor getCurrentAuditEventProcessor() {\n-        return getCurrentCatalog().getAuditEventProcessor();\n-    }\n-\n-    // Use tryLock to avoid potential dead lock\n-    private boolean tryLock(boolean mustLock) {\n-        while (true) {\n-            try {\n-                if (!lock.tryLock(Config.catalog_try_lock_timeout_ms, TimeUnit.MILLISECONDS)) {\n-                    if (LOG.isDebugEnabled()) {\n-                        // to see which thread held this lock for long time.\n-                        Thread owner = lock.getOwner();\n-                        if (owner != null) {\n-                            LOG.debug(\"catalog lock is held by: {}\", Util.dumpThread(owner, 10));\n-                        }\n-                    }\n-                    \n-                    if (mustLock) {\n-                        continue;\n-                    } else {\n-                        return false;\n-                    }\n-                }\n-                return true;\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception while getting catalog lock\", e);\n-                if (mustLock) {\n-                    continue;\n-                } else {\n-                    return lock.isHeldByCurrentThread();\n-                }\n-            }\n-        }\n-    }\n-\n-    private void unlock() {\n-        if (lock.isHeldByCurrentThread()) {\n-            this.lock.unlock();\n-        }\n-    }\n-\n-    public String getBdbDir() {\n-        return bdbDir;\n-    }\n-\n-    public String getImageDir() {\n-        return imageDir;\n-    }\n-\n-    public void initialize(String[] args) throws Exception {\n-        // set meta dir first.\n-        // we already set these variables in constructor. but Catalog is a singleton class.\n-        // so they may be set before Config is initialized.\n-        // set them here again to make sure these variables use values in fe.conf.\n-        this.metaDir = Config.meta_dir;\n-        this.bdbDir = this.metaDir + BDB_DIR;\n-        this.imageDir = this.metaDir + IMAGE_DIR;\n-\n-        // 0. get local node and helper node info\n-        getSelfHostPort();\n-        getHelperNodes(args);\n-\n-        // 1. check and create dirs and files\n-        File meta = new File(metaDir);\n-        if (!meta.exists()) {\n-            LOG.error(\"{} does not exist, will exit\", meta.getAbsolutePath());\n-            System.exit(-1);\n-        }\n-\n-        if (Config.edit_log_type.equalsIgnoreCase(\"bdb\")) {\n-            File bdbDir = new File(this.bdbDir);\n-            if (!bdbDir.exists()) {\n-                bdbDir.mkdirs();\n-            }\n-\n-            File imageDir = new File(this.imageDir);\n-            if (!imageDir.exists()) {\n-                imageDir.mkdirs();\n-            }\n-        } else {\n-            LOG.error(\"Invalid edit log type: {}\", Config.edit_log_type);\n-            System.exit(-1);\n-        }\n-\n-        // init plugin manager\n-        pluginMgr.init();\n-        auditEventProcessor.start();\n-\n-        // 2. get cluster id and role (Observer or Follower)\n-        getClusterIdAndRole();\n-\n-        // 3. Load image first and replay edits\n-        this.editLog = new EditLog(nodeName);\n-        loadImage(this.imageDir); // load image file\n-        editLog.open(); // open bdb env\n-        this.globalTransactionMgr.setEditLog(editLog);\n-        this.idGenerator.setEditLog(editLog);\n-\n-        // 4. create load and export job label cleaner thread\n-        createLabelCleaner();\n-\n-        // 5. create txn cleaner thread\n-        createTxnCleaner();\n-\n-        // 6. start state listener thread\n-        createStateListener();\n-        listener.start();\n-    }\n-\n-    // wait until FE is ready.\n-    public void waitForReady() throws InterruptedException {\n-        while (true) {\n-            if (isReady()) {\n-                LOG.info(\"catalog is ready. FE type: {}\", feType);\n-                break;\n-            }\n-\n-            Thread.sleep(2000);\n-            LOG.info(\"wait catalog to be ready. FE type: {}. is ready: {}\", feType, isReady.get());\n-        }\n-    }\n-    \n-    public boolean isReady() {\n-        return isReady.get();\n-    }\n-\n-    private void getClusterIdAndRole() throws IOException {\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-\n-        // if helper node is point to self, or there is ROLE and VERSION file in local.\n-        // get the node type from local\n-        if (isMyself() || (roleFile.exists() && versionFile.exists())) {\n-\n-            if (!isMyself()) {\n-                LOG.info(\"find ROLE and VERSION file in local, ignore helper nodes: {}\", helperNodes);\n-            }\n-\n-            // check file integrity, if has.\n-            if ((roleFile.exists() && !versionFile.exists())\n-                    || (!roleFile.exists() && versionFile.exists())) {\n-                LOG.error(\"role file and version file must both exist or both not exist. \"\n-                        + \"please specific one helper node to recover. will exit.\");\n-                System.exit(-1);\n-            }\n-\n-            // ATTN:\n-            // If the version file and role file does not exist and the helper node is itself,\n-            // this should be the very beginning startup of the cluster, so we create ROLE and VERSION file,\n-            // set isFirstTimeStartUp to true, and add itself to frontends list.\n-            // If ROLE and VERSION file is deleted for some reason, we may arbitrarily start this node as\n-            // FOLLOWER, which may cause UNDEFINED behavior.\n-            // Everything may be OK if the origin role is exactly FOLLOWER,\n-            // but if not, FE process will exit somehow.\n-            Storage storage = new Storage(this.imageDir);\n-            if (!roleFile.exists()) {\n-                // The very first time to start the first node of the cluster.\n-                // It should became a Master node (Master node's role is also FOLLOWER, which means electable)\n-\n-                // For compatibility. Because this is the very first time to start, so we arbitrarily choose\n-                // a new name for this node\n-                role = FrontendNodeType.FOLLOWER;\n-                nodeName = genFeNodeName(selfNode.first, selfNode.second, false /* new style */);\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                LOG.info(\"very first time to start this node. role: {}, node name: {}\", role.name(), nodeName);\n-            } else {\n-                role = storage.getRole();\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-\n-                nodeName = storage.getNodeName();\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // In normal case, if ROLE file exist, role and nodeName should both exist.\n-                    // But we will get a empty nodeName after upgrading.\n-                    // So for forward compatibility, we use the \"old-style\" way of naming: \"ip_port\",\n-                    // and update the ROLE file.\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true/* old style */);\n-                    storage.writeFrontendRoleAndNodeName(role, nodeName);\n-                    LOG.info(\"forward compatibility. role: {}, node name: {}\", role.name(), nodeName);\n-                }\n-            }\n-\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            if (!versionFile.exists()) {\n-                clusterId = Config.cluster_id == -1 ? Storage.newClusterID() : Config.cluster_id;\n-                token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                        Storage.newToken() : Config.auth_token;\n-                storage = new Storage(clusterId, token, this.imageDir);\n-                storage.writeClusterIdAndToken();\n-\n-                isFirstTimeStartUp = true;\n-                Frontend self = new Frontend(role, nodeName, selfNode.first, selfNode.second);\n-                // We don't need to check if frontends already contains self.\n-                // frontends must be empty cause no image is loaded and no journal is replayed yet.\n-                // And this frontend will be persisted later after opening bdbje environment.\n-                frontends.put(nodeName, self);\n-            } else {\n-                clusterId = storage.getClusterID();\n-                if (storage.getToken() == null) {\n-                    token = Strings.isNullOrEmpty(Config.auth_token) ?\n-                            Storage.newToken() : Config.auth_token;\n-                    LOG.info(\"new token={}\", token);\n-                    storage.setToken(token);\n-                    storage.writeClusterIdAndToken();\n-                } else {\n-                    token = storage.getToken();\n-                }\n-                isFirstTimeStartUp = false;\n-            }\n-        } else {\n-            // try to get role and node name from helper node,\n-            // this loop will not end until we get certain role type and name\n-            while (true) {\n-                if (!getFeNodeTypeAndNameFromHelpers()) {\n-                    LOG.warn(\"current node is not added to the group. please add it first. \"\n-                            + \"sleep 5 seconds and retry, current helper nodes: {}\", helperNodes);\n-                    try {\n-                        Thread.sleep(5000);\n-                        continue;\n-                    } catch (InterruptedException e) {\n-                        e.printStackTrace();\n-                        System.exit(-1);\n-                    }\n-                }\n-\n-                if (role == FrontendNodeType.REPLICA) {\n-                    // for compatibility\n-                    role = FrontendNodeType.FOLLOWER;\n-                }\n-                break;\n-            }\n-\n-            Preconditions.checkState(helperNodes.size() == 1);\n-            Preconditions.checkNotNull(role);\n-            Preconditions.checkNotNull(nodeName);\n-\n-            Pair<String, Integer> rightHelperNode = helperNodes.get(0);\n-\n-            Storage storage = new Storage(this.imageDir);\n-            if (roleFile.exists() && (role != storage.getRole() || !nodeName.equals(storage.getNodeName()))\n-                    || !roleFile.exists()) {\n-                storage.writeFrontendRoleAndNodeName(role, nodeName);\n-            }\n-            if (!versionFile.exists()) {\n-                // If the version file doesn't exist, download it from helper node\n-                if (!getVersionFileFromHelper(rightHelperNode)) {\n-                    LOG.error(\"fail to download version file from \" + rightHelperNode.first + \" will exit.\");\n-                    System.exit(-1);\n-                }\n-\n-                // NOTE: cluster_id will be init when Storage object is constructed,\n-                //       so we new one.\n-                storage = new Storage(this.imageDir);\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                if (Strings.isNullOrEmpty(token)) {\n-                    token = Config.auth_token;\n-                }\n-            } else {\n-                // If the version file exist, read the cluster id and check the\n-                // id with helper node to make sure they are identical\n-                clusterId = storage.getClusterID();\n-                token = storage.getToken();\n-                try {\n-                    URL idURL = new URL(\"http://\" + rightHelperNode.first + \":\" + Config.http_port + \"/check\");\n-                    HttpURLConnection conn = null;\n-                    conn = (HttpURLConnection) idURL.openConnection();\n-                    conn.setConnectTimeout(2 * 1000);\n-                    conn.setReadTimeout(2 * 1000);\n-                    String clusterIdString = conn.getHeaderField(MetaBaseAction.CLUSTER_ID);\n-                    int remoteClusterId = Integer.parseInt(clusterIdString);\n-                    if (remoteClusterId != clusterId) {\n-                        LOG.error(\"cluster id is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                        System.exit(-1);\n-                    }\n-                    String remoteToken = conn.getHeaderField(MetaBaseAction.TOKEN);\n-                    if (token == null && remoteToken != null) {\n-                        LOG.info(\"get token from helper node. token={}.\", remoteToken);\n-                        token = remoteToken;\n-                        storage.writeClusterIdAndToken();\n-                        storage.reload();\n-                    }\n-                    if (Config.enable_token_check) {\n-                        Preconditions.checkNotNull(token);\n-                        Preconditions.checkNotNull(remoteToken);\n-                        if (!token.equals(remoteToken)) {\n-                            LOG.error(\"token is not equal with helper node {}. will exit.\", rightHelperNode.first);\n-                            System.exit(-1);\n-                        }\n-                    }\n-                } catch (Exception e) {\n-                    LOG.warn(\"fail to check cluster_id and token with helper node.\", e);\n-                    System.exit(-1);\n-                }\n-            }\n-\n-            getNewImage(rightHelperNode);\n-        }\n-\n-        if (Config.cluster_id != -1 && clusterId != Config.cluster_id) {\n-            LOG.error(\"cluster id is not equal with config item cluster_id. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (role.equals(FrontendNodeType.FOLLOWER)) {\n-            isElectable = true;\n-        } else {\n-            isElectable = false;\n-        }\n-\n-        Preconditions.checkState(helperNodes.size() == 1);\n-        LOG.info(\"finished to get cluster id: {}, role: {} and node name: {}\",\n-                clusterId, role.name(), nodeName);\n-    }\n-\n-    public static String genFeNodeName(String host, int port, boolean isOldStyle) {\n-        String name = host + \"_\" + port;\n-        if (isOldStyle) {\n-            return name;\n-        } else {\n-            return name + \"_\" + System.currentTimeMillis();\n-        }\n-    }\n-\n-    // Get the role info and node name from helper node.\n-    // return false if failed.\n-    private boolean getFeNodeTypeAndNameFromHelpers() {\n-        // we try to get info from helper nodes, once we get the right helper node,\n-        // other helper nodes will be ignored and removed.\n-        Pair<String, Integer> rightHelperNode = null;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            try {\n-                URL url = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/role?host=\" + selfNode.first + \"&port=\" + selfNode.second);\n-                HttpURLConnection conn = null;\n-                conn = (HttpURLConnection) url.openConnection();\n-                if (conn.getResponseCode() != 200) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}. response code: {}\",\n-                            helperNode, conn.getResponseCode());\n-                    continue;\n-                }\n-\n-                String type = conn.getHeaderField(\"role\");\n-                if (type == null) {\n-                    LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode);\n-                    continue;\n-                }\n-                role = FrontendNodeType.valueOf(type);\n-                nodeName = conn.getHeaderField(\"name\");\n-\n-                // get role and node name before checking them, because we want to throw any exception\n-                // as early as we encounter.\n-\n-                if (role == FrontendNodeType.UNKNOWN) {\n-                    LOG.warn(\"frontend {} is not added to cluster yet. role UNKNOWN\", selfNode);\n-                    return false;\n-                }\n-\n-                if (Strings.isNullOrEmpty(nodeName)) {\n-                    // For forward compatibility, we use old-style name: \"ip_port\"\n-                    nodeName = genFeNodeName(selfNode.first, selfNode.second, true /* old style */);\n-                }\n-            } catch (Exception e) {\n-                LOG.warn(\"failed to get fe node type from helper node: {}.\", helperNode, e);\n-                continue;\n-            }\n-\n-            LOG.info(\"get fe node type {}, name {} from {}:{}\", role, nodeName, helperNode.first, Config.http_port);\n-            rightHelperNode = helperNode;\n-            break;\n-        }\n-\n-        if (rightHelperNode == null) {\n-            return false;\n-        }\n-\n-        helperNodes.clear();\n-        helperNodes.add(rightHelperNode);\n-        return true;\n-    }\n-\n-    private void getSelfHostPort() {\n-        selfNode = new Pair<String, Integer>(FrontendOptions.getLocalHostAddress(), Config.edit_log_port);\n-        LOG.debug(\"get self node: {}\", selfNode);\n-    }\n-\n-    private void getHelperNodes(String[] args) throws AnalysisException {\n-        String helpers = null;\n-        for (int i = 0; i < args.length; i++) {\n-            if (args[i].equalsIgnoreCase(\"-helper\")) {\n-                if (i + 1 >= args.length) {\n-                    System.out.println(\"-helper need parameter host:port,host:port\");\n-                    System.exit(-1);\n-                }\n-                helpers = args[i + 1];\n-                break;\n-            }\n-        }\n-\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            if (Config.enable_deploy_manager.equalsIgnoreCase(\"k8s\")) {\n-                deployManager = new K8sDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"ambari\")) {\n-                deployManager = new AmbariDeployManager(this, 5000 /* 5s interval */);\n-            } else if (Config.enable_deploy_manager.equalsIgnoreCase(\"local\")) {\n-                deployManager = new LocalFileDeployManager(this, 5000 /* 5s interval */);\n-            } else {\n-                System.err.println(\"Unknow deploy manager: \" + Config.enable_deploy_manager);\n-                System.exit(-1);\n-            }\n-\n-            getHelperNodeFromDeployManager();\n-\n-        } else {\n-            if (helpers != null) {\n-                String[] splittedHelpers = helpers.split(\",\");\n-                for (String helper : splittedHelpers) {\n-                    Pair<String, Integer> helperHostPort = SystemInfoService.validateHostAndPort(helper);\n-                    if (helperHostPort.equals(selfNode)) {\n-                        /**\n-                         * If user specified the helper node to this FE itself,\n-                         * we will stop the starting FE process and report an error.\n-                         * First, it is meaningless to point the helper to itself.\n-                         * Secondly, when some users add FE for the first time, they will mistakenly\n-                         * point the helper that should have pointed to the Master to themselves.\n-                         * In this case, some errors have caused users to be troubled.\n-                         * So here directly exit the program and inform the user to avoid unnecessary trouble.\n-                         */\n-                        throw new AnalysisException(\n-                                \"Do not specify the helper node to FE itself. \"\n-                                        + \"Please specify it to the existing running Master or Follower FE\");\n-                    }\n-                    helperNodes.add(helperHostPort);\n-                }\n-            } else {\n-                // If helper node is not designated, use local node as helper node.\n-                helperNodes.add(Pair.create(selfNode.first, Config.edit_log_port));\n-            }\n-        }\n-\n-        LOG.info(\"get helper nodes: {}\", helperNodes);\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    private void getHelperNodeFromDeployManager() {\n-        Preconditions.checkNotNull(deployManager);\n-\n-        // 1. check if this is the first time to start up\n-        File roleFile = new File(this.imageDir, Storage.ROLE_FILE);\n-        File versionFile = new File(this.imageDir, Storage.VERSION_FILE);\n-        if ((roleFile.exists() && !versionFile.exists())\n-                || (!roleFile.exists() && versionFile.exists())) {\n-            LOG.error(\"role file and version file must both exist or both not exist. \"\n-                    + \"please specific one helper node to recover. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        if (roleFile.exists()) {\n-            // This is not the first time this node start up.\n-            // It should already added to FE group, just set helper node as it self.\n-            LOG.info(\"role file exist. this is not the first time to start up\");\n-            helperNodes = Lists.newArrayList(Pair.create(selfNode.first, Config.edit_log_port));\n-            return;\n-        }\n-\n-        // This is the first time this node start up.\n-        // Get helper node from deploy manager.\n-        helperNodes = deployManager.getHelperNodes();\n-        if (helperNodes == null || helperNodes.isEmpty()) {\n-            LOG.error(\"failed to get helper node from deploy manager. exit\");\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private void transferToMaster() {\n-        // stop replayer\n-        if (replayer != null) {\n-            replayer.exit();\n-            try {\n-                replayer.join();\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"got exception when stopping the replayer thread\", e);\n-            }\n-            replayer = null;\n-        }\n-\n-        // set this after replay thread stopped. to avoid replay thread modify them.\n-        isReady.set(false);\n-        canRead.set(false);\n-\n-        editLog.open();\n-\n-        if (!haProtocol.fencing()) {\n-            LOG.error(\"fencing failed. will exit.\");\n-            System.exit(-1);\n-        }\n-\n-        long replayStartTime = System.currentTimeMillis();\n-        // replay journals. -1 means replay all the journals larger than current journal id.\n-        replayJournal(-1);\n-        long replayEndTime = System.currentTimeMillis();\n-        LOG.info(\"finish replay in \" + (replayEndTime - replayStartTime) + \" msec\");\n-\n-        checkCurrentNodeExist();\n-\n-        editLog.rollEditLog();\n-\n-        // Log meta_version\n-        long journalVersion = MetaContext.get().getMetaVersion();\n-        if (journalVersion < FeConstants.meta_version) {\n-            editLog.logMetaVersion(FeConstants.meta_version);\n-            MetaContext.get().setMetaVersion(FeConstants.meta_version);\n-        }\n-\n-        // Log the first frontend\n-        if (isFirstTimeStartUp) {\n-            // if isFirstTimeStartUp is true, frontends must contains this Node.\n-            Frontend self = frontends.get(nodeName);\n-            Preconditions.checkNotNull(self);\n-            // OP_ADD_FIRST_FRONTEND is emitted, so it can write to BDBJE even if canWrite is false\n-            editLog.logAddFirstFrontend(self);\n-        }\n-\n-        if (!isDefaultClusterCreated) {\n-            initDefaultCluster();\n-        }\n-\n-        // MUST set master ip before starting checkpoint thread.\n-        // because checkpoint thread need this info to select non-master FE to push image\n-        this.masterIp = FrontendOptions.getLocalHostAddress();\n-        this.masterRpcPort = Config.rpc_port;\n-        this.masterHttpPort = Config.http_port;\n-        MasterInfo info = new MasterInfo(this.masterIp, this.masterHttpPort, this.masterRpcPort);\n-        editLog.logMasterInfo(info);\n-\n-        // for master, the 'isReady' is set behind.\n-        // but we are sure that all metadata is replayed if we get here.\n-        // so no need to check 'isReady' flag in this method\n-        fixBugAfterMetadataReplayed(false);\n-\n-        // start all daemon threads that only running on MASTER FE\n-        startMasterOnlyDaemonThreads();\n-        // start other daemon threads that should running on all FE\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-\n-        canRead.set(true);\n-        isReady.set(true);\n-\n-        String msg = \"master finished to replay journal, can write now.\";\n-        Util.stdoutWithTime(msg);\n-        LOG.info(msg);\n-    }\n-\n-    /*\n-     * Add anything necessary here if there is meta data need to be fixed.\n-     */\n-    public void fixBugAfterMetadataReplayed(boolean waitCatalogReady) {\n-        if (waitCatalogReady) {\n-            while (!isReady()) {\n-                try {\n-                    Thread.sleep(10 * 1000);\n-                } catch (InterruptedException e) {\n-                    e.printStackTrace();\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"start to fix meta data bug\");\n-        loadManager.fixLoadJobMetaBugs(globalTransactionMgr);\n-    }\n-\n-    // start all daemon threads only running on Master\n-    private void startMasterOnlyDaemonThreads() {\n-        // start checkpoint thread\n-        checkpointer = new Checkpoint(editLog);\n-        checkpointer.setMetaContext(metaContext);\n-        // set \"checkpointThreadId\" before the checkpoint thread start, because the thread\n-        // need to check the \"checkpointThreadId\" when running.\n-        checkpointThreadId = checkpointer.getId();\n-\n-        checkpointer.start();\n-        LOG.info(\"checkpointer thread started. thread id is {}\", checkpointThreadId);\n-\n-        // heartbeat mgr\n-        heartbeatMgr.setMaster(clusterId, token, epoch);\n-        heartbeatMgr.start();\n-        // Load checker\n-        LoadChecker.init(Config.load_checker_interval_second * 1000L);\n-        LoadChecker.startAll();\n-        // New load scheduler\n-        loadManager.prepareJobs();\n-        loadJobScheduler.start();\n-        loadTimeoutChecker.start();\n-        loadEtlChecker.start();\n-        loadLoadingChecker.start();\n-        // Export checker\n-        ExportChecker.init(Config.export_checker_interval_second * 1000L);\n-        ExportChecker.startAll();\n-        // Tablet checker and scheduler\n-        tabletChecker.start();\n-        tabletScheduler.start();\n-        // Colocate tables balancer\n-        if (!Config.disable_colocate_join) {\n-            ColocateTableBalancer.getInstance().start();\n-        }\n-        // Publish Version Daemon\n-        publishVersionDaemon.start();\n-        // Start txn cleaner\n-        txnCleaner.start();\n-        // Alter\n-        getAlterInstance().start();\n-        // Consistency checker\n-        getConsistencyChecker().start();\n-        // Backup handler\n-        getBackupHandler().start();\n-        // catalog recycle bin\n-        getRecycleBin().start();\n-        // time printer\n-        createTimePrinter();\n-        timePrinter.start();\n-        // deploy manager\n-        if (!Config.enable_deploy_manager.equalsIgnoreCase(\"disable\")) {\n-            LOG.info(\"deploy manager {} start\", deployManager.getName());\n-            deployManager.start();\n-        }\n-        // start routine load scheduler\n-        routineLoadScheduler.start();\n-        routineLoadTaskScheduler.start();\n-        // start dynamic partition task\n-        dynamicPartitionScheduler.start();\n-    }\n-\n-    // start threads that should running on all FE\n-    private void startNonMasterDaemonThreads() {\n-        tabletStatMgr.start();\n-        // load and export job label cleaner thread\n-        labelCleaner.start();\n-        // ES state store\n-        esRepository.start();\n-        // domain resolver\n-        domainResolver.start();\n-    }\n-\n-    private void transferToNonMaster(FrontendNodeType newType) {\n-        isReady.set(false);\n-\n-        if (feType == FrontendNodeType.OBSERVER || feType == FrontendNodeType.FOLLOWER) {\n-            Preconditions.checkState(newType == FrontendNodeType.UNKNOWN);\n-            LOG.warn(\"{} to UNKNOWN, still offer read service\", feType.name());\n-            // not set canRead here, leave canRead as what is was.\n-            // if meta out of date, canRead will be set to false in replayer thread.\n-            metaReplayState.setTransferToUnknown();\n-            return;\n-        }\n-\n-        // transfer from INIT/UNKNOWN to OBSERVER/FOLLOWER\n-\n-        // add helper sockets\n-        if (Config.edit_log_type.equalsIgnoreCase(\"BDB\")) {\n-            for (Frontend fe : frontends.values()) {\n-                if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                    ((BDBHA) getHaProtocol()).addHelperSocket(fe.getHost(), fe.getEditLogPort());\n-                }\n-            }\n-        }\n-\n-        if (replayer == null) {\n-            createReplayer();\n-            replayer.start();\n-        }\n-\n-        // 'isReady' will be set to true in 'setCanRead()' method\n-        fixBugAfterMetadataReplayed(true);\n-\n-        startNonMasterDaemonThreads();\n-\n-        MetricRepo.init();\n-    }\n-\n-    /*\n-     * If the current node is not in the frontend list, then exit. This may\n-     * happen when this node is removed from frontend list, and the drop\n-     * frontend log is deleted because of checkpoint.\n-     */\n-    private void checkCurrentNodeExist() {\n-        if (Config.metadata_failure_recovery.equals(\"true\")) {\n-            return;\n-        }\n-\n-        Frontend fe = checkFeExist(selfNode.first, selfNode.second);\n-        if (fe == null) {\n-            LOG.error(\"current node is not added to the cluster, will exit\");\n-            System.exit(-1);\n-        } else if (fe.getRole() != role) {\n-            LOG.error(\"current node role is {} not match with frontend recorded role {}. will exit\", role,\n-                    fe.getRole());\n-            System.exit(-1);\n-        }\n-    }\n-\n-    private boolean getVersionFileFromHelper(Pair<String, Integer> helperNode) throws IOException {\n-        try {\n-            String url = \"http://\" + helperNode.first + \":\" + Config.http_port + \"/version\";\n-            File dir = new File(this.imageDir);\n-            MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000,\n-                    MetaHelper.getOutputStream(Storage.VERSION_FILE, dir));\n-            MetaHelper.complete(Storage.VERSION_FILE, dir);\n-            return true;\n-        } catch (Exception e) {\n-            LOG.warn(e);\n-        }\n-\n-        return false;\n-    }\n-\n-    private void getNewImage(Pair<String, Integer> helperNode) throws IOException {\n-        long localImageVersion = 0;\n-        Storage storage = new Storage(this.imageDir);\n-        localImageVersion = storage.getImageSeq();\n-\n-        try {\n-            URL infoUrl = new URL(\"http://\" + helperNode.first + \":\" + Config.http_port + \"/info\");\n-            StorageInfo info = getStorageInfo(infoUrl);\n-            long version = info.getImageSeq();\n-            if (version > localImageVersion) {\n-                String url = \"http://\" + helperNode.first + \":\" + Config.http_port\n-                        + \"/image?version=\" + version;\n-                String filename = Storage.IMAGE + \".\" + version;\n-                File dir = new File(this.imageDir);\n-                MetaHelper.getRemoteFile(url, HTTP_TIMEOUT_SECOND * 1000, MetaHelper.getOutputStream(filename, dir));\n-                MetaHelper.complete(filename, dir);\n-            }\n-        } catch (Exception e) {\n-            return;\n-        }\n-    }\n-\n-    private boolean isMyself() {\n-        Preconditions.checkNotNull(selfNode);\n-        Preconditions.checkNotNull(helperNodes);\n-        LOG.debug(\"self: {}. helpers: {}\", selfNode, helperNodes);\n-        // if helper nodes contain it self, remove other helpers\n-        boolean containSelf = false;\n-        for (Pair<String, Integer> helperNode : helperNodes) {\n-            if (selfNode.equals(helperNode)) {\n-                containSelf = true;\n-            }\n-        }\n-        if (containSelf) {\n-            helperNodes.clear();\n-            helperNodes.add(selfNode);\n-        }\n-\n-        return containSelf;\n-    }\n-\n-    private StorageInfo getStorageInfo(URL url) throws IOException {\n-        ObjectMapper mapper = new ObjectMapper();\n-\n-        HttpURLConnection connection = null;\n-        try {\n-            connection = (HttpURLConnection) url.openConnection();\n-            connection.setConnectTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            connection.setReadTimeout(HTTP_TIMEOUT_SECOND * 1000);\n-            return mapper.readValue(connection.getInputStream(), StorageInfo.class);\n-        } finally {\n-            if (connection != null) {\n-                connection.disconnect();\n-            }\n-        }\n-    }\n-\n-    public boolean hasReplayer() {\n-        return replayer != null;\n-    }\n-\n-    public void loadImage(String imageDir) throws IOException, DdlException {\n-        Storage storage = new Storage(imageDir);\n-        clusterId = storage.getClusterID();\n-        File curFile = storage.getCurrentImageFile();\n-        if (!curFile.exists()) {\n-            // image.0 may not exist\n-            LOG.info(\"image does not exist: {}\", curFile.getAbsolutePath());\n-            return;\n-        }\n-        replayedJournalId.set(storage.getImageSeq());\n-        LOG.info(\"start load image from {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-        long loadImageStartTime = System.currentTimeMillis();\n-        DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(curFile)));\n-\n-        long checksum = 0;\n-        try {\n-            checksum = loadHeader(dis, checksum);\n-            checksum = loadMasterInfo(dis, checksum);\n-            checksum = loadFrontends(dis, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().loadBackends(dis, checksum);\n-            checksum = loadDb(dis, checksum);\n-            // ATTN: this should be done after load Db, and before loadAlterJob\n-            recreateTabletInvertIndex();\n-            // rebuild es state state\n-            esRepository.loadTableFromCatalog();\n-\n-            checksum = loadLoadJob(dis, checksum);\n-            checksum = loadAlterJob(dis, checksum);\n-            checksum = loadRecycleBin(dis, checksum);\n-            checksum = loadGlobalVariable(dis, checksum);\n-            checksum = loadCluster(dis, checksum);\n-            checksum = loadBrokers(dis, checksum);\n-            checksum = loadResources(dis, checksum);\n-            checksum = loadExportJob(dis, checksum);\n-            checksum = loadBackupHandler(dis, checksum);\n-            checksum = loadPaloAuth(dis, checksum);\n-            // global transaction must be replayed before load jobs v2\n-            checksum = loadTransactionState(dis, checksum);\n-            checksum = loadColocateTableIndex(dis, checksum);\n-            checksum = loadRoutineLoadJobs(dis, checksum);\n-            checksum = loadLoadJobsV2(dis, checksum);\n-            checksum = loadSmallFiles(dis, checksum);\n-            checksum = loadPlugins(dis, checksum);\n-            checksum = loadDeleteHandler(dis, checksum);\n-\n-            long remoteChecksum = dis.readLong();\n-            Preconditions.checkState(remoteChecksum == checksum, remoteChecksum + \" vs. \" + checksum);\n-        } finally {\n-            dis.close();\n-        }\n-\n-        long loadImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished to load image in \" + (loadImageEndTime - loadImageStartTime) + \" ms\");\n-    }\n-\n-    private void recreateTabletInvertIndex() {\n-        if (isCheckpointThread()) {\n-            return;\n-        }\n-\n-        // create inverted index\n-        TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-        for (Database db : this.fullNameToDb.values()) {\n-            long dbId = db.getId();\n-            for (Table table : db.getTables()) {\n-                if (table.getType() != TableType.OLAP) {\n-                    continue;\n-                }\n-\n-                OlapTable olapTable = (OlapTable) table;\n-                long tableId = olapTable.getId();\n-                Collection<Partition> allPartitions = olapTable.getAllPartitions();\n-                for (Partition partition : allPartitions) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = index.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : index.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                                if (MetaContext.get().getMetaVersion() < FeMetaVersion.VERSION_48) {\n-                                    // set replica's schema hash\n-                                    replica.setSchemaHash(schemaHash);\n-                                }\n-                            }\n-                        }\n-                    } // end for indices\n-                } // end for partitions\n-            } // end for tables\n-        } // end for dbs\n-    }\n-\n-    public long loadHeader(DataInputStream dis, long checksum) throws IOException {\n-        int journalVersion = dis.readInt();\n-        long newChecksum = checksum ^ journalVersion;\n-        MetaContext.get().setMetaVersion(journalVersion);\n-\n-        long replayedJournalId = dis.readLong();\n-        newChecksum ^= replayedJournalId;\n-\n-        long catalogId = dis.readLong();\n-        newChecksum ^= catalogId;\n-        idGenerator.setId(catalogId);\n-\n-        if (journalVersion >= FeMetaVersion.VERSION_32) {\n-            isDefaultClusterCreated = dis.readBoolean();\n-        }\n-\n-        LOG.info(\"finished replay header from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadMasterInfo(DataInputStream dis, long checksum) throws IOException {\n-        masterIp = Text.readString(dis);\n-        masterRpcPort = dis.readInt();\n-        long newChecksum = checksum ^ masterRpcPort;\n-        masterHttpPort = dis.readInt();\n-        newChecksum ^= masterHttpPort;\n-\n-        LOG.info(\"finished replay masterInfo from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadFrontends(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; i++) {\n-                Frontend fe = Frontend.read(dis);\n-                replayAddFrontend(fe);\n-            }\n-            \n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                if (Catalog.getCurrentCatalogJournalVersion() < FeMetaVersion.VERSION_41) {\n-                    Frontend fe = Frontend.read(dis);\n-                    removedFrontends.add(fe.getNodeName());\n-                } else {\n-                    removedFrontends.add(Text.readString(dis));\n-                }\n-            }\n-            return newChecksum;\n-        }\n-        LOG.info(\"finished replay frontends from image\");\n-        return checksum;\n-    }\n-\n-    public long loadDb(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        int dbCount = dis.readInt();\n-        long newChecksum = checksum ^ dbCount;\n-        for (long i = 0; i < dbCount; ++i) {\n-            Database db = new Database();\n-            db.readFields(dis);\n-            newChecksum ^= db.getId();\n-            idToDb.put(db.getId(), db);\n-            fullNameToDb.put(db.getFullName(), db);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.put(db.getAttachDb(), db);\n-            }\n-            globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-        }\n-        LOG.info(\"finished replay databases from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadLoadJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        // load jobs\n-        int jobSize = dis.readInt();\n-        long newChecksum = checksum ^ jobSize;\n-        for (int i = 0; i < jobSize; i++) {\n-            long dbId = dis.readLong();\n-            newChecksum ^= dbId;\n-\n-            int loadJobCount = dis.readInt();\n-            newChecksum ^= loadJobCount;\n-            for (int j = 0; j < loadJobCount; j++) {\n-                LoadJob job = new LoadJob();\n-                job.readFields(dis);\n-                long currentTimeMs = System.currentTimeMillis();\n-\n-                // Delete the history load jobs that are older than\n-                // LABEL_KEEP_MAX_MS\n-                // This job must be FINISHED or CANCELLED\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                        || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                    load.unprotectAddLoadJob(job, true /* replay */);\n-                }\n-            }\n-        }\n-\n-        // delete jobs\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_11) {\n-            jobSize = dis.readInt();\n-            newChecksum ^= jobSize;\n-            for (int i = 0; i < jobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteCount = dis.readInt();\n-                newChecksum ^= deleteCount;\n-                for (int j = 0; j < deleteCount; j++) {\n-                    DeleteInfo deleteInfo = new DeleteInfo();\n-                    deleteInfo.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history delete jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    if ((currentTimeMs - deleteInfo.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second) {\n-                        load.unprotectAddDeleteInfo(deleteInfo);\n-                    }\n-                }\n-            }\n-        }\n-\n-        // load error hub info\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_24) {\n-            LoadErrorHub.Param param = new LoadErrorHub.Param();\n-            param.readFields(dis);\n-            load.setLoadErrorHubInfo(param);\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            // 4. load delete jobs\n-            int deleteJobSize = dis.readInt();\n-            newChecksum ^= deleteJobSize;\n-            for (int i = 0; i < deleteJobSize; i++) {\n-                long dbId = dis.readLong();\n-                newChecksum ^= dbId;\n-\n-                int deleteJobCount = dis.readInt();\n-                newChecksum ^= deleteJobCount;\n-                for (int j = 0; j < deleteJobCount; j++) {\n-                    LoadJob job = new LoadJob();\n-                    job.readFields(dis);\n-                    long currentTimeMs = System.currentTimeMillis();\n-\n-                    // Delete the history load jobs that are older than\n-                    // LABEL_KEEP_MAX_MS\n-                    // This job must be FINISHED or CANCELLED\n-                    if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.label_keep_max_second\n-                            || (job.getState() != JobState.FINISHED && job.getState() != JobState.CANCELLED)) {\n-                        load.unprotectAddLoadJob(job, true /* replay */);\n-                    }\n-                }\n-            }\n-        }\n-\n-        LOG.info(\"finished replay loadJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadExportJob(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        long newChecksum = checksum;\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_32) {\n-            int size = dis.readInt();\n-            newChecksum = checksum ^ size;\n-            for (int i = 0; i < size; ++i) {\n-                long jobId = dis.readLong();\n-                newChecksum ^= jobId;\n-                ExportJob job = new ExportJob();\n-                job.readFields(dis);\n-                exportMgr.unprotectAddJob(job);\n-            }\n-        }\n-        LOG.info(\"finished replay exportJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum) throws IOException {\n-        long newChecksum = checksum;\n-        for (JobType type : JobType.values()) {\n-            if (type == JobType.DECOMMISSION_BACKEND) {\n-                if (Catalog.getCurrentCatalogJournalVersion() >= 5) {\n-                    newChecksum = loadAlterJob(dis, newChecksum, type);\n-                }\n-            } else {\n-                newChecksum = loadAlterJob(dis, newChecksum, type);\n-            }\n-        }\n-        LOG.info(\"finished replay alterJob from image\");\n-        return newChecksum;\n-    }\n-\n-    public long loadAlterJob(DataInputStream dis, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = dis.readInt();\n-        long newChecksum = checksum ^ size;\n-        for (int i = 0; i < size; i++) {\n-            long tableId = dis.readLong();\n-            newChecksum ^= tableId;\n-            AlterJob job = AlterJob.read(dis);\n-            alterJobs.put(tableId, job);\n-\n-            // init job\n-            Database db = getDb(job.getDbId());\n-            // should check job state here because the job is finished but not removed from alter jobs list\n-            if (db != null && (job.getState() == org.apache.doris.alter.AlterJob.JobState.PENDING\n-                    || job.getState() == org.apache.doris.alter.AlterJob.JobState.RUNNING)) {\n-                job.replayInitJob(db);\n-            }\n-        }\n-\n-        if (Catalog.getCurrentCatalogJournalVersion() >= 2) {\n-            // finished or cancelled jobs\n-            long currentTimeMs = System.currentTimeMillis();\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                long tableId = dis.readLong();\n-                newChecksum ^= tableId;\n-                AlterJob job = AlterJob.read(dis);\n-                if ((currentTimeMs - job.getCreateTimeMs()) / 1000 <= Config.history_job_keep_max_second) {\n-                    // delete history jobs\n-                    finishedOrCancelledAlterJobs.add(job);\n-                }\n-            }\n-        }\n-\n-        // alter job v2\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_61) {\n-            size = dis.readInt();\n-            newChecksum ^= size;\n-            for (int i = 0; i < size; i++) {\n-                AlterJobV2 alterJobV2 = AlterJobV2.read(dis);\n-                if (type == JobType.ROLLUP || type == JobType.SCHEMA_CHANGE) {\n-                    if (type == JobType.ROLLUP) {\n-                        this.getRollupHandler().addAlterJobV2(alterJobV2);\n-                    } else {\n-                        alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                    }\n-                    // ATTN : we just want to add tablet into TabletInvertedIndex when only PendingJob is checkpointed\n-                    // to prevent TabletInvertedIndex data loss,\n-                    // So just use AlterJob.replay() instead of AlterHandler.replay().\n-                    if (alterJobV2.getJobState() == AlterJobV2.JobState.PENDING) {\n-                        alterJobV2.replay(alterJobV2);\n-                        LOG.info(\"replay pending alter job when load alter job {} \", alterJobV2.getJobId());\n-                    }\n-                } else {\n-                    alterJobsV2.put(alterJobV2.getJobId(), alterJobV2);\n-                }\n-            }\n-        }\n-\n-        return newChecksum;\n-    }\n-\n-    public long loadBackupHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_42) {\n-            getBackupHandler().readFields(dis);\n-        }\n-        getBackupHandler().setCatalog(this);\n-        LOG.info(\"finished replay backupHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveBackupHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getBackupHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadDeleteHandler(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_82) {\n-            this.deleteHandler = DeleteHandler.read(dis);\n-        }\n-        LOG.info(\"finished replay deleteHandler from image\");\n-        return checksum;\n-    }\n-\n-    public long saveDeleteHandler(DataOutputStream dos, long checksum) throws IOException {\n-        getDeleteHandler().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPaloAuth(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_43) {\n-            // CAN NOT use PaloAuth.read(), cause this auth instance is already passed to DomainResolver\n-            auth.readFields(dis);\n-        }\n-        LOG.info(\"finished replay paloAuth from image\");\n-        return checksum;\n-    }\n-\n-    public long loadTransactionState(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_45) {\n-            int size = dis.readInt();\n-            long newChecksum = checksum ^ size;\n-            globalTransactionMgr.readFields(dis);\n-            LOG.info(\"finished replay transactionState from image\");\n-            return newChecksum;\n-        }\n-        return checksum;\n-    }\n-\n-    public long loadRecycleBin(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_10) {\n-            recycleBin.readFields(dis);\n-            if (!isCheckpointThread()) {\n-                // add tablet in Recycle bin to TabletInvertedIndex\n-                recycleBin.addTabletToInvertedIndex();\n-            }\n-            // create DatabaseTransactionMgr for db in recycle bin.\n-            // these dbs do not exist in `idToDb` of the catalog.\n-            for (Long dbId : recycleBin.getAllDbIds()) {\n-                globalTransactionMgr.addDatabaseTransactionMgr(dbId);\n-            }\n-        }\n-        LOG.info(\"finished replay recycleBin from image\");\n-        return checksum;\n-    }\n-\n-    public long loadColocateTableIndex(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_46) {\n-            Catalog.getCurrentColocateIndex().readFields(dis);\n-        }\n-        LOG.info(\"finished replay colocateTableIndex from image\");\n-        return checksum;\n-    }\n-\n-    public long loadRoutineLoadJobs(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_49) {\n-            Catalog.getCurrentCatalog().getRoutineLoadManager().readFields(dis);\n-        }\n-        LOG.info(\"finished replay routineLoadJobs from image\");\n-        return checksum;\n-    }\n-\n-    public long loadLoadJobsV2(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_50) {\n-            loadManager.readFields(in);\n-        }\n-        LOG.info(\"finished replay loadJobsV2 from image\");\n-        return checksum;\n-    }\n-\n-    public long loadResources(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_87) {\n-            resourceMgr = ResourceMgr.read(in);\n-        }\n-        LOG.info(\"finished replay resources from image\");\n-        return checksum;\n-    }\n-\n-    public long loadSmallFiles(DataInputStream in, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_52) {\n-            smallFileMgr.readFields(in);\n-        }\n-        LOG.info(\"finished replay smallFiles from image\");\n-        return checksum;\n-    }\n-\n-    // Only called by checkpoint thread\n-    public void saveImage() throws IOException {\n-        // Write image.ckpt\n-        Storage storage = new Storage(this.imageDir);\n-        File curFile = storage.getImageFile(replayedJournalId.get());\n-        File ckpt = new File(this.imageDir, Storage.IMAGE_NEW);\n-        saveImage(ckpt, replayedJournalId.get());\n-\n-        // Move image.ckpt to image.dataVersion\n-        LOG.info(\"Move \" + ckpt.getAbsolutePath() + \" to \" + curFile.getAbsolutePath());\n-        if (!ckpt.renameTo(curFile)) {\n-            curFile.delete();\n-            throw new IOException();\n-        }\n-    }\n-\n-    public void saveImage(File curFile, long replayedJournalId) throws IOException {\n-        if (!curFile.exists()) {\n-            curFile.createNewFile();\n-        }\n-\n-        // save image does not need any lock. because only checkpoint thread will call this method.\n-        LOG.info(\"start save image to {}. is ckpt: {}\", curFile.getAbsolutePath(), Catalog.isCheckpointThread());\n-\n-        long checksum = 0;\n-        long saveImageStartTime = System.currentTimeMillis();\n-        try (DataOutputStream dos = new DataOutputStream(new FileOutputStream(curFile))) {\n-            checksum = saveHeader(dos, replayedJournalId, checksum);\n-            checksum = saveMasterInfo(dos, checksum);\n-            checksum = saveFrontends(dos, checksum);\n-            checksum = Catalog.getCurrentSystemInfo().saveBackends(dos, checksum);\n-            checksum = saveDb(dos, checksum);\n-            checksum = saveLoadJob(dos, checksum);\n-            checksum = saveAlterJob(dos, checksum);\n-            checksum = saveRecycleBin(dos, checksum);\n-            checksum = saveGlobalVariable(dos, checksum);\n-            checksum = saveCluster(dos, checksum);\n-            checksum = saveBrokers(dos, checksum);\n-            checksum = saveResources(dos, checksum);\n-            checksum = saveExportJob(dos, checksum);\n-            checksum = saveBackupHandler(dos, checksum);\n-            checksum = savePaloAuth(dos, checksum);\n-            checksum = saveTransactionState(dos, checksum);\n-            checksum = saveColocateTableIndex(dos, checksum);\n-            checksum = saveRoutineLoadJobs(dos, checksum);\n-            checksum = saveLoadJobsV2(dos, checksum);\n-            checksum = saveSmallFiles(dos, checksum);\n-            checksum = savePlugins(dos, checksum);\n-            checksum = saveDeleteHandler(dos, checksum);\n-            dos.writeLong(checksum);\n-        }\n-\n-        long saveImageEndTime = System.currentTimeMillis();\n-        LOG.info(\"finished save image {} in {} ms. checksum is {}\",\n-                curFile.getAbsolutePath(), (saveImageEndTime - saveImageStartTime), checksum);\n-    }\n-\n-    public long saveHeader(DataOutputStream dos, long replayedJournalId, long checksum) throws IOException {\n-        // Write meta version\n-        checksum ^= FeConstants.meta_version;\n-        dos.writeInt(FeConstants.meta_version);\n-\n-        // Write replayed journal id\n-        checksum ^= replayedJournalId;\n-        dos.writeLong(replayedJournalId);\n-\n-        // Write id\n-        long id = idGenerator.getBatchEndId();\n-        checksum ^= id;\n-        dos.writeLong(id);\n-\n-        dos.writeBoolean(isDefaultClusterCreated);\n-\n-        return checksum;\n-    }\n-\n-    public long saveMasterInfo(DataOutputStream dos, long checksum) throws IOException {\n-        Text.writeString(dos, masterIp);\n-\n-        checksum ^= masterRpcPort;\n-        dos.writeInt(masterRpcPort);\n-\n-        checksum ^= masterHttpPort;\n-        dos.writeInt(masterHttpPort);\n-\n-        return checksum;\n-    }\n-\n-    public long saveFrontends(DataOutputStream dos, long checksum) throws IOException {\n-        int size = frontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (Frontend fe : frontends.values()) {\n-            fe.write(dos);\n-        }\n-\n-        size = removedFrontends.size();\n-        checksum ^= size;\n-\n-        dos.writeInt(size);\n-        for (String feName : removedFrontends) {\n-            Text.writeString(dos, feName);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveDb(DataOutputStream dos, long checksum) throws IOException {\n-        int dbCount = idToDb.size() - nameToCluster.keySet().size();\n-        checksum ^= dbCount;\n-        dos.writeInt(dbCount);\n-        for (Map.Entry<Long, Database> entry : idToDb.entrySet()) {\n-            Database db = entry.getValue();\n-            String dbName = db.getFullName();\n-            // Don't write information_schema db meta\n-            if (!InfoSchemaDb.isInfoSchemaDb(dbName)) {\n-                checksum ^= entry.getKey();\n-                db.readLock();\n-                try {\n-                    db.write(dos);\n-                } finally {\n-                    db.readUnlock();\n-                }\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveLoadJob(DataOutputStream dos, long checksum) throws IOException {\n-        // 1. save load.dbToLoadJob\n-        Map<Long, List<LoadJob>> dbToLoadJob = load.getDbToLoadJobs();\n-        int jobSize = dbToLoadJob.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToLoadJob.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> loadJobs = entry.getValue();\n-            int loadJobCount = loadJobs.size();\n-            checksum ^= loadJobCount;\n-            dos.writeInt(loadJobCount);\n-            for (LoadJob job : loadJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        // 2. save delete jobs\n-        Map<Long, List<DeleteInfo>> dbToDeleteInfos = load.getDbToDeleteInfos();\n-        jobSize = dbToDeleteInfos.size();\n-        checksum ^= jobSize;\n-        dos.writeInt(jobSize);\n-        for (Entry<Long, List<DeleteInfo>> entry : dbToDeleteInfos.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<DeleteInfo> deleteInfos = entry.getValue();\n-            int deletInfoCount = deleteInfos.size();\n-            checksum ^= deletInfoCount;\n-            dos.writeInt(deletInfoCount);\n-            for (DeleteInfo deleteInfo : deleteInfos) {\n-                deleteInfo.write(dos);\n-            }\n-        }\n-\n-        // 3. load error hub info\n-        LoadErrorHub.Param param = load.getLoadErrorHubInfo();\n-        param.write(dos);\n-\n-        // 4. save delete load job info\n-        Map<Long, List<LoadJob>> dbToDeleteJobs = load.getDbToDeleteJobs();\n-        int deleteJobSize = dbToDeleteJobs.size();\n-        checksum ^= deleteJobSize;\n-        dos.writeInt(deleteJobSize);\n-        for (Entry<Long, List<LoadJob>> entry : dbToDeleteJobs.entrySet()) {\n-            long dbId = entry.getKey();\n-            checksum ^= dbId;\n-            dos.writeLong(dbId);\n-\n-            List<LoadJob> deleteJobs = entry.getValue();\n-            int deleteJobCount = deleteJobs.size();\n-            checksum ^= deleteJobCount;\n-            dos.writeInt(deleteJobCount);\n-            for (LoadJob job : deleteJobs) {\n-                job.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveExportJob(DataOutputStream dos, long checksum) throws IOException {\n-        Map<Long, ExportJob> idToJob = exportMgr.getIdToJob();\n-        int size = idToJob.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (ExportJob job : idToJob.values()) {\n-            long jobId = job.getId();\n-            checksum ^= jobId;\n-            dos.writeLong(jobId);\n-            job.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum) throws IOException {\n-        for (JobType type : JobType.values()) {\n-            checksum = saveAlterJob(dos, checksum, type);\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveAlterJob(DataOutputStream dos, long checksum, JobType type) throws IOException {\n-        Map<Long, AlterJob> alterJobs = null;\n-        ConcurrentLinkedQueue<AlterJob> finishedOrCancelledAlterJobs = null;\n-        Map<Long, AlterJobV2> alterJobsV2 = Maps.newHashMap();\n-        if (type == JobType.ROLLUP) {\n-            alterJobs = this.getRollupHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getRollupHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getRollupHandler().getAlterJobsV2();\n-        } else if (type == JobType.SCHEMA_CHANGE) {\n-            alterJobs = this.getSchemaChangeHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getSchemaChangeHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-            alterJobsV2 = this.getSchemaChangeHandler().getAlterJobsV2();\n-        } else if (type == JobType.DECOMMISSION_BACKEND) {\n-            alterJobs = this.getClusterHandler().unprotectedGetAlterJobs();\n-            finishedOrCancelledAlterJobs = this.getClusterHandler().unprotectedGetFinishedOrCancelledAlterJobs();\n-        }\n-\n-        // alter jobs\n-        int size = alterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (Entry<Long, AlterJob> entry : alterJobs.entrySet()) {\n-            long tableId = entry.getKey();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            entry.getValue().write(dos);\n-        }\n-\n-        // finished or cancelled jobs\n-        size = finishedOrCancelledAlterJobs.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJob alterJob : finishedOrCancelledAlterJobs) {\n-            long tableId = alterJob.getTableId();\n-            checksum ^= tableId;\n-            dos.writeLong(tableId);\n-            alterJob.write(dos);\n-        }\n-\n-        // alter job v2\n-        size = alterJobsV2.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        for (AlterJobV2 alterJobV2 : alterJobsV2.values()) {\n-            alterJobV2.write(dos);\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long savePaloAuth(DataOutputStream dos, long checksum) throws IOException {\n-        auth.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveTransactionState(DataOutputStream dos, long checksum) throws IOException {\n-        int size = globalTransactionMgr.getTransactionNum();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-        globalTransactionMgr.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRecycleBin(DataOutputStream dos, long checksum) throws IOException {\n-        CatalogRecycleBin recycleBin = Catalog.getCurrentRecycleBin();\n-        recycleBin.write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveColocateTableIndex(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentColocateIndex().write(dos);\n-        return checksum;\n-    }\n-\n-    public long saveRoutineLoadJobs(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getRoutineLoadManager().write(dos);\n-        return checksum;\n-    }\n-\n-    // global variable persistence\n-    public long loadGlobalVariable(DataInputStream in, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_22) {\n-            VariableMgr.read(in);\n-        }\n-        LOG.info(\"finished replay globalVariable from image\");\n-        return checksum;\n-    }\n-\n-    public long saveGlobalVariable(DataOutputStream out, long checksum) throws IOException {\n-        VariableMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void replayGlobalVariable(SessionVariable variable) throws IOException, DdlException {\n-        VariableMgr.replayGlobalVariable(variable);\n-    }\n-\n-    public long saveLoadJobsV2(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getLoadManager().write(out);\n-        return checksum;\n-    }\n-\n-\tpublic long saveResources(DataOutputStream out, long checksum) throws IOException {\n-        Catalog.getCurrentCatalog().getResourceMgr().write(out);\n-        return checksum;\n-    }\n-\n-    private long saveSmallFiles(DataOutputStream out, long checksum) throws IOException {\n-        smallFileMgr.write(out);\n-        return checksum;\n-    }\n-\n-    public void createLabelCleaner() {\n-        labelCleaner = new MasterDaemon(\"LoadLabelCleaner\", Config.label_clean_interval_second * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                load.removeOldLoadJobs();\n-                load.removeOldDeleteJobs();\n-                loadManager.removeOldLoadJob();\n-                exportMgr.removeOldExportJobs();\n-            }\n-        };\n-    }\n-\n-    public void createTxnCleaner() {\n-        txnCleaner = new MasterDaemon(\"txnCleaner\", Config.transaction_clean_interval_second) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                globalTransactionMgr.removeExpiredAndTimeoutTxns();\n-            }\n-        };\n-    }\n-\n-    public void createReplayer() {\n-        replayer = new Daemon(\"replayer\", REPLAY_INTERVAL_MS) {\n-            protected void runOneCycle() {\n-                boolean err = false;\n-                boolean hasLog = false;\n-                try {\n-                    hasLog = replayJournal(-1);\n-                    metaReplayState.setOk();\n-                } catch (InsufficientLogException insufficientLogEx) {\n-                    // Copy the missing log files from a member of the\n-                    // replication group who owns the files\n-                    LOG.error(\"catch insufficient log exception. please restart.\", insufficientLogEx);\n-                    NetworkRestore restore = new NetworkRestore();\n-                    NetworkRestoreConfig config = new NetworkRestoreConfig();\n-                    config.setRetainLogFiles(false);\n-                    restore.execute(insufficientLogEx, config);\n-                    System.exit(-1);\n-                } catch (Throwable e) {\n-                    LOG.error(\"replayer thread catch an exception when replay journal.\", e);\n-                    metaReplayState.setException(e);\n-                    try {\n-                        Thread.sleep(5000);\n-                    } catch (InterruptedException e1) {\n-                        LOG.error(\"sleep got exception. \", e);\n-                    }\n-                    err = true;\n-                }\n-\n-                setCanRead(hasLog, err);\n-            }\n-        };\n-        replayer.setMetaContext(metaContext);\n-    }\n-\n-    private void setCanRead(boolean hasLog, boolean err) {\n-        if (err) {\n-            canRead.set(false);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        if (Config.ignore_meta_check) {\n-            // can still offer read, but is not ready\n-            canRead.set(true);\n-            isReady.set(false);\n-            return;\n-        }\n-\n-        long currentTimeMs = System.currentTimeMillis();\n-        if (currentTimeMs - synchronizedTimeMs > Config.meta_delay_toleration_second * 1000) {\n-            // we still need this log to observe this situation\n-            // but service may be continued when there is no log being replayed.\n-            LOG.warn(\"meta out of date. current time: {}, synchronized time: {}, has log: {}, fe type: {}\",\n-                    currentTimeMs, synchronizedTimeMs, hasLog, feType);\n-            if (hasLog || feType == FrontendNodeType.UNKNOWN) {\n-                // 1. if we read log from BDB, which means master is still alive.\n-                // So we need to set meta out of date.\n-                // 2. if we didn't read any log from BDB and feType is UNKNOWN,\n-                // which means this non-master node is disconnected with master.\n-                // So we need to set meta out of date either.\n-                metaReplayState.setOutOfDate(currentTimeMs, synchronizedTimeMs);\n-                canRead.set(false);\n-                isReady.set(false);\n-            }\n-\n-            // sleep 5s to avoid numerous 'meta out of date' log\n-            try {\n-                Thread.sleep(5000L);\n-            } catch (InterruptedException e) {\n-                LOG.error(\"unhandled exception when sleep\", e);\n-            }\n-\n-        } else {\n-            canRead.set(true);\n-            isReady.set(true);\n-        }\n-    }\n-\n-    public void notifyNewFETypeTransfer(FrontendNodeType newType) {\n-        try {\n-            String msg = \"notify new FE type transfer: \" + newType;\n-            LOG.warn(msg);\n-            Util.stdoutWithTime(msg);\n-            this.typeTransferQueue.put(newType);\n-        } catch (InterruptedException e) {\n-            LOG.error(\"failed to put new FE type: {}\", newType, e);\n-        }\n-    }\n-\n-    public void createStateListener() {\n-        listener = new Daemon(\"stateListener\", STATE_CHANGE_CHECK_INTERVAL_MS) {\n-            @Override\n-            protected synchronized void runOneCycle() {\n-\n-                while (true) {\n-                    FrontendNodeType newType = null;\n-                    try {\n-                        newType = typeTransferQueue.take();\n-                    } catch (InterruptedException e) {\n-                        LOG.error(\"got exception when take FE type from queue\", e);\n-                        Util.stdoutWithTime(\"got exception when take FE type from queue. \" + e.getMessage());\n-                        System.exit(-1);\n-                    }\n-                    Preconditions.checkNotNull(newType);\n-                    LOG.info(\"begin to transfer FE type from {} to {}\", feType, newType);\n-                    if (feType == newType) {\n-                        return;\n-                    }\n-\n-                    /*\n-                     * INIT -> MASTER: transferToMaster\n-                     * INIT -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * UNKNOWN -> MASTER: transferToMaster\n-                     * UNKNOWN -> FOLLOWER/OBSERVER: transferToNonMaster\n-                     * FOLLOWER -> MASTER: transferToMaster\n-                     * FOLLOWER/OBSERVER -> INIT/UNKNOWN: set isReady to false\n-                     */\n-                    switch (feType) {\n-                        case INIT: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                case UNKNOWN:\n-                                    break;\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case UNKNOWN: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case FOLLOWER:\n-                                case OBSERVER: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case FOLLOWER: {\n-                            switch (newType) {\n-                                case MASTER: {\n-                                    transferToMaster();\n-                                    break;\n-                                }\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case OBSERVER: {\n-                            switch (newType) {\n-                                case UNKNOWN: {\n-                                    transferToNonMaster(newType);\n-                                    break;\n-                                }\n-                                default:\n-                                    break;\n-                            }\n-                            break;\n-                        }\n-                        case MASTER: {\n-                            // exit if master changed to any other type\n-                            String msg = \"transfer FE type from MASTER to \" + newType.name() + \". exit\";\n-                            LOG.error(msg);\n-                            Util.stdoutWithTime(msg);\n-                            System.exit(-1);\n-                        }\n-                        default:\n-                            break;\n-                    } // end switch formerFeType\n-\n-                    feType = newType;\n-                    LOG.info(\"finished to transfer FE type to {}\", feType);\n-                }\n-            } // end runOneCycle\n-        };\n-\n-        listener.setMetaContext(metaContext);\n-    }\n-\n-    public synchronized boolean replayJournal(long toJournalId) {\n-        long newToJournalId = toJournalId;\n-        if (newToJournalId == -1) {\n-            newToJournalId = getMaxJournalId();\n-        }\n-        if (newToJournalId <= replayedJournalId.get()) {\n-            return false;\n-        }\n-\n-        LOG.info(\"replayed journal id is {}, replay to journal id is {}\", replayedJournalId, newToJournalId);\n-        JournalCursor cursor = editLog.read(replayedJournalId.get() + 1, newToJournalId);\n-        if (cursor == null) {\n-            LOG.warn(\"failed to get cursor from {} to {}\", replayedJournalId.get() + 1, newToJournalId);\n-            return false;\n-        }\n-\n-        long startTime = System.currentTimeMillis();\n-        boolean hasLog = false;\n-        while (true) {\n-            JournalEntity entity = cursor.next();\n-            if (entity == null) {\n-                break;\n-            }\n-            hasLog = true;\n-            EditLog.loadJournal(this, entity);\n-            replayedJournalId.incrementAndGet();\n-            LOG.debug(\"journal {} replayed.\", replayedJournalId);\n-            if (feType != FrontendNodeType.MASTER) {\n-                journalObservable.notifyObservers(replayedJournalId.get());\n-            }\n-            if (MetricRepo.isInit.get()) {\n-                // Metric repo may not init after this replay thread start\n-                MetricRepo.COUNTER_EDIT_LOG_READ.increase(1L);\n-            }\n-        }\n-        long cost = System.currentTimeMillis() - startTime;\n-        if (cost >= 1000) {\n-            LOG.warn(\"replay journal cost too much time: {} replayedJournalId: {}\", cost, replayedJournalId);\n-        }\n-\n-        return hasLog;\n-    }\n-\n-    public void createTimePrinter() {\n-        // time printer will write timestamp edit log every 10 seconds\n-        timePrinter = new MasterDaemon(\"timePrinter\", 10 * 1000L) {\n-            @Override\n-            protected void runAfterCatalogReady() {\n-                Timestamp stamp = new Timestamp();\n-                editLog.logTimestamp(stamp);\n-            }\n-        };\n-    }\n-\n-    public void addFrontend(FrontendNodeType role, String host, int editLogPort) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, editLogPort);\n-            if (fe != null) {\n-                throw new DdlException(\"frontend already exists \" + fe);\n-            }\n-\n-            String nodeName = genFeNodeName(host, editLogPort, false /* new name style */);\n-\n-            if (removedFrontends.contains(nodeName)) {\n-                throw new DdlException(\"frontend name already exists \" + nodeName + \". Try again\");\n-            }\n-\n-            fe = new Frontend(role, nodeName, host, editLogPort);\n-            frontends.put(nodeName, fe);\n-            if (role == FrontendNodeType.FOLLOWER || role == FrontendNodeType.REPLICA) {\n-                ((BDBHA) getHaProtocol()).addHelperSocket(host, editLogPort);\n-                helperNodes.add(Pair.create(host, editLogPort));\n-            }\n-            editLog.logAddFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropFrontend(FrontendNodeType role, String host, int port) throws DdlException {\n-        if (host.equals(selfNode.first) && port == selfNode.second && feType == FrontendNodeType.MASTER) {\n-            throw new DdlException(\"can not drop current master node.\");\n-        }\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Frontend fe = checkFeExist(host, port);\n-            if (fe == null) {\n-                throw new DdlException(\"frontend does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            if (fe.getRole() != role) {\n-                throw new DdlException(role.toString() + \" does not exist[\" + host + \":\" + port + \"]\");\n-            }\n-            frontends.remove(fe.getNodeName());\n-            removedFrontends.add(fe.getNodeName());\n-\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                haProtocol.removeElectableNode(fe.getNodeName());\n-                helperNodes.remove(Pair.create(host, port));\n-            }\n-            editLog.logRemoveFrontend(fe);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Frontend checkFeExist(String host, int port) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host) && fe.getEditLogPort() == port) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByHost(String host) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getHost().equals(host)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Frontend getFeByName(String name) {\n-        for (Frontend fe : frontends.values()) {\n-            if (fe.getNodeName().equals(name)) {\n-                return fe;\n-            }\n-        }\n-        return null;\n-    }\n-\n-\n-    // The interface which DdlExecutor needs.\n-    public void createDb(CreateDbStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        String fullDbName = stmt.getFullDbName();\n-        long id = 0L;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_SELECT_CLUSTER, clusterName);\n-            }\n-            if (fullNameToDb.containsKey(fullDbName)) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create database[{}] which already exists\", fullDbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, fullDbName);\n-                }\n-            } else {\n-                id = getNextId();\n-                Database db = new Database(id, fullDbName);\n-                db.setClusterName(clusterName);\n-                unprotectCreateDb(db);\n-                editLog.logCreateDb(db);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-        LOG.info(\"createDb dbName = \" + fullDbName + \", id = \" + id);\n-    }\n-\n-    // For replay edit log, need't lock metadata\n-    public void unprotectCreateDb(Database db) {\n-        idToDb.put(db.getId(), db);\n-        fullNameToDb.put(db.getFullName(), db);\n-        final Cluster cluster = nameToCluster.get(db.getClusterName());\n-        cluster.addDb(db.getFullName(), db.getId());\n-        globalTransactionMgr.addDatabaseTransactionMgr(db.getId());\n-    }\n-\n-    // for test\n-    public void addCluster(Cluster cluster) {\n-        nameToCluster.put(cluster.getName(), cluster);\n-        idToCluster.put(cluster.getId(), cluster);\n-    }\n-\n-    public void replayCreateDb(Database db) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateDb(db);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void dropDb(DropDbStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-\n-        // 1. check if database exists\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!fullNameToDb.containsKey(dbName)) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop database[{}] which does not exist\", dbName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_DB_DROP_EXISTS, dbName);\n-                }\n-            }\n-\n-            // 2. drop tables in db\n-            Database db = this.fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                if (stmt.isNeedCheckCommittedTxns()) {\n-                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), null, null)) {\n-                       throw new DdlException(\"There are still some transactions in the COMMITTED state waiting to be completed. \" +\n-                               \"The database [\" + dbName +\"] cannot be dropped. If you want to forcibly drop(cannot be recovered),\" +\n-                               \" please use \\\"DROP database force\\\".\");\n-                    }\n-                }\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getAttachDb())) {\n-                    // We try to drop a hard link.\n-                    final DropLinkDbAndUpdateDbInfo info = new DropLinkDbAndUpdateDbInfo();\n-                    fullNameToDb.remove(db.getAttachDb());\n-                    db.setDbState(DbState.NORMAL);\n-                    info.setUpdateDbState(DbState.NORMAL);\n-                    final Cluster cluster = nameToCluster\n-                            .get(ClusterNamespace.getClusterNameFromFullName(db.getAttachDb()));\n-                    final BaseParam param = new BaseParam();\n-                    param.addStringParam(db.getAttachDb());\n-                    param.addLongParam(db.getId());\n-                    cluster.removeLinkDb(param);\n-                    info.setDropDbCluster(cluster.getName());\n-                    info.setDropDbId(db.getId());\n-                    info.setDropDbName(db.getAttachDb());\n-                    editLog.logDropLinkDb(info);\n-                    return;\n-                }\n-\n-                if (db.getDbState() == DbState.LINK && dbName.equals(db.getFullName())) {\n-                    // We try to drop a db which other dbs attach to it,\n-                    // which is not allowed.\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                if (dbName.equals(db.getAttachDb()) && db.getDbState() == DbState.MOVE) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                            ClusterNamespace.getNameFromFullName(dbName));\n-                    return;\n-                }\n-\n-                // save table names for recycling\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            // 3. remove db from catalog\n-            idToDb.remove(db.getId());\n-            fullNameToDb.remove(db.getFullName());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-            editLog.logDropDb(dbName);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"finish drop database[{}]\", dbName);\n-    }\n-\n-    public void unprotectDropDb(Database db) {\n-        for (Table table : db.getTables()) {\n-            unprotectDropTable(db, table.getId());\n-        }\n-    }\n-\n-    public void replayDropLinkDb(DropLinkDbAndUpdateDbInfo info) {\n-        tryLock(true);\n-        try {\n-            final Database db = this.fullNameToDb.remove(info.getDropDbName());\n-            db.setDbState(info.getUpdateDbState());\n-            final Cluster cluster = nameToCluster\n-                    .get(info.getDropDbCluster());\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(db.getAttachDb());\n-            param.addLongParam(db.getId());\n-            cluster.removeLinkDb(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropDb(String dbName) throws DdlException {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            db.writeLock();\n-            try {\n-                Set<String> tableNames = db.getTableNamesWithLock();\n-                unprotectDropDb(db);\n-                Catalog.getCurrentRecycleBin().recycleDatabase(db, tableNames);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-\n-            fullNameToDb.remove(dbName);\n-            idToDb.remove(db.getId());\n-            final Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(dbName, db.getId());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void recoverDatabase(RecoverDbStmt recoverStmt) throws DdlException {\n-        // check is new db with same name already exist\n-        if (getDb(recoverStmt.getDbName()) != null) {\n-            throw new DdlException(\"Database[\" + recoverStmt.getDbName() + \"] already exist.\");\n-        }\n-\n-        Database db = Catalog.getCurrentRecycleBin().recoverDatabase(recoverStmt.getDbName());\n-\n-        // add db to catalog\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (fullNameToDb.containsKey(db.getFullName())) {\n-                throw new DdlException(\"Database[\" + db.getFullName() + \"] already exist.\");\n-                // it's ok that we do not put db back to CatalogRecycleBin\n-                // cause this db cannot recover any more\n-            }\n-\n-            fullNameToDb.put(db.getFullName(), db);\n-            idToDb.put(db.getId(), db);\n-\n-            // log\n-            RecoverInfo recoverInfo = new RecoverInfo(db.getId(), -1L, -1L);\n-            editLog.logRecoverDb(recoverInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"recover database[{}]\", db.getId());\n-    }\n-\n-    public void recoverTable(RecoverTableStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table != null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-            }\n-\n-            if (!Catalog.getCurrentRecycleBin().recoverTable(db, tableName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void recoverPartition(RecoverPartitionStmt recoverStmt) throws DdlException {\n-        String dbName = recoverStmt.getDbName();\n-\n-        Database db = null;\n-        if ((db = getDb(dbName)) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        String tableName = recoverStmt.getTableName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"table[\" + tableName + \"] is not OLAP table\");\n-            }\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            String partitionName = recoverStmt.getPartitionName();\n-            if (olapTable.getPartition(partitionName) != null) {\n-                throw new DdlException(\"partition[\" + partitionName + \"] already exist in table[\" + tableName + \"]\");\n-            }\n-\n-            Catalog.getCurrentRecycleBin().recoverPartition(db.getId(), olapTable, partitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseDatabase(long dbId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseDatabase(dbId);\n-    }\n-\n-    public void replayRecoverDatabase(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = Catalog.getCurrentRecycleBin().replayRecoverDatabase(dbId);\n-\n-        // add db to catalog\n-        replayCreateDb(db);\n-\n-        LOG.info(\"replay recover db[{}]\", dbId);\n-    }\n-\n-    public void alterDatabaseQuota(AlterDatabaseQuotaStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        QuotaType quotaType = stmt.getQuotaType();\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(stmt.getQuota());\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(stmt.getQuota());\n-        }\n-        long quota = stmt.getQuota();\n-        DatabaseInfo dbInfo = new DatabaseInfo(dbName, \"\", quota, quotaType);\n-        editLog.logAlterDb(dbInfo);\n-    }\n-\n-    public void replayAlterDatabaseQuota(String dbName, long quota, QuotaType quotaType) {\n-        Database db = getDb(dbName);\n-        Preconditions.checkNotNull(db);\n-        if (quotaType == QuotaType.DATA) {\n-            db.setDataQuotaWithLock(quota);\n-        } else if (quotaType == QuotaType.REPLICA) {\n-            db.setReplicaQuotaWithLock(quota);\n-        }\n-    }\n-\n-    public void renameDatabase(AlterDatabaseRename stmt) throws DdlException {\n-        String fullDbName = stmt.getDbName();\n-        String newFullDbName = stmt.getNewDbName();\n-        String clusterName = stmt.getClusterName();\n-\n-        if (fullDbName.equals(newFullDbName)) {\n-            throw new DdlException(\"Same database name\");\n-        }\n-\n-        Database db = null;\n-        Cluster cluster = null;\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            // check if db exists\n-            db = fullNameToDb.get(fullDbName);\n-            if (db == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, fullDbName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK || db.getDbState() == DbState.MOVE) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_RENAME_DB_ERR, fullDbName);\n-            }\n-            // check if name is already used\n-            if (fullNameToDb.get(newFullDbName) != null) {\n-                throw new DdlException(\"Database name[\" + newFullDbName + \"] is already used\");\n-            }\n-\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            cluster.addDb(newFullDbName, db.getId());\n-            // 1. rename db\n-            db.setNameWithLock(newFullDbName);\n-\n-            // 2. add to meta. check again\n-            fullNameToDb.remove(fullDbName);\n-            fullNameToDb.put(newFullDbName, db);\n-\n-            DatabaseInfo dbInfo = new DatabaseInfo(fullDbName, newFullDbName, -1L, QuotaType.NONE);\n-            editLog.logDatabaseRename(dbInfo);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"rename database[{}] to [{}]\", fullDbName, newFullDbName);\n-    }\n-\n-    public void replayRenameDatabase(String dbName, String newDbName) {\n-        tryLock(true);\n-        try {\n-            Database db = fullNameToDb.get(dbName);\n-            Cluster cluster = nameToCluster.get(db.getClusterName());\n-            cluster.removeDb(db.getFullName(), db.getId());\n-            db.setName(newDbName);\n-            cluster.addDb(newDbName, db.getId());\n-            fullNameToDb.remove(dbName);\n-            fullNameToDb.put(newDbName, db);\n-        } finally {\n-            unlock();\n-        }\n-\n-        LOG.info(\"replay rename database {} to {}\", dbName, newDbName);\n-    }\n-\n-    public void createTable(CreateTableStmt stmt) throws DdlException {\n-        String engineName = stmt.getEngineName();\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check if db exists\n-        Database db = getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // only internal table should check quota and cluster capacity\n-        if (!stmt.isExternal()) {\n-            // check cluster capacity\n-            Catalog.getCurrentSystemInfo().checkClusterCapacity(stmt.getClusterName());\n-            // check db quota\n-            db.checkQuota();\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create table[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        if (engineName.equals(\"olap\")) {\n-            createOlapTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"mysql\")) {\n-            createMysqlTable(db, stmt);\n-            return;\n-        } else if (engineName.equals(\"broker\")) {\n-            createBrokerTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"elasticsearch\") || engineName.equalsIgnoreCase(\"es\")) {\n-            createEsTable(db, stmt);\n-            return;\n-        } else if (engineName.equalsIgnoreCase(\"hive\")) {\n-            createHiveTable(db, stmt);\n-            return;\n-        } else {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_UNKNOWN_STORAGE_ENGINE, engineName);\n-        }\n-        Preconditions.checkState(false);\n-        return;\n-    }\n-\n-    public void addPartition(Database db, String tableName, AddPartitionClause addPartitionClause) throws DdlException {\n-        SingleRangePartitionDesc singlePartitionDesc = addPartitionClause.getSingeRangePartitionDesc();\n-        DistributionDesc distributionDesc = addPartitionClause.getDistributionDesc();\n-        boolean isTempPartition = addPartitionClause.isTempPartition();\n-\n-        DistributionInfo distributionInfo = null;\n-        OlapTable olapTable = null;\n-\n-        Map<Long, MaterializedIndexMeta> indexIdToMeta;\n-        Set<String> bfColumns = null;\n-\n-        String partitionName = singlePartitionDesc.getPartitionName();\n-\n-        // check\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            // check state\n-            olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-            }\n-\n-            // check partition type\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (partitionInfo.getType() != PartitionType.RANGE) {\n-                throw new DdlException(\"Only support adding partition to range partitioned table\");\n-            }\n-\n-            // check partition name\n-            if (olapTable.checkPartitionNameExist(partitionName)) {\n-                if (singlePartitionDesc.isSetIfNotExists()) {\n-                    LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                }\n-            }\n-\n-            Map<String, String> properties = singlePartitionDesc.getProperties();\n-            // partition properties should inherit table properties\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM, replicationNum.toString());\n-            }\n-            if (!properties.containsKey(PropertyAnalyzer.PROPERTIES_INMEMORY)) {\n-                properties.put(PropertyAnalyzer.PROPERTIES_INMEMORY, olapTable.isInMemory().toString());\n-            }\n-\n-            RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-            singlePartitionDesc.analyze(rangePartitionInfo.getPartitionColumns().size(), properties);\n-            rangePartitionInfo.checkAndCreateRange(singlePartitionDesc, isTempPartition);\n-\n-            // get distributionInfo\n-            List<Column> baseSchema = olapTable.getBaseSchema();\n-            DistributionInfo defaultDistributionInfo = olapTable.getDefaultDistributionInfo();\n-            if (distributionDesc != null) {\n-                distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-                // for now. we only support modify distribution's bucket num\n-                if (distributionInfo.getType() != defaultDistributionInfo.getType()) {\n-                    throw new DdlException(\"Cannot assign different distribution type. default is: \"\n-                            + defaultDistributionInfo.getType());\n-                }\n-\n-                if (distributionInfo.getType() == DistributionInfoType.HASH) {\n-                    HashDistributionInfo hashDistributionInfo = (HashDistributionInfo) distributionInfo;\n-                    List<Column> newDistriCols = hashDistributionInfo.getDistributionColumns();\n-                    List<Column> defaultDistriCols = ((HashDistributionInfo) defaultDistributionInfo)\n-                            .getDistributionColumns();\n-                    if (!newDistriCols.equals(defaultDistriCols)) {\n-                        throw new DdlException(\"Cannot assign hash distribution with different distribution cols. \"\n-                                + \"default is: \" + defaultDistriCols);\n-                    }\n-                    if (hashDistributionInfo.getBucketNum() <= 0) {\n-                        throw new DdlException(\"Cannot assign hash distribution buckets less than 1\");\n-                    }\n-                }\n-            } else {\n-                distributionInfo = defaultDistributionInfo;\n-            }\n-\n-            // check colocation\n-            if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-                String fullGroupName = db.getId() + \"_\" + olapTable.getColocateGroup();\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                Preconditions.checkNotNull(groupSchema);\n-                groupSchema.checkDistribution(distributionInfo);\n-                groupSchema.checkReplicationNum(singlePartitionDesc.getReplicationNum());\n-            }\n-\n-            indexIdToMeta = olapTable.getCopiedIndexIdToMeta();\n-            bfColumns = olapTable.getCopiedBfColumns();\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        Preconditions.checkNotNull(distributionInfo);\n-        Preconditions.checkNotNull(olapTable);\n-        Preconditions.checkNotNull(indexIdToMeta);\n-\n-        // create partition outside db lock\n-        DataProperty dataProperty = singlePartitionDesc.getPartitionDataProperty();\n-        Preconditions.checkNotNull(dataProperty);\n-\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-        try {\n-            long partitionId = getNextId();\n-            Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                    olapTable.getId(),\n-                    olapTable.getBaseIndexId(),\n-                    partitionId, partitionName,\n-                    indexIdToMeta,\n-                    olapTable.getKeysType(),\n-                    distributionInfo,\n-                    dataProperty.getStorageMedium(),\n-                    singlePartitionDesc.getReplicationNum(),\n-                    singlePartitionDesc.getVersionInfo(),\n-                    bfColumns, olapTable.getBfFpp(),\n-                    tabletIdSet, olapTable.getCopiedIndexes(),\n-                    singlePartitionDesc.isInMemory(),\n-                    olapTable.getStorageFormat(),\n-                    singlePartitionDesc.getTabletType()\n-                    );\n-\n-            // check again\n-            db.writeLock();\n-            try {\n-                Table table = db.getTable(tableName);\n-                if (table == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-\n-                if (table.getType() != TableType.OLAP) {\n-                    throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-                }\n-\n-                olapTable = (OlapTable) table;\n-                if (olapTable.getState() != OlapTableState.NORMAL) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s state is not NORMAL\");\n-                }\n-\n-                // check partition name\n-                if (olapTable.checkPartitionNameExist(partitionName)) {\n-                    if (singlePartitionDesc.isSetIfNotExists()) {\n-                        LOG.info(\"add partition[{}] which already exists\", partitionName);\n-                        return;\n-                    } else {\n-                        ErrorReport.reportDdlException(ErrorCode.ERR_SAME_NAME_PARTITION, partitionName);\n-                    }\n-                }\n-\n-                // check if meta changed\n-                // rollup index may be added or dropped during add partition operation.\n-                // schema may be changed during add partition operation.\n-                boolean metaChanged = false;\n-                if (olapTable.getIndexNameToId().size() != indexIdToMeta.size()) {\n-                    metaChanged = true;\n-                } else {\n-                    // compare schemaHash\n-                    for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                        long indexId = entry.getKey();\n-                        if (!indexIdToMeta.containsKey(indexId)) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                        if (indexIdToMeta.get(indexId).getSchemaHash() != entry.getValue().getSchemaHash()) {\n-                            metaChanged = true;\n-                            break;\n-                        }\n-                    }\n-                }\n-\n-                if (metaChanged) {\n-                    throw new DdlException(\"Table[\" + tableName + \"]'s meta has been changed. try again.\");\n-                }\n-\n-                // check partition type\n-                PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                if (partitionInfo.getType() != PartitionType.RANGE) {\n-                    throw new DdlException(\"Only support adding partition to range partitioned table\");\n-                }\n-\n-                // update partition info\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                rangePartitionInfo.handleNewSinglePartitionDesc(singlePartitionDesc, partitionId, isTempPartition);\n-\n-                if (isTempPartition) {\n-                    olapTable.addTempPartition(partition);\n-                } else {\n-                    olapTable.addPartition(partition);\n-                }\n-\n-                // log\n-                PartitionPersistInfo info = new PartitionPersistInfo(db.getId(), olapTable.getId(), partition,\n-                        rangePartitionInfo.getRange(partitionId), dataProperty,\n-                        rangePartitionInfo.getReplicationNum(partitionId),\n-                        rangePartitionInfo.getIsInMemory(partitionId),\n-                        isTempPartition);\n-                editLog.logAddPartition(info);\n-\n-                LOG.info(\"succeed in creating partition[{}], temp: {}\", partitionId, isTempPartition);\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-    }\n-\n-    public void replayAddPartition(PartitionPersistInfo info) throws DdlException {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            Partition partition = info.getPartition();\n-\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.isTempPartition()) {\n-                olapTable.addTempPartition(partition);\n-            } else {\n-                olapTable.addPartition(partition);\n-            }\n-\n-            ((RangePartitionInfo) partitionInfo).unprotectHandleNewSinglePartitionDesc(partition.getId(),\n-                    info.isTempPartition(), info.getRange(), info.getDataProperty(), info.getReplicationNum(),\n-                    info.isInMemory());\n-\n-            if (!isCheckpointThread()) {\n-                // add to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (MaterializedIndex index : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                    long indexId = index.getId();\n-                    int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                    TabletMeta tabletMeta = new TabletMeta(info.getDbId(), info.getTableId(), partition.getId(),\n-                            index.getId(), schemaHash, info.getDataProperty().getStorageMedium());\n-                    for (Tablet tablet : index.getTablets()) {\n-                        long tabletId = tablet.getId();\n-                        invertedIndex.addTablet(tabletId, tabletMeta);\n-                        for (Replica replica : tablet.getReplicas()) {\n-                            invertedIndex.addReplica(tabletId, replica);\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void dropPartition(Database db, OlapTable olapTable, DropPartitionClause clause) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-\n-        String partitionName = clause.getPartitionName();\n-        boolean isTempPartition = clause.isTempPartition();\n-\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        if (!olapTable.checkPartitionNameExist(partitionName, isTempPartition)) {\n-            if (clause.isSetIfExists()) {\n-                LOG.info(\"drop partition[{}] which does not exist\", partitionName);\n-                return;\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DROP_PARTITION_NON_EXISTENT, partitionName);\n-            }\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-        if (partitionInfo.getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Alter table [\" + olapTable.getName() + \"] failed. Not a partitioned table\");\n-        }\n-\n-        // drop\n-        if (isTempPartition) {\n-            olapTable.dropTempPartition(partitionName, true);\n-        } else {\n-            if (clause.isNeedCheckCommittedTxns()) {\n-                Partition partition = olapTable.getPartition(partitionName);\n-                if (partition != null) {\n-                    if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), olapTable.getId(), partition.getId())) {\n-                        throw new DdlException(\"There are still some transactions in the COMMITTED state waiting to be completed.\" +\n-                                \" The partition [\" + partitionName + \"] cannot be dropped. If you want to forcibly drop(cannot be recovered),\" +\n-                                \" please use \\\"DROP partition force\\\".\");\n-                    }\n-                }\n-            }\n-            olapTable.dropPartition(db.getId(), partitionName);\n-        }\n-\n-        // log\n-        DropPartitionInfo info = new DropPartitionInfo(db.getId(), olapTable.getId(), partitionName, isTempPartition);\n-        editLog.logDropPartition(info);\n-\n-        LOG.info(\"succeed in droping partition[{}]\", partitionName);\n-    }\n-\n-    public void replayDropPartition(DropPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            if (info.isTempPartition()) {\n-                olapTable.dropTempPartition(info.getPartitionName(), true);\n-            } else {\n-                olapTable.dropPartition(info.getDbId(), info.getPartitionName());\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayErasePartition(long partitionId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayErasePartition(partitionId);\n-    }\n-\n-    public void replayRecoverPartition(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(info.getTableId());\n-            Catalog.getCurrentRecycleBin().replayRecoverPartition((OlapTable) table, info.getPartitionId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void modifyPartitionProperty(Database db, OlapTable olapTable, String partitionName, Map<String, String> properties)\n-            throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        if (olapTable.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + olapTable.getName() + \"]'s state is not NORMAL\");\n-        }\n-\n-        Partition partition = olapTable.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\n-                    \"Partition[\" + partitionName + \"] does not exist in table[\" + olapTable.getName() + \"]\");\n-        }\n-\n-        PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-        // 1. data property\n-        DataProperty oldDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        DataProperty newDataProperty = null;\n-        try {\n-            newDataProperty = PropertyAnalyzer.analyzeDataProperty(properties, oldDataProperty);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(newDataProperty);\n-\n-        if (newDataProperty.equals(oldDataProperty)) {\n-            newDataProperty = null;\n-        }\n-\n-        // 2. replication num\n-        short oldReplicationNum = partitionInfo.getReplicationNum(partition.getId());\n-        short newReplicationNum = (short) -1;\n-        try {\n-            newReplicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, oldReplicationNum);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (newReplicationNum == oldReplicationNum) {\n-            newReplicationNum = (short) -1;\n-        } else if (Catalog.getCurrentColocateIndex().isColocateTable(olapTable.getId())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_TABLE_MUST_HAS_SAME_REPLICATION_NUM, oldReplicationNum);\n-        }\n-\n-        // 3. in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties,\n-                PropertyAnalyzer.PROPERTIES_INMEMORY, partitionInfo.getIsInMemory(partition.getId()));\n-\n-        // 4. tablet type\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // check if has other undefined properties\n-        if (properties != null && !properties.isEmpty()) {\n-            MapJoiner mapJoiner = Joiner.on(\", \").withKeyValueSeparator(\" = \");\n-            throw new DdlException(\"Unknown properties: \" + mapJoiner.join(properties));\n-        }\n-\n-        // modify meta here\n-        // date property\n-        if (newDataProperty != null) {\n-            partitionInfo.setDataProperty(partition.getId(), newDataProperty);\n-            LOG.debug(\"modify partition[{}-{}-{}] data property to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newDataProperty.toString());\n-        }\n-\n-        // replication num\n-        if (newReplicationNum != (short) -1) {\n-            partitionInfo.setReplicationNum(partition.getId(), newReplicationNum);\n-            LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    newReplicationNum);\n-        }\n-\n-        // in memory\n-        if (isInMemory != partitionInfo.getIsInMemory(partition.getId())) {\n-            partitionInfo.setIsInMemory(partition.getId(), isInMemory);\n-            LOG.debug(\"modify partition[{}-{}-{}] in memory to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    isInMemory);\n-        }\n-\n-        // tablet type\n-        // TODO: serialize to edit log\n-        if (tabletType != partitionInfo.getTabletType(partition.getId())) {\n-            partitionInfo.setTabletType(partition.getId(), tabletType);\n-            LOG.debug(\"modify partition[{}-{}-{}] tablet type to {}\", db.getId(), olapTable.getId(), partitionName,\n-                    tabletType);\n-        }\n-\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), olapTable.getId(), partition.getId(),\n-                newDataProperty, newReplicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-    }\n-\n-    public void replayModifyPartition(ModifyPartitionInfo info) {\n-        Database db = this.getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            if (info.getDataProperty() != null) {\n-                partitionInfo.setDataProperty(info.getPartitionId(), info.getDataProperty());\n-            }\n-            if (info.getReplicationNum() != (short) -1) {\n-                partitionInfo.setReplicationNum(info.getPartitionId(), info.getReplicationNum());\n-            }\n-            partitionInfo.setIsInMemory(info.getPartitionId(), info.isInMemory());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private Partition createPartitionWithIndices(String clusterName, long dbId, long tableId,\n-                                                 long baseIndexId, long partitionId, String partitionName,\n-                                                 Map<Long, MaterializedIndexMeta> indexIdToMeta,\n-                                                 KeysType keysType,\n-                                                 DistributionInfo distributionInfo,\n-                                                 TStorageMedium storageMedium,\n-                                                 short replicationNum,\n-                                                 Pair<Long, Long> versionInfo,\n-                                                 Set<String> bfColumns,\n-                                                 double bfFpp,\n-                                                 Set<Long> tabletIdSet,\n-                                                 List<Index> indexes,\n-                                                 boolean isInMemory,\n-                                                 TStorageFormat storageFormat,\n-                                                 TTabletType tabletType) throws DdlException {\n-        // create base index first.\n-        Preconditions.checkArgument(baseIndexId != -1);\n-        MaterializedIndex baseIndex = new MaterializedIndex(baseIndexId, IndexState.NORMAL);\n-\n-        // create partition with base index\n-        Partition partition = new Partition(partitionId, partitionName, baseIndex, distributionInfo);\n-\n-        // add to index map\n-        Map<Long, MaterializedIndex> indexMap = new HashMap<Long, MaterializedIndex>();\n-        indexMap.put(baseIndexId, baseIndex);\n-\n-        // create rollup index if has\n-        for (long indexId : indexIdToMeta.keySet()) {\n-            if (indexId == baseIndexId) {\n-                continue;\n-            }\n-\n-            MaterializedIndex rollup = new MaterializedIndex(indexId, IndexState.NORMAL);\n-            indexMap.put(indexId, rollup);\n-        }\n-\n-        // version and version hash\n-        if (versionInfo != null) {\n-            partition.updateVisibleVersionAndVersionHash(versionInfo.first, versionInfo.second);\n-        }\n-        long version = partition.getVisibleVersion();\n-        long versionHash = partition.getVisibleVersionHash();\n-\n-        for (Map.Entry<Long, MaterializedIndex> entry : indexMap.entrySet()) {\n-            long indexId = entry.getKey();\n-            MaterializedIndex index = entry.getValue();\n-            MaterializedIndexMeta indexMeta = indexIdToMeta.get(indexId);\n-\n-            // create tablets\n-            int schemaHash = indexMeta.getSchemaHash();\n-            TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, storageMedium);\n-            createTablets(clusterName, index, ReplicaState.NORMAL, distributionInfo, version, versionHash,\n-                    replicationNum, tabletMeta, tabletIdSet);\n-\n-            boolean ok = false;\n-            String errMsg = null;\n-\n-            // add create replica task for olap\n-            short shortKeyColumnCount = indexMeta.getShortKeyColumnCount();\n-            TStorageType storageType = indexMeta.getStorageType();\n-            List<Column> schema = indexMeta.getSchema();\n-            int totalTaskNum = index.getTablets().size() * replicationNum;\n-            MarkedCountDownLatch<Long, Long> countDownLatch = new MarkedCountDownLatch<Long, Long>(totalTaskNum);\n-            AgentBatchTask batchTask = new AgentBatchTask();\n-            for (Tablet tablet : index.getTablets()) {\n-                long tabletId = tablet.getId();\n-                for (Replica replica : tablet.getReplicas()) {\n-                    long backendId = replica.getBackendId();\n-                    countDownLatch.addMark(backendId, tabletId);\n-                    CreateReplicaTask task = new CreateReplicaTask(backendId, dbId, tableId,\n-                            partitionId, indexId, tabletId,\n-                            shortKeyColumnCount, schemaHash,\n-                            version, versionHash,\n-                            keysType,\n-                            storageType, storageMedium,\n-                            schema, bfColumns, bfFpp,\n-                            countDownLatch,\n-                            indexes,\n-                            isInMemory,\n-                            tabletType);\n-                    task.setStorageFormat(storageFormat);\n-                    batchTask.addTask(task);\n-                    // add to AgentTaskQueue for handling finish report.\n-                    // not for resending task\n-                    AgentTaskQueue.addTask(task);\n-                }\n-            }\n-            AgentTaskExecutor.submit(batchTask);\n-\n-            // estimate timeout\n-            long timeout = Config.tablet_create_timeout_second * 1000L * totalTaskNum;\n-            timeout = Math.min(timeout, Config.max_create_table_timeout_second * 1000);\n-            try {\n-                ok = countDownLatch.await(timeout, TimeUnit.MILLISECONDS);\n-            } catch (InterruptedException e) {\n-                LOG.warn(\"InterruptedException: \", e);\n-                ok = false;\n-            }\n-\n-            if (!ok || !countDownLatch.getStatus().ok()) {\n-                errMsg = \"Failed to create partition[\" + partitionName + \"]. Timeout.\";\n-                // clear tasks\n-                AgentTaskQueue.removeBatchTask(batchTask, TTaskType.CREATE);\n-\n-                if (!countDownLatch.getStatus().ok()) {\n-                    errMsg += \" Error: \" + countDownLatch.getStatus().getErrorMsg();\n-                } else {\n-                    List<Entry<Long, Long>> unfinishedMarks = countDownLatch.getLeftMarks();\n-                    // only show at most 3 results\n-                    List<Entry<Long, Long>> subList = unfinishedMarks.subList(0, Math.min(unfinishedMarks.size(), 3));\n-                    if (!subList.isEmpty()) {\n-                        errMsg += \" Unfinished mark: \" + Joiner.on(\", \").join(subList);\n-                    }\n-                }\n-                LOG.warn(errMsg);\n-                throw new DdlException(errMsg);\n-            }\n-\n-            if (index.getId() != baseIndexId) {\n-                // add rollup index to partition\n-                partition.createRollupIndex(index);\n-            }\n-        } // end for indexMap\n-        return partition;\n-    }\n-\n-    // Create olap table and related base index synchronously.\n-    private void createOlapTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        LOG.debug(\"begin create olap table: {}\", tableName);\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            // gen partition id first\n-            if (partitionDesc instanceof RangePartitionDesc) {\n-                RangePartitionDesc rangeDesc = (RangePartitionDesc) partitionDesc;\n-                for (SingleRangePartitionDesc desc : rangeDesc.getSingleRangePartitionDescs()) {\n-                    long partitionId = getNextId();\n-                    partitionNameToId.put(desc.getPartitionName(), partitionId);\n-                }\n-            }\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            if (DynamicPartitionUtil.checkDynamicPartitionPropertiesExist(stmt.getProperties())) {\n-                throw new DdlException(\"Only support dynamic partition properties on range partition table\");\n-            }\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        // get keys type\n-        KeysDesc keysDesc = stmt.getKeysDesc();\n-        Preconditions.checkNotNull(keysDesc);\n-        KeysType keysType = keysDesc.getKeysType();\n-\n-        // create distribution info\n-        DistributionDesc distributionDesc = stmt.getDistributionDesc();\n-        Preconditions.checkNotNull(distributionDesc);\n-        DistributionInfo distributionInfo = distributionDesc.toDistributionInfo(baseSchema);\n-\n-        // calc short key column count\n-        short shortKeyColumnCount = Catalog.calcShortKeyColumnCount(baseSchema, stmt.getProperties());\n-        LOG.debug(\"create table[{}] short key column count: {}\", tableName, shortKeyColumnCount);\n-\n-        // indexes\n-        TableIndexes indexes = new TableIndexes(stmt.getIndexes());\n-\n-        // create table\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        OlapTable olapTable = new OlapTable(tableId, tableName, baseSchema, keysType, partitionInfo,\n-                distributionInfo, indexes);\n-        olapTable.setComment(stmt.getComment());\n-\n-        // set base index id\n-        long baseIndexId = getNextId();\n-        olapTable.setBaseIndexId(baseIndexId);\n-\n-        // set base index info to table\n-        // this should be done before create partition.\n-        Map<String, String> properties = stmt.getProperties();\n-\n-        // analyze bloom filter columns\n-        Set<String> bfColumns = null;\n-        double bfFpp = 0;\n-        try {\n-            bfColumns = PropertyAnalyzer.analyzeBloomFilterColumns(properties, baseSchema);\n-            if (bfColumns != null && bfColumns.isEmpty()) {\n-                bfColumns = null;\n-            }\n-\n-            bfFpp = PropertyAnalyzer.analyzeBloomFilterFpp(properties);\n-            if (bfColumns != null && bfFpp == 0) {\n-                bfFpp = FeConstants.default_bloom_filter_fpp;\n-            } else if (bfColumns == null) {\n-                bfFpp = 0;\n-            }\n-\n-            olapTable.setBloomFilterInfo(bfColumns, bfFpp);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // analyze replication_num\n-        short replicationNum = FeConstants.default_replication_num;\n-        try {\n-            boolean isReplicationNumSet = properties != null && properties.containsKey(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM);\n-            replicationNum = PropertyAnalyzer.analyzeReplicationNum(properties, replicationNum);\n-            if (isReplicationNumSet) {\n-                olapTable.setReplicationNum(replicationNum);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // set in memory\n-        boolean isInMemory = PropertyAnalyzer.analyzeBooleanProp(properties, PropertyAnalyzer.PROPERTIES_INMEMORY, false);\n-        olapTable.setIsInMemory(isInMemory);\n-\n-        TTabletType tabletType = TTabletType.TABLET_TYPE_DISK;\n-        try {\n-            tabletType = PropertyAnalyzer.analyzeTabletType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-            // if this is an unpartitioned table, we should analyze data property and replication num here.\n-            // if this is a partitioned table, there properties are already analyzed in RangePartitionDesc analyze phase.\n-\n-            // use table name as this single partition name\n-            long partitionId = partitionNameToId.get(tableName);\n-            DataProperty dataProperty = null;\n-            try {\n-                dataProperty = PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(),\n-                        DataProperty.DEFAULT_DATA_PROPERTY);\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(dataProperty);\n-            partitionInfo.setDataProperty(partitionId, dataProperty);\n-            partitionInfo.setReplicationNum(partitionId, replicationNum);\n-            partitionInfo.setIsInMemory(partitionId, isInMemory);\n-            partitionInfo.setTabletType(partitionId, tabletType);\n-        }\n-\n-        // check colocation properties\n-        try {\n-            String colocateGroup = PropertyAnalyzer.analyzeColocate(properties);\n-            if (colocateGroup != null) {\n-                if (Config.disable_colocate_join) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_COLOCATE_FEATURE_DISABLED);\n-                }\n-                String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-                ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-                if (groupSchema != null) {\n-                    // group already exist, check if this table can be added to this group\n-                    groupSchema.checkColocateSchema(olapTable);\n-                }\n-                // add table to this group, if group does not exist, create a new one\n-                getColocateTableIndex().addTableToGroup(db.getId(), olapTable, colocateGroup,\n-                        null /* generate group id inside */);\n-                olapTable.setColocateGroup(colocateGroup);\n-            }\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-\n-        // get base index storage type. default is COLUMN\n-        TStorageType baseIndexStorageType = null;\n-        try {\n-            baseIndexStorageType = PropertyAnalyzer.analyzeStorageType(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(baseIndexStorageType);\n-        // set base index meta\n-        int schemaVersion = 0;\n-        try {\n-            schemaVersion = PropertyAnalyzer.analyzeSchemaVersion(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        int schemaHash = Util.schemaHash(schemaVersion, baseSchema, bfColumns, bfFpp);\n-        olapTable.setIndexMeta(baseIndexId, tableName, baseSchema, schemaVersion, schemaHash,\n-                shortKeyColumnCount, baseIndexStorageType, keysType);\n-\n-        for (AlterClause alterClause : stmt.getRollupAlterClauseList()) {\n-            AddRollupClause addRollupClause = (AddRollupClause)alterClause;\n-\n-            Long baseRollupIndex = olapTable.getIndexIdByName(tableName);\n-\n-            // get storage type for rollup index\n-            TStorageType rollupIndexStorageType = null;\n-            try {\n-                rollupIndexStorageType = PropertyAnalyzer.analyzeStorageType(addRollupClause.getProperties());\n-            } catch (AnalysisException e) {\n-                throw new DdlException(e.getMessage());\n-            }\n-            Preconditions.checkNotNull(rollupIndexStorageType);\n-            // set rollup index meta to olap table\n-            List<Column> rollupColumns = getRollupHandler().checkAndPrepareMaterializedView(addRollupClause,\n-                    olapTable, baseRollupIndex, false);\n-            short rollupShortKeyColumnCount = Catalog.calcShortKeyColumnCount(rollupColumns, alterClause.getProperties());\n-            int rollupSchemaHash = Util.schemaHash(schemaVersion, rollupColumns, bfColumns, bfFpp);\n-            long rollupIndexId = getCurrentCatalog().getNextId();\n-            olapTable.setIndexMeta(rollupIndexId, addRollupClause.getRollupName(), rollupColumns, schemaVersion,\n-                    rollupSchemaHash, rollupShortKeyColumnCount, rollupIndexStorageType, keysType);\n-        }\n-\n-        // analyze version info\n-        Pair<Long, Long> versionInfo = null;\n-        try {\n-            versionInfo = PropertyAnalyzer.analyzeVersionInfo(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        Preconditions.checkNotNull(versionInfo);\n-\n-        // get storage format\n-        TStorageFormat storageFormat = TStorageFormat.DEFAULT; // default means it's up to BE's config\n-        try {\n-            storageFormat = PropertyAnalyzer.analyzeStorageFormat(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        olapTable.setStorageFormat(storageFormat);\n-\n-        // a set to record every new tablet created when create table\n-        // if failed in any step, use this set to do clear things\n-        Set<Long> tabletIdSet = new HashSet<Long>();\n-\n-        // create partition\n-        try {\n-            if (partitionInfo.getType() == PartitionType.UNPARTITIONED) {\n-                // this is a 1-level partitioned table\n-                // use table name as partition name\n-                String partitionName = tableName;\n-                long partitionId = partitionNameToId.get(partitionName);\n-                // create partition\n-                Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(),\n-                        olapTable.getId(), olapTable.getBaseIndexId(),\n-                        partitionId, partitionName,\n-                        olapTable.getIndexIdToMeta(),\n-                        keysType,\n-                        distributionInfo,\n-                        partitionInfo.getDataProperty(partitionId).getStorageMedium(),\n-                        partitionInfo.getReplicationNum(partitionId),\n-                        versionInfo, bfColumns, bfFpp,\n-                        tabletIdSet, olapTable.getCopiedIndexes(),\n-                        isInMemory, storageFormat, tabletType);\n-                olapTable.addPartition(partition);\n-            } else if (partitionInfo.getType() == PartitionType.RANGE) {\n-                try {\n-                    // just for remove entries in stmt.getProperties(),\n-                    // and then check if there still has unknown properties\n-                    PropertyAnalyzer.analyzeDataProperty(stmt.getProperties(), DataProperty.DEFAULT_DATA_PROPERTY);\n-                    DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(olapTable, properties);\n-\n-                    if (properties != null && !properties.isEmpty()) {\n-                        // here, all properties should be checked\n-                        throw new DdlException(\"Unknown properties: \" + properties);\n-                    }\n-                } catch (AnalysisException e) {\n-                    throw new DdlException(e.getMessage());\n-                }\n-\n-                // this is a 2-level partitioned tables\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Map.Entry<String, Long> entry : partitionNameToId.entrySet()) {\n-                    DataProperty dataProperty = rangePartitionInfo.getDataProperty(entry.getValue());\n-                    Partition partition = createPartitionWithIndices(db.getClusterName(), db.getId(), olapTable.getId(),\n-                            olapTable.getBaseIndexId(), entry.getValue(), entry.getKey(),\n-                            olapTable.getIndexIdToMeta(),\n-                            keysType, distributionInfo,\n-                            dataProperty.getStorageMedium(),\n-                            partitionInfo.getReplicationNum(entry.getValue()),\n-                            versionInfo, bfColumns, bfFpp,\n-                            tabletIdSet, olapTable.getCopiedIndexes(),\n-                            isInMemory, storageFormat,\n-                            rangePartitionInfo.getTabletType(entry.getValue()));\n-                    olapTable.addPartition(partition);\n-                }\n-            } else {\n-                throw new DdlException(\"Unsupport partition method: \" + partitionInfo.getType().name());\n-            }\n-\n-            if (!db.createTableWithLock(olapTable, false, stmt.isSetIfNotExists())) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exists\");\n-            }\n-\n-            // we have added these index to memory, only need to persist here\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                GroupId groupId = getColocateTableIndex().getGroup(tableId);\n-                List<List<Long>> backendsPerBucketSeq = getColocateTableIndex().getBackendsPerBucketSeq(groupId);\n-                ColocatePersistInfo info = ColocatePersistInfo.createForAddTable(groupId, tableId, backendsPerBucketSeq);\n-                editLog.logColocateAddTable(info);\n-            }\n-            LOG.info(\"successfully create table[{};{}]\", tableName, tableId);\n-            // register or remove table from DynamicPartition after table created\n-            DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), olapTable);\n-            dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                    tableName, DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        } catch (DdlException e) {\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-\n-            // only remove from memory, because we have not persist it\n-            if (getColocateTableIndex().isColocateTable(tableId)) {\n-                getColocateTableIndex().removeTable(tableId);\n-            }\n-\n-            throw e;\n-        }\n-        return;\n-    }\n-\n-    private void createMysqlTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        MysqlTable mysqlTable = new MysqlTable(tableId, tableName, columns, stmt.getProperties());\n-        mysqlTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(mysqlTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-        return;\n-    }\n-\n-    private Table createEsTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        // create columns\n-        List<Column> baseSchema = stmt.getColumns();\n-        validateColumns(baseSchema);\n-\n-        // create partition info\n-        PartitionDesc partitionDesc = stmt.getPartitionDesc();\n-        PartitionInfo partitionInfo = null;\n-        Map<String, Long> partitionNameToId = Maps.newHashMap();\n-        if (partitionDesc != null) {\n-            partitionInfo = partitionDesc.toPartitionInfo(baseSchema, partitionNameToId, false);\n-        } else {\n-            long partitionId = getNextId();\n-            // use table name as single partition name\n-            partitionNameToId.put(tableName, partitionId);\n-            partitionInfo = new SinglePartitionInfo();\n-        }\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        EsTable esTable = new EsTable(tableId, tableName, baseSchema, stmt.getProperties(), partitionInfo);\n-        esTable.setComment(stmt.getComment());\n-\n-        if (!db.createTableWithLock(esTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table{} with id {}\", tableName, tableId);\n-        return esTable;\n-    }\n-\n-    private void createBrokerTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        BrokerTable brokerTable = new BrokerTable(tableId, tableName, columns, stmt.getProperties());\n-        brokerTable.setComment(stmt.getComment());\n-        brokerTable.setBrokerProperties(stmt.getExtProperties());\n-\n-        if (!db.createTableWithLock(brokerTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-\n-        return;\n-    }\n-\n-    private void createHiveTable(Database db, CreateTableStmt stmt) throws DdlException {\n-        String tableName = stmt.getTableName();\n-        List<Column> columns = stmt.getColumns();\n-        long tableId = getNextId();\n-        HiveTable hiveTable = new HiveTable(tableId, tableName, columns, stmt.getProperties());\n-        hiveTable.setComment(stmt.getComment());\n-        if (!db.createTableWithLock(hiveTable, false, stmt.isSetIfNotExists())) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CANT_CREATE_TABLE, tableName, \"table already exist\");\n-        }\n-        LOG.info(\"successfully create table[{}-{}]\", tableName, tableId);\n-    }\n-\n-    public static void getDdlStmt(Table table, List<String> createTableStmt, List<String> addPartitionStmt,\n-                                  List<String> createRollupStmt, boolean separatePartition, boolean hidePassword) {\n-        StringBuilder sb = new StringBuilder();\n-\n-        // 1. create table\n-        // 1.1 view\n-        if (table.getType() == TableType.VIEW) {\n-            View view = (View) table;\n-            sb.append(\"CREATE VIEW `\").append(table.getName()).append(\"` AS \").append(view.getInlineViewDef());\n-            sb.append(\";\");\n-            createTableStmt.add(sb.toString());\n-            return;\n-        }\n-\n-        // 1.2 other table type\n-        sb.append(\"CREATE \");\n-        if (table.getType() == TableType.MYSQL || table.getType() == TableType.ELASTICSEARCH\n-                || table.getType() == TableType.BROKER || table.getType() == TableType.HIVE) {\n-            sb.append(\"EXTERNAL \");\n-        }\n-        sb.append(\"TABLE \");\n-        sb.append(\"`\").append(table.getName()).append(\"` (\\n\");\n-        int idx = 0;\n-        for (Column column : table.getBaseSchema()) {\n-            if (idx++ != 0) {\n-                sb.append(\",\\n\");\n-            }\n-            // There MUST BE 2 space in front of each column description line\n-            // sqlalchemy requires this to parse SHOW CREATE TAEBL stmt.\n-            sb.append(\"  \").append(column.toSql());\n-        }\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-            if (CollectionUtils.isNotEmpty(olapTable.getIndexes())) {\n-                for (Index index : olapTable.getIndexes()) {\n-                    sb.append(\",\\n\");\n-                    sb.append(\"  \").append(index.toSql());\n-                }\n-            }\n-        }\n-        sb.append(\"\\n) ENGINE=\");\n-        sb.append(table.getType().name());\n-\n-        if (table.getType() == TableType.OLAP) {\n-            OlapTable olapTable = (OlapTable) table;\n-\n-            // keys\n-            sb.append(\"\\n\").append(olapTable.getKeysType().toSql()).append(\"(\");\n-            List<String> keysColumnNames = Lists.newArrayList();\n-            for (Column column : olapTable.getBaseSchema()) {\n-                if (column.isKey()) {\n-                    keysColumnNames.add(\"`\" + column.getName() + \"`\");\n-                }\n-            }\n-            sb.append(Joiner.on(\", \").join(keysColumnNames)).append(\")\");\n-\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-            List<Long> partitionId = null;\n-            if (separatePartition) {\n-                partitionId = Lists.newArrayList();\n-            }\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\").append(partitionInfo.toSql(olapTable, partitionId));\n-            }\n-\n-            // distribution\n-            DistributionInfo distributionInfo = olapTable.getDefaultDistributionInfo();\n-            sb.append(\"\\n\").append(distributionInfo.toSql());\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-\n-            // replicationNum\n-            Short replicationNum = olapTable.getDefaultReplicationNum();\n-            sb.append(\"\\\"\").append(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM).append(\"\\\" = \\\"\");\n-            sb.append(replicationNum).append(\"\\\"\");\n-\n-            // bloom filter\n-            Set<String> bfColumnNames = olapTable.getCopiedBfColumns();\n-            if (bfColumnNames != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_BF_COLUMNS).append(\"\\\" = \\\"\");\n-                sb.append(Joiner.on(\", \").join(olapTable.getCopiedBfColumns())).append(\"\\\"\");\n-            }\n-\n-            if (separatePartition) {\n-                // version info\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_VERSION_INFO).append(\"\\\" = \\\"\");\n-                Partition partition = null;\n-                if (olapTable.getPartitionInfo().getType() == PartitionType.UNPARTITIONED) {\n-                    partition = olapTable.getPartition(olapTable.getName());\n-                } else {\n-                    Preconditions.checkState(partitionId.size() == 1);\n-                    partition = olapTable.getPartition(partitionId.get(0));\n-                }\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-            }\n-\n-            // colocateTable\n-            String colocateTable = olapTable.getColocateGroup();\n-            if (colocateTable != null) {\n-                sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH).append(\"\\\" = \\\"\");\n-                sb.append(colocateTable).append(\"\\\"\");\n-            }\n-\n-            // dynamic partition\n-            if (olapTable.dynamicPartitionExists()) {\n-                sb.append(olapTable.getTableProperty().getDynamicPartitionProperty().toString());\n-            }\n-\n-            // in memory\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_INMEMORY).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.isInMemory()).append(\"\\\"\");\n-\n-            // storage type\n-            sb.append(\",\\n\\\"\").append(PropertyAnalyzer.PROPERTIES_STORAGE_FORMAT).append(\"\\\" = \\\"\");\n-            sb.append(olapTable.getStorageFormat()).append(\"\\\"\");\n-\n-            sb.append(\"\\n)\");\n-        } else if (table.getType() == TableType.MYSQL) {\n-            MysqlTable mysqlTable = (MysqlTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"host\\\" = \\\"\").append(mysqlTable.getHost()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"port\\\" = \\\"\").append(mysqlTable.getPort()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(mysqlTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : mysqlTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(mysqlTable.getMysqlDatabaseName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(mysqlTable.getMysqlTableName()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.BROKER) {\n-            BrokerTable brokerTable = (BrokerTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"broker_name\\\" = \\\"\").append(brokerTable.getBrokerName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"path\\\" = \\\"\").append(Joiner.on(\",\").join(brokerTable.getEncodedPaths())).append(\"\\\",\\n\");\n-            sb.append(\"\\\"column_separator\\\" = \\\"\").append(brokerTable.getReadableColumnSeparator()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"line_delimiter\\\" = \\\"\").append(brokerTable.getReadableLineDelimiter()).append(\"\\\",\\n\");\n-            sb.append(\")\");\n-            if (!brokerTable.getBrokerProperties().isEmpty()) {\n-                sb.append(\"\\nBROKER PROPERTIES (\\n\");\n-                sb.append(new PrintableMap<>(brokerTable.getBrokerProperties(), \" = \", true, true,\n-                        hidePassword).toString());\n-                sb.append(\"\\n)\");\n-            }\n-        } else if (table.getType() == TableType.ELASTICSEARCH) {\n-            EsTable esTable = (EsTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-\n-            // partition\n-            PartitionInfo partitionInfo = esTable.getPartitionInfo();\n-            if (partitionInfo.getType() == PartitionType.RANGE) {\n-                sb.append(\"\\n\");\n-                sb.append(\"PARTITION BY RANGE(\");\n-                idx = 0;\n-                RangePartitionInfo rangePartitionInfo = (RangePartitionInfo) partitionInfo;\n-                for (Column column : rangePartitionInfo.getPartitionColumns()) {\n-                    if (idx != 0) {\n-                        sb.append(\", \");\n-                    }\n-                    sb.append(\"`\").append(column.getName()).append(\"`\");\n-                }\n-                sb.append(\")\\n()\");\n-            }\n-\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"hosts\\\" = \\\"\").append(esTable.getHosts()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"user\\\" = \\\"\").append(esTable.getUserName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"password\\\" = \\\"\").append(hidePassword ? \"\" : esTable.getPasswd()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"index\\\" = \\\"\").append(esTable.getIndexName()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"type\\\" = \\\"\").append(esTable.getMappingType()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"transport\\\" = \\\"\").append(esTable.getTransport()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_docvalue_scan\\\" = \\\"\").append(esTable.isDocValueScanEnable()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"enable_keyword_sniff\\\" = \\\"\").append(esTable.isKeywordSniffEnable()).append(\"\\\"\\n\");\n-            sb.append(\")\");\n-        } else if (table.getType() == TableType.HIVE) {\n-            HiveTable hiveTable = (HiveTable) table;\n-            if (!Strings.isNullOrEmpty(table.getComment())) {\n-                sb.append(\"\\nCOMMENT \\\"\").append(table.getComment()).append(\"\\\"\");\n-            }\n-            // properties\n-            sb.append(\"\\nPROPERTIES (\\n\");\n-            sb.append(\"\\\"database\\\" = \\\"\").append(hiveTable.getHiveDb()).append(\"\\\",\\n\");\n-            sb.append(\"\\\"table\\\" = \\\"\").append(hiveTable.getHiveTable()).append(\"\\\",\\n\");\n-            sb.append(new PrintableMap<>(hiveTable.getHiveProperties(), \" = \", true, true, false).toString());\n-            sb.append(\"\\n)\");\n-        }\n-        sb.append(\";\");\n-\n-        createTableStmt.add(sb.toString());\n-\n-        // 2. add partition\n-        if (separatePartition && (table instanceof OlapTable)\n-                && ((OlapTable) table).getPartitionInfo().getType() == PartitionType.RANGE\n-                && ((OlapTable) table).getPartitions().size() > 1) {\n-            OlapTable olapTable = (OlapTable) table;\n-            RangePartitionInfo partitionInfo = (RangePartitionInfo) olapTable.getPartitionInfo();\n-            boolean first = true;\n-            for (Map.Entry<Long, Range<PartitionKey>> entry : partitionInfo.getSortedRangeMap(false)) {\n-                if (first) {\n-                    first = false;\n-                    continue;\n-                }\n-                sb = new StringBuilder();\n-                Partition partition = olapTable.getPartition(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName());\n-                sb.append(\" ADD PARTITION \").append(partition.getName()).append(\" VALUES [\");\n-                sb.append(entry.getValue().lowerEndpoint().toSql());\n-                sb.append(\", \").append(entry.getValue().upperEndpoint().toSql()).append(\")\");\n-                sb.append(\"(\\\"version_info\\\" = \\\"\");\n-                sb.append(Joiner.on(\",\").join(partition.getVisibleVersion(), partition.getVisibleVersionHash()))\n-                        .append(\"\\\"\");\n-                sb.append(\");\");\n-                addPartitionStmt.add(sb.toString());\n-            }\n-        }\n-\n-        // 3. rollup\n-        if (createRollupStmt != null && (table instanceof OlapTable)) {\n-            OlapTable olapTable = (OlapTable) table;\n-            for (Map.Entry<Long, MaterializedIndexMeta> entry : olapTable.getIndexIdToMeta().entrySet()) {\n-                if (entry.getKey() == olapTable.getBaseIndexId()) {\n-                    continue;\n-                }\n-                MaterializedIndexMeta materializedIndexMeta = entry.getValue();\n-                sb = new StringBuilder();\n-                String indexName = olapTable.getIndexNameById(entry.getKey());\n-                sb.append(\"ALTER TABLE \").append(table.getName()).append(\" ADD ROLLUP \").append(indexName);\n-                sb.append(\"(\");\n-\n-                List<Column> indexSchema = materializedIndexMeta.getSchema();\n-                for (int i = 0; i < indexSchema.size(); i++) {\n-                    Column column = indexSchema.get(i);\n-                    sb.append(column.getName());\n-                    if (i != indexSchema.size() - 1) {\n-                        sb.append(\", \");\n-                    }\n-                }\n-                sb.append(\");\");\n-                createRollupStmt.add(sb.toString());\n-            }\n-        }\n-    }\n-\n-    public void replayCreateTable(String dbName, Table table) {\n-        Database db = this.fullNameToDb.get(dbName);\n-        db.createTableWithLock(table, true, false);\n-\n-        if (!isCheckpointThread()) {\n-            // add to inverted index\n-            if (table.getType() == TableType.OLAP) {\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                OlapTable olapTable = (OlapTable) table;\n-                long dbId = db.getId();\n-                long tableId = table.getId();\n-                for (Partition partition : olapTable.getAllPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(dbId, tableId, partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                } // end for partitions\n-                DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(dbId, olapTable);\n-            }\n-        }\n-    }\n-\n-    private void createTablets(String clusterName, MaterializedIndex index, ReplicaState replicaState,\n-                               DistributionInfo distributionInfo, long version, long versionHash, short replicationNum,\n-                               TabletMeta tabletMeta, Set<Long> tabletIdSet) throws DdlException {\n-        Preconditions.checkArgument(replicationNum > 0);\n-\n-        DistributionInfoType distributionInfoType = distributionInfo.getType();\n-        if (distributionInfoType == DistributionInfoType.HASH) {\n-            ColocateTableIndex colocateIndex = Catalog.getCurrentColocateIndex();\n-            List<List<Long>> backendsPerBucketSeq = null;\n-            GroupId groupId = null;\n-            if (colocateIndex.isColocateTable(tabletMeta.getTableId())) {\n-                // if this is a colocate table, try to get backend seqs from colocation index.\n-                Database db = Catalog.getCurrentCatalog().getDb(tabletMeta.getDbId());\n-                groupId = colocateIndex.getGroup(tabletMeta.getTableId());\n-                // Use db write lock here to make sure the backendsPerBucketSeq is consistent when the backendsPerBucketSeq is updating.\n-                // This lock will release very fast.\n-                db.writeLock();\n-                try {\n-                    backendsPerBucketSeq = colocateIndex.getBackendsPerBucketSeq(groupId);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-            }\n-\n-            // chooseBackendsArbitrary is true, means this may be the first table of colocation group,\n-            // or this is just a normal table, and we can choose backends arbitrary.\n-            // otherwise, backends should be chosen from backendsPerBucketSeq;\n-            boolean chooseBackendsArbitrary = backendsPerBucketSeq == null || backendsPerBucketSeq.isEmpty();\n-            if (chooseBackendsArbitrary) {\n-                backendsPerBucketSeq = Lists.newArrayList();\n-            }\n-            for (int i = 0; i < distributionInfo.getBucketNum(); ++i) {\n-                // create a new tablet with random chosen backends\n-                Tablet tablet = new Tablet(getNextId());\n-\n-                // add tablet to inverted index first\n-                index.addTablet(tablet, tabletMeta);\n-                tabletIdSet.add(tablet.getId());\n-\n-                // get BackendIds\n-                List<Long> chosenBackendIds;\n-                if (chooseBackendsArbitrary) {\n-                    // This is the first colocate table in the group, or just a normal table,\n-                    // randomly choose backends\n-                    if (Config.enable_strict_storage_medium_check) {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName, tabletMeta.getStorageMedium());\n-                    } else {\n-                        chosenBackendIds = chosenBackendIdBySeq(replicationNum, clusterName);\n-                    }\n-                    backendsPerBucketSeq.add(chosenBackendIds);\n-                } else {\n-                    // get backends from existing backend sequence\n-                    chosenBackendIds = backendsPerBucketSeq.get(i);\n-                }\n-                \n-                // create replicas\n-                for (long backendId : chosenBackendIds) {\n-                    long replicaId = getNextId();\n-                    Replica replica = new Replica(replicaId, backendId, replicaState, version, versionHash,\n-                            tabletMeta.getOldSchemaHash());\n-                    tablet.addReplica(replica);\n-                }\n-                Preconditions.checkState(chosenBackendIds.size() == replicationNum, chosenBackendIds.size() + \" vs. \"+ replicationNum);\n-            }\n-\n-            if (groupId != null) {\n-                colocateIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-        } else {\n-            throw new DdlException(\"Unknown distribution type: \" + distributionInfoType);\n-        }\n-    }\n-\n-    // create replicas for tablet with random chosen backends\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName, TStorageMedium storageMedium) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIdsByStorageMedium(replicationNum,\n-                true, true, clusterName, storageMedium);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host with storage medium is \" + storageMedium + \" in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    private List<Long> chosenBackendIdBySeq(int replicationNum, String clusterName) throws DdlException {\n-        List<Long> chosenBackendIds = Catalog.getCurrentSystemInfo().seqChooseBackendIds(replicationNum, true, true, clusterName);\n-        if (chosenBackendIds == null) {\n-            throw new DdlException(\"Failed to find enough host in all backends. need: \" + replicationNum);\n-        }\n-        return chosenBackendIds;\n-    }\n-\n-    // Drop table\n-    public void dropTable(DropTableStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTableName();\n-\n-        // check database\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        Table table = null;\n-        db.writeLock();\n-        try {\n-            table = db.getTable(tableName);\n-            if (table == null) {\n-                if (stmt.isSetIfExists()) {\n-                    LOG.info(\"drop table[{}] which does not exist\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-                }\n-            }\n-\n-            // Check if a view\n-            if (stmt.isView()) {\n-                if (!(table instanceof View)) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"VIEW\");\n-                }\n-            } else {\n-                if (table instanceof View) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_WRONG_OBJECT, dbName, tableName, \"TABLE\");\n-                }\n-            }\n-\n-            if (stmt.isNeedCheckCommittedTxns()) {\n-                if (Catalog.getCurrentCatalog().getGlobalTransactionMgr().existCommittedTxns(db.getId(), table.getId(), null)) {\n-                    throw new DdlException(\"There are still some transactions in the COMMITTED state waiting to be completed. \" +\n-                            \"The table [\" + tableName +\"] cannot be dropped. If you want to forcibly drop(cannot be recovered),\" +\n-                            \" please use \\\"DROP table force\\\".\");\n-                }\n-            }\n-\n-            unprotectDropTable(db, table.getId());\n-\n-            DropInfo info = new DropInfo(db.getId(), table.getId(), -1L);\n-            editLog.logDropTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-\n-        LOG.info(\"finished dropping table: {} from db: {}\", tableName, dbName);\n-    }\n-\n-    public boolean unprotectDropTable(Database db, long tableId) {\n-        Table table = db.getTable(tableId);\n-        // delete from db meta\n-        if (table == null) {\n-            return false;\n-        }\n-\n-        if (table.getType() == TableType.ELASTICSEARCH) {\n-            esRepository.deRegisterTable(tableId);\n-        } else if (table.getType() == TableType.OLAP) {\n-            // drop all temp partitions of this table, so that there is no temp partitions in recycle bin,\n-            // which make things easier.\n-            ((OlapTable) table).dropAllTempPartitions();\n-        }\n-\n-        db.dropTable(table.getName());\n-\n-        Catalog.getCurrentRecycleBin().recycleTable(db.getId(), table);\n-\n-        LOG.info(\"finished dropping table[{}] in db[{}]\", table.getName(), db.getFullName());\n-        return true;\n-    }\n-\n-    public void replayDropTable(Database db, long tableId) {\n-        db.writeLock();\n-        try {\n-            unprotectDropTable(db, tableId);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayEraseTable(long tableId) throws DdlException {\n-        Catalog.getCurrentRecycleBin().replayEraseTable(tableId);\n-    }\n-\n-    public void replayRecoverTable(RecoverInfo info) {\n-        long dbId = info.getDbId();\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            Catalog.getCurrentRecycleBin().replayRecoverTable(db, info.getTableId());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    private void unprotectAddReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay add a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-\n-        // for compatibility\n-        int schemaHash = info.getSchemaHash();\n-        if (schemaHash == -1) {\n-            schemaHash = olapTable.getSchemaHashByIndexId(info.getIndexId());\n-        }\n-\n-        Replica replica = new Replica(info.getReplicaId(), info.getBackendId(), info.getVersion(),\n-                info.getVersionHash(), schemaHash, info.getDataSize(), info.getRowCount(),\n-                ReplicaState.NORMAL,\n-                info.getLastFailedVersion(),\n-                info.getLastFailedVersionHash(),\n-                info.getLastSuccessVersion(),\n-                info.getLastSuccessVersionHash());\n-        tablet.addReplica(replica);\n-    }\n-\n-    private void unprotectUpdateReplica(ReplicaPersistInfo info) {\n-        LOG.debug(\"replay update a replica {}\", info);\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        Replica replica = tablet.getReplicaByBackendId(info.getBackendId());\n-        Preconditions.checkNotNull(replica, info);\n-        replica.updateVersionInfo(info.getVersion(), info.getVersionHash(), info.getDataSize(), info.getRowCount());\n-        replica.setBad(false);\n-    }\n-\n-    public void replayAddReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectAddReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayUpdateReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectUpdateReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void unprotectDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        OlapTable olapTable = (OlapTable) db.getTable(info.getTableId());\n-        Partition partition = olapTable.getPartition(info.getPartitionId());\n-        MaterializedIndex materializedIndex = partition.getIndex(info.getIndexId());\n-        Tablet tablet = materializedIndex.getTablet(info.getTabletId());\n-        tablet.deleteReplicaByBackendId(info.getBackendId());\n-    }\n-\n-    public void replayDeleteReplica(ReplicaPersistInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            unprotectDeleteReplica(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayAddFrontend(Frontend fe) {\n-        tryLock(true);\n-        try {\n-            Frontend existFe = checkFeExist(fe.getHost(), fe.getEditLogPort());\n-            if (existFe != null) {\n-                LOG.warn(\"fe {} already exist.\", existFe);\n-                if (existFe.getRole() != fe.getRole()) {\n-                    /*\n-                     * This may happen if:\n-                     * 1. first, add a FE as OBSERVER.\n-                     * 2. This OBSERVER is restarted with ROLE and VERSION file being DELETED.\n-                     *    In this case, this OBSERVER will be started as a FOLLOWER, and add itself to the frontends.\n-                     * 3. this \"FOLLOWER\" begin to load image or replay journal,\n-                     *    then find the origin OBSERVER in image or journal.\n-                     * This will cause UNDEFINED behavior, so it is better to exit and fix it manually.\n-                     */\n-                    System.err.println(\"Try to add an already exist FE with different role\" + fe.getRole());\n-                    System.exit(-1);\n-                }\n-                return;\n-            }\n-            frontends.put(fe.getNodeName(), fe);\n-            if (fe.getRole() == FrontendNodeType.FOLLOWER || fe.getRole() == FrontendNodeType.REPLICA) {\n-                // DO NOT add helper sockets here, cause BDBHA is not instantiated yet.\n-                // helper sockets will be added after start BDBHA\n-                // But add to helperNodes, just for show\n-                helperNodes.add(Pair.create(fe.getHost(), fe.getEditLogPort()));\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayDropFrontend(Frontend frontend) {\n-        tryLock(true);\n-        try {\n-            Frontend removedFe = frontends.remove(frontend.getNodeName());\n-            if (removedFe == null) {\n-                LOG.error(frontend.toString() + \" does not exist.\");\n-                return;\n-            }\n-            if (removedFe.getRole() == FrontendNodeType.FOLLOWER\n-                    || removedFe.getRole() == FrontendNodeType.REPLICA) {\n-                helperNodes.remove(Pair.create(removedFe.getHost(), removedFe.getEditLogPort()));\n-            }\n-\n-            removedFrontends.add(removedFe.getNodeName());\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public int getClusterId() {\n-        return this.clusterId;\n-    }\n-\n-    public String getToken() {\n-        return token;\n-    }\n-\n-    public Database getDb(String name) {\n-        if (fullNameToDb.containsKey(name)) {\n-            return fullNameToDb.get(name);\n-        } else {\n-            // This maybe a information_schema db request, and information_schema db name is case insensitive.\n-            // So, we first extract db name to check if it is information_schema.\n-            // Then we reassemble the origin cluster name with lower case db name,\n-            // and finally get information_schema db from the name map.\n-            String dbName = ClusterNamespace.getNameFromFullName(name);\n-            if (dbName.equalsIgnoreCase(InfoSchemaDb.DATABASE_NAME)) {\n-                String clusterName = ClusterNamespace.getClusterNameFromFullName(name);\n-                return fullNameToDb.get(ClusterNamespace.getFullName(clusterName, dbName.toLowerCase()));\n-            }\n-        }\n-        return null;\n-    }\n-\n-    public Database getDb(long dbId) {\n-        return idToDb.get(dbId);\n-    }\n-\n-    public EditLog getEditLog() {\n-        return editLog;\n-    }\n-\n-    // Get the next available, need't lock because of nextId is atomic.\n-    public long getNextId() {\n-        long id = idGenerator.getNextId();\n-        return id;\n-    }\n-\n-    public List<String> getDbNames() {\n-        return Lists.newArrayList(fullNameToDb.keySet());\n-    }\n-\n-    public List<String> getClusterDbNames(String clusterName) throws AnalysisException {\n-        final Cluster cluster = nameToCluster.get(clusterName);\n-        if (cluster == null) {\n-            throw new AnalysisException(\"No cluster selected\");\n-        }\n-        return Lists.newArrayList(cluster.getDbNames());\n-    }\n-\n-    public List<Long> getDbIds() {\n-        return Lists.newArrayList(idToDb.keySet());\n-    }\n-\n-    public HashMap<Long, TStorageMedium> getPartitionIdToStorageMediumMap() {\n-        HashMap<Long, TStorageMedium> storageMediumMap = new HashMap<Long, TStorageMedium>();\n-\n-        // record partition which need to change storage medium\n-        // dbId -> (tableId -> partitionId)\n-        HashMap<Long, Multimap<Long, Long>> changedPartitionsMap = new HashMap<Long, Multimap<Long, Long>>();\n-        long currentTimeMs = System.currentTimeMillis();\n-        List<Long> dbIds = getDbIds();\n-\n-        for (long dbId : dbIds) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while doing backend report\", dbId);\n-                continue;\n-            }\n-\n-            db.readLock();\n-            try {\n-                for (Table table : db.getTables()) {\n-                    if (table.getType() != TableType.OLAP) {\n-                        continue;\n-                    }\n-\n-                    long tableId = table.getId();\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-                    for (Partition partition : olapTable.getAllPartitions()) {\n-                        long partitionId = partition.getId();\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        Preconditions.checkNotNull(dataProperty, partition.getName() + \", pId:\" + partitionId + \", db: \" + dbId + \", tbl: \" + tableId);\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            // record and change when holding write lock\n-                            Multimap<Long, Long> multimap = changedPartitionsMap.get(dbId);\n-                            if (multimap == null) {\n-                                multimap = HashMultimap.create();\n-                                changedPartitionsMap.put(dbId, multimap);\n-                            }\n-                            multimap.put(tableId, partitionId);\n-                        } else {\n-                            storageMediumMap.put(partitionId, dataProperty.getStorageMedium());\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.readUnlock();\n-            }\n-        } // end for dbs\n-\n-        // handle data property changed\n-        for (Long dbId : changedPartitionsMap.keySet()) {\n-            Database db = getDb(dbId);\n-            if (db == null) {\n-                LOG.warn(\"db {} does not exist while checking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Multimap<Long, Long> tableIdToPartitionIds = changedPartitionsMap.get(dbId);\n-\n-            // use try lock to avoid blocking a long time.\n-            // if block too long, backend report rpc will timeout.\n-            if (!db.tryWriteLock(Database.TRY_LOCK_TIMEOUT_MS, TimeUnit.MILLISECONDS)) {\n-                LOG.warn(\"try get db {} writelock but failed when hecking backend storage medium\", dbId);\n-                continue;\n-            }\n-            Preconditions.checkState(db.isWriteLockHeldByCurrentThread());\n-            try {\n-                for (Long tableId : tableIdToPartitionIds.keySet()) {\n-                    Table table = db.getTable(tableId);\n-                    if (table == null) {\n-                        continue;\n-                    }\n-                    OlapTable olapTable = (OlapTable) table;\n-                    PartitionInfo partitionInfo = olapTable.getPartitionInfo();\n-\n-                    Collection<Long> partitionIds = tableIdToPartitionIds.get(tableId);\n-                    for (Long partitionId : partitionIds) {\n-                        Partition partition = olapTable.getPartition(partitionId);\n-                        if (partition == null) {\n-                            continue;\n-                        }\n-                        DataProperty dataProperty = partitionInfo.getDataProperty(partition.getId());\n-                        if (dataProperty.getStorageMedium() == TStorageMedium.SSD\n-                                && dataProperty.getCooldownTimeMs() < currentTimeMs) {\n-                            // expire. change to HDD.\n-                            partitionInfo.setDataProperty(partition.getId(), new DataProperty(TStorageMedium.HDD));\n-                            storageMediumMap.put(partitionId, TStorageMedium.HDD);\n-                            LOG.debug(\"partition[{}-{}-{}] storage medium changed from SSD to HDD\",\n-                                    dbId, tableId, partitionId);\n-\n-                            // log\n-                            ModifyPartitionInfo info =\n-                                    new ModifyPartitionInfo(db.getId(), olapTable.getId(),\n-                                            partition.getId(),\n-                                            DataProperty.DEFAULT_DATA_PROPERTY,\n-                                            (short) -1,\n-                                            partitionInfo.getIsInMemory(partition.getId()));\n-                            editLog.logModifyPartition(info);\n-                        }\n-                    } // end for partitions\n-                } // end for tables\n-            } finally {\n-                db.writeUnlock();\n-            }\n-        } // end for dbs\n-        return storageMediumMap;\n-    }\n-\n-    public ConsistencyChecker getConsistencyChecker() {\n-        return this.consistencyChecker;\n-    }\n-\n-    public Alter getAlterInstance() {\n-        return this.alter;\n-    }\n-\n-    public SchemaChangeHandler getSchemaChangeHandler() {\n-        return (SchemaChangeHandler) this.alter.getSchemaChangeHandler();\n-    }\n-\n-    public MaterializedViewHandler getRollupHandler() {\n-        return (MaterializedViewHandler) this.alter.getMaterializedViewHandler();\n-    }\n-\n-    public SystemHandler getClusterHandler() {\n-        return (SystemHandler) this.alter.getClusterHandler();\n-    }\n-\n-    public BackupHandler getBackupHandler() {\n-        return this.backupHandler;\n-    }\n-\n-    public DeleteHandler getDeleteHandler() {\n-        return this.deleteHandler;\n-    }\n-\n-    public Load getLoadInstance() {\n-        return this.load;\n-    }\n-\n-    public LoadManager getLoadManager() {\n-        return loadManager;\n-    }\n-\n-    public MasterTaskExecutor getLoadTaskScheduler() {\n-        return loadTaskScheduler;\n-    }\n-\n-    public RoutineLoadManager getRoutineLoadManager() {\n-        return routineLoadManager;\n-    }\n-\n-    public RoutineLoadTaskScheduler getRoutineLoadTaskScheduler(){\n-        return routineLoadTaskScheduler;\n-    }\n-\n-    public ExportMgr getExportMgr() {\n-        return this.exportMgr;\n-    }\n-\n-    public SmallFileMgr getSmallFileMgr() {\n-        return this.smallFileMgr;\n-    }\n-\n-    public long getReplayedJournalId() {\n-        return this.replayedJournalId.get();\n-    }\n-\n-    public HAProtocol getHaProtocol() {\n-        return this.haProtocol;\n-    }\n-\n-    public Long getMaxJournalId() {\n-        return this.editLog.getMaxJournalId();\n-    }\n-\n-    public long getEpoch() {\n-        return this.epoch;\n-    }\n-\n-    public void setEpoch(long epoch) {\n-        this.epoch = epoch;\n-    }\n-\n-    public FrontendNodeType getRole() {\n-        return this.role;\n-    }\n-\n-    public Pair<String, Integer> getHelperNode() {\n-        Preconditions.checkState(helperNodes.size() >= 1);\n-        return this.helperNodes.get(0);\n-    }\n-\n-    public List<Pair<String, Integer>> getHelperNodes() {\n-        return Lists.newArrayList(helperNodes);\n-    }\n-\n-    public Pair<String, Integer> getSelfNode() {\n-        return this.selfNode;\n-    }\n-\n-    public String getNodeName() {\n-        return this.nodeName;\n-    }\n-\n-    public FrontendNodeType getFeType() {\n-        return this.feType;\n-    }\n-\n-    public int getMasterRpcPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterRpcPort;\n-    }\n-\n-    public int getMasterHttpPort() {\n-        if (!isReady()) {\n-            return 0;\n-        }\n-        return this.masterHttpPort;\n-    }\n-\n-    public String getMasterIp() {\n-        if (!isReady()) {\n-            return \"\";\n-        }\n-        return this.masterIp;\n-    }\n-\n-    public EsRepository getEsRepository() {\n-        return this.esRepository;\n-    }\n-\n-    public void setMaster(MasterInfo info) {\n-        this.masterIp = info.getIp();\n-        this.masterHttpPort = info.getHttpPort();\n-        this.masterRpcPort = info.getRpcPort();\n-    }\n-\n-    public boolean canRead() {\n-        return this.canRead.get();\n-    }\n-\n-    public boolean isElectable() {\n-        return this.isElectable;\n-    }\n-\n-    public boolean isMaster() {\n-        return feType == FrontendNodeType.MASTER;\n-    }\n-\n-    public void setSynchronizedTime(long time) {\n-        this.synchronizedTimeMs = time;\n-    }\n-\n-    public void setEditLog(EditLog editLog) {\n-        this.editLog = editLog;\n-    }\n-\n-    public void setNextId(long id) {\n-        idGenerator.setId(id);\n-    }\n-\n-    public void setHaProtocol(HAProtocol protocol) {\n-        this.haProtocol = protocol;\n-    }\n-\n-    public static short calcShortKeyColumnCount(List<Column> columns, Map<String, String> properties)\n-            throws DdlException {\n-        List<Column> indexColumns = new ArrayList<Column>();\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                indexColumns.add(column);\n-            }\n-        }\n-        LOG.debug(\"index column size: {}\", indexColumns.size());\n-        Preconditions.checkArgument(indexColumns.size() > 0);\n-\n-        // figure out shortKeyColumnCount\n-        short shortKeyColumnCount = (short) -1;\n-        try {\n-            shortKeyColumnCount = PropertyAnalyzer.analyzeShortKeyColumnCount(properties);\n-        } catch (AnalysisException e) {\n-            throw new DdlException(e.getMessage());\n-        }\n-        if (shortKeyColumnCount != (short) -1) {\n-            // use user specified short key column count\n-            if (shortKeyColumnCount <= 0) {\n-                throw new DdlException(\"Invalid short key: \" + shortKeyColumnCount);\n-            }\n-\n-            if (shortKeyColumnCount > indexColumns.size()) {\n-                throw new DdlException(\"Short key is too large. should less than: \" + indexColumns.size());\n-            }\n-\n-            for (int pos = 0; pos < shortKeyColumnCount; pos++) {\n-                if (indexColumns.get(pos).getDataType() == PrimitiveType.VARCHAR && pos != shortKeyColumnCount - 1) {\n-                    throw new DdlException(\"Varchar should not in the middle of short keys.\");\n-                }\n-            }\n-        } else {\n-            /*\n-             * Calc short key column count. NOTE: short key column count is\n-             * calculated as follow: 1. All index column are taking into\n-             * account. 2. Max short key column count is Min(Num of\n-             * indexColumns, META_MAX_SHORT_KEY_NUM). 3. Short key list can\n-             * contains at most one VARCHAR column. And if contains, it should\n-             * be at the last position of the short key list.\n-             */\n-            shortKeyColumnCount = 0;\n-            int shortKeySizeByte = 0;\n-            int maxShortKeyColumnCount = Math.min(indexColumns.size(), FeConstants.shortkey_max_column_count);\n-            for (int i = 0; i < maxShortKeyColumnCount; i++) {\n-                Column column = indexColumns.get(i);\n-                shortKeySizeByte += column.getOlapColumnIndexSize();\n-                if (shortKeySizeByte > FeConstants.shortkey_maxsize_bytes) {\n-                    if (column.getDataType().isCharFamily()) {\n-                        ++shortKeyColumnCount;\n-                    }\n-                    break;\n-                }\n-                if (column.getType().isFloatingPointType()) {\n-                    break;\n-                }\n-                if (column.getDataType() == PrimitiveType.VARCHAR) {\n-                    ++shortKeyColumnCount;\n-                    break;\n-                }\n-                ++shortKeyColumnCount;\n-            }\n-            if (shortKeyColumnCount == 0) {\n-                throw new DdlException(\"The first column could not be float or double type, use decimal instead\");\n-            }\n-\n-        } // end calc shortKeyColumnCount\n-\n-        return shortKeyColumnCount;\n-    }\n-\n-    /*\n-     * used for handling AlterTableStmt (for client is the ALTER TABLE command).\n-     * including SchemaChangeHandler and RollupHandler\n-     */\n-    public void alterTable(AlterTableStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterTable(stmt);\n-    }\n-\n-    /**\n-     * used for handling AlterViewStmt (the ALTER VIEW command).\n-     */\n-    public void alterView(AlterViewStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterView(stmt, ConnectContext.get());\n-    }\n-\n-    public void createMaterializedView(CreateMaterializedViewStmt stmt)\n-            throws AnalysisException, DdlException {\n-        this.alter.processCreateMaterializedView(stmt);\n-    }\n-\n-    public void dropMaterializedView(DropMaterializedViewStmt stmt) throws DdlException, MetaNotFoundException {\n-        this.alter.processDropMaterializedView(stmt);\n-    }\n-\n-    /*\n-     * used for handling CacnelAlterStmt (for client is the CANCEL ALTER\n-     * command). including SchemaChangeHandler and RollupHandler\n-     */\n-    public void cancelAlter(CancelAlterTableStmt stmt) throws DdlException {\n-        if (stmt.getAlterType() == AlterType.ROLLUP) {\n-            this.getRollupHandler().cancel(stmt);\n-        } else if (stmt.getAlterType() == AlterType.COLUMN) {\n-            this.getSchemaChangeHandler().cancel(stmt);\n-        } else {\n-            throw new DdlException(\"Cancel \" + stmt.getAlterType() + \" does not implement yet\");\n-        }\n-    }\n-\n-    /*\n-     * used for handling backup opt\n-     */\n-    public void backup(BackupStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void restore(RestoreStmt stmt) throws DdlException {\n-        getBackupHandler().process(stmt);\n-    }\n-\n-    public void cancelBackup(CancelBackupStmt stmt) throws DdlException {\n-        getBackupHandler().cancel(stmt);\n-    }\n-\n-    public void renameTable(Database db, OlapTable table, TableRenameClause tableRenameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String tableName = table.getName();\n-        String newTableName = tableRenameClause.getNewTableName();\n-        if (tableName.equals(newTableName)) {\n-            throw new DdlException(\"Same table name\");\n-        }\n-\n-        // check if name is already used\n-        if (db.getTable(newTableName) != null) {\n-            throw new DdlException(\"Table name[\" + newTableName + \"] is already used\");\n-        }\n-\n-        // check if rollup has same name\n-        for (String idxName : table.getIndexNameToId().keySet()) {\n-            if (idxName.equals(newTableName)) {\n-                throw new DdlException(\"New name conflicts with rollup index name: \" + idxName);\n-            }\n-        }\n-\n-        table.setName(newTableName);\n-\n-        db.dropTable(tableName);\n-        db.createTable(table);\n-\n-        TableInfo tableInfo = TableInfo.createForTableRename(db.getId(), table.getId(), newTableName);\n-        editLog.logTableRename(tableInfo);\n-        LOG.info(\"rename table[{}] to {}\", tableName, newTableName);\n-    }\n-\n-    public void replayRenameTable(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        String newTableName = tableInfo.getNewTableName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String tableName = table.getName();\n-            db.dropTable(tableName);\n-            table.setName(newTableName);\n-            db.createTable(table);\n-\n-            LOG.info(\"replay rename table[{}] to {}\", tableName, newTableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    // the invoker should keep db write lock\n-    public void modifyTableColocate(Database db, OlapTable table, String colocateGroup, boolean isReplay,\n-            GroupId assignedGroupId)\n-            throws DdlException {\n-\n-        String oldGroup = table.getColocateGroup();\n-        GroupId groupId = null;\n-        if (!Strings.isNullOrEmpty(colocateGroup)) {\n-            String fullGroupName = db.getId() + \"_\" + colocateGroup;\n-            ColocateGroupSchema groupSchema = colocateTableIndex.getGroupSchema(fullGroupName);\n-            if (groupSchema == null) {\n-                // user set a new colocate group,\n-                // check if all partitions all this table has same buckets num and same replication number\n-                PartitionInfo partitionInfo = table.getPartitionInfo();\n-                if (partitionInfo.getType() == PartitionType.RANGE) {\n-                    int bucketsNum = -1;\n-                    short replicationNum = -1;\n-                    for (Partition partition : table.getPartitions()) {\n-                        if (bucketsNum == -1) {\n-                            bucketsNum = partition.getDistributionInfo().getBucketNum();\n-                        } else if (bucketsNum != partition.getDistributionInfo().getBucketNum()) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different buckets number\");\n-                        }\n-                        \n-                        if (replicationNum == -1) {\n-                            replicationNum = partitionInfo.getReplicationNum(partition.getId());\n-                        } else if (replicationNum != partitionInfo.getReplicationNum(partition.getId())) {\n-                            throw new DdlException(\"Partitions in table \" + table.getName() + \" have different replication number\");\n-                        }\n-                    }\n-                }\n-            } else {\n-                // set to an already exist colocate group, check if this table can be added to this group.\n-                groupSchema.checkColocateSchema(table);\n-            }\n-            \n-            List<List<Long>> backendsPerBucketSeq = null;\n-            if (groupSchema == null) {\n-                // assign to a newly created group, set backends sequence.\n-                // we arbitrarily choose a tablet backends sequence from this table,\n-                // let the colocation balancer do the work.\n-                backendsPerBucketSeq = table.getArbitraryTabletBucketsSeq();\n-            }\n-            // change group after getting backends sequence(if has), in case 'getArbitraryTabletBucketsSeq' failed\n-            groupId = colocateTableIndex.changeGroup(db.getId(), table, oldGroup, colocateGroup, assignedGroupId);\n-\n-            if (groupSchema == null) {\n-                Preconditions.checkNotNull(backendsPerBucketSeq);\n-                colocateTableIndex.addBackendsPerBucketSeq(groupId, backendsPerBucketSeq);\n-            }\n-\n-            // set this group as unstable\n-            colocateTableIndex.markGroupUnstable(groupId, false /* edit log is along with modify table log */);\n-            table.setColocateGroup(colocateGroup);\n-        } else {\n-            // unset colocation group\n-            if (Strings.isNullOrEmpty(oldGroup)) {\n-                // this table is not a colocate table, do nothing\n-                return;\n-            }\n-\n-            // when replayModifyTableColocate, we need the groupId info\n-            String fullGroupName = db.getId() + \"_\" + oldGroup;\n-            groupId = colocateTableIndex.getGroupSchema(fullGroupName).getGroupId();\n-\n-            colocateTableIndex.removeTable(table.getId());\n-            table.setColocateGroup(null);\n-        }\n-\n-        if (!isReplay) {\n-            Map<String, String> properties = Maps.newHashMapWithExpectedSize(1);\n-            properties.put(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH, colocateGroup);\n-            TablePropertyInfo info = new TablePropertyInfo(table.getId(), groupId, properties);\n-            editLog.logModifyTableColocate(info);\n-        }\n-        LOG.info(\"finished modify table's colocation property. table: {}, is replay: {}\",\n-                table.getName(), isReplay);\n-    }\n-\n-    public void replayModifyTableColocate(TablePropertyInfo info) {\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getPropertyMap();\n-\n-        Database db = getDb(info.getGroupId().dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            modifyTableColocate(db, table, properties.get(PropertyAnalyzer.PROPERTIES_COLOCATE_WITH), true,\n-                    info.getGroupId());\n-        } catch (DdlException e) {\n-            // should not happen\n-            LOG.warn(\"failed to replay modify table colocate\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameRollup(Database db, OlapTable table, RollupRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        String rollupName = renameClause.getRollupName();\n-        // check if it is base table name\n-        if (rollupName.equals(table.getName())) {\n-            throw new DdlException(\"Using ALTER TABLE RENAME to change table name\");\n-        }\n-\n-        String newRollupName = renameClause.getNewRollupName();\n-        if (rollupName.equals(newRollupName)) {\n-            throw new DdlException(\"Same rollup name\");\n-        }\n-\n-        Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-        if (indexNameToIdMap.get(rollupName) == null) {\n-            throw new DdlException(\"Rollup index[\" + rollupName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (indexNameToIdMap.get(newRollupName) != null) {\n-            throw new DdlException(\"Rollup name[\" + newRollupName + \"] is already used\");\n-        }\n-\n-        long indexId = indexNameToIdMap.remove(rollupName);\n-        indexNameToIdMap.put(newRollupName, indexId);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForRollupRename(db.getId(), table.getId(), indexId, newRollupName);\n-        editLog.logRollupRename(tableInfo);\n-        LOG.info(\"rename rollup[{}] to {}\", rollupName, newRollupName);\n-    }\n-\n-    public void replayRenameRollup(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long indexId = tableInfo.getIndexId();\n-        String newRollupName = tableInfo.getNewRollupName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            String rollupName = table.getIndexNameById(indexId);\n-            Map<String, Long> indexNameToIdMap = table.getIndexNameToId();\n-            indexNameToIdMap.remove(rollupName);\n-            indexNameToIdMap.put(newRollupName, indexId);\n-\n-            LOG.info(\"replay rename rollup[{}] to {}\", rollupName, newRollupName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renamePartition(Database db, OlapTable table, PartitionRenameClause renameClause) throws DdlException {\n-        if (table.getState() != OlapTableState.NORMAL) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is under \" + table.getState());\n-        }\n-\n-        if (table.getPartitionInfo().getType() != PartitionType.RANGE) {\n-            throw new DdlException(\"Table[\" + table.getName() + \"] is single partitioned. \"\n-                    + \"no need to rename partition name.\");\n-        }\n-\n-        String partitionName = renameClause.getPartitionName();\n-        String newPartitionName = renameClause.getNewPartitionName();\n-        if (partitionName.equalsIgnoreCase(newPartitionName)) {\n-            throw new DdlException(\"Same partition name\");\n-        }\n-\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition[\" + partitionName + \"] does not exists\");\n-        }\n-\n-        // check if name is already used\n-        if (table.checkPartitionNameExist(newPartitionName)) {\n-            throw new DdlException(\"Partition name[\" + newPartitionName + \"] is already used\");\n-        }\n-\n-        table.renamePartition(partitionName, newPartitionName);\n-\n-        // log\n-        TableInfo tableInfo = TableInfo.createForPartitionRename(db.getId(), table.getId(), partition.getId(),\n-                newPartitionName);\n-        editLog.logPartitionRename(tableInfo);\n-        LOG.info(\"rename partition[{}] to {}\", partitionName, newPartitionName);\n-    }\n-\n-    public void replayRenamePartition(TableInfo tableInfo) throws DdlException {\n-        long dbId = tableInfo.getDbId();\n-        long tableId = tableInfo.getTableId();\n-        long partitionId = tableInfo.getPartitionId();\n-        String newPartitionName = tableInfo.getNewPartitionName();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            Partition partition = table.getPartition(partitionId);\n-            table.renamePartition(partition.getName(), newPartitionName);\n-\n-            LOG.info(\"replay rename partition[{}] to {}\", partition.getName(), newPartitionName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void renameColumn(Database db, OlapTable table, ColumnRenameClause renameClause) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void replayRenameColumn(TableInfo tableInfo) throws DdlException {\n-        throw new DdlException(\"not implmented\");\n-    }\n-\n-    public void modifyTableDynamicPartition(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Map<String, String> logProperties = new HashMap<>(properties);\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            DynamicPartitionUtil.checkAndSetDynamicPartitionProperty(table, properties);\n-        } else {\n-            Map<String, String> analyzedDynamicPartition = DynamicPartitionUtil.analyzeDynamicPartition(properties);\n-            tableProperty.modifyTableProperties(analyzedDynamicPartition);\n-            tableProperty.buildDynamicProperty();\n-        }\n-\n-        DynamicPartitionUtil.registerOrRemoveDynamicPartitionTable(db.getId(), table);\n-        dynamicPartitionScheduler.createOrUpdateRuntimeInfo(\n-                table.getName(), DynamicPartitionScheduler.LAST_UPDATE_TIME, TimeUtils.getCurrentFormatTime());\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), logProperties);\n-        editLog.logDynamicPartition(info);\n-    }\n-\n-    /**\n-     * Set replication number for unpartitioned table.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     * @throws DdlException\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableReplicationNum(Database db, OlapTable table, Map<String, String> properties) throws DdlException {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        String defaultReplicationNumName = \"default.\"+ PropertyAnalyzer.PROPERTIES_REPLICATION_NUM;\n-        PartitionInfo partitionInfo = table.getPartitionInfo();\n-        if (partitionInfo.getType() == PartitionType.RANGE) {\n-            throw new DdlException(\"This is a range partitioned table, you should specify partitions with MODIFY PARTITION clause.\" +\n-                    \" If you want to set default replication number, please use '\" + defaultReplicationNumName +\n-                    \"' instead of '\" + PropertyAnalyzer.PROPERTIES_REPLICATION_NUM + \"' to escape misleading.\");\n-        }\n-        String partitionName = table.getName();\n-        Partition partition = table.getPartition(partitionName);\n-        if (partition == null) {\n-            throw new DdlException(\"Partition does not exist. name: \" + partitionName);\n-        }\n-\n-        short replicationNum = Short.valueOf(properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-        boolean isInMemory = partitionInfo.getIsInMemory(partition.getId());\n-        DataProperty newDataProperty = partitionInfo.getDataProperty(partition.getId());\n-        partitionInfo.setReplicationNum(partition.getId(), replicationNum);\n-        // log\n-        ModifyPartitionInfo info = new ModifyPartitionInfo(db.getId(), table.getId(), partition.getId(),\n-                newDataProperty, replicationNum, isInMemory);\n-        editLog.logModifyPartition(info);\n-        LOG.debug(\"modify partition[{}-{}-{}] replication num to {}\", db.getId(), table.getId(), partition.getName(),\n-                replicationNum);\n-    }\n-\n-    /**\n-     * Set default replication number for a specified table.\n-     * You can see the default replication number by Show Create Table stmt.\n-     * @param db\n-     * @param table\n-     * @param properties\n-     */\n-    // The caller need to hold the db write lock\n-    public void modifyTableDefaultReplicationNum(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildReplicationNum();\n-        // log\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyReplicationNum(info);\n-        LOG.debug(\"modify table[{}] replication num to {}\", table.getName(),\n-                properties.get(PropertyAnalyzer.PROPERTIES_REPLICATION_NUM));\n-    }\n-\n-    // The caller need to hold the db write lock\n-    public void modifyTableInMemoryMeta(Database db, OlapTable table, Map<String, String> properties) {\n-        Preconditions.checkArgument(db.isWriteLockHeldByCurrentThread());\n-        TableProperty tableProperty = table.getTableProperty();\n-        if (tableProperty == null) {\n-            tableProperty = new TableProperty(properties);\n-        } else {\n-            tableProperty.modifyTableProperties(properties);\n-        }\n-        tableProperty.buildInMemory();\n-\n-        // need to update partition info meta\n-        for(Partition partition: table.getPartitions()) {\n-            table.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-        }\n-\n-        ModifyTablePropertyOperationLog info = new ModifyTablePropertyOperationLog(db.getId(), table.getId(), properties);\n-        editLog.logModifyInMemory(info);\n-    }\n-\n-    public void replayModifyTableProperty(short opCode, ModifyTablePropertyOperationLog info) {\n-        long dbId = info.getDbId();\n-        long tableId = info.getTableId();\n-        Map<String, String> properties = info.getProperties();\n-\n-        Database db = getDb(dbId);\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(tableId);\n-            TableProperty tableProperty = olapTable.getTableProperty();\n-            if (tableProperty == null) {\n-                olapTable.setTableProperty(new TableProperty(properties).buildProperty(opCode));\n-            } else {\n-                tableProperty.modifyTableProperties(properties);\n-                tableProperty.buildProperty(opCode);\n-            }\n-\n-            // need to replay partition info meta\n-            if (opCode == OperationType.OP_MODIFY_IN_MEMORY) {\n-                for(Partition partition: olapTable.getPartitions()) {\n-                    olapTable.getPartitionInfo().setIsInMemory(partition.getId(), tableProperty.IsInMemory());\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * used for handling AlterClusterStmt\n-     * (for client is the ALTER CLUSTER command).\n-     */\n-    public void alterCluster(AlterSystemStmt stmt) throws DdlException, UserException {\n-        this.alter.processAlterCluster(stmt);\n-    }\n-\n-    public void cancelAlterCluster(CancelAlterSystemStmt stmt) throws DdlException {\n-        this.alter.getClusterHandler().cancel(stmt);\n-    }\n-\n-    /*\n-     * generate and check columns' order and key's existence\n-     */\n-    private void validateColumns(List<Column> columns) throws DdlException {\n-        if (columns.isEmpty()) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_COLUMNS);\n-        }\n-\n-        boolean encounterValue = false;\n-        boolean hasKey = false;\n-        for (Column column : columns) {\n-            if (column.isKey()) {\n-                if (encounterValue) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_OLAP_KEY_MUST_BEFORE_VALUE);\n-                }\n-                hasKey = true;\n-            } else {\n-                encounterValue = true;\n-            }\n-        }\n-\n-        if (!hasKey) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_MUST_HAVE_KEYS);\n-        }\n-    }\n-\n-    // Change current database of this session.\n-    public void changeDb(ConnectContext ctx, String qualifiedDb) throws DdlException {\n-        if (!auth.checkDbPriv(ctx, qualifiedDb, PrivPredicate.SHOW)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_DB_ACCESS_DENIED, ctx.getQualifiedUser(), qualifiedDb);\n-        }\n-\n-        if (getDb(qualifiedDb) == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, qualifiedDb);\n-        }\n-\n-        ctx.setDatabase(qualifiedDb);\n-    }\n-\n-    // for test only\n-    public void clear() {\n-        if (SingletonHolder.INSTANCE.idToDb != null) {\n-            SingletonHolder.INSTANCE.idToDb.clear();\n-        }\n-        if (SingletonHolder.INSTANCE.fullNameToDb != null) {\n-            SingletonHolder.INSTANCE.fullNameToDb.clear();\n-        }\n-        if (load.getIdToLoadJob() != null) {\n-            load.getIdToLoadJob().clear();\n-            // load = null;\n-        }\n-\n-        SingletonHolder.INSTANCE.getRollupHandler().unprotectedGetAlterJobs().clear();\n-        SingletonHolder.INSTANCE.getSchemaChangeHandler().unprotectedGetAlterJobs().clear();\n-        System.gc();\n-    }\n-\n-    public void createView(CreateViewStmt stmt) throws DdlException {\n-        String dbName = stmt.getDbName();\n-        String tableName = stmt.getTable();\n-\n-        // check if db exists\n-        Database db = this.getDb(stmt.getDbName());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbName);\n-        }\n-\n-        // check if table exists in db\n-        db.readLock();\n-        try {\n-            if (db.getTable(tableName) != null) {\n-                if (stmt.isSetIfNotExists()) {\n-                    LOG.info(\"create view[{}] which already exists\", tableName);\n-                    return;\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_TABLE_EXISTS_ERROR, tableName);\n-                }\n-            }\n-        } finally {\n-            db.readUnlock();\n-        }\n-\n-        List<Column> columns = stmt.getColumns();\n-\n-        long tableId = Catalog.getCurrentCatalog().getNextId();\n-        View newView = new View(tableId, tableName, columns);\n-        newView.setComment(stmt.getComment());\n-        newView.setInlineViewDefWithSqlMode(stmt.getInlineViewDef(),\n-                ConnectContext.get().getSessionVariable().getSqlMode());\n-        // init here in case the stmt string from view.toSql() has some syntax error.\n-        try {\n-            newView.init();\n-        } catch (UserException e) {\n-            throw new DdlException(\"failed to init view stmt\", e);\n-        }\n-      \n-        if (!db.createTableWithLock(newView, false, stmt.isSetIfNotExists())) {\n-            throw new DdlException(\"Failed to create view[\" + tableName + \"].\");\n-        }\n-        LOG.info(\"successfully create view[\" + tableName + \"-\" + newView.getId() + \"]\");\n-    }\n-\n-    /**\n-     * Returns the function that best matches 'desc' that is registered with the\n-     * catalog using 'mode' to check for matching. If desc matches multiple\n-     * functions in the catalog, it will return the function with the strictest\n-     * matching mode. If multiple functions match at the same matching mode,\n-     * ties are broken by comparing argument types in lexical order. Argument\n-     * types are ordered by argument precision (e.g. double is preferred over\n-     * float) and then by alphabetical order of argument type name, to guarantee\n-     * deterministic results.\n-     */\n-    public Function getFunction(Function desc, Function.CompareMode mode) {\n-        return functionSet.getFunction(desc, mode);\n-    }\n-\n-    public List<Function> getBuiltinFunctions() {\n-        return functionSet.getBulitinFunctions();\n-    }\n-\n-    public boolean isNonNullResultWithNullParamFunction(String funcName) {\n-        return functionSet.isNonNullResultWithNullParamFunctions(funcName);\n-    }\n-\n-    /**\n-     * create cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void createCluster(CreateClusterStmt stmt) throws DdlException {\n-        final String clusterName = stmt.getClusterName();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (nameToCluster.containsKey(clusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_HAS_EXIST, clusterName);\n-            } else {\n-                List<Long> backendList = systemInfo.createCluster(clusterName, stmt.getInstanceNum());\n-                // 1: BE returned is less than requested, throws DdlException.\n-                // 2: BE returned is more than or equal to 0, succeeds.\n-                if (backendList != null || stmt.getInstanceNum() == 0) {\n-                    final long id = getNextId();\n-                    final Cluster cluster = new Cluster(clusterName, id);\n-                    cluster.setBackendIdList(backendList);\n-                    unprotectCreateCluster(cluster);\n-                    if (clusterName.equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                        for (Database db : idToDb.values()) {\n-                            if (db.getClusterName().equals(SystemInfoService.DEFAULT_CLUSTER)) {\n-                                cluster.addDb(db.getFullName(), db.getId());\n-                            }\n-                        }\n-                    }\n-                    editLog.logCreateCluster(cluster);\n-                    LOG.info(\"finish to create cluster: {}\", clusterName);\n-                } else {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-            }\n-        } finally {\n-            unlock();\n-        }\n-\n-        // create super user for this cluster\n-        UserIdentity adminUser = new UserIdentity(PaloAuth.ADMIN_USER, \"%\");\n-        try {\n-            adminUser.analyze(stmt.getClusterName());\n-        } catch (AnalysisException e) {\n-            LOG.error(\"should not happen\", e);\n-        }\n-        auth.createUser(new CreateUserStmt(new UserDesc(adminUser, \"\", true)));\n-    }\n-\n-    private void unprotectCreateCluster(Cluster cluster) {\n-        final Iterator<Long> iterator = cluster.getBackendIdList().iterator();\n-        while (iterator.hasNext()) {\n-            final Long id = iterator.next();\n-            final Backend backend = systemInfo.getBackend(id);\n-            backend.setOwnerClusterName(cluster.getName());\n-            backend.setBackendState(BackendState.using);\n-        }\n-\n-        idToCluster.put(cluster.getId(), cluster);\n-        nameToCluster.put(cluster.getName(), cluster);\n-\n-        // create info schema db\n-        final InfoSchemaDb infoDb = new InfoSchemaDb(cluster.getName());\n-        infoDb.setClusterName(cluster.getName());\n-        unprotectCreateDb(infoDb);\n-\n-        // only need to create default cluster once.\n-        if (cluster.getName().equalsIgnoreCase(SystemInfoService.DEFAULT_CLUSTER)) {\n-            isDefaultClusterCreated = true;\n-        }\n-    }\n-\n-    /**\n-     * replay create cluster\n-     *\n-     * @param cluster\n-     */\n-    public void replayCreateCluster(Cluster cluster) {\n-        tryLock(true);\n-        try {\n-            unprotectCreateCluster(cluster);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * drop cluster and cluster's db must be have deleted\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void dropCluster(DropClusterStmt stmt) throws DdlException {\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            final String clusterName = stmt.getClusterName();\n-            final Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-            final List<Backend> backends = systemInfo.getClusterBackends(clusterName);\n-            for (Backend backend : backends) {\n-                if (backend.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            // check if there still have databases undropped, except for information_schema db\n-            if (cluster.getDbNames().size() > 1) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DELETE_DB_EXIST, clusterName);\n-            }\n-\n-            systemInfo.releaseBackends(clusterName, false /* is not replay */);\n-            final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId());\n-            unprotectDropCluster(info, false /* is not replay */);\n-            editLog.logDropCluster(info);\n-        } finally {\n-            unlock();\n-        }\n-\n-        // drop user of this cluster\n-        // set is replay to true, not write log\n-        auth.dropUserOfCluster(stmt.getClusterName(), true /* is replay */);\n-    }\n-\n-    private void unprotectDropCluster(ClusterInfo info, boolean isReplay) {\n-        systemInfo.releaseBackends(info.getClusterName(), isReplay);\n-        idToCluster.remove(info.getClusterId());\n-        nameToCluster.remove(info.getClusterName());\n-        final Database infoSchemaDb = fullNameToDb.get(InfoSchemaDb.getFullInfoSchemaDbName(info.getClusterName()));\n-        fullNameToDb.remove(infoSchemaDb.getFullName());\n-        idToDb.remove(infoSchemaDb.getId());\n-    }\n-\n-    public void replayDropCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            unprotectDropCluster(info, true/* is replay */);\n-        } finally {\n-            unlock();\n-        }\n-\n-        auth.dropUserOfCluster(info.getClusterName(), true /* is replay */);\n-    }\n-\n-    public void replayExpandCluster(ClusterInfo info) {\n-        tryLock(true);\n-        try {\n-            final Cluster cluster = nameToCluster.get(info.getClusterName());\n-            cluster.addBackends(info.getBackendIdList());\n-\n-            for (Long beId : info.getBackendIdList()) {\n-                Backend be = Catalog.getCurrentSystemInfo().getBackend(beId);\n-                if (be == null) {\n-                    continue;\n-                }\n-                be.setOwnerClusterName(info.getClusterName());\n-                be.setBackendState(BackendState.using);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * modify cluster: Expansion or shrink\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void processModifyCluster(AlterClusterStmt stmt) throws UserException {\n-        final String clusterName = stmt.getAlterClusterName();\n-        final int newInstanceNum = stmt.getInstanceNum();\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            Cluster cluster = nameToCluster.get(clusterName);\n-            if (cluster == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-            }\n-\n-            // check if this cluster has backend in decommission\n-            final List<Long> backendIdsInCluster = cluster.getBackendIdList();\n-            for (Long beId : backendIdsInCluster) {\n-                Backend be = systemInfo.getBackend(beId);\n-                if (be.isDecommissioned()) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_IN_DECOMMISSION, clusterName);\n-                }\n-            }\n-\n-            final int oldInstanceNum = backendIdsInCluster.size();\n-            if (newInstanceNum > oldInstanceNum) {\n-                // expansion\n-                final List<Long> expandBackendIds = systemInfo.calculateExpansionBackends(clusterName,\n-                        newInstanceNum - oldInstanceNum);\n-                if (expandBackendIds == null) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BE_NOT_ENOUGH);\n-                }\n-                cluster.addBackends(expandBackendIds);\n-                final ClusterInfo info = new ClusterInfo(clusterName, cluster.getId(), expandBackendIds);\n-                editLog.logExpandCluster(info);\n-            } else if (newInstanceNum < oldInstanceNum) {\n-                // shrink\n-                final List<Long> decomBackendIds = systemInfo.calculateDecommissionBackends(clusterName,\n-                        oldInstanceNum - newInstanceNum);\n-                if (decomBackendIds == null || decomBackendIds.size() == 0) {\n-                    ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_BACKEND_ERROR);\n-                }\n-\n-                List<String> hostPortList = Lists.newArrayList();\n-                for (Long id : decomBackendIds) {\n-                    final Backend backend = systemInfo.getBackend(id);\n-                    hostPortList.add(new StringBuilder().append(backend.getHost()).append(\":\")\n-                            .append(backend.getHeartbeatPort()).toString());\n-                }\n-\n-                // here we reuse the process of decommission backends. but set backend's decommission type to\n-                // ClusterDecommission, which means this backend will not be removed from the system\n-                // after decommission is done.\n-                final DecommissionBackendClause clause = new DecommissionBackendClause(hostPortList);\n-                try {\n-                    clause.analyze(null);\n-                    clause.setType(DecommissionType.ClusterDecommission);\n-                    AlterSystemStmt alterStmt = new AlterSystemStmt(clause);\n-                    alterStmt.setClusterName(clusterName);\n-                    this.alter.processAlterCluster(alterStmt);\n-                } catch (AnalysisException e) {\n-                    Preconditions.checkState(false, \"should not happend: \" + e.getMessage());\n-                }\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_ALTER_BE_NO_CHANGE, newInstanceNum);\n-            }\n-\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * @param ctx\n-     * @param clusterName\n-     * @throws DdlException\n-     */\n-    public void changeCluster(ConnectContext ctx, String clusterName) throws DdlException {\n-        if (!Catalog.getCurrentCatalog().getAuth().checkCanEnterCluster(ConnectContext.get(), clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_AUTHORITY,\n-                    ConnectContext.get().getQualifiedUser(), \"enter\");\n-        }\n-\n-        if (!nameToCluster.containsKey(clusterName)) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_NO_EXISTS, clusterName);\n-        }\n-\n-        ctx.setCluster(clusterName);\n-    }\n-\n-    /**\n-     * migrate db to link dest cluster\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void migrateDb(MigrateDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-            if (!destCluster.containLink(destDbName, srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-\n-            final Database db = fullNameToDb.get(srcDbName);\n-\n-            // if the max replication num of the src db is larger then the backends num of the dest cluster,\n-            // the migration will not be processed.\n-            final int maxReplicationNum = db.getMaxReplicationNum();\n-            if (maxReplicationNum > destCluster.getBackendIdList().size()) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_BE_NOT_ENOUGH, destClusterName);\n-            }\n-\n-            if (db.getDbState() == DbState.LINK) {\n-                final BaseParam param = new BaseParam();\n-                param.addStringParam(destDbName);\n-                param.addLongParam(db.getId());\n-                param.addStringParam(srcDbName);\n-                param.addStringParam(destClusterName);\n-                param.addStringParam(srcClusterName);\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                destCluster.removeLinkDb(param);\n-                destCluster.addDb(destDbName, db.getId());\n-                db.writeLock();\n-                try {\n-                    db.setDbState(DbState.MOVE);\n-                    // set cluster to the dest cluster.\n-                    // and Clone process will do the migration things.\n-                    db.setClusterName(destClusterName);\n-                    db.setName(destDbName);\n-                    db.setAttachDb(srcDbName);\n-                } finally {\n-                    db.writeUnlock();\n-                }\n-                editLog.logMigrateCluster(param);\n-            } else {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATION_NO_LINK, srcDbName, destDbName);\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayMigrateDb(BaseParam param) {\n-        final String desDbName = param.getStringParam();\n-        final String srcDbName = param.getStringParam(1);\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcClusterName = param.getStringParam(3);\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Database db = fullNameToDb.get(srcDbName);\n-            if (db.getDbState() == DbState.LINK) {\n-                fullNameToDb.remove(db.getFullName());\n-                srcCluster.removeDb(db.getFullName(), db.getId());\n-                desCluster.removeLinkDb(param);\n-                desCluster.addDb(param.getStringParam(), db.getId());\n-\n-                db.writeLock();\n-                db.setName(desDbName);\n-                db.setAttachDb(srcDbName);\n-                db.setDbState(DbState.MOVE);\n-                db.setClusterName(desClusterName);\n-                db.writeUnlock();\n-            }\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public void replayLinkDb(BaseParam param) {\n-        final String desClusterName = param.getStringParam(2);\n-        final String srcDbName = param.getStringParam(1);\n-        final String desDbName = param.getStringParam();\n-\n-        tryLock(true);\n-        try {\n-            final Cluster desCluster = this.nameToCluster.get(desClusterName);\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-            srcDb.writeLock();\n-            srcDb.setDbState(DbState.LINK);\n-            srcDb.setAttachDb(desDbName);\n-            srcDb.writeUnlock();\n-            desCluster.addLinkDb(param);\n-            fullNameToDb.put(desDbName, srcDb);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    /**\n-     * link src db to dest db. we use java's quotation Mechanism to realize db hard links\n-     *\n-     * @param stmt\n-     * @throws DdlException\n-     */\n-    public void linkDb(LinkDbStmt stmt) throws DdlException {\n-        final String srcClusterName = stmt.getSrcCluster();\n-        final String destClusterName = stmt.getDestCluster();\n-        final String srcDbName = stmt.getSrcDb();\n-        final String destDbName = stmt.getDestDb();\n-\n-        if (!tryLock(false)) {\n-            throw new DdlException(\"Failed to acquire catalog lock. Try again\");\n-        }\n-        try {\n-            if (!nameToCluster.containsKey(srcClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_CLUSTER_NOT_EXIST, srcClusterName);\n-            }\n-\n-            if (!nameToCluster.containsKey(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DEST_CLUSTER_NOT_EXIST, destClusterName);\n-            }\n-\n-            if (srcClusterName.equals(destClusterName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_MIGRATE_SAME_CLUSTER);\n-            }\n-\n-            if (fullNameToDb.containsKey(destDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_DB_CREATE_EXISTS, destDbName);\n-            }\n-\n-            final Cluster srcCluster = this.nameToCluster.get(srcClusterName);\n-            final Cluster destCluster = this.nameToCluster.get(destClusterName);\n-\n-            if (!srcCluster.containDb(srcDbName)) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_SRC_DB_NOT_EXIST, srcDbName);\n-            }\n-            final Database srcDb = fullNameToDb.get(srcDbName);\n-\n-            if (srcDb.getDbState() != DbState.NORMAL) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_CLUSTER_DB_STATE_LINK_OR_MIGRATE,\n-                        ClusterNamespace.getNameFromFullName(srcDbName));\n-            }\n-\n-            srcDb.writeLock();\n-            try {\n-                srcDb.setDbState(DbState.LINK);\n-                srcDb.setAttachDb(destDbName);\n-            } finally {\n-                srcDb.writeUnlock();\n-            }\n-\n-            final long id = getNextId();\n-            final BaseParam param = new BaseParam();\n-            param.addStringParam(destDbName);\n-            param.addStringParam(srcDbName);\n-            param.addLongParam(id);\n-            param.addLongParam(srcDb.getId());\n-            param.addStringParam(destClusterName);\n-            param.addStringParam(srcClusterName);\n-            destCluster.addLinkDb(param);\n-            fullNameToDb.put(destDbName, srcDb);\n-            editLog.logLinkCluster(param);\n-        } finally {\n-            unlock();\n-        }\n-    }\n-\n-    public Cluster getCluster(String clusterName) {\n-        return nameToCluster.get(clusterName);\n-    }\n-\n-    public List<String> getClusterNames() {\n-        return new ArrayList<String>(nameToCluster.keySet());\n-    }\n-\n-    /**\n-     * get migrate progress , when finish migration, next clonecheck will reset dbState\n-     *\n-     * @return\n-     */\n-    public Set<BaseParam> getMigrations() {\n-        final Set<BaseParam> infos = Sets.newHashSet();\n-        for (Database db : fullNameToDb.values()) {\n-            db.readLock();\n-            try {\n-                if (db.getDbState() == DbState.MOVE) {\n-                    int tabletTotal = 0;\n-                    int tabletQuorum = 0;\n-                    final Set<Long> beIds = Sets.newHashSet(systemInfo.getClusterBackendIds(db.getClusterName()));\n-                    final Set<String> tableNames = db.getTableNamesWithLock();\n-                    for (String tableName : tableNames) {\n-\n-                        Table table = db.getTable(tableName);\n-                        if (table == null || table.getType() != TableType.OLAP) {\n-                            continue;\n-                        }\n-\n-                        OlapTable olapTable = (OlapTable) table;\n-                        for (Partition partition : olapTable.getPartitions()) {\n-                            final short replicationNum = olapTable.getPartitionInfo()\n-                                    .getReplicationNum(partition.getId());\n-                            for (MaterializedIndex materializedIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                                if (materializedIndex.getState() != IndexState.NORMAL) {\n-                                    continue;\n-                                }\n-                                for (Tablet tablet : materializedIndex.getTablets()) {\n-                                    int replicaNum = 0;\n-                                    int quorum = replicationNum / 2 + 1;\n-                                    for (Replica replica : tablet.getReplicas()) {\n-                                        if (replica.getState() != ReplicaState.CLONE\n-                                                && beIds.contains(replica.getBackendId())) {\n-                                            replicaNum++;\n-                                        }\n-                                    }\n-                                    if (replicaNum > quorum) {\n-                                        replicaNum = quorum;\n-                                    }\n-\n-                                    tabletQuorum = tabletQuorum + replicaNum;\n-                                    tabletTotal = tabletTotal + quorum;\n-                                }\n-                            }\n-                        }\n-                    }\n-                    final BaseParam info = new BaseParam();\n-                    info.addStringParam(db.getClusterName());\n-                    info.addStringParam(db.getAttachDb());\n-                    info.addStringParam(db.getFullName());\n-                    final float percentage = tabletTotal > 0 ? (float) tabletQuorum / (float) tabletTotal : 0f;\n-                    info.addFloatParam(percentage);\n-                    infos.add(info);\n-                }\n-            } finally {\n-                db.readUnlock();\n-            }\n-        }\n-\n-        return infos;\n-    }\n-\n-    public long loadCluster(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_30) {\n-            int clusterCount = dis.readInt();\n-            checksum ^= clusterCount;\n-            for (long i = 0; i < clusterCount; ++i) {\n-                final Cluster cluster = Cluster.read(dis);\n-                checksum ^= cluster.getId();\n-\n-                List<Long> latestBackendIds = systemInfo.getClusterBackendIds(cluster.getName());\n-                if (latestBackendIds.size() != cluster.getBackendIdList().size()) {\n-                    LOG.warn(\"Cluster:\" + cluster.getName() + \", backends in Cluster is \"\n-                            + cluster.getBackendIdList().size() + \", backends in SystemInfoService is \"\n-                            + cluster.getBackendIdList().size());\n-                }\n-                // The number of BE in cluster is not same as in SystemInfoService, when perform 'ALTER\n-                // SYSTEM ADD BACKEND TO ...' or 'ALTER SYSTEM ADD BACKEND ...', because both of them are \n-                // for adding BE to some Cluster, but loadCluster is after loadBackend.\n-                cluster.setBackendIdList(latestBackendIds);\n-\n-                String dbName =  InfoSchemaDb.getFullInfoSchemaDbName(cluster.getName());\n-                InfoSchemaDb db;\n-                // Use real Catalog instance to avoid InfoSchemaDb id continuously increment\n-                // when checkpoint thread load image.\n-                if (Catalog.getCurrentCatalog().getFullNameToDb().containsKey(dbName)) {\n-                    db = (InfoSchemaDb)Catalog.getCurrentCatalog().getFullNameToDb().get(dbName);\n-                } else {\n-                    db = new InfoSchemaDb(cluster.getName());\n-                    db.setClusterName(cluster.getName());\n-                }\n-                String errMsg = \"InfoSchemaDb id shouldn't larger than 10000, please restart your FE server\";\n-                // Every time we construct the InfoSchemaDb, which id will increment.\n-                // When InfoSchemaDb id larger than 10000 and put it to idToDb,\n-                // which may be overwrite the normal db meta in idToDb,\n-                // so we ensure InfoSchemaDb id less than 10000.\n-                Preconditions.checkState(db.getId() < NEXT_ID_INIT_VALUE, errMsg);\n-                idToDb.put(db.getId(), db);\n-                fullNameToDb.put(db.getFullName(), db);\n-                cluster.addDb(dbName, db.getId());\n-                idToCluster.put(cluster.getId(), cluster);\n-                nameToCluster.put(cluster.getName(), cluster);\n-            }\n-        }\n-        LOG.info(\"finished replay cluster from image\");\n-        return checksum;\n-    }\n-\n-    public void initDefaultCluster() {\n-        final List<Long> backendList = Lists.newArrayList();\n-        final List<Backend> defaultClusterBackends = systemInfo.getClusterBackends(SystemInfoService.DEFAULT_CLUSTER);\n-        for (Backend backend : defaultClusterBackends) {\n-            backendList.add(backend.getId());\n-        }\n-\n-        final long id = getNextId();\n-        final Cluster cluster = new Cluster(SystemInfoService.DEFAULT_CLUSTER, id);\n-\n-        // make sure one host hold only one backend.\n-        Set<String> beHost = Sets.newHashSet();\n-        for (Backend be : defaultClusterBackends) {\n-            if (beHost.contains(be.getHost())) {\n-                // we can not handle this situation automatically.\n-                LOG.error(\"found more than one backends in same host: {}\", be.getHost());\n-                System.exit(-1);\n-            } else {\n-                beHost.add(be.getHost());\n-            }\n-        }\n-\n-        // we create default_cluster to meet the need for ease of use, because\n-        // most users hava no multi tenant needs.\n-        cluster.setBackendIdList(backendList);\n-        unprotectCreateCluster(cluster);\n-        for (Database db : idToDb.values()) {\n-            db.setClusterName(SystemInfoService.DEFAULT_CLUSTER);\n-            cluster.addDb(db.getFullName(), db.getId());\n-        }\n-\n-        // no matter default_cluster is created or not,\n-        // mark isDefaultClusterCreated as true\n-        isDefaultClusterCreated = true;\n-        editLog.logCreateCluster(cluster);\n-    }\n-\n-    public void replayUpdateDb(DatabaseInfo info) {\n-        final Database db = fullNameToDb.get(info.getDbName());\n-        db.setClusterName(info.getClusterName());\n-        db.setDbState(info.getDbState());\n-    }\n-\n-    public long saveCluster(DataOutputStream dos, long checksum) throws IOException {\n-        final int clusterCount = idToCluster.size();\n-        checksum ^= clusterCount;\n-        dos.writeInt(clusterCount);\n-        for (Map.Entry<Long, Cluster> entry : idToCluster.entrySet()) {\n-            long clusterId = entry.getKey();\n-            if (clusterId >= NEXT_ID_INIT_VALUE) {\n-                checksum ^= clusterId;\n-                final Cluster cluster = entry.getValue();\n-                cluster.write(dos);\n-            }\n-        }\n-        return checksum;\n-    }\n-\n-    public long saveBrokers(DataOutputStream dos, long checksum) throws IOException {\n-        Map<String, List<FsBroker>> addressListMap = brokerMgr.getBrokerListMap();\n-        int size = addressListMap.size();\n-        checksum ^= size;\n-        dos.writeInt(size);\n-\n-        for (Map.Entry<String, List<FsBroker>> entry : addressListMap.entrySet()) {\n-            Text.writeString(dos, entry.getKey());\n-            final List<FsBroker> addrs = entry.getValue();\n-            size = addrs.size();\n-            checksum ^= size;\n-            dos.writeInt(size);\n-            for (FsBroker addr : addrs) {\n-                addr.write(dos);\n-            }\n-        }\n-\n-        return checksum;\n-    }\n-\n-    public long loadBrokers(DataInputStream dis, long checksum) throws IOException, DdlException {\n-        if (MetaContext.get().getMetaVersion() >= FeMetaVersion.VERSION_31) {\n-            int count = dis.readInt();\n-            checksum ^= count;\n-            for (long i = 0; i < count; ++i) {\n-                String brokerName = Text.readString(dis);\n-                int size = dis.readInt();\n-                checksum ^= size;\n-                List<FsBroker> addrs = Lists.newArrayList();\n-                for (int j = 0; j < size; j++) {\n-                    FsBroker addr = FsBroker.readIn(dis);\n-                    addrs.add(addr);\n-                }\n-                brokerMgr.replayAddBrokers(brokerName, addrs);\n-            }\n-            LOG.info(\"finished replay brokerMgr from image\");\n-        }\n-        return checksum;\n-    }\n-\n-    public void replayUpdateClusterAndBackends(BackendIdsUpdateInfo info) {\n-        for (long id : info.getBackendList()) {\n-            final Backend backend = systemInfo.getBackend(id);\n-            final Cluster cluster = nameToCluster.get(backend.getOwnerClusterName());\n-            cluster.removeBackend(id);\n-            backend.setDecommissioned(false);\n-            backend.clearClusterName();\n-            backend.setBackendState(BackendState.free);\n-        }\n-    }\n-\n-    public String dumpImage() {\n-        LOG.info(\"begin to dump meta data\");\n-        String dumpFilePath;\n-        Map<Long, Database> lockedDbMap = Maps.newTreeMap();\n-        tryLock(true);\n-        try {\n-            // sort all dbs\n-            for (long dbId : getDbIds()) {\n-                Database db = getDb(dbId);\n-                Preconditions.checkNotNull(db);\n-                lockedDbMap.put(dbId, db);\n-            }\n-\n-            // lock all dbs\n-            for (Database db : lockedDbMap.values()) {\n-                db.readLock();\n-            }\n-            LOG.info(\"acquired all the dbs' read lock.\");\n-\n-            load.readLock();\n-\n-            LOG.info(\"acquired all jobs' read lock.\");\n-            long journalId = getMaxJournalId();\n-            File dumpFile = new File(Config.meta_dir, \"image.\" + journalId);\n-            dumpFilePath = dumpFile.getAbsolutePath();\n-            try {\n-                LOG.info(\"begin to dump {}\", dumpFilePath);\n-                saveImage(dumpFile, journalId);\n-            } catch (IOException e) {\n-                LOG.error(\"failed to dump image to {}\", dumpFilePath, e);\n-            }\n-        } finally {\n-            // unlock all\n-            load.readUnlock();\n-            for (Database db : lockedDbMap.values()) {\n-                db.readUnlock();\n-            }\n-\n-            unlock();\n-        }\n-\n-        LOG.info(\"finished dumpping image to {}\", dumpFilePath);\n-        return dumpFilePath;\n-    }\n-\n-    /*\n-     * Truncate specified table or partitions.\n-     * The main idea is:\n-     * \n-     * 1. using the same schema to create new table(partitions)\n-     * 2. use the new created table(partitions) to replace the old ones.\n-     * \n-     * if no partition specified, it will truncate all partitions of this table, including all temp partitions,\n-     * otherwise, it will only truncate those specified partitions.\n-     * \n-     */\n-    public void truncateTable(TruncateTableStmt truncateTableStmt) throws DdlException {\n-        TableRef tblRef = truncateTableStmt.getTblRef();\n-        TableName dbTbl = tblRef.getName();\n-\n-        // check, and save some info which need to be checked again later\n-        Map<String, Long> origPartitions = Maps.newHashMap();\n-        OlapTable copiedTbl = null;\n-        Database db = getDb(dbTbl.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, dbTbl.getDb());\n-        }\n-\n-        boolean truncateEntireTable = tblRef.getPartitionNames() == null;\n-        db.readLock();\n-        try {\n-            Table table = db.getTable(dbTbl.getTbl());\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, dbTbl.getTbl());\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Only support truncate OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-            \n-            if (!truncateEntireTable) {\n-                for (String partName : tblRef.getPartitionNames().getPartitionNames()) {\n-                    Partition partition = olapTable.getPartition(partName);\n-                    if (partition == null) {\n-                        throw new DdlException(\"Partition \" + partName + \" does not exist\");\n-                    }\n-                    \n-                    origPartitions.put(partName, partition.getId());\n-                }\n-            } else {\n-                for (Partition partition : olapTable.getPartitions()) {\n-                    origPartitions.put(partition.getName(), partition.getId());\n-                }\n-            }\n-            \n-            copiedTbl = olapTable.selectiveCopy(origPartitions.keySet(), true, IndexExtState.VISIBLE);\n-        } finally {\n-            db.readUnlock();\n-        }\n-        \n-        // 2. use the copied table to create partitions\n-        List<Partition> newPartitions = Lists.newArrayList();\n-        // tabletIdSet to save all newly created tablet ids.\n-        Set<Long> tabletIdSet = Sets.newHashSet();\n-        try {\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                // the new partition must use new id\n-                // If we still use the old partition id, the behavior of current load jobs on this partition\n-                // will be undefined.\n-                // By using a new id, load job will be aborted(just like partition is dropped),\n-                // which is the right behavior.\n-                long oldPartitionId = entry.getValue();\n-                long newPartitionId = getNextId();\n-                Partition newPartition = createPartitionWithIndices(db.getClusterName(),\n-                        db.getId(), copiedTbl.getId(), copiedTbl.getBaseIndexId(),\n-                        newPartitionId, entry.getKey(),\n-                        copiedTbl.getIndexIdToMeta(),\n-                        copiedTbl.getKeysType(),\n-                        copiedTbl.getDefaultDistributionInfo(),\n-                        copiedTbl.getPartitionInfo().getDataProperty(oldPartitionId).getStorageMedium(),\n-                        copiedTbl.getPartitionInfo().getReplicationNum(oldPartitionId),\n-                        null /* version info */,\n-                        copiedTbl.getCopiedBfColumns(),\n-                        copiedTbl.getBfFpp(),\n-                        tabletIdSet,\n-                        copiedTbl.getCopiedIndexes(),\n-                        copiedTbl.isInMemory(),\n-                        copiedTbl.getStorageFormat(),\n-                        copiedTbl.getPartitionInfo().getTabletType(oldPartitionId));\n-                newPartitions.add(newPartition);\n-            }\n-        } catch (DdlException e) {\n-            // create partition failed, remove all newly created tablets\n-            for (Long tabletId : tabletIdSet) {\n-                Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-            }\n-            throw e;\n-        }\n-        Preconditions.checkState(origPartitions.size() == newPartitions.size());\n-\n-        // all partitions are created successfully, try to replace the old partitions.\n-        // before replacing, we need to check again.\n-        // Things may be changed outside the database lock.\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(copiedTbl.getId());\n-            if (olapTable == null) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"] is dropped\");\n-            }\n-            \n-            if (olapTable.getState() != OlapTableState.NORMAL) {\n-                throw new DdlException(\"Table' state is not NORMAL: \" + olapTable.getState());\n-            }\n-\n-            // check partitions\n-            for (Map.Entry<String, Long> entry : origPartitions.entrySet()) {\n-                Partition partition = copiedTbl.getPartition(entry.getValue());\n-                if (partition == null || !partition.getName().equals(entry.getKey())) {\n-                    throw new DdlException(\"Partition [\" + entry.getKey() + \"] is changed\");\n-                }\n-            }\n-\n-            // check if meta changed\n-            // rollup index may be added or dropped, and schema may be changed during creating partition operation.\n-            boolean metaChanged = false;\n-            if (olapTable.getIndexNameToId().size() != copiedTbl.getIndexNameToId().size()) {\n-                metaChanged = true;\n-            } else {\n-                // compare schemaHash\n-                Map<Long, Integer> copiedIndexIdToSchemaHash = copiedTbl.getIndexIdToSchemaHash();\n-                for (Map.Entry<Long, Integer> entry : olapTable.getIndexIdToSchemaHash().entrySet()) {\n-                    long indexId = entry.getKey();\n-                    if (!copiedIndexIdToSchemaHash.containsKey(indexId)) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                    if (!copiedIndexIdToSchemaHash.get(indexId).equals(entry.getValue())) {\n-                        metaChanged = true;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            if (metaChanged) {\n-                throw new DdlException(\"Table[\" + copiedTbl.getName() + \"]'s meta has been changed. try again.\");\n-            }\n-\n-            // replace\n-            truncateTableInternal(olapTable, newPartitions, truncateEntireTable);\n-\n-            // write edit log\n-            TruncateTableInfo info = new TruncateTableInfo(db.getId(), olapTable.getId(), newPartitions,\n-                    truncateEntireTable);\n-            editLog.logTruncateTable(info);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-        \n-        LOG.info(\"finished to truncate table {}, partitions: {}\",\n-                tblRef.getName().toSql(), tblRef.getPartitionNames());\n-    }\n-\n-    private void truncateTableInternal(OlapTable olapTable, List<Partition> newPartitions, boolean isEntireTable) {\n-        // use new partitions to replace the old ones.\n-        Set<Long> oldTabletIds = Sets.newHashSet();\n-        for (Partition newPartition : newPartitions) {\n-            Partition oldPartition = olapTable.replacePartition(newPartition);\n-            // save old tablets to be removed\n-            for (MaterializedIndex index : oldPartition.getMaterializedIndices(IndexExtState.ALL)) {\n-                index.getTablets().stream().forEach(t -> {\n-                    oldTabletIds.add(t.getId());\n-                });\n-            }\n-        }\n-\n-        if (isEntireTable) {\n-            // drop all temp partitions\n-            olapTable.dropAllTempPartitions();\n-        }\n-\n-        // remove the tablets in old partitions\n-        for (Long tabletId : oldTabletIds) {\n-            Catalog.getCurrentInvertedIndex().deleteTablet(tabletId);\n-        }\n-    }\n-\n-    public void replayTruncateTable(TruncateTableInfo info) {\n-        Database db = getDb(info.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(info.getTblId());\n-            truncateTableInternal(olapTable, info.getPartitions(), info.isEntireTable());\n-\n-            if (!Catalog.isCheckpointThread()) {\n-                // add tablet to inverted index\n-                TabletInvertedIndex invertedIndex = Catalog.getCurrentInvertedIndex();\n-                for (Partition partition : info.getPartitions()) {\n-                    long partitionId = partition.getId();\n-                    TStorageMedium medium = olapTable.getPartitionInfo().getDataProperty(\n-                            partitionId).getStorageMedium();\n-                    for (MaterializedIndex mIndex : partition.getMaterializedIndices(IndexExtState.ALL)) {\n-                        long indexId = mIndex.getId();\n-                        int schemaHash = olapTable.getSchemaHashByIndexId(indexId);\n-                        TabletMeta tabletMeta = new TabletMeta(db.getId(), olapTable.getId(),\n-                                partitionId, indexId, schemaHash, medium);\n-                        for (Tablet tablet : mIndex.getTablets()) {\n-                            long tabletId = tablet.getId();\n-                            invertedIndex.addTablet(tabletId, tabletMeta);\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                invertedIndex.addReplica(tabletId, replica);\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void createFunction(CreateFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.addFunction(stmt.getFunction());\n-    }\n-\n-    public void replayCreateFunction(Function function) {\n-        String dbName = function.getFunctionName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayAddFunction(function);\n-    }\n-\n-    public void dropFunction(DropFunctionStmt stmt) throws UserException {\n-        FunctionName name = stmt.getFunctionName();\n-        Database db = getDb(name.getDb());\n-        if (db == null) {\n-            ErrorReport.reportDdlException(ErrorCode.ERR_BAD_DB_ERROR, name.getDb());\n-        }\n-        db.dropFunction(stmt.getFunction());\n-    }\n-\n-    public void replayDropFunction(FunctionSearchDesc functionSearchDesc) {\n-        String dbName = functionSearchDesc.getName().getDb();\n-        Database db = getDb(dbName);\n-        if (db == null) {\n-            throw new Error(\"unknown database when replay log, db=\" + dbName);\n-        }\n-        db.replayDropFunction(functionSearchDesc);\n-    }\n-\n-    public void setConfig(AdminSetConfigStmt stmt) throws DdlException {\n-        Map<String, String> configs = stmt.getConfigs();\n-        Preconditions.checkState(configs.size() == 1);\n-\n-        for (Map.Entry<String, String> entry : configs.entrySet()) {\n-            ConfigBase.setMutableConfig(entry.getKey(), entry.getValue());\n-        }\n-    }\n-\n-    public void replayBackendTabletsInfo(BackendTabletsInfo backendTabletsInfo) {\n-        List<Pair<Long, Integer>> tabletsWithSchemaHash = backendTabletsInfo.getTabletSchemaHash();\n-        for (Pair<Long, Integer> tabletInfo : tabletsWithSchemaHash) {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletInfo.first,\n-                    backendTabletsInfo.getBackendId());\n-            if (replica == null) {\n-                LOG.warn(\"replica does not found when replay. tablet {}, backend {}\",\n-                        tabletInfo.first, backendTabletsInfo.getBackendId());\n-                continue;\n-            }\n-\n-            if (replica.getSchemaHash() != tabletInfo.second) {\n-                continue;\n-            }\n-\n-            replica.setBad(backendTabletsInfo.isBad());\n-        }\n-    }\n-\n-    // Convert table's distribution type from random to hash.\n-    // random distribution is no longer supported.\n-    public void convertDistributionType(Database db, OlapTable tbl) throws DdlException {\n-        db.writeLock();\n-        try {\n-            if (!tbl.convertRandomDistributionToHashDistribution()) {\n-                throw new DdlException(\"Table \" + tbl.getName() + \" is not random distributed\");\n-            }\n-            TableInfo tableInfo = TableInfo.createForModifyDistribution(db.getId(), tbl.getId());\n-            editLog.logModifyDistributionType(tableInfo);\n-            LOG.info(\"finished to modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayConvertDistributionType(TableInfo tableInfo) {\n-        Database db = getDb(tableInfo.getDbId());\n-        db.writeLock();\n-        try {\n-            OlapTable tbl = (OlapTable) db.getTable(tableInfo.getTableId());\n-            tbl.convertRandomDistributionToHashDistribution();\n-            LOG.info(\"replay modify distribution type of table: \" + tbl.getName());\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    /*\n-     * The entry of replacing partitions with temp partitions.\n-     */\n-    public void replaceTempPartition(Database db, String tableName, ReplacePartitionClause clause) throws DdlException {\n-        List<String> partitionNames = clause.getPartitionNames();\n-        List<String> tempPartitonNames = clause.getTempPartitionNames();\n-        boolean isStrictRange = clause.isStrictRange();\n-        boolean useTempPartitionName = clause.useTempPartitionName();\n-        db.writeLock();\n-        try {\n-            Table table = db.getTable(tableName);\n-            if (table == null) {\n-                ErrorReport.reportDdlException(ErrorCode.ERR_BAD_TABLE_ERROR, tableName);\n-            }\n-\n-            if (table.getType() != TableType.OLAP) {\n-                throw new DdlException(\"Table[\" + tableName + \"] is not OLAP table\");\n-            }\n-\n-            OlapTable olapTable = (OlapTable) table;\n-            // check partition exist\n-            for (String partName : partitionNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, false)) {\n-                    throw new DdlException(\"Partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-            for (String partName : tempPartitonNames) {\n-                if (!olapTable.checkPartitionNameExist(partName, true)) {\n-                    throw new DdlException(\"Temp partition[\" + partName + \"] does not exist\");\n-                }\n-            }\n-\n-            olapTable.replaceTempPartitions(partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-\n-            // write log\n-            ReplacePartitionOperationLog info = new ReplacePartitionOperationLog(db.getId(), olapTable.getId(),\n-                    partitionNames, tempPartitonNames, isStrictRange, useTempPartitionName);\n-            editLog.logReplaceTempPartition(info);\n-            LOG.info(\"finished to replace partitions {} with temp partitions {} from table: {}\",\n-                    clause.getPartitionNames(), clause.getTempPartitionNames(), tableName);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void replayReplaceTempPartition(ReplacePartitionOperationLog replaceTempPartitionLog) {\n-        Database db = getDb(replaceTempPartitionLog.getDbId());\n-        if (db == null) {\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            OlapTable olapTable = (OlapTable) db.getTable(replaceTempPartitionLog.getTblId());\n-            if (olapTable == null) {\n-                return;\n-            }\n-            olapTable.replaceTempPartitions(replaceTempPartitionLog.getPartitions(),\n-                    replaceTempPartitionLog.getTempPartitions(),\n-                    replaceTempPartitionLog.isStrictRange(),\n-                    replaceTempPartitionLog.useTempPartitionName());\n-        } catch (DdlException e) {\n-            LOG.warn(\"should not happen. {}\", e);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-\n-    public void installPlugin(InstallPluginStmt stmt) throws UserException, IOException {\n-        pluginMgr.installPlugin(stmt);\n-    }\n-\n-    public long savePlugins(DataOutputStream dos, long checksum) throws IOException {\n-        Catalog.getCurrentPluginMgr().write(dos);\n-        return checksum;\n-    }\n-\n-    public long loadPlugins(DataInputStream dis, long checksum) throws IOException {\n-        if (Catalog.getCurrentCatalogJournalVersion() >= FeMetaVersion.VERSION_78) {\n-            Catalog.getCurrentPluginMgr().readFields(dis);\n-        }\n-        LOG.info(\"finished replay plugins from image\");\n-        return checksum;\n-    }\n-\n-    public void replayInstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.replayLoadDynamicPlugin(pluginInfo);\n-        } catch (Exception e) {\n-            LOG.warn(\"replay install plugin failed.\", e);\n-        }\n-    }\n-\n-    public void uninstallPlugin(UninstallPluginStmt stmt) throws IOException, UserException {\n-        PluginInfo info = pluginMgr.uninstallPlugin(stmt.getPluginName());\n-        if (null != info) {\n-            editLog.logUninstallPlugin(info);\n-        }\n-        LOG.info(\"uninstall plugin = \" + stmt.getPluginName());\n-    }\n-\n-    public void replayUninstallPlugin(PluginInfo pluginInfo)  {\n-        try {\n-            pluginMgr.uninstallPlugin(pluginInfo.getName());\n-        } catch (Exception e) {\n-            LOG.warn(\"replay uninstall plugin failed.\", e);\n-        }\n-    }\n-\n-    // entry of checking tablets operation\n-    public void checkTablets(AdminCheckTabletsStmt stmt) {\n-        CheckType type = stmt.getType();\n-        switch (type) {\n-            case CONSISTENCY:\n-                consistencyChecker.addTabletsToCheck(stmt.getTabletIds());\n-                break;\n-            default:\n-                break;\n-        }\n-    }\n-\n-    // Set specified replica's status. If replica does not exist, just ignore it.\n-    public void setReplicaStatus(AdminSetReplicaStatusStmt stmt) {\n-        long tabletId = stmt.getTabletId();\n-        long backendId = stmt.getBackendId();\n-        ReplicaStatus status = stmt.getStatus();\n-        setReplicaStatusInternal(tabletId, backendId, status, false);\n-    }\n-\n-    public void replaySetReplicaStatus(SetReplicaStatusOperationLog log) {\n-        setReplicaStatusInternal(log.getTabletId(), log.getBackendId(), log.getReplicaStatus(), true);\n-    }\n-\n-    private void setReplicaStatusInternal(long tabletId, long backendId, ReplicaStatus status, boolean isReplay) {\n-        TabletMeta meta = tabletInvertedIndex.getTabletMeta(tabletId);\n-        if (meta == null) {\n-            LOG.info(\"tablet {} does not exist\", tabletId);\n-            return;\n-        }\n-        long dbId = meta.getDbId();\n-        Database db = getDb(dbId);\n-        if (db == null) {\n-            LOG.info(\"database {} of tablet {} does not exist\", dbId, tabletId);\n-            return;\n-        }\n-        db.writeLock();\n-        try {\n-            Replica replica = tabletInvertedIndex.getReplica(tabletId, backendId);\n-            if (replica == null) {\n-                LOG.info(\"replica of tablet {} does not exist\", tabletId);\n-                return;\n-            }\n-            if (status == ReplicaStatus.BAD || status == ReplicaStatus.OK) {\n-                if (replica.setBad(status == ReplicaStatus.BAD)) {\n-                    if (!isReplay) {\n-                        SetReplicaStatusOperationLog log = new SetReplicaStatusOperationLog(backendId, tabletId, status);\n-                        getEditLog().logSetReplicaStatus(log);\n-                    }\n-                    LOG.info(\"set replica {} of tablet {} on backend {} as {}. is replay: {}\",\n-                            replica.getId(), tabletId, backendId, status, isReplay);\n-                }\n-            }\n-        } finally {\n-            db.writeUnlock();\n-        }\n-    }\n-}\n-\n"}}, {"oid": "2a88bfba98c0af0015757af23efaebb08a72a068", "url": "https://github.com/apache/incubator-doris/commit/2a88bfba98c0af0015757af23efaebb08a72a068", "message": "Support check commited txns in Drop meta Stmt", "committedDate": "2020-07-27T06:45:45Z", "type": "commit"}, {"oid": "ae15642293ec866eb84ad39f3a9f2cef3d1cfedc", "url": "https://github.com/apache/incubator-doris/commit/ae15642293ec866eb84ad39f3a9f2cef3d1cfedc", "message": "Fix drop meta check", "committedDate": "2020-07-27T06:45:45Z", "type": "commit"}, {"oid": "a53df5f8f1cf99575f85937c3bc7381c49be9d63", "url": "https://github.com/apache/incubator-doris/commit/a53df5f8f1cf99575f85937c3bc7381c49be9d63", "message": "Add Dropp stmt for force meta drop", "committedDate": "2020-07-27T06:45:45Z", "type": "commit"}, {"oid": "9580912f416b94d74211a4fe68bf9ef1dfc643e2", "url": "https://github.com/apache/incubator-doris/commit/9580912f416b94d74211a4fe68bf9ef1dfc643e2", "message": "Support commited txns check before drop db, table or partition", "committedDate": "2020-07-27T06:45:46Z", "type": "commit"}, {"oid": "d57a8883f620eeb0b386ef5b3e5fc7c53241d038", "url": "https://github.com/apache/incubator-doris/commit/d57a8883f620eeb0b386ef5b3e5fc7c53241d038", "message": "fix by review", "committedDate": "2020-07-27T06:45:46Z", "type": "commit"}, {"oid": "3f9484a91ef3c899dd6fdc2a9db75e58db2c05b4", "url": "https://github.com/apache/incubator-doris/commit/3f9484a91ef3c899dd6fdc2a9db75e58db2c05b4", "message": "use drop force stmt to replace dropp stmt", "committedDate": "2020-07-27T06:45:46Z", "type": "commit"}, {"oid": "59779e894a10e7864596680d7cc1f39b5e71b5e2", "url": "https://github.com/apache/incubator-doris/commit/59779e894a10e7864596680d7cc1f39b5e71b5e2", "message": "make force drop operation do not recycle meta", "committedDate": "2020-07-27T06:57:32Z", "type": "commit"}, {"oid": "5ee5998c3ba5347267adc74f5af838c45b48d911", "url": "https://github.com/apache/incubator-doris/commit/5ee5998c3ba5347267adc74f5af838c45b48d911", "message": "fix", "committedDate": "2020-07-27T07:08:20Z", "type": "commit"}, {"oid": "70b89d59dcca659f63c4bb8316e2f7a44736a488", "url": "https://github.com/apache/incubator-doris/commit/70b89d59dcca659f63c4bb8316e2f7a44736a488", "message": "fix by review", "committedDate": "2020-07-27T07:15:23Z", "type": "commit"}, {"oid": "70b89d59dcca659f63c4bb8316e2f7a44736a488", "url": "https://github.com/apache/incubator-doris/commit/70b89d59dcca659f63c4bb8316e2f7a44736a488", "message": "fix by review", "committedDate": "2020-07-27T07:15:23Z", "type": "forcePushed"}, {"oid": "e130fa8b04aab869e083239f4efbd0df57e11e41", "url": "https://github.com/apache/incubator-doris/commit/e130fa8b04aab869e083239f4efbd0df57e11e41", "message": "revert add fe-core target file", "committedDate": "2020-07-27T07:28:32Z", "type": "commit"}, {"oid": "e130fa8b04aab869e083239f4efbd0df57e11e41", "url": "https://github.com/apache/incubator-doris/commit/e130fa8b04aab869e083239f4efbd0df57e11e41", "message": "revert add fe-core target file", "committedDate": "2020-07-27T07:28:32Z", "type": "forcePushed"}]}