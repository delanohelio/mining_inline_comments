{"pr_number": 4491, "pr_title": "[Spark load][Bug] Fix column terminator for spark load", "pr_createdAt": "2020-08-31T16:39:02Z", "pr_url": "https://github.com/apache/incubator-doris/pull/4491", "timeline": [{"oid": "e9799e3619fe9e484f20bb685ebe0b8936682ace", "url": "https://github.com/apache/incubator-doris/commit/e9799e3619fe9e484f20bb685ebe0b8936682ace", "message": "save", "committedDate": "2020-08-31T13:21:33Z", "type": "commit"}, {"oid": "bb769d02dc8bd944e1a5490a6cf55a2e6cb0db66", "url": "https://github.com/apache/incubator-doris/commit/bb769d02dc8bd944e1a5490a6cf55a2e6cb0db66", "message": "save code", "committedDate": "2020-08-31T15:09:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDcyNjU2MA==", "url": "https://github.com/apache/incubator-doris/pull/4491#discussion_r480726560", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        this.columnSeparator = Strings.isNullOrEmpty(columnSeparator)?'\\t': columnSeparator.charAt(0);\n          \n          \n            \n                        this.columnSeparator = Strings.isNullOrEmpty(columnSeparator) ? '\\t' : columnSeparator.charAt(0);", "author": "morningman", "createdAt": "2020-09-01T03:47:59Z", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/etl/EtlJobConfig.java", "diffHunk": "@@ -502,7 +504,7 @@ public EtlFileGroup(SourceType sourceType, List<String> filePaths, List<String>\n             this.filePaths = filePaths;\n             this.fileFieldNames = fileFieldNames;\n             this.columnsFromPath = columnsFromPath;\n-            this.columnSeparator = columnSeparator;\n+            this.columnSeparator = Strings.isNullOrEmpty(columnSeparator)?'\\t': columnSeparator.charAt(0);", "originalCommit": "bb769d02dc8bd944e1a5490a6cf55a2e6cb0db66", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "706364c95919a2850d5d04bccf03159fb09b10f1", "chunk": "diff --git a/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/etl/EtlJobConfig.java b/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/etl/EtlJobConfig.java\nindex 27f0a1e51..713592ca9 100644\n--- a/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/etl/EtlJobConfig.java\n+++ b/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/etl/EtlJobConfig.java\n\n@@ -504,13 +503,20 @@ public class EtlJobConfig implements Serializable {\n             this.filePaths = filePaths;\n             this.fileFieldNames = fileFieldNames;\n             this.columnsFromPath = columnsFromPath;\n-            this.columnSeparator = Strings.isNullOrEmpty(columnSeparator)?'\\t': columnSeparator.charAt(0);\n             this.lineDelimiter = lineDelimiter;\n             this.isNegative = isNegative;\n             this.fileFormat = fileFormat;\n             this.columnMappings = columnMappings;\n             this.where = where;\n             this.partitions = partitions;\n+\n+            // Handle some special characters\n+            char sep = Strings.isNullOrEmpty(columnSeparator) ? '\\t' : columnSeparator.charAt(0);\n+            if (\".$|()[]{}^?*+\\\\\".indexOf(sep) != -1) {\n+                this.columnSeparator = new String(new char[]{'\\\\', sep});\n+            } else {\n+                this.columnSeparator = Character.toString(sep);\n+            }\n         }\n \n         // for data from table\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDczMTA5OA==", "url": "https://github.com/apache/incubator-doris/pull/4491#discussion_r480731098", "bodyText": "Make it execute only once.", "author": "morningman", "createdAt": "2020-09-01T03:51:23Z", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -559,7 +558,14 @@ private void processRollupTree(RollupTreeNode rootNode,\n         JavaRDD<Row> rowRDD = sourceDataRdd.flatMap(\n                 record -> {\n                     scannedRowsAcc.add(1);\n-                    String[] attributes = record.split(fileGroup.columnSeparator);\n+                    char sep = fileGroup.columnSeparator;", "originalCommit": "bb769d02dc8bd944e1a5490a6cf55a2e6cb0db66", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "706364c95919a2850d5d04bccf03159fb09b10f1", "chunk": "diff --git a/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java b/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java\nindex 1cc25f455..46378b8fd 100644\n--- a/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java\n+++ b/fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java\n\n@@ -558,14 +558,7 @@ public final class SparkDpp implements java.io.Serializable {\n         JavaRDD<Row> rowRDD = sourceDataRdd.flatMap(\n                 record -> {\n                     scannedRowsAcc.add(1);\n-                    char sep = fileGroup.columnSeparator;\n-                    String regex = null;\n-                    if (\".$|()[]{}^?*+\\\\\".indexOf(sep) != -1) {\n-                        regex = new String(new char[]{'\\\\', sep});\n-                    } else {\n-                        regex = Character.toString(sep);\n-                    }\n-                    String[] attributes = record.split(regex);\n+                    String[] attributes = record.split(fileGroup.columnSeparator);\n                     List<Row> result = new ArrayList<>();\n                     boolean validRow = true;\n                     if (attributes.length != columnSize) {\n"}}, {"oid": "706364c95919a2850d5d04bccf03159fb09b10f1", "url": "https://github.com/apache/incubator-doris/commit/706364c95919a2850d5d04bccf03159fb09b10f1", "message": "execute once", "committedDate": "2020-09-01T06:27:38Z", "type": "commit"}, {"oid": "027fe9af21406dd3d2029ffb0b1552933efcf87a", "url": "https://github.com/apache/incubator-doris/commit/027fe9af21406dd3d2029ffb0b1552933efcf87a", "message": "save code", "committedDate": "2020-09-01T06:42:52Z", "type": "commit"}]}