{"pr_number": 2824, "pr_title": "[Load] Fix bug of wrong file group aggregation when handling broker load job", "pr_createdAt": "2020-01-28T09:45:21Z", "pr_url": "https://github.com/apache/incubator-doris/pull/2824", "timeline": [{"oid": "ed1b09b41d88a8c134fdc7b2c3c4f7e029dea229", "url": "https://github.com/apache/incubator-doris/commit/ed1b09b41d88a8c134fdc7b2c3c4f7e029dea229", "message": "first", "committedDate": "2020-01-28T09:08:08Z", "type": "commit"}, {"oid": "f681655711d379e2c4e07b97fae41ca00e97691b", "url": "https://github.com/apache/incubator-doris/commit/f681655711d379e2c4e07b97fae41ca00e97691b", "message": "fix partition empty bug", "committedDate": "2020-01-29T02:21:27Z", "type": "commit"}, {"oid": "c22c30813cbfadf5325d96fd044641eee5e558f8", "url": "https://github.com/apache/incubator-doris/commit/c22c30813cbfadf5325d96fd044641eee5e558f8", "message": "remove unused code", "committedDate": "2020-01-29T02:34:35Z", "type": "commit"}, {"oid": "35cf28d2e69cd885c582c857a750763834bdca63", "url": "https://github.com/apache/incubator-doris/commit/35cf28d2e69cd885c582c857a750763834bdca63", "message": "fix overlapping problem", "committedDate": "2020-01-29T09:49:13Z", "type": "commit"}, {"oid": "1529831a0e84f63137a652eefc9d51d5a3fda2d7", "url": "https://github.com/apache/incubator-doris/commit/1529831a0e84f63137a652eefc9d51d5a3fda2d7", "message": "add missing ut file", "committedDate": "2020-01-29T10:43:59Z", "type": "commit"}, {"oid": "e8b786d2e4bf52d3c20f274670792579f7f5d70a", "url": "https://github.com/apache/incubator-doris/commit/e8b786d2e4bf52d3c20f274670792579f7f5d70a", "message": "fix bug", "committedDate": "2020-01-29T12:40:13Z", "type": "commit"}, {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "url": "https://github.com/apache/incubator-doris/commit/063543a16e9d39e94f0dba6361c1ef1f7ad91093", "message": "fix ut", "committedDate": "2020-01-29T12:43:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxMzQ4MA==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373913480", "bodyText": "Private contructor could be mocked also.", "author": "EmmyMiao87", "createdAt": "2020-02-03T03:56:27Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java", "diffHunk": "@@ -77,8 +77,8 @@\n     // filter the data which has been conformed\n     private Expr whereExpr;\n \n-    // Used for recovery from edit log\n-    private BrokerFileGroup() {", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk3ODY3NA==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373978674", "bodyText": "OK", "author": "morningman", "createdAt": "2020-02-03T08:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxMzQ4MA=="}], "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java\nindex fdf04e178..48015a917 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java\n\n@@ -78,7 +78,7 @@ public class BrokerFileGroup implements Writable {\n     private Expr whereExpr;\n \n     // for unit test and edit log persistence\n-    public BrokerFileGroup() {\n+    private BrokerFileGroup() {\n     }\n \n     // Used for broker table, no need to parse\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxNTU1Mg==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373915552", "bodyText": "The max filter ratio belongs to the whole broker load instead of single data desc . So it does not change the \"max_filter_ratio\" when the system aggregate data desc automatically. Right?", "author": "EmmyMiao87", "createdAt": "2020-02-03T04:10:36Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk4Nzk2Nw==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373987967", "bodyText": "No, it's not. for example\nAssume that file1 should be loaded to p1 and p2,\nand file2 should be loaded to p2 and p3.\nboth file1 and file2 contain data half in p1 and half in p3. so if we set max_filter_ratio to 0.5, this load\nshould succeed.\nIn this case, if we do the aggregation, the result should be:\nfile1 to p1\nfile2 to p3\nfile1+file2 to p2\nand if max_filter_ratio still be 0.5, the load will be failed.\nassume each file has 10 lines:\n[file1 to p1] results in, eg, 5 lines ok, 5 lines error.\n[file2 to p3] results in 5 lines ok, 5 lines error.\n[file1+file2 to p2] reuslts in 0 line ok, 20 lines error.\nso the total error line is 30, and total line is 40, 30/40 > 0.5", "author": "morningman", "createdAt": "2020-02-03T09:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxNTU1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\nindex ddb4c4857..12bafa5c3 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n\n@@ -173,10 +173,6 @@ public class BrokerFileGroupAggInfo implements Writable {\n         }\n     }\n \n-    public BrokerFileGroupAggInfo() {\n-\n-    }\n-\n     public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n         FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n         List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk1NjI0Nw==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373956247", "bodyText": "Maybe public class is better.", "author": "EmmyMiao87", "createdAt": "2020-02-03T07:41:57Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\nindex ddb4c4857..12bafa5c3 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n\n@@ -173,10 +173,6 @@ public class BrokerFileGroupAggInfo implements Writable {\n         }\n     }\n \n-    public BrokerFileGroupAggInfo() {\n-\n-    }\n-\n     public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n         FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n         List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk1OTE0Mg==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373959142", "bodyText": "It can be omitted.", "author": "EmmyMiao87", "createdAt": "2020-02-03T07:51:50Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions\n+    public static class FileGroupAggKey {\n+        private long tableId;\n+        private Set<Long> partitionIds; // empty means partition is not specified\n+\n+        public FileGroupAggKey(long tableId, List<Long> partitionIds) {\n+            this.tableId = tableId;\n+            if (partitionIds != null) {\n+                this.partitionIds = Sets.newHashSet(partitionIds);\n+            } else {\n+                this.partitionIds = Sets.newHashSet();\n+            }\n+        }\n+\n+        public long getTableId() {\n+            return tableId;\n+        }\n+\n+        public Set<Long> getPartitionIds() {\n+            return partitionIds;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (this == obj) {\n+                return true;\n+            }\n+            if (!(obj instanceof FileGroupAggKey)) {\n+                return false;\n+            }\n+\n+            FileGroupAggKey other = (FileGroupAggKey) obj;\n+            return other.tableId == this.tableId && other.partitionIds.equals(this.partitionIds);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(tableId, partitionIds);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"[\").append(tableId).append(\": \").append(partitionIds).append(\"]\");\n+            return sb.toString();\n+        }\n+    }\n+\n+    public BrokerFileGroupAggInfo() {", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NDA2OQ==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373994069", "bodyText": "OK", "author": "morningman", "createdAt": "2020-02-03T09:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk1OTE0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\nindex ddb4c4857..12bafa5c3 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n\n@@ -173,10 +173,6 @@ public class BrokerFileGroupAggInfo implements Writable {\n         }\n     }\n \n-    public BrokerFileGroupAggInfo() {\n-\n-    }\n-\n     public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n         FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n         List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NDcyMg==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373964722", "bodyText": "This class has been consistent in the old version. Maybe you should keep it .", "author": "EmmyMiao87", "createdAt": "2020-02-03T08:09:42Z", "path": "fe/src/main/java/org/apache/doris/load/PullLoadSourceInfo.java", "diffHunk": "@@ -1,119 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NDQ4OQ==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373994489", "bodyText": "It ok, I checked it, it does no use at all now. and the old load job are gone before Doris version 0.11.", "author": "morningman", "createdAt": "2020-02-03T09:24:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NDcyMg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NjQ3OA==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373966478", "bodyText": "Maybe in is a PullLoadSourceInfo??", "author": "EmmyMiao87", "createdAt": "2020-02-03T08:15:33Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions\n+    public static class FileGroupAggKey {\n+        private long tableId;\n+        private Set<Long> partitionIds; // empty means partition is not specified\n+\n+        public FileGroupAggKey(long tableId, List<Long> partitionIds) {\n+            this.tableId = tableId;\n+            if (partitionIds != null) {\n+                this.partitionIds = Sets.newHashSet(partitionIds);\n+            } else {\n+                this.partitionIds = Sets.newHashSet();\n+            }\n+        }\n+\n+        public long getTableId() {\n+            return tableId;\n+        }\n+\n+        public Set<Long> getPartitionIds() {\n+            return partitionIds;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (this == obj) {\n+                return true;\n+            }\n+            if (!(obj instanceof FileGroupAggKey)) {\n+                return false;\n+            }\n+\n+            FileGroupAggKey other = (FileGroupAggKey) obj;\n+            return other.tableId == this.tableId && other.partitionIds.equals(this.partitionIds);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(tableId, partitionIds);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"[\").append(tableId).append(\": \").append(partitionIds).append(\"]\");\n+            return sb.toString();\n+        }\n+    }\n+\n+    public BrokerFileGroupAggInfo() {\n+\n+    }\n+\n+    public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n+        FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n+        List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n+        if (fileGroupList == null) {\n+            // check if there are overlapping partitions of same table\n+            if (tableIdToPartitioIds.containsKey(fileGroup.getTableId()) \n+                    && tableIdToPartitioIds.get(fileGroup.getTableId()).stream().anyMatch(id -> fileGroup.getPartitionIds().contains(id))) {\n+                throw new DdlException(\"There are overlapping partitions of same table in data descrition of load job stmt\");\n+            }\n+            \n+            fileGroupList = Lists.newArrayList();\n+            aggKeyToFileGroups.put(fileGroupAggKey, fileGroupList);\n+        }\n+        // exist, aggregate them\n+        fileGroupList.add(fileGroup);\n+\n+        // update tableIdToPartitioIds\n+        Set<Long> partitionIds = tableIdToPartitioIds.get(fileGroup.getTableId());\n+        if (partitionIds == null) {\n+            partitionIds = Sets.newHashSet();\n+            tableIdToPartitioIds.put(fileGroup.getTableId(), partitionIds);\n+        }\n+        if (fileGroup.getPartitionIds() != null) {\n+            partitionIds.addAll(fileGroup.getPartitionIds());\n+        }\n+    }\n+\n+    public Set<Long> getAllTableIds() {\n+        return aggKeyToFileGroups.keySet().stream().map(k -> k.tableId).collect(Collectors.toSet());\n+    }\n+\n+    public Map<FileGroupAggKey, List<BrokerFileGroup>> getAggKeyToFileGroups() {\n+        return aggKeyToFileGroups;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(aggKeyToFileGroups);\n+        return sb.toString();\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+        // The pull load source info doesn't need to be persisted.\n+        // It will be recreated by origin stmt in prepare of load job.\n+        // write 0 just for compatibility\n+        out.writeInt(0);\n+    }\n+\n+    public void readFields(DataInput in) throws IOException {\n+        in.readInt();", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk5NDg4OQ==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373994889", "bodyText": "It won't, we already solve this in version 0.11", "author": "morningman", "createdAt": "2020-02-03T09:25:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NjQ3OA=="}], "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\nindex ddb4c4857..12bafa5c3 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n\n@@ -173,10 +173,6 @@ public class BrokerFileGroupAggInfo implements Writable {\n         }\n     }\n \n-    public BrokerFileGroupAggInfo() {\n-\n-    }\n-\n     public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n         FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n         List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NzMzNA==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373967334", "bodyText": "Maybe Map<tableId, List> is useful.", "author": "EmmyMiao87", "createdAt": "2020-02-03T08:18:13Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();", "originalCommit": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "814537a575b701e7fe1d306b26dc40ac00c1a042", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\nindex ddb4c4857..12bafa5c3 100644\n--- a/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n+++ b/fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java\n\n@@ -173,10 +173,6 @@ public class BrokerFileGroupAggInfo implements Writable {\n         }\n     }\n \n-    public BrokerFileGroupAggInfo() {\n-\n-    }\n-\n     public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n         FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n         List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n"}}, {"oid": "814537a575b701e7fe1d306b26dc40ac00c1a042", "url": "https://github.com/apache/incubator-doris/commit/814537a575b701e7fe1d306b26dc40ac00c1a042", "message": "fix by review", "committedDate": "2020-02-03T09:26:58Z", "type": "commit"}, {"oid": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4", "url": "https://github.com/apache/incubator-doris/commit/f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4", "message": "add compatible code", "committedDate": "2020-02-03T11:20:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDA0OTgxMQ==", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r374049811", "bodyText": "A warning log is better ~", "author": "EmmyMiao87", "createdAt": "2020-02-03T11:23:37Z", "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -224,7 +224,15 @@ public void write(DataOutput out) throws IOException {\n     }\n \n     public void readFields(DataInput in) throws IOException {\n-        in.readInt();\n+        int mapSize = in.readInt();\n+        // just for compatibility, the following read objects are useless", "originalCommit": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}