{"pr_number": 3955, "pr_title": "Support checking database used data quota when data load job begin a new txn", "pr_createdAt": "2020-06-27T07:55:29Z", "pr_url": "https://github.com/apache/incubator-doris/pull/3955", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyNTA5Ng==", "url": "https://github.com/apache/incubator-doris/pull/3955#discussion_r449725096", "bodyText": "I think this can be set to true by default.", "author": "morningman", "createdAt": "2020-07-04T00:53:58Z", "path": "fe/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -873,6 +873,18 @@\n      */\n     @ConfField(mutable = true, masterOnly = true)\n     public static boolean disable_load_job = false;\n+\n+    /*\n+     * if this is set to true, all load job will check db data quota when call begin txn api\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static boolean enable_check_data_quota_on_load = false;", "originalCommit": "21a49bb293503a3aeb83b0f6597ce9b5abccb766", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc1NTY2OA==", "url": "https://github.com/apache/incubator-doris/pull/3955#discussion_r449755668", "bodyText": "done", "author": "caiconghui", "createdAt": "2020-07-04T09:13:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyNTA5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "923b16566ea129734e44219f6ba3d78a53746987", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/common/Config.java b/fe/src/main/java/org/apache/doris/common/Config.java\ndeleted file mode 100644\nindex 6afb16d2f..000000000\n--- a/fe/src/main/java/org/apache/doris/common/Config.java\n+++ /dev/null\n\n@@ -1,1110 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.common;\n-\n-import org.apache.doris.PaloFe;\n-\n-public class Config extends ConfigBase {\n-    \n-    /*\n-     * The max size of one sys log and audit log\n-     */\n-    @ConfField public static int log_roll_size_mb = 1024; // 1 GB\n-\n-    /*\n-     * sys_log_dir:\n-     *      This specifies FE log dir. FE will produces 2 log files:\n-     *      fe.log:      all logs of FE process.\n-     *      fe.warn.log  all WARNING and ERROR log of FE process.\n-     *      \n-     * sys_log_level:\n-     *      INFO, WARNING, ERROR, FATAL\n-     *      \n-     * sys_log_roll_num:\n-     *      Maximal FE log files to be kept within an sys_log_roll_interval.\n-     *      default is 10, which means there will be at most 10 log files in a day\n-     *      \n-     * sys_log_verbose_modules:\n-     *      Verbose modules. VERBOSE level is implemented by log4j DEBUG level.\n-     *      eg:\n-     *          sys_log_verbose_modules = org.apache.doris.catalog\n-     *      This will only print debug log of files in package org.apache.doris.catalog and all its sub packages.\n-     *      \n-     * sys_log_roll_interval:\n-     *      DAY:  log suffix is yyyyMMdd\n-     *      HOUR: log suffix is yyyyMMddHH\n-     *      \n-     * sys_log_delete_age:\n-     *      default is 7 days, if log's last modify time is 7 days ago, it will be deleted.\n-     *      support format:\n-     *          7d      7 days\n-     *          10h     10 hours\n-     *          60m     60 mins\n-     *          120s    120 seconds\n-     */\n-    @ConfField\n-    public static String sys_log_dir = PaloFe.DORIS_HOME_DIR + \"/log\";\n-    @ConfField public static String sys_log_level = \"INFO\"; \n-    @ConfField public static int sys_log_roll_num = 10;\n-    @ConfField public static String[] sys_log_verbose_modules = {};\n-    @ConfField public static String sys_log_roll_interval = \"DAY\";\n-    @ConfField public static String sys_log_delete_age = \"7d\";\n-    @Deprecated\n-    @ConfField public static String sys_log_roll_mode = \"SIZE-MB-1024\";\n-\n-    /*\n-     * audit_log_dir:\n-     *      This specifies FE audit log dir.\n-     *      Audit log fe.audit.log contains all requests with related infos such as user, host, cost, status, etc.\n-     * \n-     * audit_log_roll_num:\n-     *      Maximal FE audit log files to be kept within an audit_log_roll_interval.\n-     *      \n-     * audit_log_modules:\n-     *       Slow query contains all queries which cost exceed *qe_slow_log_ms*\n-     *       \n-     * qe_slow_log_ms:\n-     *      If the response time of a query exceed this threshold, it will be recored in audit log as slow_query.\n-     *      \n-     * audit_log_roll_interval:\n-     *      DAY:  log suffix is yyyyMMdd\n-     *      HOUR: log suffix is yyyyMMddHH\n-     *      \n-     * audit_log_delete_age:\n-     *      default is 30 days, if log's last modify time is 30 days ago, it will be deleted.\n-     *      support format:\n-     *          7d      7 days\n-     *          10h     10 hours\n-     *          60m     60 mins\n-     *          120s    120 seconds\n-     */\n-    @ConfField public static String audit_log_dir = PaloFe.DORIS_HOME_DIR + \"/log\";\n-    @ConfField public static int audit_log_roll_num = 90;\n-    @ConfField public static String[] audit_log_modules = {\"slow_query\", \"query\"};\n-    @ConfField(mutable = true) public static long qe_slow_log_ms = 5000;\n-    @ConfField public static String audit_log_roll_interval = \"DAY\";\n-    @ConfField public static String audit_log_delete_age = \"30d\";\n-    @Deprecated\n-    @ConfField public static String audit_log_roll_mode = \"TIME-DAY\";\n-\n-    /*\n-     * plugin_dir:\n-     *      plugin install directory\n-     */\n-    @ConfField public static String plugin_dir = System.getenv(\"DORIS_HOME\") + \"/plugins\";\n-\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean plugin_enable = false;\n-\n-    /*\n-     * Labels of finished or cancelled load jobs will be removed after *label_keep_max_second*\n-     * The removed labels can be reused.\n-     * Set a short time will lower the FE memory usage.\n-     * (Because all load jobs' info is kept in memory before being removed)\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int label_keep_max_second = 3 * 24 * 3600; // 3 days\n-    /*\n-     * The max keep time of some kind of jobs.\n-     * like schema change job and rollup job.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int history_job_keep_max_second = 7 * 24 * 3600; // 7 days\n-    /*\n-     * Load label cleaner will run every *label_clean_interval_second* to clean the outdated jobs.\n-     */\n-    @ConfField public static int label_clean_interval_second = 4 * 3600; // 4 hours\n-    /*\n-     * the transaction will be cleaned after transaction_clean_interval_second seconds if the transaction is visible or aborted\n-     * we should make this interval as short as possible and each clean cycle as soon as possible\n-     */\n-    @ConfField public static int transaction_clean_interval_second = 30;\n-\n-    // Configurations for meta data durability\n-    /*\n-     * Doris meta data will be saved here.\n-     * The storage of this dir is highly recommended as to be:\n-     * 1. High write performance (SSD)\n-     * 2. Safe (RAID)\n-     */\n-    @ConfField public static String meta_dir = PaloFe.DORIS_HOME_DIR + \"/doris-meta\";\n-    \n-    /*\n-     * temp dir is used to save intermediate results of some process, such as backup and restore process.\n-     * file in this dir will be cleaned after these process is finished.\n-     */\n-    @ConfField public static String tmp_dir = PaloFe.DORIS_HOME_DIR + \"/temp_dir\";\n-    \n-    /*\n-     * Edit log type.\n-     * BDB: write log to bdbje\n-     * LOCAL: deprecated.\n-     */\n-    @ConfField\n-    public static String edit_log_type = \"BDB\";\n-    /*\n-     * bdbje port\n-     */\n-    @ConfField\n-    public static int edit_log_port = 9010;\n-    /*\n-     * Master FE will save image every *edit_log_roll_num* meta journals.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int edit_log_roll_num = 50000;\n-    /*\n-     * Non-master FE will stop offering service\n-     * if meta data delay gap exceeds *meta_delay_toleration_second*\n-     */\n-    @ConfField public static int meta_delay_toleration_second = 300;    // 5 min\n-    /*\n-     * Master FE sync policy of bdbje.\n-     * If you only deploy one Follower FE, set this to 'SYNC'. If you deploy more than 3 Follower FE,\n-     * you can set this and the following 'replica_sync_policy' to WRITE_NO_SYNC.\n-     * more info, see: http://docs.oracle.com/cd/E17277_02/html/java/com/sleepycat/je/Durability.SyncPolicy.html\n-     */\n-    @ConfField public static String master_sync_policy = \"SYNC\"; // SYNC, NO_SYNC, WRITE_NO_SYNC\n-    /*\n-     * Follower FE sync policy of bdbje.\n-     */\n-    @ConfField public static String replica_sync_policy = \"SYNC\"; // SYNC, NO_SYNC, WRITE_NO_SYNC\n-    /*\n-     * Replica ack policy of bdbje.\n-     * more info, see: http://docs.oracle.com/cd/E17277_02/html/java/com/sleepycat/je/Durability.ReplicaAckPolicy.html\n-     */\n-    @ConfField public static String replica_ack_policy = \"SIMPLE_MAJORITY\"; // ALL, NONE, SIMPLE_MAJORITY\n-    \n-    /*\n-     * The heartbeat timeout of bdbje between master and follower.\n-     * the default is 30 seconds, which is same as default value in bdbje.\n-     * If the network is experiencing transient problems, of some unexpected long java GC annoying you,\n-     * you can try to increase this value to decrease the chances of false timeouts\n-     */\n-    @ConfField public static int bdbje_heartbeat_timeout_second = 30;\n-\n-    /*\n-     * The lock timeout of bdbje operation\n-     * If there are many LockTimeoutException in FE WARN log, you can try to increase this value\n-     */\n-    @ConfField\n-    public static int bdbje_lock_timeout_second = 1;\n-\n-    /*\n-     * num of thread to handle heartbeat events in heartbeat_mgr.\n-     */\n-    @ConfField(masterOnly = true)\n-    public static int heartbeat_mgr_threads_num = 8;\n-\n-     /*\n-     * blocking queue size to store heartbeat task in heartbeat_mgr.\n-     */\n-    @ConfField(masterOnly = true)\n-    public static int heartbeat_mgr_blocking_queue_size = 1024;\n-\n-    /*\n-    * max num of thread to handle agent task in agent task thread-pool.\n-    */\n-    @ConfField(masterOnly = true)\n-    public static int max_agent_task_threads_num = 4096;\n-\n-    /*\n-     * the max txn number which bdbje can rollback when trying to rejoin the group\n-     */\n-    @ConfField public static int txn_rollback_limit = 100;\n-\n-    /*\n-     * Specified an IP for frontend, instead of the ip get by *InetAddress.getByName*.\n-     * This can be used when *InetAddress.getByName* get an unexpected IP address.\n-     * Default is \"0.0.0.0\", which means not set.\n-     * CAN NOT set this as a hostname, only IP.\n-     */\n-    @ConfField public static String frontend_address = \"0.0.0.0\";\n-\n-    /*\n-     * Declare a selection strategy for those servers have many ips.\n-     * Note that there should at most one ip match this list.\n-     * this is a list in semicolon-delimited format, in CIDR notation, e.g. 10.10.10.0/24\n-     * If no ip match this rule, will choose one randomly.\n-     */\n-    @ConfField public static String priority_networks = \"\";\n-\n-    /*\n-     * If true, FE will reset bdbje replication group(that is, to remove all electable nodes info)\n-     * and is supposed to start as Master.\n-     * If all the electable nodes can not start, we can copy the meta data\n-     * to another node and set this config to true to try to restart the FE.\n-     */\n-    @ConfField public static String metadata_failure_recovery = \"false\";\n-\n-    /*\n-     * If true, non-master FE will ignore the meta data delay gap between Master FE and its self,\n-     * even if the metadata delay gap exceeds *meta_delay_toleration_second*.\n-     * Non-master FE will still offer read service.\n-     *\n-     * This is helpful when you try to stop the Master FE for a relatively long time for some reason,\n-     * but still wish the non-master FE can offer read service.\n-     */\n-    @ConfField(mutable = true)\n-    public static boolean ignore_meta_check = false;\n-\n-    /*\n-     * Set the maximum acceptable clock skew between non-master FE to Master FE host.\n-     * This value is checked whenever a non-master FE establishes a connection to master FE via BDBJE.\n-     * The connection is abandoned if the clock skew is larger than this value.\n-     */\n-    @ConfField public static long max_bdbje_clock_delta_ms = 5000; // 5s\n-\n-    /*\n-     * Fe http port\n-     * Currently, all FEs' http port must be same.\n-     */\n-    @ConfField public static int http_port = 8030;\n-\n-    /*\n-     * The backlog_num for netty http server\n-     * When you enlarge this backlog_num, you should ensure it's value larger than\n-     * the linux /proc/sys/net/core/somaxconn config\n-     */\n-    @ConfField public static int http_backlog_num = 1024;\n-\n-    /*\n-     * The connection timeout and socket timeout config for thrift server\n-     * The value for thrift_client_timeout_ms is set to be larger than zero to prevent\n-     * some hang up problems in java.net.SocketInputStream.socketRead0\n-     */\n-    @ConfField public static int thrift_client_timeout_ms = 30000;\n-\n-    /*\n-     * The backlog_num for thrift server\n-     * When you enlarge this backlog_num, you should ensure it's value larger than\n-     * the linux /proc/sys/net/core/somaxconn config\n-     */\n-    @ConfField public static int thrift_backlog_num = 1024;\n-\n-    /*\n-     * FE thrift server port\n-     */\n-    @ConfField public static int rpc_port = 9020;\n-    /*\n-     * FE mysql server port\n-     */\n-    @ConfField public static int query_port = 9030;\n-\n-    /*\n-    * mysql service nio option.\n-     */\n-    @ConfField public static boolean mysql_service_nio_enabled = false;\n-\n-    /*\n-     * num of thread to handle io events in mysql.\n-     */\n-    @ConfField public static int mysql_service_io_threads_num = 4;\n-\n-    /*\n-     * max num of thread to handle task in mysql.\n-     */\n-    @ConfField public static int max_mysql_service_task_threads_num = 4096;\n-\n-    /*\n-     * Cluster name will be shown as the title of web page\n-     */\n-    @ConfField public static String cluster_name = \"Baidu Palo\";\n-    /*\n-     * node(FE or BE) will be considered belonging to the same Palo cluster if they have same cluster id.\n-     * Cluster id is usually a random integer generated when master FE start at first time.\n-     * You can also sepecify one.\n-     */\n-    @ConfField public static int cluster_id = -1;\n-    /*\n-     * Cluster token used for internal authentication.\n-     */\n-    @ConfField public static String auth_token = \"\";\n-\n-    // Configurations for load, clone, create table, alter table etc. We will rarely change them\n-    /*\n-     * Maximal waiting time for creating a single replica.\n-     * eg.\n-     *      if you create a table with #m tablets and #n replicas for each tablet,\n-     *      the create table request will run at most (m * n * tablet_create_timeout_second) before timeout.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int tablet_create_timeout_second = 1;\n-    /*\n-     * In order not to wait too long for create table(index), set a max timeout.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_create_table_timeout_second = 60;\n-    \n-    /*\n-     * Maximal waiting time for all publish version tasks of one transaction to be finished\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int publish_version_timeout_second = 30; // 30 seconds\n-    \n-    /*\n-     * minimal intervals between two publish version action\n-     */\n-    @ConfField public static int publish_version_interval_ms = 10;\n-\n-\n-    /*\n-     * The thrift server max worker threads\n-     */\n-    @ConfField public static int thrift_server_max_worker_threads = 4096;\n-\n-    /*\n-     * Maximal wait seconds for straggler node in load\n-     * eg.\n-     *      there are 3 replicas A, B, C\n-     *      load is already quorum finished(A,B) at t1 and C is not finished\n-     *      if (current_time - t1) > 300s, then palo will treat C as a failure node\n-     *      will call transaction manager to commit the transaction and tell transaction manager \n-     *      that C is failed\n-     * \n-     * This is also used when waiting for publish tasks\n-     * \n-     * TODO this parameter is the default value for all job and the DBA could specify it for separate job\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int load_straggler_wait_second = 300;\n-    \n-    /*\n-     * Maximal memory layout length of a row. default is 100 KB.\n-     * In BE, the maximal size of a RowBlock is 100MB(Configure as max_unpacked_row_block_size in be.conf).\n-     * And each RowBlock contains 1024 rows. So the maximal size of a row is approximately 100 KB.\n-     * \n-     * eg.\n-     *      schema: k1(int), v1(decimal), v2(varchar(2000))\n-     *      then the memory layout length of a row is: 8(int) + 40(decimal) + 2000(varchar) = 2048 (Bytes)\n-     *      \n-     * See memory layout length of all types, run 'help create table' in mysql-client.\n-     * \n-     * If you want to increase this number to support more columns in a row, you also need to increase the \n-     * max_unpacked_row_block_size in be.conf. But the performance impact is unknown.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_layout_length_per_row = 100000; // 100k\n-\n-    /*\n-     * The load scheduler running interval.\n-     * A load job will transfer its state from PENDING to LOADING to FINISHED.\n-     * The load scheduler will transfer load job from PENDING to LOADING\n-     *      while the txn callback will transfer load job from LOADING to FINISHED.\n-     * So a load job will cost at most one interval to finish when the concurrency has not reached the upper limit.\n-     */\n-    @ConfField public static int load_checker_interval_second = 5;\n-\n-    /*\n-     * Concurrency of HIGH priority pending load jobs.\n-     * Load job priority is defined as HIGH or NORMAL.\n-     * All mini batch load jobs are HIGH priority, other types of load jobs are NORMAL priority.\n-     * Priority is set to avoid that a slow load job occupies a thread for a long time.\n-     * This is just a internal optimized scheduling policy.\n-     * Currently, you can not specified the job priority manually,\n-     * and do not change this if you know what you are doing.\n-     */\n-    @ConfField public static int load_pending_thread_num_high_priority = 3;\n-    /*\n-     * Concurrency of NORMAL priority pending load jobs.\n-     * Do not change this if you know what you are doing.\n-     */\n-    @ConfField public static int load_pending_thread_num_normal_priority = 10;\n-    /*\n-     * Concurrency of HIGH priority etl load jobs.\n-     * Do not change this if you know what you are doing.\n-     */\n-    @ConfField public static int load_etl_thread_num_high_priority = 3;\n-    /*\n-     * Concurrency of NORMAL priority etl load jobs.\n-     * Do not change this if you know what you are doing.\n-     */\n-    @ConfField public static int load_etl_thread_num_normal_priority = 10;\n-    /*\n-     * Concurrency of delete jobs.\n-     */\n-    @ConfField public static int delete_thread_num = 10;\n-    /*\n-     * Not available.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int load_input_size_limit_gb = 0; // GB, 0 is no limit\n-    /*\n-     * Not available.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int load_running_job_num_limit = 0; // 0 is no limit\n-    /*\n-     * Default broker load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int broker_load_default_timeout_second = 14400; // 4 hour\n-\n-    /*\n-     * Default non-streaming mini load timeout\n-     */\n-    @Deprecated\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int mini_load_default_timeout_second = 3600; // 1 hour\n-    \n-    /*\n-     * Default insert load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int insert_load_default_timeout_second = 3600; // 1 hour\n-    \n-    /*\n-     * Default stream load and streaming mini load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int stream_load_default_timeout_second = 600; // 600s\n-\n-    /*\n-     * Max load timeout applicable to all type of load except for stream load\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_load_timeout_second = 259200; // 3days\n-\n-    /*\n-     * Max stream load and streaming mini load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_stream_load_timeout_second = 259200; // 3days\n-\n-    /*\n-    * Min stream load timeout applicable to all type of load\n-    */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int min_load_timeout_second = 1; // 1s\n-\n-    /*\n-     * Default hadoop load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int hadoop_load_default_timeout_second = 86400 * 3; // 3 day\n-\n-    /*\n-     * Default spark load timeout\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int spark_load_default_timeout_second = 86400; // 1 day\n-\n-    /*\n-     * Default number of waiting jobs for routine load and version 2 of load\n-     * This is a desired number.\n-     * In some situation, such as switch the master, the current number is maybe more then desired_max_waiting_jobs\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int desired_max_waiting_jobs = 100;\n-\n-\n-    /*\n-     * maximun concurrent running txn num including prepare, commit txns under a single db\n-     * txn manager will reject coming txns\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_running_txn_num_per_db = 100;\n-\n-    /*\n-     * The load task executor pool size. This pool size limits the max running load tasks.\n-     * Currently, it only limits the load task of broker load, pending and loading phases.\n-     * It should be less than 'max_running_txn_num_per_db'\n-     */\n-    @ConfField(mutable = false, masterOnly = true)\n-    public static int async_load_task_pool_size = 10;\n-\n-    /*\n-     * Same meaning as *tablet_create_timeout_second*, but used when delete a tablet.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int tablet_delete_timeout_second = 2;\n-    /*\n-     * Clone checker's running interval.\n-     */\n-    @ConfField public static int clone_checker_interval_second = 300;\n-    /*\n-     * Default timeout of a single clone job. Set long enough to fit your replica size.\n-     * The larger the replica data size is, the more time is will cost to finish clone.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int clone_job_timeout_second = 7200; // 2h\n-    /*\n-     * Concurrency of LOW priority clone jobs.\n-     * Concurrency of High priority clone jobs is currently unlimit.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int clone_max_job_num = 100;\n-    /*\n-     * LOW priority clone job's delay trigger time.\n-     * A clone job contains a tablet which need to be cloned(recovery or migration).\n-     * If the priority is LOW, it will be delayed *clone_low_priority_delay_second*\n-     * after the job creation and then be executed.\n-     * This is to avoid a large number of clone jobs running at same time only because a host is down for a short time.\n-     *\n-     * NOTICE that this config(and *clone_normal_priority_delay_second* as well)\n-     * will not work if it's smaller then *clone_checker_interval_second*\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int clone_low_priority_delay_second = 600;\n-    /*\n-     * NORMAL priority clone job's delay trigger time.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int clone_normal_priority_delay_second = 300;\n-    /*\n-     * HIGH priority clone job's delay trigger time.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int clone_high_priority_delay_second = 0;\n-    /*\n-     * the minimal delay seconds between a replica is failed and fe try to recovery it using clone.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int replica_delay_recovery_second = 0;\n-    /*\n-     * Balance threshold of data size in BE.\n-     * The balance algorithm is:\n-     * 1. Calculate the average used capacity(AUC) of the entire cluster. (total data size / total backends num)\n-     * 2. The high water level is (AUC * (1 + clone_capacity_balance_threshold))\n-     * 3. The low water level is (AUC * (1 - clone_capacity_balance_threshold))\n-     * The Clone checker will try to move replica from high water level BE to low water level BE.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static double clone_capacity_balance_threshold = 0.2;\n-    /*\n-     * Balance threshold of num of replicas in Backends.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static double clone_distribution_balance_threshold = 0.2;\n-    /*\n-     * The high water of disk capacity used percent.\n-     * This is used for calculating load score of a backend.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static double capacity_used_percent_high_water = 0.75;\n-    /*\n-     * Maximal timeout of ALTER TABLE request. Set long enough to fit your table data size.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int alter_table_timeout_second = 86400; // 1day\n-    /*\n-     * If a backend is down for *max_backend_down_time_second*, a BACKEND_DOWN event will be triggered.\n-     * Do not set this if you know what you are doing.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_backend_down_time_second = 3600; // 1h\n-    /*\n-     * When create a table(or partition), you can specify its storage medium(HDD or SSD).\n-     * If not set, this specifies the default medium when creat.\n-     */\n-    @ConfField public static String default_storage_medium = \"HDD\";\n-    /*\n-     * When create a table(or partition), you can specify its storage medium(HDD or SSD).\n-     * If set to SSD, this specifies the default duration that tablets will stay on SSD.\n-     * After that, tablets will be moved to HDD automatically.\n-     * You can set storage cooldown time in CREATE TABLE stmt.\n-     */\n-    @ConfField public static long storage_cooldown_second = 30 * 24 * 3600L; // 30 days\n-    /*\n-     * After dropping database(table/partition), you can recover it by using RECOVER stmt.\n-     * And this specifies the maximal data retention time. After time, the data will be deleted permanently.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long catalog_trash_expire_second = 86400L; // 1day\n-    /*\n-     * Maximal bytes that a single broker scanner will read.\n-     * Do not set this if you know what you are doing.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long min_bytes_per_broker_scanner = 67108864L; // 64MB\n-    /*\n-     * Maximal concurrency of broker scanners.\n-     * Do not set this if you know what you are doing.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_broker_concurrency = 10;\n-\n-    /*\n-     * Export checker's running interval.\n-     */\n-    @ConfField public static int export_checker_interval_second = 5;\n-    /*\n-     * Limitation of the concurrency of running export jobs.\n-     * Default is 5.\n-     * 0 is unlimited\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int export_running_job_num_limit = 5;\n-    /*\n-     * Default timeout of export jobs.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int export_task_default_timeout_second = 2 * 3600; // 2h\n-    /*\n-     * Number of tablets per export query plan\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int export_tablet_num_per_task = 5;\n-\n-    // Configurations for consistency check\n-    /*\n-     * Consistency checker will run from *consistency_check_start_time* to *consistency_check_end_time*.\n-     * Default is from 23:00 to 04:00\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static String consistency_check_start_time = \"23\";\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static String consistency_check_end_time = \"4\";\n-    /*\n-     * Default timeout of a single consistency check task. Set long enough to fit your tablet size.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long check_consistency_default_timeout_second = 600; // 10 min\n-\n-    // Configurations for query engine\n-    /*\n-     * Maximal number of connections per FE.\n-     */\n-    @ConfField public static int qe_max_connection = 1024;\n-\n-    /*\n-     * Maximal number of thread in connection-scheduler-pool.\n-     */\n-    @ConfField public static int max_connection_scheduler_threads_num = 4096;\n-\n-    /*\n-    * The memory_limit for colocote join PlanFragment instance =\n-    * exec_mem_limit / min (query_colocate_join_memory_limit_penalty_factor, instance_num)\n-    */\n-    @ConfField(mutable = true)\n-    public static int query_colocate_join_memory_limit_penalty_factor = 8;\n-\n-    /*\n-     * Deprecated after 0.10\n-     */\n-    @ConfField\n-    public static boolean disable_colocate_join = false;\n-    /*\n-     * The default user resource publishing timeout.\n-     */\n-    @ConfField public static int meta_publish_timeout_ms = 1000;\n-    @ConfField public static boolean proxy_auth_enable = false;\n-    @ConfField public static String proxy_auth_magic_prefix = \"x@8\";\n-    /*\n-     * Limit on the number of expr children of an expr tree.\n-     * Exceed this limit may cause long analysis time while holding database read lock.\n-     * Do not set this if you know what you are doing.\n-     */\n-    @ConfField(mutable = true)\n-    public static int expr_children_limit = 10000;\n-    /*\n-     * Limit on the depth of an expr tree.\n-     * Exceed this limit may cause long analysis time while holding db read lock.\n-     * Do not set this if you know what you are doing.\n-     */\n-    @ConfField(mutable = true)\n-    public static int expr_depth_limit = 3000;\n-\n-    // Configurations for backup and restore\n-    /*\n-     * Plugins' path for BACKUP and RESTORE operations. Currently deprecated.\n-     */\n-    @Deprecated\n-    @ConfField public static String backup_plugin_path = \"/tools/trans_file_tool/trans_files.sh\";\n-\n-    // Configurations for hadoop dpp\n-    /*\n-     * The following configurations are not available.\n-     */\n-    @ConfField public static String dpp_hadoop_client_path = \"/lib/hadoop-client/hadoop/bin/hadoop\";\n-    @ConfField public static long dpp_bytes_per_reduce = 100 * 1024 * 1024L; // 100M\n-    @ConfField public static String dpp_default_cluster = \"palo-dpp\";\n-    @ConfField public static String dpp_default_config_str = \"\"\n-            + \"{\"\n-            + \"hadoop_configs : '\"\n-            + \"mapred.job.priority=NORMAL;\"\n-            + \"mapred.job.map.capacity=50;\"\n-            + \"mapred.job.reduce.capacity=50;\"\n-            + \"mapred.hce.replace.streaming=false;\"\n-            + \"abaci.long.stored.job=true;\"\n-            + \"dce.shuffle.enable=false;\"\n-            + \"dfs.client.authserver.force_stop=true;\"\n-            + \"dfs.client.auth.method=0\"\n-            + \"'}\";\n-    @ConfField public static String dpp_config_str = \"\"\n-            + \"{palo-dpp : {\"\n-            + \"hadoop_palo_path : '/dir',\"\n-            + \"hadoop_configs : '\"\n-            + \"fs.default.name=hdfs://host:port;\"\n-            + \"mapred.job.tracker=host:port;\"\n-            + \"hadoop.job.ugi=user,password\"\n-            + \"'}\"\n-            + \"}\";\n-\n-    // For forward compatibility, will be removed later.\n-    // check token when download image file.\n-    @ConfField public static boolean enable_token_check = true;\n-\n-    /*\n-     * Set to true if you deploy Palo using thirdparty deploy manager\n-     * Valid options are:\n-     *      disable:    no deploy manager\n-     *      k8s:        Kubernetes\n-     *      ambari:     Ambari\n-     *      local:      Local File (for test or Boxer2 BCC version)\n-     */\n-    @ConfField public static String enable_deploy_manager = \"disable\";\n-    \n-    // If use k8s deploy manager locally, set this to true and prepare the certs files\n-    @ConfField public static boolean with_k8s_certs = false;\n-    \n-    // Set runtime locale when exec some cmds\n-    @ConfField public static String locale = \"zh_CN.UTF-8\";\n-\n-    // default timeout of backup job\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int backup_job_default_timeout_ms = 86400 * 1000; // 1 day\n-    \n-    /*\n-     * 'storage_high_watermark_usage_percent' limit the max capacity usage percent of a Backend storage path.\n-     * 'storage_min_left_capacity_bytes' limit the minimum left capacity of a Backend storage path.\n-     * If both limitations are reached, this storage path can not be chose as tablet balance destination.\n-     * But for tablet recovery, we may exceed these limit for keeping data integrity as much as possible.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int storage_high_watermark_usage_percent = 85;\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long storage_min_left_capacity_bytes = 2 * 1024 * 1024 * 1024; // 2G\n-\n-    /*\n-     * If capacity of disk reach the 'storage_flood_stage_usage_percent' and 'storage_flood_stage_left_capacity_bytes',\n-     * the following operation will be rejected:\n-     * 1. load job\n-     * 2. restore job\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int storage_flood_stage_usage_percent = 95;\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long storage_flood_stage_left_capacity_bytes = 1 * 1024 * 1024 * 1024; // 100MB\n-\n-    // update interval of tablet stat\n-    // All frontends will get tablet stat from all backends at each interval\n-    @ConfField public static int tablet_stat_update_interval_second = 300;  // 5 min\n-\n-    // May be necessary to modify the following BRPC configurations in high concurrency scenarios. \n-    // The number of concurrent requests BRPC can processed\n-    @ConfField public static int brpc_number_of_concurrent_requests_processed = 4096;\n-\n-    // BRPC idle wait time (ms)\n-    @ConfField public static int brpc_idle_wait_max_time = 10000;\n-    \n-    /*\n-     * if set to false, auth check will be disable, in case some goes wrong with the new privilege system. \n-     */\n-    @ConfField public static boolean enable_auth_check = true;\n-    \n-    /*\n-     * Max bytes a broker scanner can process in one broker load job.\n-     * Commonly, each Backends has one broker scanner.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long max_bytes_per_broker_scanner = 3 * 1024 * 1024 * 1024L; // 3G\n-    \n-    /*\n-     * Max number of load jobs, include PENDING\u3001ETL\u3001LOADING\u3001QUORUM_FINISHED.\n-     * If exceed this number, load job is not allowed to be submitted.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long max_unfinished_load_job = 1000;\n-    \n-    /*\n-     * If set to true, Planner will try to select replica of tablet on same host as this Frontend.\n-     * This may reduce network transmission in following case:\n-     * 1. N hosts with N Backends and N Frontends deployed.\n-     * 2. The data has N replicas.\n-     * 3. High concurrency queries are sent to all Frontends evenly\n-     * In this case, all Frontends can only use local replicas to do the query.\n-     */\n-    @ConfField(mutable = true)\n-    public static boolean enable_local_replica_selection = false;\n-    \n-    /*\n-     * The timeout of executing async remote fragment.\n-     * In normal case, the async remote fragment will be executed in a short time. If system are under high load\n-     * condition\uff0ctry to set this timeout longer.\n-     */\n-    @ConfField(mutable = true)\n-    public static long remote_fragment_exec_timeout_ms = 5000; // 5 sec\n-    \n-    /*\n-     * The number of query retries. \n-     * A query may retry if we encounter RPC exception and no result has been sent to user.\n-     * You may reduce this number to void Avalanche disaster.\n-     */\n-    @ConfField(mutable = true)\n-    public static int max_query_retry_time = 2;\n-\n-    /*\n-     * The tryLock timeout configuration of catalog lock.\n-     * Normally it does not need to change, unless you need to test something.\n-     */\n-    @ConfField(mutable = true)\n-    public static long catalog_try_lock_timeout_ms = 5000; // 5 sec\n-    \n-    /*\n-     * if this is set to true\n-     *    all pending load job will failed when call begin txn api\n-     *    all prepare load job will failed when call commit txn api\n-     *    all committed load job will waiting to be published\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean disable_load_job = false;\n-\n-    /*\n-     * if this is set to true, all load job will check db data quota when call begin txn api\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean enable_check_data_quota_on_load = false;\n-\n-    /*\n-     * One master daemon thread will update database used data quota for db txn manager every db_used_data_quota_update_interval_secs\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int db_used_data_quota_update_interval_secs = 300;\n-    \n-    /*\n-     * Load using hadoop cluster will be deprecated in future.\n-     * Set to true to disable this kind of load.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean disable_hadoop_load = false;\n-    \n-    /*\n-     * fe will call es api to get es index shard info every es_state_sync_interval_secs\n-     */\n-    @ConfField\n-    public static long es_state_sync_interval_second = 10;\n-    \n-    /*\n-     * the factor of delay time before deciding to repair tablet.\n-     * if priority is VERY_HIGH, repair it immediately.\n-     * HIGH, delay tablet_repair_delay_factor_second * 1;\n-     * NORMAL: delay tablet_repair_delay_factor_second * 2;\n-     * LOW: delay tablet_repair_delay_factor_second * 3;\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long tablet_repair_delay_factor_second = 60;\n-    \n-    /*\n-     * the default slot number per path in tablet scheduler\n-     * TODO(cmy): remove this config and dynamically adjust it by clone task statistic\n-     */\n-    @ConfField public static int schedule_slot_num_per_path = 2;\n-    \n-    /*\n-     * Deprecated after 0.10\n-     */\n-    @ConfField public static boolean use_new_tablet_scheduler = true;\n-\n-    /*\n-     * the threshold of cluster balance score, if a backend's load score is 10% lower than average score,\n-     * this backend will be marked as LOW load, if load score is 10% higher than average score, HIGH load\n-     * will be marked.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static double balance_load_score_threshold = 0.1; // 10%\n-\n-    /*\n-     * if set to true, TabletScheduler will not do balance.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean disable_balance = false;\n-\n-    // if the number of scheduled tablets in TabletScheduler exceed max_scheduling_tablets\n-    // skip checking.\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_scheduling_tablets = 2000;\n-\n-    // if the number of balancing tablets in TabletScheduler exceed max_balancing_tablets,\n-    // no more balance check\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_balancing_tablets = 100;\n-\n-    // This threshold is to avoid piling up too many report task in FE, which may cause OOM exception.\n-    // In some large Doris cluster, eg: 100 Backends with ten million replicas, a tablet report may cost\n-    // several seconds after some modification of metadata(drop partition, etc..).\n-    // And one Backend will report tablets info every 1 min, so unlimited receiving reports is unacceptable.\n-    // TODO(cmy): we will optimize the processing speed of tablet report in future, but now, just discard\n-    // the report if queue size exceeding limit.\n-    // Some online time cost:\n-    // 1. disk report: 0-1 ms\n-    // 2. task report: 0-1 ms\n-    // 3. tablet report \n-    //      10000 replicas: 200ms\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int report_queue_size = 100;\n-    \n-    /*\n-     * If set to true, metric collector will be run as a daemon timer to collect metrics at fix interval\n-     */\n-    @ConfField public static boolean enable_metric_calculator = true;\n-\n-    /*\n-     * the max routine load job num, including NEED_SCHEDULED, RUNNING, PAUSE\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_routine_load_job_num = 100;\n-\n-    /*\n-     * the max concurrent routine load task num of a single routine load job\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_routine_load_task_concurrent_num = 5;\n-\n-    /*\n-     * the max concurrent routine load task num per BE.\n-     * This is to limit the num of routine load tasks sending to a BE, and it should also less\n-     * than BE config 'routine_load_thread_pool_size'(default 10),\n-     * which is the routine load task thread pool size on BE.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_routine_load_task_num_per_be = 5;\n-\n-    /*\n-     * The max number of files store in SmallFileMgr \n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_small_file_number = 100;\n-\n-    /*\n-     * The max size of a single file store in SmallFileMgr \n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_small_file_size_bytes = 1024 * 1024; // 1MB\n-\n-    /*\n-     * Save small files\n-     */\n-    @ConfField public static String small_file_dir = PaloFe.DORIS_HOME_DIR + \"/small_files\";\n-    \n-    /*\n-     * The following 2 configs can set to true to disable the automatic colocate tables's relocate and balance.\n-     * if 'disable_colocate_relocate' is set to true, ColocateTableBalancer will not relocate colocate tables when Backend unavailable.\n-     * if 'disable_colocate_balance' is set to true, ColocateTableBalancer will not balance colocate tables.\n-     */\n-    @ConfField(mutable = true, masterOnly = true) public static boolean disable_colocate_relocate = false;\n-    @ConfField(mutable = true, masterOnly = true) public static boolean disable_colocate_balance = false;\n-\n-    /*\n-     * If set to true, the insert stmt with processing error will still return a label to user.\n-     * And user can use this label to check the load job's status.\n-     * The default value is false, which means if insert operation encounter errors,\n-     * exception will be thrown to user client directly without load label.\n-     */\n-    @ConfField(mutable = true, masterOnly = true) public static boolean using_old_load_usage_pattern = false;\n-\n-    /*\n-     * This will limit the max recursion depth of hash distribution pruner.\n-     * eg: where a in (5 elements) and b in (4 elements) and c in (3 elements) and d in (2 elements).\n-     * a/b/c/d are distribution columns, so the recursion depth will be 5 * 4 * 3 * 2 = 120, larger than 100,\n-     * So that distribution pruner will no work and just return all buckets.\n-     * \n-     * Increase the depth can support distribution pruning for more elements, but may cost more CPU.\n-     */\n-    @ConfField(mutable = true, masterOnly = false)\n-    public static int max_distribution_pruner_recursion_depth = 100;\n-\n-    /*\n-     * If the jvm memory used percent(heap or old mem pool) exceed this threshold, checkpoint thread will\n-     * not work to avoid OOM.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static long metadata_checkopoint_memory_threshold = 60;\n-\n-    /*\n-     * If set to true, the checkpoint thread will make the checkpoint regardless of the jvm memory used percent.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean force_do_metadata_checkpoint = false;\n-\n-    /*\n-     * The multi cluster feature will be deprecated in version 0.12\n-     * set this config to true will disable all operations related to cluster feature, include:\n-     *   create/drop cluster\n-     *   add free backend/add backend to cluster/decommission cluster balance\n-     *   change the backends num of cluster\n-     *   link/migration db\n-     */\n-    @ConfField(mutable = true)\n-    public static boolean disable_cluster_feature = true;\n-\n-    /*\n-     * Decide how often to check dynamic partition\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int dynamic_partition_check_interval_seconds = 600;\n-\n-    /*\n-     * If set to true, dynamic partition feature will open\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean dynamic_partition_enable = false;\n-\n-    /*\n-     * control rollup job concurrent limit\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_running_rollup_job_num_per_table = 1;\n-\n-    /*\n-     * If set to true, Doris will check if the compiled and running versions of Java are compatible\n-     */\n-    @ConfField\n-    public static boolean check_java_version = true;\n-\n-    /*\n-     * control materialized view\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean enable_materialized_view = true;\n-\n-    /**\n-     * it can't auto-resume routine load job as long as one of the backends is down\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int max_tolerable_backend_down_num = 0;\n-\n-    /**\n-     * a period for auto resume routine load\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static int period_of_auto_resume_min = 5;\n-\n-    /*\n-     * If set to true, the backend will be automatically dropped after finishing decommission.\n-     * If set to false, the backend will not be dropped and remaining in DECOMMISSION state.\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean drop_backend_after_decommission = true;\n-\n-    /*\n-     * enable spark load for temporary use\n-     */\n-    @ConfField(mutable = true, masterOnly = true)\n-    public static boolean enable_spark_load = false;\n-}\n-\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyNTIwOQ==", "url": "https://github.com/apache/incubator-doris/pull/3955#discussion_r449725209", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                       checkDatabaseDataQuota();\n          \n          \n            \n                        checkDatabaseDataQuota();", "author": "morningman", "createdAt": "2020-07-04T00:55:37Z", "path": "fe/src/main/java/org/apache/doris/transaction/DatabaseTransactionMgr.java", "diffHunk": "@@ -246,6 +249,10 @@ private void getTxnStateInfo(TransactionState txnState, List<String> info) {\n     public long beginTransaction(List<Long> tableIdList, String label, TUniqueId requestId,\n                                  TransactionState.TxnCoordinator coordinator, TransactionState.LoadJobSourceType sourceType, long listenerId, long timeoutSecond)\n             throws DuplicatedRequestException, LabelAlreadyUsedException, BeginTransactionException, AnalysisException {\n+        if (Config.enable_check_data_quota_on_load) {\n+           checkDatabaseDataQuota();", "originalCommit": "21a49bb293503a3aeb83b0f6597ce9b5abccb766", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc1NTY3Ng==", "url": "https://github.com/apache/incubator-doris/pull/3955#discussion_r449755676", "bodyText": "fix", "author": "caiconghui", "createdAt": "2020-07-04T09:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyNTIwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "923b16566ea129734e44219f6ba3d78a53746987", "chunk": "diff --git a/fe/src/main/java/org/apache/doris/transaction/DatabaseTransactionMgr.java b/fe/src/main/java/org/apache/doris/transaction/DatabaseTransactionMgr.java\ndeleted file mode 100644\nindex 23949015a..000000000\n--- a/fe/src/main/java/org/apache/doris/transaction/DatabaseTransactionMgr.java\n+++ /dev/null\n\n@@ -1,1417 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-package org.apache.doris.transaction;\n-\n-import org.apache.doris.catalog.Catalog;\n-import org.apache.doris.catalog.Database;\n-import org.apache.doris.catalog.MaterializedIndex;\n-import org.apache.doris.catalog.OlapTable;\n-import org.apache.doris.catalog.Partition;\n-import org.apache.doris.catalog.PartitionInfo;\n-import org.apache.doris.catalog.Replica;\n-import org.apache.doris.catalog.Table;\n-import org.apache.doris.catalog.Tablet;\n-import org.apache.doris.catalog.TabletInvertedIndex;\n-import org.apache.doris.catalog.TabletMeta;\n-import org.apache.doris.common.AnalysisException;\n-import org.apache.doris.common.Config;\n-import org.apache.doris.common.DuplicatedRequestException;\n-import org.apache.doris.common.ErrorCode;\n-import org.apache.doris.common.ErrorReport;\n-import org.apache.doris.common.FeNameFormat;\n-import org.apache.doris.common.LabelAlreadyUsedException;\n-import org.apache.doris.common.LoadException;\n-import org.apache.doris.common.MetaNotFoundException;\n-import org.apache.doris.common.Pair;\n-import org.apache.doris.common.UserException;\n-import org.apache.doris.common.util.DebugUtil;\n-import org.apache.doris.common.util.TimeUtils;\n-import org.apache.doris.common.util.Util;\n-import org.apache.doris.metric.MetricRepo;\n-import org.apache.doris.mysql.privilege.PrivPredicate;\n-import org.apache.doris.persist.EditLog;\n-import org.apache.doris.qe.ConnectContext;\n-import org.apache.doris.task.AgentBatchTask;\n-import org.apache.doris.task.AgentTaskExecutor;\n-import org.apache.doris.task.AgentTaskQueue;\n-import org.apache.doris.task.ClearTransactionTask;\n-import org.apache.doris.task.PublishVersionTask;\n-import org.apache.doris.thrift.TTaskType;\n-import org.apache.doris.thrift.TUniqueId;\n-\n-import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.base.Joiner;\n-import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import com.google.common.collect.Sets;\n-\n-import org.apache.commons.collections.CollectionUtils;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-\n-import java.io.DataOutput;\n-import java.io.IOException;\n-import java.util.ArrayDeque;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.concurrent.locks.ReentrantReadWriteLock;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Transaction Manager in database level, as a component in GlobalTransactionMgr\n- * DatabaseTransactionMgr mainly be responsible for the following content:\n- * 1. provide read/write lock in database level\n- * 2. provide basic txn infos interface in database level to GlobalTransactionMgr\n- * 3. do some transaction management, such as add/update/delete transaction.\n- * Attention: all api in DatabaseTransactionMgr should be only invoked by GlobalTransactionMgr\n- */\n-\n-public class DatabaseTransactionMgr {\n-\n-    private static final Logger LOG = LogManager.getLogger(DatabaseTransactionMgr.class);\n-\n-    private long dbId;\n-\n-    // the lock is used to control the access to transaction states\n-    // no other locks should be inside this lock\n-    private ReentrantReadWriteLock transactionLock = new ReentrantReadWriteLock(true);\n-\n-    // transactionId -> running TransactionState\n-    private Map<Long, TransactionState> idToRunningTransactionState = Maps.newHashMap();\n-\n-    // transactionId -> final status TransactionState\n-    private Map<Long, TransactionState> idToFinalStatusTransactionState = Maps.newHashMap();\n-\n-\n-    // to store transtactionStates with final status\n-    private ArrayDeque<TransactionState> finalStatusTransactionStateDeque = new ArrayDeque<>();\n-\n-    // label -> txn ids\n-    // this is used for checking if label already used. a label may correspond to multiple txns,\n-    // and only one is success.\n-    // this member should be consistent with idToTransactionState,\n-    // which means if a txn exist in idToRunningTransactionState or idToFinalStatusTransactionState\n-    // it must exists in dbIdToTxnLabels, and vice versa\n-    private Map<String, Set<Long>> labelToTxnIds = Maps.newHashMap();\n-\n-\n-    // count the number of running txns of database, except for the routine load txn\n-    private int runningTxnNums = 0;\n-\n-    // count only the number of running routine load txns of database\n-    private int runningRoutineLoadTxnNums = 0;\n-\n-    private Catalog catalog;\n-\n-    private EditLog editLog;\n-\n-    private TransactionIdGenerator idGenerator;\n-\n-    private List<ClearTransactionTask> clearTransactionTasks = Lists.newArrayList();\n-\n-    // not realtime usedQuota value to make a fast check for database data quota\n-    private volatile long usedQuotaDataBytes = -1;\n-\n-    protected void readLock() {\n-        this.transactionLock.readLock().lock();\n-    }\n-\n-    protected void readUnlock() {\n-        this.transactionLock.readLock().unlock();\n-    }\n-\n-    protected void writeLock() {\n-        this.transactionLock.writeLock().lock();\n-    }\n-\n-    protected void writeUnlock() {\n-        this.transactionLock.writeLock().unlock();\n-    }\n-\n-    public DatabaseTransactionMgr(long dbId, Catalog catalog, TransactionIdGenerator idGenerator) {\n-        this.dbId = dbId;\n-        this.catalog = catalog;\n-        this.idGenerator = idGenerator;\n-        this.editLog = catalog.getEditLog();\n-    }\n-\n-    public long getDbId() {\n-        return dbId;\n-    }\n-\n-    public TransactionState getTransactionState(Long transactionId) {\n-        readLock();\n-        try {\n-            TransactionState transactionState = idToRunningTransactionState.get(transactionId);\n-            if (transactionState != null) {\n-                return transactionState;\n-            } else {\n-                return idToFinalStatusTransactionState.get(transactionId);\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-    }\n-\n-    private TransactionState unprotectedGetTransactionState(Long transactionId) {\n-        TransactionState transactionState = idToRunningTransactionState.get(transactionId);\n-        if (transactionState != null) {\n-            return transactionState;\n-        } else {\n-            return idToFinalStatusTransactionState.get(transactionId);\n-        }\n-    }\n-\n-    @VisibleForTesting\n-    protected Set<Long> unprotectedGetTxnIdsByLabel(String label) {\n-        return labelToTxnIds.get(label);\n-    }\n-\n-    @VisibleForTesting\n-    protected int getRunningTxnNums() {\n-        return runningTxnNums;\n-    }\n-\n-    @VisibleForTesting\n-    protected int getRunningRoutineLoadTxnNums() {\n-        return runningRoutineLoadTxnNums;\n-    }\n-\n-    @VisibleForTesting\n-    protected int getFinishedTxnNums() {\n-        return finalStatusTransactionStateDeque.size();\n-    }\n-\n-    public List<List<String>> getTxnStateInfoList(boolean running, int limit) {\n-        List<List<String>> infos = Lists.newArrayList();\n-        Collection<TransactionState> transactionStateCollection = null;\n-        readLock();\n-        try {\n-            if (running) {\n-                transactionStateCollection = idToRunningTransactionState.values();\n-            } else {\n-                transactionStateCollection = finalStatusTransactionStateDeque;\n-            }\n-            // get transaction order by txn id desc limit 'limit'\n-            transactionStateCollection.stream()\n-                    .sorted(TransactionState.TXN_ID_COMPARATOR)\n-                    .limit(limit)\n-                    .forEach(t -> {\n-                        List<String> info = Lists.newArrayList();\n-                        getTxnStateInfo(t, info);\n-                        infos.add(info);\n-                    });\n-        } finally {\n-            readUnlock();\n-        }\n-        return infos;\n-    }\n-\n-    private void getTxnStateInfo(TransactionState txnState, List<String> info) {\n-        info.add(String.valueOf(txnState.getTransactionId()));\n-        info.add(txnState.getLabel());\n-        info.add(txnState.getCoordinator().toString());\n-        info.add(txnState.getTransactionStatus().name());\n-        info.add(txnState.getSourceType().name());\n-        info.add(TimeUtils.longToTimeString(txnState.getPrepareTime()));\n-        info.add(TimeUtils.longToTimeString(txnState.getCommitTime()));\n-        info.add(TimeUtils.longToTimeString(txnState.getPublishVersionTime()));\n-        info.add(TimeUtils.longToTimeString(txnState.getFinishTime()));\n-        info.add(txnState.getReason());\n-        info.add(String.valueOf(txnState.getErrorReplicas().size()));\n-        info.add(String.valueOf(txnState.getCallbackId()));\n-        info.add(String.valueOf(txnState.getTimeoutMs()));\n-        info.add(txnState.getErrMsg());\n-    }\n-\n-    public long beginTransaction(List<Long> tableIdList, String label, TUniqueId requestId,\n-                                 TransactionState.TxnCoordinator coordinator, TransactionState.LoadJobSourceType sourceType, long listenerId, long timeoutSecond)\n-            throws DuplicatedRequestException, LabelAlreadyUsedException, BeginTransactionException, AnalysisException {\n-        if (Config.enable_check_data_quota_on_load) {\n-           checkDatabaseDataQuota();\n-        }\n-\n-        writeLock();\n-        try {\n-            Preconditions.checkNotNull(coordinator);\n-            Preconditions.checkNotNull(label);\n-            FeNameFormat.checkLabel(label);\n-\n-            /*\n-             * Check if label already used, by following steps\n-             * 1. get all existing transactions\n-             * 2. if there is a PREPARE transaction, check if this is a retry request. If yes, return the\n-             *    existing txn id.\n-             * 3. if there is a non-aborted transaction, throw label already used exception.\n-             */\n-            Set<Long> existingTxnIds = unprotectedGetTxnIdsByLabel(label);\n-            if (existingTxnIds != null && !existingTxnIds.isEmpty()) {\n-                List<TransactionState> notAbortedTxns = Lists.newArrayList();\n-                for (long txnId : existingTxnIds) {\n-                    TransactionState txn = unprotectedGetTransactionState(txnId);\n-                    Preconditions.checkNotNull(txn);\n-                    if (txn.getTransactionStatus() != TransactionStatus.ABORTED) {\n-                        notAbortedTxns.add(txn);\n-                    }\n-                }\n-                // there should be at most 1 txn in PREPARE/COMMITTED/VISIBLE status\n-                Preconditions.checkState(notAbortedTxns.size() <= 1, notAbortedTxns);\n-                if (!notAbortedTxns.isEmpty()) {\n-                    TransactionState notAbortedTxn = notAbortedTxns.get(0);\n-                    if (requestId != null && notAbortedTxn.getTransactionStatus() == TransactionStatus.PREPARE\n-                            && notAbortedTxn.getRequsetId() != null && notAbortedTxn.getRequsetId().equals(requestId)) {\n-                        // this may be a retry request for same job, just return existing txn id.\n-                        throw new DuplicatedRequestException(DebugUtil.printId(requestId),\n-                                notAbortedTxn.getTransactionId(), \"\");\n-                    }\n-                    throw new LabelAlreadyUsedException(label, notAbortedTxn.getTransactionStatus());\n-                }\n-            }\n-\n-            checkRunningTxnExceedLimit(sourceType);\n-\n-            long tid = idGenerator.getNextTransactionId();\n-            LOG.info(\"begin transaction: txn id {} with label {} from coordinator {}\", tid, label, coordinator);\n-            TransactionState transactionState = new TransactionState(dbId, tableIdList, tid, label, requestId, sourceType,\n-                    coordinator, listenerId, timeoutSecond * 1000);\n-            transactionState.setPrepareTime(System.currentTimeMillis());\n-            unprotectUpsertTransactionState(transactionState, false);\n-\n-            if (MetricRepo.isInit.get()) {\n-                MetricRepo.COUNTER_TXN_BEGIN.increase(1L);\n-            }\n-\n-            return tid;\n-        } catch (DuplicatedRequestException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            if (MetricRepo.isInit.get()) {\n-                MetricRepo.COUNTER_TXN_REJECT.increase(1L);\n-            }\n-            throw e;\n-        } finally {\n-            writeUnlock();\n-        }\n-    }\n-\n-\n-    private void checkDatabaseDataQuota() throws AnalysisException {\n-        Database db = catalog.getDb(dbId);\n-        if (db == null) {\n-            throw new AnalysisException(\"Database[\" + dbId + \"] does not exist\");\n-        }\n-\n-        if (usedQuotaDataBytes == -1) {\n-            usedQuotaDataBytes = db.getUsedDataQuotaWithLock();\n-        }\n-\n-        long dataQuotaBytes = db.getDataQuota();\n-        if (usedQuotaDataBytes >= dataQuotaBytes) {\n-            Pair<Double, String> quotaUnitPair = DebugUtil.getByteUint(dataQuotaBytes);\n-            String readableQuota = DebugUtil.DECIMAL_FORMAT_SCALE_3.format(quotaUnitPair.first) + \" \"\n-                    + quotaUnitPair.second;\n-            throw new AnalysisException(\"Database[\" + db.getFullName()\n-                    + \"] data size exceeds quota[\" + readableQuota + \"]\");\n-        }\n-    }\n-\n-    public void updateDatabaseUsedQuotaData(long usedQuotaDataBytes) {\n-        this.usedQuotaDataBytes = usedQuotaDataBytes;\n-    }\n-\n-    /**\n-     * commit transaction process as follows\uff1a\n-     * 1. validate whether `Load` is cancelled\n-     * 2. validate whether `Table` is deleted\n-     * 3. validate replicas consistency\n-     * 4. update transaction state version\n-     * 5. persistent transactionState\n-     * 6. update nextVersion because of the failure of persistent transaction resulting in error version\n-     */\n-    public void commitTransaction(long transactionId, List<TabletCommitInfo> tabletCommitInfos,\n-                                  TxnCommitAttachment txnCommitAttachment)\n-            throws UserException {\n-        // 1. check status\n-        // the caller method already own db lock, we do not obtain db lock here\n-        Database db = catalog.getDb(dbId);\n-        if (null == db) {\n-            throw new MetaNotFoundException(\"could not find db [\" + dbId + \"]\");\n-        }\n-\n-        TransactionState transactionState = null;\n-        readLock();\n-        try {\n-            transactionState = unprotectedGetTransactionState(transactionId);\n-        } finally {\n-            readUnlock();\n-        }\n-        if (transactionState == null\n-                || transactionState.getTransactionStatus() == TransactionStatus.ABORTED) {\n-            throw new TransactionCommitFailedException(\n-                    transactionState == null ? \"transaction not found\" : transactionState.getReason());\n-        }\n-\n-        if (transactionState.getTransactionStatus() == TransactionStatus.VISIBLE) {\n-            LOG.debug(\"transaction is already visible: {}\", transactionId);\n-            return;\n-        }\n-        if (transactionState.getTransactionStatus() == TransactionStatus.COMMITTED) {\n-            LOG.debug(\"transaction is already committed: {}\", transactionId);\n-            return;\n-        }\n-\n-        if (tabletCommitInfos == null || tabletCommitInfos.isEmpty()) {\n-            throw new TransactionCommitFailedException(TransactionCommitFailedException.NO_DATA_TO_LOAD_MSG);\n-        }\n-\n-        // update transaction state extra if exists\n-        if (txnCommitAttachment != null) {\n-            transactionState.setTxnCommitAttachment(txnCommitAttachment);\n-        }\n-\n-        TabletInvertedIndex tabletInvertedIndex = catalog.getTabletInvertedIndex();\n-        Map<Long, Set<Long>> tabletToBackends = new HashMap<>();\n-        Map<Long, Set<Long>> tableToPartition = new HashMap<>();\n-        // 2. validate potential exists problem: db->table->partition\n-        // guarantee exist exception during a transaction\n-        // if index is dropped, it does not matter.\n-        // if table or partition is dropped during load, just ignore that tablet,\n-        // because we should allow dropping rollup or partition during load\n-        List<Long> tabletIds = tabletCommitInfos.stream().map(\n-                tabletCommitInfo -> tabletCommitInfo.getTabletId()).collect(Collectors.toList());\n-        List<TabletMeta> tabletMetaList = tabletInvertedIndex.getTabletMetaList(tabletIds);\n-        for (int i = 0; i < tabletMetaList.size(); i++) {\n-            TabletMeta tabletMeta = tabletMetaList.get(i);\n-            if (tabletMeta == TabletInvertedIndex.NOT_EXIST_TABLET_META) {\n-                continue;\n-            }\n-            long tabletId = tabletIds.get(i);\n-            long tableId = tabletMeta.getTableId();\n-            OlapTable tbl = (OlapTable) db.getTable(tableId);\n-            if (tbl == null) {\n-                // this can happen when tableId == -1 (tablet being dropping)\n-                // or table really not exist.\n-                continue;\n-            }\n-\n-            if (tbl.getState() == OlapTable.OlapTableState.RESTORE) {\n-                throw new LoadException(\"Table \" + tbl.getName() + \" is in restore process. \"\n-                        + \"Can not load into it\");\n-            }\n-\n-            long partitionId = tabletMeta.getPartitionId();\n-            if (tbl.getPartition(partitionId) == null) {\n-                // this can happen when partitionId == -1 (tablet being dropping)\n-                // or partition really not exist.\n-                continue;\n-            }\n-\n-            if (!tableToPartition.containsKey(tableId)) {\n-                tableToPartition.put(tableId, new HashSet<>());\n-            }\n-            tableToPartition.get(tableId).add(partitionId);\n-            if (!tabletToBackends.containsKey(tabletId)) {\n-                tabletToBackends.put(tabletId, new HashSet<>());\n-            }\n-            tabletToBackends.get(tabletId).add(tabletCommitInfos.get(i).getBackendId());\n-        }\n-\n-        if (tableToPartition.isEmpty()) {\n-            // table or all partitions are being dropped\n-            throw new TransactionCommitFailedException(TransactionCommitFailedException.NO_DATA_TO_LOAD_MSG);\n-        }\n-\n-        Set<Long> errorReplicaIds = Sets.newHashSet();\n-        Set<Long> totalInvolvedBackends = Sets.newHashSet();\n-        for (long tableId : tableToPartition.keySet()) {\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            if (table == null) {\n-                throw new MetaNotFoundException(\"Table does not exist: \" + tableId);\n-            }\n-            for (Partition partition : table.getAllPartitions()) {\n-                if (!tableToPartition.get(tableId).contains(partition.getId())) {\n-                    continue;\n-                }\n-\n-                List<MaterializedIndex> allIndices;\n-                if (transactionState.getLoadedTblIndexes().isEmpty()) {\n-                    allIndices = partition.getMaterializedIndices(MaterializedIndex.IndexExtState.ALL);\n-                } else {\n-                    allIndices = Lists.newArrayList();\n-                    for (long indexId : transactionState.getLoadedTblIndexes().get(tableId)) {\n-                        MaterializedIndex index = partition.getIndex(indexId);\n-                        if (index != null) {\n-                            allIndices.add(index);\n-                        }\n-                    }\n-                }\n-\n-                if (table.getState() == OlapTable.OlapTableState.ROLLUP || table.getState() == OlapTable.OlapTableState.SCHEMA_CHANGE) {\n-                    /*\n-                     * This is just a optimization that do our best to not let publish version tasks\n-                     * timeout if table is under rollup or schema change. Because with a short\n-                     * timeout, a replica's publish version task is more likely to fail. And if\n-                     * quorum replicas of a tablet fail to publish, the alter job will fail.\n-                     *\n-                     * If the table is not under rollup or schema change, the failure of a replica's\n-                     * publish version task has a minor effect because the replica can be repaired\n-                     * by tablet repair process very soon. But the tablet repair process will not\n-                     * repair rollup replicas.\n-                     *\n-                     * This a kind of best-effort-optimization, if FE restart after commit and\n-                     * before publish, this 'prolong' information will be lost.\n-                     */\n-                    transactionState.prolongPublishTimeout();\n-                }\n-\n-                int quorumReplicaNum = table.getPartitionInfo().getReplicationNum(partition.getId()) / 2 + 1;\n-                for (MaterializedIndex index : allIndices) {\n-                    for (Tablet tablet : index.getTablets()) {\n-                        int successReplicaNum = 0;\n-                        long tabletId = tablet.getId();\n-                        Set<Long> tabletBackends = tablet.getBackendIds();\n-                        totalInvolvedBackends.addAll(tabletBackends);\n-                        Set<Long> commitBackends = tabletToBackends.get(tabletId);\n-                        // save the error replica ids for current tablet\n-                        // this param is used for log\n-                        Set<Long> errorBackendIdsForTablet = Sets.newHashSet();\n-                        for (long tabletBackend : tabletBackends) {\n-                            Replica replica = tabletInvertedIndex.getReplica(tabletId, tabletBackend);\n-                            if (replica == null) {\n-                                throw new TransactionCommitFailedException(\"could not find replica for tablet [\"\n-                                        + tabletId + \"], backend [\" + tabletBackend + \"]\");\n-                            }\n-                            // if the tablet have no replica's to commit or the tablet is a rolling up tablet, the commit backends maybe null\n-                            // if the commit backends is null, set all replicas as error replicas\n-                            if (commitBackends != null && commitBackends.contains(tabletBackend)) {\n-                                // if the backend load success but the backend has some errors previously, then it is not a normal replica\n-                                // ignore it but not log it\n-                                // for example, a replica is in clone state\n-                                if (replica.getLastFailedVersion() < 0) {\n-                                    ++successReplicaNum;\n-                                }\n-                            } else {\n-                                errorBackendIdsForTablet.add(tabletBackend);\n-                                errorReplicaIds.add(replica.getId());\n-                                // not remove rollup task here, because the commit maybe failed\n-                                // remove rollup task when commit successfully\n-                            }\n-                        }\n-\n-                        if (successReplicaNum < quorumReplicaNum) {\n-                            LOG.warn(\"Failed to commit txn [{}]. \"\n-                                            + \"Tablet [{}] success replica num is {} < quorum replica num {} \"\n-                                            + \"while error backends {}\",\n-                                    transactionId, tablet.getId(), successReplicaNum, quorumReplicaNum,\n-                                    Joiner.on(\",\").join(errorBackendIdsForTablet));\n-                            throw new TabletQuorumFailedException(transactionId, tablet.getId(),\n-                                    successReplicaNum, quorumReplicaNum,\n-                                    errorBackendIdsForTablet);\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-\n-        // before state transform\n-        transactionState.beforeStateTransform(TransactionStatus.COMMITTED);\n-        // transaction state transform\n-        boolean txnOperated = false;\n-        writeLock();\n-        try {\n-            unprotectedCommitTransaction(transactionState, errorReplicaIds, tableToPartition, totalInvolvedBackends,\n-                    db);\n-            txnOperated = true;\n-        } finally {\n-            writeUnlock();\n-            // after state transform\n-            transactionState.afterStateTransform(TransactionStatus.COMMITTED, txnOperated);\n-        }\n-\n-        // 6. update nextVersion because of the failure of persistent transaction resulting in error version\n-        updateCatalogAfterCommitted(transactionState, db);\n-        LOG.info(\"transaction:[{}] successfully committed\", transactionState);\n-    }\n-\n-    public boolean publishTransaction(Database db, long transactionId, long timeoutMillis) throws TransactionCommitFailedException {\n-        TransactionState transactionState = null;\n-        readLock();\n-        try {\n-            transactionState = unprotectedGetTransactionState(transactionId);\n-        } finally {\n-            readUnlock();\n-        }\n-\n-        switch (transactionState.getTransactionStatus()) {\n-            case COMMITTED:\n-            case VISIBLE:\n-                break;\n-            default:\n-                LOG.warn(\"transaction commit failed, db={}, txn={}\", db.getFullName(), transactionId);\n-                throw new TransactionCommitFailedException(\"transaction commit failed\");\n-        }\n-\n-        long currentTimeMillis = System.currentTimeMillis();\n-        long timeoutTimeMillis = currentTimeMillis + timeoutMillis;\n-        while (currentTimeMillis < timeoutTimeMillis &&\n-                transactionState.getTransactionStatus() == TransactionStatus.COMMITTED) {\n-            try {\n-                transactionState.waitTransactionVisible(timeoutMillis);\n-            } catch (InterruptedException e) {\n-            }\n-            currentTimeMillis = System.currentTimeMillis();\n-        }\n-        return transactionState.getTransactionStatus() == TransactionStatus.VISIBLE;\n-    }\n-\n-    public void deleteTransaction(TransactionState transactionState) {\n-        writeLock();\n-        try {\n-            // here we only delete the oldest element, so if element exist in finalStatusTransactionStateDeque,\n-            // it must at the front of the finalStatusTransactionStateDeque\n-            if (!finalStatusTransactionStateDeque.isEmpty() &&\n-            transactionState.getTransactionId() == finalStatusTransactionStateDeque.getFirst().getTransactionId()) {\n-                finalStatusTransactionStateDeque.pop();\n-                idToFinalStatusTransactionState.remove(transactionState.getTransactionId());\n-                Set<Long> txnIds = unprotectedGetTxnIdsByLabel(transactionState.getLabel());\n-                txnIds.remove(transactionState.getTransactionId());\n-                if (txnIds.isEmpty()) {\n-                    labelToTxnIds.remove(transactionState.getLabel());\n-                }\n-            }\n-        } finally {\n-            writeUnlock();\n-        }\n-    }\n-\n-    public TransactionStatus getLabelState(String label) {\n-        readLock();\n-        try {\n-            Set<Long> existingTxnIds = unprotectedGetTxnIdsByLabel(label);\n-            if (existingTxnIds == null || existingTxnIds.isEmpty()) {\n-                return TransactionStatus.UNKNOWN;\n-            }\n-            // find the latest txn (which id is largest)\n-            long maxTxnId = existingTxnIds.stream().max(Comparator.comparingLong(Long::valueOf)).get();\n-            return unprotectedGetTransactionState(maxTxnId).getTransactionStatus();\n-        } finally {\n-            readUnlock();\n-        }\n-    }\n-\n-    public List<TransactionState> getCommittedTxnList() {\n-        readLock();\n-        try {\n-            // only send task to committed transaction\n-            return idToRunningTransactionState.values().stream()\n-                    .filter(transactionState -> (transactionState.getTransactionStatus() == TransactionStatus.COMMITTED))\n-                    .sorted(Comparator.comparing(TransactionState::getCommitTime))\n-                    .collect(Collectors.toList());\n-        } finally {\n-            readUnlock();\n-        }\n-    }\n-\n-    public void finishTransaction(long transactionId, Set<Long> errorReplicaIds) throws UserException {\n-        TransactionState transactionState = null;\n-        readLock();\n-        try {\n-            transactionState = unprotectedGetTransactionState(transactionId);\n-        } finally {\n-            readUnlock();\n-        }\n-        // add all commit errors and publish errors to a single set\n-        if (errorReplicaIds == null) {\n-            errorReplicaIds = Sets.newHashSet();\n-        }\n-        Set<Long> originalErrorReplicas = transactionState.getErrorReplicas();\n-        if (originalErrorReplicas != null) {\n-            errorReplicaIds.addAll(originalErrorReplicas);\n-        }\n-\n-        Database db = catalog.getDb(transactionState.getDbId());\n-        if (db == null) {\n-            writeLock();\n-            try {\n-                transactionState.setTransactionStatus(TransactionStatus.ABORTED);\n-                transactionState.setReason(\"db is dropped\");\n-                LOG.warn(\"db is dropped during transaction, abort transaction {}\", transactionState);\n-                unprotectUpsertTransactionState(transactionState, false);\n-                return;\n-            } finally {\n-                writeUnlock();\n-            }\n-        }\n-        db.writeLock();\n-        try {\n-            boolean hasError = false;\n-            for (TableCommitInfo tableCommitInfo : transactionState.getIdToTableCommitInfos().values()) {\n-                long tableId = tableCommitInfo.getTableId();\n-                OlapTable table = (OlapTable) db.getTable(tableId);\n-                // table maybe dropped between commit and publish, ignore this error\n-                if (table == null) {\n-                    transactionState.removeTable(tableId);\n-                    LOG.warn(\"table {} is dropped, skip version check and remove it from transaction state {}\",\n-                            tableId,\n-                            transactionState);\n-                    continue;\n-                }\n-                PartitionInfo partitionInfo = table.getPartitionInfo();\n-                for (PartitionCommitInfo partitionCommitInfo : tableCommitInfo.getIdToPartitionCommitInfo().values()) {\n-                    long partitionId = partitionCommitInfo.getPartitionId();\n-                    Partition partition = table.getPartition(partitionId);\n-                    // partition maybe dropped between commit and publish version, ignore this error\n-                    if (partition == null) {\n-                        tableCommitInfo.removePartition(partitionId);\n-                        LOG.warn(\"partition {} is dropped, skip version check and remove it from transaction state {}\",\n-                                partitionId,\n-                                transactionState);\n-                        continue;\n-                    }\n-                    if (partition.getVisibleVersion() != partitionCommitInfo.getVersion() - 1) {\n-                        LOG.debug(\"transactionId {} partition commitInfo version {} is not equal with \" +\n-                                        \"partition visible version {} plus one, need wait\",\n-                                transactionId,\n-                                partitionCommitInfo.getVersion(),\n-                                partition.getVisibleVersion());\n-                        String errMsg = String.format(\"wait for publishing partition %d version %d. self version: %d. table %d\",\n-                                partitionId, partition.getVisibleVersion() + 1, partitionCommitInfo.getVersion(), tableId);\n-                        transactionState.setErrorMsg(errMsg);\n-                        return;\n-                    }\n-                    int quorumReplicaNum = partitionInfo.getReplicationNum(partitionId) / 2 + 1;\n-\n-                    List<MaterializedIndex> allIndices;\n-                    if (transactionState.getLoadedTblIndexes().isEmpty()) {\n-                        allIndices = partition.getMaterializedIndices(MaterializedIndex.IndexExtState.ALL);\n-                    } else {\n-                        allIndices = Lists.newArrayList();\n-                        for (long indexId : transactionState.getLoadedTblIndexes().get(tableId)) {\n-                            MaterializedIndex index = partition.getIndex(indexId);\n-                            if (index != null) {\n-                                allIndices.add(index);\n-                            }\n-                        }\n-                    }\n-\n-                    for (MaterializedIndex index : allIndices) {\n-                        for (Tablet tablet : index.getTablets()) {\n-                            int healthReplicaNum = 0;\n-                            for (Replica replica : tablet.getReplicas()) {\n-                                if (!errorReplicaIds.contains(replica.getId())\n-                                        && replica.getLastFailedVersion() < 0) {\n-                                    // this means the replica is a healthy replica,\n-                                    // it is healthy in the past and does not have error in current load\n-                                    if (replica.checkVersionCatchUp(partition.getVisibleVersion(),\n-                                            partition.getVisibleVersionHash(), true)) {\n-                                        // during rollup, the rollup replica's last failed version < 0,\n-                                        // it may be treated as a normal replica.\n-                                        // the replica is not failed during commit or publish\n-                                        // during upgrade, one replica's last version maybe invalid,\n-                                        // has to compare version hash.\n-\n-                                        // Here we still update the replica's info even if we failed to publish\n-                                        // this txn, for the following case:\n-                                        // replica A,B,C is successfully committed, but only A is successfully\n-                                        // published,\n-                                        // B and C is crashed, now we need a Clone task to repair this tablet.\n-                                        // So, here we update A's version info, so that clone task will clone\n-                                        // the latest version of data.\n-\n-                                        replica.updateVersionInfo(partitionCommitInfo.getVersion(),\n-                                                partitionCommitInfo.getVersionHash(),\n-                                                replica.getDataSize(), replica.getRowCount());\n-                                        ++healthReplicaNum;\n-                                    } else {\n-                                        // this means the replica has error in the past, but we did not observe it\n-                                        // during upgrade, one job maybe in quorum finished state, for example, A,B,C 3 replica\n-                                        // A,B 's version is 10, C's version is 10 but C' 10 is abnormal should be rollback\n-                                        // then we will detect this and set C's last failed version to 10 and last success version to 11\n-                                        // this logic has to be replayed in checkpoint thread\n-                                        replica.updateVersionInfo(replica.getVersion(), replica.getVersionHash(),\n-                                                partition.getVisibleVersion(), partition.getVisibleVersionHash(),\n-                                                partitionCommitInfo.getVersion(), partitionCommitInfo.getVersionHash());\n-                                        LOG.warn(\"transaction state {} has error, the replica [{}] not appeared in error replica list \"\n-                                                + \" and its version not equal to partition commit version or commit version - 1\"\n-                                                + \" if its not a upgrate stage, its a fatal error. \", transactionState, replica);\n-                                    }\n-                                } else if (replica.getVersion() == partitionCommitInfo.getVersion()\n-                                        && replica.getVersionHash() == partitionCommitInfo.getVersionHash()) {\n-                                    // the replica's version and version hash is equal to current transaction partition's version and version hash\n-                                    // the replica is normal, then remove it from error replica ids\n-                                    errorReplicaIds.remove(replica.getId());\n-                                    ++healthReplicaNum;\n-                                }\n-                            }\n-\n-                            if (healthReplicaNum < quorumReplicaNum) {\n-                                LOG.info(\"publish version failed for transaction {} on tablet {}, with only {} replicas less than quorum {}\",\n-                                        transactionState, tablet, healthReplicaNum, quorumReplicaNum);\n-                                String errMsg = String.format(\"publish on tablet %d failed. succeed replica num %d less than quorum %d.\"\n-                                        + \" table: %d, partition: %d, publish version: %d\",\n-                                        tablet.getId(), healthReplicaNum, quorumReplicaNum, tableId, partitionId, partition.getVisibleVersion() + 1);\n-                                transactionState.setErrorMsg(errMsg);\n-                                hasError = true;\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-            if (hasError) {\n-                return;\n-            }\n-            boolean txnOperated = false;\n-            writeLock();\n-            try {\n-                transactionState.setErrorReplicas(errorReplicaIds);\n-                transactionState.setFinishTime(System.currentTimeMillis());\n-                transactionState.clearErrorMsg();\n-                transactionState.setTransactionStatus(TransactionStatus.VISIBLE);\n-                unprotectUpsertTransactionState(transactionState, false);\n-                txnOperated = true;\n-                // TODO(cmy): We found a very strange problem. When delete-related transactions are processed here,\n-                // subsequent `updateCatalogAfterVisible()` is called, but it does not seem to be executed here\n-                // (because the relevant editlog does not see the log of visible transactions).\n-                // So I add a log here for observation.\n-                LOG.debug(\"after set transaction {} to visible\", transactionState);\n-            } finally {\n-                writeUnlock();\n-                transactionState.afterStateTransform(TransactionStatus.VISIBLE, txnOperated);\n-            }\n-            updateCatalogAfterVisible(transactionState, db);\n-        } finally {\n-            db.writeUnlock();\n-        }\n-        LOG.info(\"finish transaction {} successfully\", transactionState);\n-    }\n-\n-    protected void unprotectedCommitTransaction(TransactionState transactionState, Set<Long> errorReplicaIds,\n-                                               Map<Long, Set<Long>> tableToPartition, Set<Long> totalInvolvedBackends,\n-                                               Database db) {\n-        // transaction state is modified during check if the transaction could committed\n-        if (transactionState.getTransactionStatus() != TransactionStatus.PREPARE) {\n-            return;\n-        }\n-        // update transaction state version\n-        transactionState.setCommitTime(System.currentTimeMillis());\n-        transactionState.setTransactionStatus(TransactionStatus.COMMITTED);\n-        transactionState.setErrorReplicas(errorReplicaIds);\n-        for (long tableId : tableToPartition.keySet()) {\n-            TableCommitInfo tableCommitInfo = new TableCommitInfo(tableId);\n-            for (long partitionId : tableToPartition.get(tableId)) {\n-                OlapTable table = (OlapTable) db.getTable(tableId);\n-                Partition partition = table.getPartition(partitionId);\n-                PartitionCommitInfo partitionCommitInfo = new PartitionCommitInfo(partitionId,\n-                        partition.getNextVersion(),\n-                        partition.getNextVersionHash());\n-                tableCommitInfo.addPartitionCommitInfo(partitionCommitInfo);\n-            }\n-            transactionState.putIdToTableCommitInfo(tableId, tableCommitInfo);\n-        }\n-        // persist transactionState\n-        unprotectUpsertTransactionState(transactionState, false);\n-\n-        // add publish version tasks. set task to null as a placeholder.\n-        // tasks will be created when publishing version.\n-        for (long backendId : totalInvolvedBackends) {\n-            transactionState.addPublishVersionTask(backendId, null);\n-        }\n-    }\n-\n-    // for add/update/delete TransactionState\n-    protected void unprotectUpsertTransactionState(TransactionState transactionState, boolean isReplay) {\n-        // if this is a replay operation, we should not log it\n-        if (!isReplay) {\n-            if (transactionState.getTransactionStatus() != TransactionStatus.PREPARE\n-                    || transactionState.getSourceType() == TransactionState.LoadJobSourceType.FRONTEND) {\n-                // if this is a prepare txn, and load source type is not FRONTEND\n-                // no need to persist it. if prepare txn lost, the following commit will just be failed.\n-                // user only need to retry this txn.\n-                // The FRONTEND type txn is committed and running asynchronously, so we have to persist it.\n-                editLog.logInsertTransactionState(transactionState);\n-            }\n-        }\n-        if (!transactionState.getTransactionStatus().isFinalStatus()) {\n-            if (idToRunningTransactionState.put(transactionState.getTransactionId(), transactionState) == null) {\n-                if (transactionState.getSourceType() == TransactionState.LoadJobSourceType.ROUTINE_LOAD_TASK) {\n-                    runningRoutineLoadTxnNums++;\n-                } else {\n-                    runningTxnNums++;\n-                }\n-            }\n-        } else {\n-            if (idToRunningTransactionState.remove(transactionState.getTransactionId()) != null) {\n-                if (transactionState.getSourceType() == TransactionState.LoadJobSourceType.ROUTINE_LOAD_TASK) {\n-                    runningRoutineLoadTxnNums--;\n-                } else {\n-                    runningTxnNums--;\n-                }\n-            }\n-            idToFinalStatusTransactionState.put(transactionState.getTransactionId(), transactionState);\n-            finalStatusTransactionStateDeque.add(transactionState);\n-        }\n-        updateTxnLabels(transactionState);\n-    }\n-\n-    private void updateTxnLabels(TransactionState transactionState) {\n-        Set<Long> txnIds = labelToTxnIds.get(transactionState.getLabel());\n-        if (txnIds == null) {\n-            txnIds = Sets.newHashSet();\n-            labelToTxnIds.put(transactionState.getLabel(), txnIds);\n-        }\n-        txnIds.add(transactionState.getTransactionId());\n-    }\n-\n-    public void abortTransaction(String label, String reason) throws UserException {\n-        Preconditions.checkNotNull(label);\n-        long transactionId = -1;\n-        readLock();\n-        try {\n-            Set<Long> existingTxns = unprotectedGetTxnIdsByLabel(label);\n-            if (existingTxns == null || existingTxns.isEmpty()) {\n-                throw new TransactionNotFoundException(\"transaction not found, label=\" + label);\n-            }\n-            // find PREPARE txn. For one load label, there should be only one PREPARE txn.\n-            TransactionState prepareTxn = null;\n-            for (Long txnId : existingTxns) {\n-                TransactionState txn = unprotectedGetTransactionState(txnId);\n-                if (txn.getTransactionStatus() == TransactionStatus.PREPARE) {\n-                    prepareTxn = txn;\n-                    break;\n-                }\n-            }\n-\n-            if (prepareTxn == null) {\n-                throw new TransactionNotFoundException(\"running transaction not found, label=\" + label);\n-            }\n-\n-            transactionId = prepareTxn.getTransactionId();\n-        } finally {\n-            readUnlock();\n-        }\n-        abortTransaction(transactionId, reason, null);\n-    }\n-\n-    public void abortTransaction(long transactionId, String reason, TxnCommitAttachment txnCommitAttachment) throws UserException {\n-        if (transactionId < 0) {\n-            LOG.info(\"transaction id is {}, less than 0, maybe this is an old type load job, ignore abort operation\", transactionId);\n-            return;\n-        }\n-        TransactionState transactionState = null;\n-        readLock();\n-        try {\n-            transactionState = idToRunningTransactionState.get(transactionId);\n-        } finally {\n-            readUnlock();\n-        }\n-        if (transactionState == null) {\n-            throw new TransactionNotFoundException(\"transaction not found\", transactionId);\n-        }\n-\n-        // update transaction state extra if exists\n-        if (txnCommitAttachment != null) {\n-            transactionState.setTxnCommitAttachment(txnCommitAttachment);\n-        }\n-\n-        // before state transform\n-        transactionState.beforeStateTransform(TransactionStatus.ABORTED);\n-        boolean txnOperated = false;\n-        writeLock();\n-        try {\n-            txnOperated = unprotectAbortTransaction(transactionId, reason);\n-        } finally {\n-            writeUnlock();\n-            transactionState.afterStateTransform(TransactionStatus.ABORTED, txnOperated, reason);\n-        }\n-\n-        // send clear txn task to BE to clear the transactions on BE.\n-        // This is because parts of a txn may succeed in some BE, and these parts of txn should be cleared\n-        // explicitly, or it will be remained on BE forever\n-        // (However the report process will do the diff and send clear txn tasks to BE, but that is our\n-        // last defense)\n-        if (txnOperated && transactionState.getTransactionStatus() == TransactionStatus.ABORTED) {\n-            clearBackendTransactions(transactionState);\n-        }\n-    }\n-\n-    private boolean unprotectAbortTransaction(long transactionId, String reason)\n-            throws UserException {\n-        TransactionState transactionState = unprotectedGetTransactionState(transactionId);\n-        if (transactionState == null) {\n-            throw new TransactionNotFoundException(\"transaction not found\", transactionId);\n-        }\n-        if (transactionState.getTransactionStatus() == TransactionStatus.ABORTED) {\n-            return false;\n-        }\n-        if (transactionState.getTransactionStatus() == TransactionStatus.COMMITTED\n-                || transactionState.getTransactionStatus() == TransactionStatus.VISIBLE) {\n-            throw new UserException(\"transaction's state is already \"\n-                    + transactionState.getTransactionStatus() + \", could not abort\");\n-        }\n-        transactionState.setFinishTime(System.currentTimeMillis());\n-        transactionState.setReason(reason);\n-        transactionState.setTransactionStatus(TransactionStatus.ABORTED);\n-        unprotectUpsertTransactionState(transactionState, false);\n-        for (PublishVersionTask task : transactionState.getPublishVersionTasks().values()) {\n-            AgentTaskQueue.removeTask(task.getBackendId(), TTaskType.PUBLISH_VERSION, task.getSignature());\n-        }\n-        return true;\n-    }\n-\n-    private void clearBackendTransactions(TransactionState transactionState) {\n-        Preconditions.checkState(transactionState.getTransactionStatus() == TransactionStatus.ABORTED);\n-        // for aborted transaction, we don't know which backends are involved, so we have to send clear task\n-        // to all backends.\n-        List<Long> allBeIds = Catalog.getCurrentSystemInfo().getBackendIds(false);\n-        AgentBatchTask batchTask = null;\n-        synchronized (clearTransactionTasks) {\n-            for (Long beId : allBeIds) {\n-                ClearTransactionTask task = new ClearTransactionTask(beId, transactionState.getTransactionId(), Lists.newArrayList());\n-                clearTransactionTasks.add(task);\n-            }\n-\n-            // try to group send tasks, not sending every time a txn is aborted. to avoid too many task rpc.\n-            if (clearTransactionTasks.size() > allBeIds.size() * 2) {\n-                batchTask = new AgentBatchTask();\n-                for (ClearTransactionTask clearTransactionTask : clearTransactionTasks) {\n-                    batchTask.addTask(clearTransactionTask);\n-                }\n-                clearTransactionTasks.clear();\n-            }\n-        }\n-\n-        if (batchTask != null) {\n-            AgentTaskExecutor.submit(batchTask);\n-        }\n-    }\n-\n-\n-    protected List<List<Comparable>> getTableTransInfo(long txnId) throws AnalysisException {\n-        List<List<Comparable>> tableInfos = new ArrayList<>();\n-        readLock();\n-        try {\n-            TransactionState transactionState = unprotectedGetTransactionState(txnId);\n-            if (null == transactionState) {\n-                throw new AnalysisException(\"Transaction[\" + txnId + \"] does not exist.\");\n-            }\n-\n-            for (Map.Entry<Long, TableCommitInfo> entry : transactionState.getIdToTableCommitInfos().entrySet()) {\n-                List<Comparable> tableInfo = new ArrayList<>();\n-                tableInfo.add(entry.getKey());\n-                tableInfo.add(Joiner.on(\", \").join(entry.getValue().getIdToPartitionCommitInfo().values().stream().map(\n-                        PartitionCommitInfo::getPartitionId).collect(Collectors.toList())));\n-                tableInfos.add(tableInfo);\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return tableInfos;\n-    }\n-\n-    protected List<List<Comparable>> getPartitionTransInfo(long txnId, long tableId) throws AnalysisException {\n-        List<List<Comparable>> partitionInfos = new ArrayList<List<Comparable>>();\n-        readLock();\n-        try {\n-            TransactionState transactionState = unprotectedGetTransactionState(txnId);\n-            if (null == transactionState) {\n-                throw new AnalysisException(\"Transaction[\" + txnId + \"] does not exist.\");\n-            }\n-\n-            TableCommitInfo tableCommitInfo = transactionState.getIdToTableCommitInfos().get(tableId);\n-            Map<Long, PartitionCommitInfo> idToPartitionCommitInfo = tableCommitInfo.getIdToPartitionCommitInfo();\n-            for (Map.Entry<Long, PartitionCommitInfo> entry : idToPartitionCommitInfo.entrySet()) {\n-                List<Comparable> partitionInfo = new ArrayList<Comparable>();\n-                partitionInfo.add(entry.getKey());\n-                partitionInfo.add(entry.getValue().getVersion());\n-                partitionInfo.add(entry.getValue().getVersionHash());\n-                partitionInfos.add(partitionInfo);\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return partitionInfos;\n-    }\n-\n-    public void removeExpiredTxns(long currentMillis) {\n-        writeLock();\n-        try {\n-            while (!finalStatusTransactionStateDeque.isEmpty()) {\n-                TransactionState transactionState = finalStatusTransactionStateDeque.getFirst();\n-                if (transactionState.isExpired(currentMillis)) {\n-                    finalStatusTransactionStateDeque.pop();\n-                    Set<Long> txnIds = unprotectedGetTxnIdsByLabel(transactionState.getLabel());\n-                    txnIds.remove(transactionState.getTransactionId());\n-                    if (txnIds.isEmpty()) {\n-                        labelToTxnIds.remove(transactionState.getLabel());\n-                    }\n-                    editLog.logDeleteTransactionState(transactionState);\n-                    LOG.info(\"transaction [\" + transactionState.getTransactionId() + \"] is expired, remove it from transaction manager\");\n-                } else {\n-                    break;\n-                }\n-\n-            }\n-        } finally {\n-            writeUnlock();\n-        }\n-    }\n-\n-    public int getTransactionNum() {\n-        return idToRunningTransactionState.size() + finalStatusTransactionStateDeque.size();\n-    }\n-\n-\n-    public TransactionState getTransactionStateByCallbackIdAndStatus(long callbackId, Set<TransactionStatus> status) {\n-        readLock();\n-        try {\n-            for (TransactionState txn : idToRunningTransactionState.values()) {\n-                if (txn.getCallbackId() == callbackId && status.contains(txn.getTransactionStatus())) {\n-                    return txn;\n-                }\n-            }\n-            for (TransactionState txn : finalStatusTransactionStateDeque) {\n-                if (txn.getCallbackId() == callbackId && status.contains(txn.getTransactionStatus())) {\n-                    return txn;\n-                }\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return null;\n-    }\n-\n-    public TransactionState getTransactionStateByCallbackId(long callbackId) {\n-        readLock();\n-        try {\n-            for (TransactionState txn : idToRunningTransactionState.values()) {\n-                if (txn.getCallbackId() == callbackId) {\n-                    return txn;\n-                }\n-            }\n-            for (TransactionState txn : finalStatusTransactionStateDeque) {\n-                if (txn.getCallbackId() == callbackId) {\n-                    return txn;\n-                }\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return null;\n-    }\n-\n-    public List<Pair<Long, Long>> getTransactionIdByCoordinateBe(String coordinateHost, int limit) {\n-        ArrayList<Pair<Long, Long>> txnInfos = new ArrayList<>();\n-        readLock();\n-        try {\n-            idToRunningTransactionState.values().stream()\n-                    .filter(t -> (t.getCoordinator().sourceType == TransactionState.TxnSourceType.BE\n-                            && t.getCoordinator().ip.equals(coordinateHost)))\n-                    .limit(limit)\n-                    .forEach(t -> txnInfos.add(new Pair<>(t.getDbId(), t.getTransactionId())));\n-        } finally {\n-            readUnlock();\n-        }\n-        return txnInfos;\n-    }\n-\n-    // get show info of a specified txnId\n-    public List<List<String>> getSingleTranInfo(long dbId, long txnId) throws AnalysisException {\n-        List<List<String>> infos = new ArrayList<List<String>>();\n-        readLock();\n-        try {\n-            Database db = Catalog.getCurrentCatalog().getDb(dbId);\n-            if (db == null) {\n-                throw new AnalysisException(\"Database[\" + dbId + \"] does not exist\");\n-            }\n-\n-            TransactionState txnState = unprotectedGetTransactionState(txnId);\n-            if (txnState == null) {\n-                throw new AnalysisException(\"transaction with id \" + txnId + \" does not exist\");\n-            }\n-\n-            if (ConnectContext.get() != null) {\n-                // check auth\n-                Set<Long> tblIds = txnState.getIdToTableCommitInfos().keySet();\n-                for (Long tblId : tblIds) {\n-                    Table tbl = db.getTable(tblId);\n-                    if (tbl != null) {\n-                        if (!Catalog.getCurrentCatalog().getAuth().checkTblPriv(ConnectContext.get(), db.getFullName(),\n-                                tbl.getName(), PrivPredicate.SHOW)) {\n-                            ErrorReport.reportAnalysisException(ErrorCode.ERR_TABLEACCESS_DENIED_ERROR,\n-                                    \"SHOW TRANSACTION\",\n-                                    ConnectContext.get().getQualifiedUser(),\n-                                    ConnectContext.get().getRemoteIP(),\n-                                    tbl.getName());\n-                        }\n-                    }\n-                }\n-            }\n-\n-            List<String> info = Lists.newArrayList();\n-            getTxnStateInfo(txnState, info);\n-            infos.add(info);\n-        } finally {\n-            readUnlock();\n-        }\n-        return infos;\n-    }\n-\n-    protected void checkRunningTxnExceedLimit(TransactionState.LoadJobSourceType sourceType) throws BeginTransactionException {\n-        switch (sourceType) {\n-            case ROUTINE_LOAD_TASK:\n-                // no need to check limit for routine load task:\n-                // 1. the number of running routine load tasks is limited by Config.max_routine_load_task_num_per_be\n-                // 2. if we add routine load txn to runningTxnNums, runningTxnNums will always be occupied by routine load,\n-                //    and other txn may not be able to submitted.\n-                break;\n-            default:\n-                if (runningTxnNums >= Config.max_running_txn_num_per_db) {\n-                    throw new BeginTransactionException(\"current running txns on db \" + dbId + \" is \"\n-                            + runningTxnNums + \", larger than limit \" + Config.max_running_txn_num_per_db);\n-                }\n-                break;\n-        }\n-    }\n-\n-    private void updateCatalogAfterCommitted(TransactionState transactionState, Database db) {\n-        Set<Long> errorReplicaIds = transactionState.getErrorReplicas();\n-        for (TableCommitInfo tableCommitInfo : transactionState.getIdToTableCommitInfos().values()) {\n-            long tableId = tableCommitInfo.getTableId();\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            for (PartitionCommitInfo partitionCommitInfo : tableCommitInfo.getIdToPartitionCommitInfo().values()) {\n-                long partitionId = partitionCommitInfo.getPartitionId();\n-                Partition partition = table.getPartition(partitionId);\n-                List<MaterializedIndex> allIndices = partition.getMaterializedIndices(MaterializedIndex.IndexExtState.ALL);\n-                for (MaterializedIndex index : allIndices) {\n-                    List<Tablet> tablets = index.getTablets();\n-                    for (Tablet tablet : tablets) {\n-                        for (Replica replica : tablet.getReplicas()) {\n-                            if (errorReplicaIds.contains(replica.getId())) {\n-                                // should not use partition.getNextVersion and partition.getNextVersionHash because partition's next version hash is generated locally\n-                                // should get from transaction state\n-                                replica.updateLastFailedVersion(partitionCommitInfo.getVersion(),\n-                                        partitionCommitInfo.getVersionHash());\n-                            }\n-                        }\n-                    }\n-                }\n-                partition.setNextVersion(partition.getNextVersion() + 1);\n-                // Although committed version(hash) is not visible to user,\n-                // but they need to be synchronized among Frontends.\n-                // because we use committed version(hash) to create clone task, if the first Master FE\n-                // send clone task with committed version hash X, and than Master changed, the new Master FE\n-                // received the clone task report with version hash X, which not equals to it own committed\n-                // version hash, than the clone task is failed.\n-                partition.setNextVersionHash(Util.generateVersionHash() /* next version hash */,\n-                        partitionCommitInfo.getVersionHash() /* committed version hash*/);\n-            }\n-        }\n-    }\n-\n-    private boolean updateCatalogAfterVisible(TransactionState transactionState, Database db) {\n-        Set<Long> errorReplicaIds = transactionState.getErrorReplicas();\n-        for (TableCommitInfo tableCommitInfo : transactionState.getIdToTableCommitInfos().values()) {\n-            long tableId = tableCommitInfo.getTableId();\n-            OlapTable table = (OlapTable) db.getTable(tableId);\n-            for (PartitionCommitInfo partitionCommitInfo : tableCommitInfo.getIdToPartitionCommitInfo().values()) {\n-                long partitionId = partitionCommitInfo.getPartitionId();\n-                long newCommitVersion = partitionCommitInfo.getVersion();\n-                long newCommitVersionHash = partitionCommitInfo.getVersionHash();\n-                Partition partition = table.getPartition(partitionId);\n-                List<MaterializedIndex> allIndices = partition.getMaterializedIndices(MaterializedIndex.IndexExtState.ALL);\n-                for (MaterializedIndex index : allIndices) {\n-                    for (Tablet tablet : index.getTablets()) {\n-                        for (Replica replica : tablet.getReplicas()) {\n-                            long lastFailedVersion = replica.getLastFailedVersion();\n-                            long lastFailedVersionHash = replica.getLastFailedVersionHash();\n-                            long newVersion = newCommitVersion;\n-                            long newVersionHash = newCommitVersionHash;\n-                            long lastSucessVersion = replica.getLastSuccessVersion();\n-                            long lastSuccessVersionHash = replica.getLastSuccessVersionHash();\n-                            if (!errorReplicaIds.contains(replica.getId())) {\n-                                if (replica.getLastFailedVersion() > 0) {\n-                                    // if the replica is a failed replica, then not changing version and version hash\n-                                    newVersion = replica.getVersion();\n-                                    newVersionHash = replica.getVersionHash();\n-                                } else if (!replica.checkVersionCatchUp(partition.getVisibleVersion(),\n-                                        partition.getVisibleVersionHash(), true)) {\n-                                    // this means the replica has error in the past, but we did not observe it\n-                                    // during upgrade, one job maybe in quorum finished state, for example, A,B,C 3 replica\n-                                    // A,B 's version is 10, C's version is 10 but C' 10 is abnormal should be rollback\n-                                    // then we will detect this and set C's last failed version to 10 and last success version to 11\n-                                    // this logic has to be replayed in checkpoint thread\n-                                    lastFailedVersion = partition.getVisibleVersion();\n-                                    lastFailedVersionHash = partition.getVisibleVersionHash();\n-                                    newVersion = replica.getVersion();\n-                                    newVersionHash = replica.getVersionHash();\n-                                }\n-\n-                                // success version always move forward\n-                                lastSucessVersion = newCommitVersion;\n-                                lastSuccessVersionHash = newCommitVersionHash;\n-                            } else {\n-                                // for example, A,B,C 3 replicas, B,C failed during publish version, then B C will be set abnormal\n-                                // all loading will failed, B,C will have to recovery by clone, it is very inefficient and maybe lost data\n-                                // Using this method, B,C will publish failed, and fe will publish again, not update their last failed version\n-                                // if B is publish successfully in next turn, then B is normal and C will be set abnormal so that quorum is maintained\n-                                // and loading will go on.\n-                                newVersion = replica.getVersion();\n-                                newVersionHash = replica.getVersionHash();\n-                                if (newCommitVersion > lastFailedVersion) {\n-                                    lastFailedVersion = newCommitVersion;\n-                                    lastFailedVersionHash = newCommitVersionHash;\n-                                }\n-                            }\n-                            replica.updateVersionInfo(newVersion, newVersionHash, lastFailedVersion, lastFailedVersionHash, lastSucessVersion, lastSuccessVersionHash);\n-                        }\n-                    }\n-                } // end for indices\n-                long version = partitionCommitInfo.getVersion();\n-                long versionHash = partitionCommitInfo.getVersionHash();\n-                partition.updateVisibleVersionAndVersionHash(version, versionHash);\n-                if (LOG.isDebugEnabled()) {\n-                    LOG.debug(\"transaction state {} set partition {}'s version to [{}] and version hash to [{}]\",\n-                            transactionState, partition.getId(), version, versionHash);\n-                }\n-            }\n-        }\n-        return true;\n-    }\n-\n-    public boolean isPreviousTransactionsFinished(long endTransactionId, List<Long> tableIdList) {\n-        readLock();\n-        try {\n-            for (Map.Entry<Long, TransactionState> entry : idToRunningTransactionState.entrySet()) {\n-                if (entry.getValue().getDbId() != dbId || !isIntersectionNotEmpty(entry.getValue().getTableIdList(),\n-                        tableIdList) || !entry.getValue().isRunning()) {\n-                    continue;\n-                }\n-                if (entry.getKey() <= endTransactionId) {\n-                    LOG.debug(\"find a running txn with txn_id={} on db: {}, less than watermark txn_id {}\",\n-                            entry.getKey(), dbId, endTransactionId);\n-                    return false;\n-                }\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return true;\n-    }\n-\n-    /**\n-     * check if there exists a intersection between the source tableId list and target tableId list\n-     * if one of them is null or empty, that means that we don't know related tables in tableList,\n-     * we think the two lists may have intersection for right ordered txns\n-     */\n-    public boolean isIntersectionNotEmpty(List<Long> sourceTableIdList, List<Long> targetTableIdList) {\n-        if (CollectionUtils.isEmpty(sourceTableIdList) || CollectionUtils.isEmpty(targetTableIdList)) {\n-            return true;\n-        }\n-        for (Long srcValue : sourceTableIdList) {\n-            for (Long targetValue : targetTableIdList) {\n-                if (srcValue.equals(targetValue)) {\n-                    return true;\n-                }\n-            }\n-        }\n-        return false;\n-    }\n-\n-\n-    public List<Long> getTimeoutTxns(long currentMillis) {\n-        List<Long> timeoutTxns = Lists.newArrayList();\n-        readLock();\n-        try {\n-            for (TransactionState transactionState : idToRunningTransactionState.values()) {\n-                if (transactionState.isTimeout(currentMillis)) {\n-                    // txn is running but timeout, abort it.\n-                    timeoutTxns.add(transactionState.getTransactionId());\n-                }\n-            }\n-        } finally {\n-            readUnlock();\n-        }\n-        return timeoutTxns;\n-    }\n-\n-    public void removeExpiredAndTimeoutTxns(long currentMillis) {\n-        removeExpiredTxns(currentMillis);\n-        List<Long> timeoutTxns = getTimeoutTxns(currentMillis);\n-        // abort timeout txns\n-        for (Long txnId : timeoutTxns) {\n-            try {\n-                abortTransaction(txnId, \"timeout by txn manager\", null);\n-                LOG.info(\"transaction [\" + txnId + \"] is timeout, abort it by transaction manager\");\n-            } catch (UserException e) {\n-                // abort may be failed. it is acceptable. just print a log\n-                LOG.warn(\"abort timeout txn {} failed. msg: {}\", txnId, e.getMessage());\n-            }\n-        }\n-    }\n-\n-    public void replayUpsertTransactionState(TransactionState transactionState) {\n-        writeLock();\n-        try {\n-            // set transaction status will call txn state change listener\n-            transactionState.replaySetTransactionStatus();\n-            Database db = catalog.getDb(transactionState.getDbId());\n-            if (transactionState.getTransactionStatus() == TransactionStatus.COMMITTED) {\n-                LOG.info(\"replay a committed transaction {}\", transactionState);\n-                updateCatalogAfterCommitted(transactionState, db);\n-            } else if (transactionState.getTransactionStatus() == TransactionStatus.VISIBLE) {\n-                LOG.info(\"replay a visible transaction {}\", transactionState);\n-                updateCatalogAfterVisible(transactionState, db);\n-            }\n-            unprotectUpsertTransactionState(transactionState, true);\n-        } finally {\n-            writeUnlock();\n-        }\n-    }\n-\n-    public List<List<String>> getDbTransStateInfo() {\n-        List<List<String>> infos = Lists.newArrayList();\n-        readLock();\n-        try {\n-            infos.add(Lists.newArrayList(\"running\", String.valueOf(\n-                    runningTxnNums + runningRoutineLoadTxnNums)));\n-            long finishedNum = getFinishedTxnNums();\n-            infos.add(Lists.newArrayList(\"finished\", String.valueOf(finishedNum)));\n-        } finally {\n-            readUnlock();\n-        }\n-        return infos;\n-    }\n-\n-    public void unprotectWriteAllTransactionStates(DataOutput out) throws IOException {\n-        for (Map.Entry<Long, TransactionState> entry : idToRunningTransactionState.entrySet()) {\n-            entry.getValue().write(out);\n-        }\n-\n-        for (TransactionState transactionState : finalStatusTransactionStateDeque) {\n-            transactionState.write(out);\n-        }\n-    }\n-\n-}\n"}}, {"oid": "923b16566ea129734e44219f6ba3d78a53746987", "url": "https://github.com/apache/incubator-doris/commit/923b16566ea129734e44219f6ba3d78a53746987", "message": "Support checking database used data quota when begin a new txn", "committedDate": "2020-07-23T03:04:15Z", "type": "commit"}, {"oid": "fbb80222c437de337e5a16370d16987e11497b7c", "url": "https://github.com/apache/incubator-doris/commit/fbb80222c437de337e5a16370d16987e11497b7c", "message": "Update doc content for enable_check_data_quota_on_load and db_used_data_quota_update_interval_secs config", "committedDate": "2020-07-23T03:04:15Z", "type": "commit"}, {"oid": "e25de031552089bd1d6b8f4b7fa700147469d03b", "url": "https://github.com/apache/incubator-doris/commit/e25de031552089bd1d6b8f4b7fa700147469d03b", "message": "Add debug log for UpdateDbUsedDataQuotaDaemon", "committedDate": "2020-07-23T03:04:15Z", "type": "commit"}, {"oid": "ac2383f6828f479ccbf0944f54b78e150db7177f", "url": "https://github.com/apache/incubator-doris/commit/ac2383f6828f479ccbf0944f54b78e150db7177f", "message": "fix by review", "committedDate": "2020-07-23T03:04:15Z", "type": "commit"}, {"oid": "9af3c7fde26caa75dfdc005ae0dfdbc0e020a4cc", "url": "https://github.com/apache/incubator-doris/commit/9af3c7fde26caa75dfdc005ae0dfdbc0e020a4cc", "message": "remove enable_check_data_quota_on_load config", "committedDate": "2020-07-23T03:04:15Z", "type": "commit"}, {"oid": "9af3c7fde26caa75dfdc005ae0dfdbc0e020a4cc", "url": "https://github.com/apache/incubator-doris/commit/9af3c7fde26caa75dfdc005ae0dfdbc0e020a4cc", "message": "remove enable_check_data_quota_on_load config", "committedDate": "2020-07-23T03:04:15Z", "type": "forcePushed"}, {"oid": "2d6e17e3a9df680e71cbb1508eda75e416b4b1db", "url": "https://github.com/apache/incubator-doris/commit/2d6e17e3a9df680e71cbb1508eda75e416b4b1db", "message": "fix compile failed", "committedDate": "2020-07-23T03:25:19Z", "type": "commit"}]}