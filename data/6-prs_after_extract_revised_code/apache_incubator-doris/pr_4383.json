{"pr_number": 4383, "pr_title": "[SparkLoad]Use the yarn command to get status and kill the application", "pr_createdAt": "2020-08-18T08:07:16Z", "pr_url": "https://github.com/apache/incubator-doris/pull/4383", "timeline": [{"oid": "9e3f0fedd25ea52eccdbb4eb1820a7753cb7a853", "url": "https://github.com/apache/incubator-doris/commit/9e3f0fedd25ea52eccdbb4eb1820a7753cb7a853", "message": "save code", "committedDate": "2020-08-17T13:29:37Z", "type": "commit"}, {"oid": "b806b0b93add0f96c2f85ad989fe12d9fe2c1113", "url": "https://github.com/apache/incubator-doris/commit/b806b0b93add0f96c2f85ad989fe12d9fe2c1113", "message": "save code", "committedDate": "2020-08-18T07:52:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA5MDc0NQ==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r472090745", "bodyText": "Preconditions.checkState(entry.size() <= 2, line);", "author": "morningman", "createdAt": "2020-08-18T10:55:03Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java", "diffHunk": "@@ -0,0 +1,121 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.common.LoadException;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.Maps;\n+\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl;\n+import org.apache.hadoop.yarn.util.ConverterUtils;\n+\n+import java.text.NumberFormat;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Covert output string of command `yarn application -status` to application report.\n+ * Input sample:\n+ * -------------------\n+ * Application Report :\n+ * \tApplication-Id : application_1573630236805_6763648\n+ * \tApplication-Name : doris_label_test\n+ * \tApplication-Type : SPARK-2.4.1\n+ * \tUser : test\n+ * \tQueue : test-queue\n+ * \tStart-Time : 1597654469958\n+ * \tFinish-Time : 1597654801939\n+ * \tProgress : 100%\n+ * \tState : FINISHED\n+ * \tFinal-State : SUCCEEDED\n+ * \tTracking-URL : 127.0.0.1:8004/history/application_1573630236805_6763648/1\n+ * \tRPC Port : 40236\n+ * \tAM Host : host-name\n+ * \t------------------\n+ *\n+ * \tOutput:\n+ * \tApplicationReport\n+ */\n+public class YarnApplicationReport {\n+    private static final String APPLICATION_ID = \"Application-Id\";\n+    private static final String APPLICATION_TYPE = \"Application-Type\";\n+    private static final String APPLICATION_NAME = \"Application-Name\";\n+    private static final String USER = \"User\";\n+    private static final String QUEUE = \"Queue\";\n+    private static final String START_TIME = \"Start-Time\";\n+    private static final String FINISH_TIME = \"Finish-Time\";\n+    private static final String PROGRESS = \"Progress\";\n+    private static final String STATE = \"State\";\n+    private static final String FINAL_STATE = \"Final-State\";\n+    private static final String TRACKING_URL = \"Tracking-URL\";\n+    private static final String RPC_PORT = \"RPC Port\";\n+    private static final String AM_HOST = \"AM Host\";\n+    private static final String DIAGNOSTICS = \"Diagnostics\";\n+\n+    private ApplicationReport report;\n+\n+    public YarnApplicationReport(String output) throws LoadException {\n+        this.report = new ApplicationReportPBImpl();\n+        parseFromOutput(output);\n+    }\n+\n+    public ApplicationReport getReport() {\n+        return report;\n+    }\n+\n+    private void parseFromOutput(String output) throws LoadException {\n+        Map<String, String> reportMap = Maps.newHashMap();\n+        List<String> lines = Splitter.onPattern(\"\\n\").trimResults().splitToList(output);\n+        // Application-Id : application_1573630236805_6763648 ==> (Application-Id, application_1573630236805_6763648)\n+        for (String line : lines) {\n+            List<String> entry = Splitter.onPattern(\":\").limit(2).trimResults().splitToList(line);\n+            Preconditions.checkState(entry.size() <= 2);", "originalCommit": "b806b0b93add0f96c2f85ad989fe12d9fe2c1113", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjgzNjcxNQ==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r472836715", "bodyText": "changed", "author": "xy720", "createdAt": "2020-08-19T08:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA5MDc0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "6ea1493fa405059ba4a1dce8f0574326e25e7c61", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java\nindex 3270fd7d9..cd540c280 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java\n\n@@ -89,7 +89,7 @@ public class YarnApplicationReport {\n         // Application-Id : application_1573630236805_6763648 ==> (Application-Id, application_1573630236805_6763648)\n         for (String line : lines) {\n             List<String> entry = Splitter.onPattern(\":\").limit(2).trimResults().splitToList(line);\n-            Preconditions.checkState(entry.size() <= 2);\n+            Preconditions.checkState(entry.size() <= 2, line);\n             if (entry.size() > 1) {\n                 reportMap.put(entry.get(0), entry.get(1));\n             } else {\n"}}, {"oid": "6ea1493fa405059ba4a1dce8f0574326e25e7c61", "url": "https://github.com/apache/incubator-doris/commit/6ea1493fa405059ba4a1dce8f0574326e25e7c61", "message": "add ut", "committedDate": "2020-08-19T08:03:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzUxNjA3NA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r473516074", "bodyText": "immutable", "author": "morningman", "createdAt": "2020-08-20T01:36:50Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -543,6 +543,15 @@\n     @ConfField\n     public static String spark_resource_path = \"\";\n \n+    /**\n+     * Default yarn client path\n+     */\n+    @ConfField(mutable = true, masterOnly = true)", "originalCommit": "6ea1493fa405059ba4a1dce8f0574326e25e7c61", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c7f00c00bd736231392289d8db7712a86114f068", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/common/Config.java b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\nindex b9db19af7..df94bf9c8 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\n\n@@ -546,7 +546,6 @@ public class Config extends ConfigBase {\n     /**\n      * Default yarn client path\n      */\n-    @ConfField(mutable = true, masterOnly = true)\n     public static String yarn_client_path = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-client/hadoop/bin/yarn\";\n \n     @ConfField\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzUxNzI0OA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r473517248", "bodyText": "Better to create a function called getYarnClienthPath() and check if the binary file exist in that function.", "author": "morningman", "createdAt": "2020-08-20T01:38:32Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -262,19 +273,22 @@ public EtlStatus getEtlJobStatus(SparkAppHandle handle, String appId, long loadJ\n         return status;\n     }\n \n-    public void killEtlJob(SparkAppHandle handle, String appId, long loadJobId, SparkResource resource) {\n+    public void killEtlJob(SparkAppHandle handle, String appId, long loadJobId, SparkResource resource) throws LoadException {\n         if (resource.isYarnMaster()) {\n             Preconditions.checkNotNull(appId);\n-            YarnClient client = startYarnClient(resource);\n-            try {\n-                try {\n-                    client.killApplication(ConverterUtils.toApplicationId(appId));\n-                    LOG.info(\"yarn application -kill {}\", appId);\n-                } catch (YarnException | IOException e) {\n-                    LOG.warn(\"yarn application kill failed. app id: {}, load job id: {}\", appId, loadJobId, e);\n-                }\n-            } finally {\n-                stopYarnClient(client);\n+            // prepare yarn config\n+            String configDir = resource.prepareYarnConfig();\n+            // yarn client path\n+            String yarnClient = Config.yarn_client_path;", "originalCommit": "6ea1493fa405059ba4a1dce8f0574326e25e7c61", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MjQxNA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477152414", "bodyText": "ok", "author": "xy720", "createdAt": "2020-08-26T09:10:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzUxNzI0OA=="}], "type": "inlineReview", "revised_code": {"commit": "c7f00c00bd736231392289d8db7712a86114f068", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\nindex 48941e8cb..ce03d0caa 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n\n@@ -273,7 +264,7 @@ public class SparkEtlJobHandler {\n         return status;\n     }\n \n-    public void killEtlJob(SparkAppHandle handle, String appId, long loadJobId, SparkResource resource) throws LoadException {\n+    public void killEtlJob(SparkLoadAppHandle handle, String appId, long loadJobId, SparkResource resource) throws LoadException {\n         if (resource.isYarnMaster()) {\n             Preconditions.checkNotNull(appId);\n             // prepare yarn config\n"}}, {"oid": "c7f00c00bd736231392289d8db7712a86114f068", "url": "https://github.com/apache/incubator-doris/commit/c7f00c00bd736231392289d8db7712a86114f068", "message": "save code", "committedDate": "2020-08-25T10:26:08Z", "type": "commit"}, {"oid": "635e1152170e14daec5170b79607beb3f93a6d97", "url": "https://github.com/apache/incubator-doris/commit/635e1152170e14daec5170b79607beb3f93a6d97", "message": "license", "committedDate": "2020-08-25T10:36:35Z", "type": "commit"}, {"oid": "1eb1a7143a58f5257066180a7f036b7ad54917a3", "url": "https://github.com/apache/incubator-doris/commit/1eb1a7143a58f5257066180a7f036b7ad54917a3", "message": "save code", "committedDate": "2020-08-25T10:49:34Z", "type": "commit"}, {"oid": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "url": "https://github.com/apache/incubator-doris/commit/f85f6926f33caf0b0297180466ec98b5ff8176e0", "message": "save code", "committedDate": "2020-08-25T11:06:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ0NjQwNg==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476446406", "bodyText": "Remove unused code", "author": "morningman", "createdAt": "2020-08-25T13:26:26Z", "path": "fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java", "diffHunk": "@@ -95,94 +132,103 @@ public void setUp() {\n                 .SparkLibrary(\"\", \"\", SparkRepository.SparkLibrary.LibType.SPARK2X, 0L));\n     }\n \n-    @Test\n-    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n-                                 @Injectable SparkAppHandle handle) throws IOException, LoadException {\n-        new Expectations() {\n-            {\n-                launcher.startApplication((SparkAppHandle.Listener) any);\n-                result = handle;\n-                handle.getAppId();\n-                returns(null, null, appId);\n-                handle.getState();\n-                returns(State.CONNECTED, State.SUBMITTED, State.RUNNING);\n-            }\n-        };\n-\n-        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n-        SparkResource resource = new SparkResource(resourceName);\n-        new Expectations(resource) {\n-            {\n-                resource.prepareArchive();\n-                result = archive;\n-            }\n-        };\n+//    @Test", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java b/fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java\nindex e70c63f0c..904a8b171 100644\n--- a/fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java\n+++ b/fe/fe-core/src/test/java/org/apache/doris/load/loadv2/SparkEtlJobHandlerTest.java\n\n@@ -132,79 +137,78 @@ public class SparkEtlJobHandlerTest {\n                 .SparkLibrary(\"\", \"\", SparkRepository.SparkLibrary.LibType.SPARK2X, 0L));\n     }\n \n-//    @Test\n-//    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n-//                                 @Injectable SparkLoadAppAppHandle handle) throws IOException, LoadException {\n-//        new Expectations() {\n-//            {\n-//                launcher.startApplication((SparkLoadAppAppHandle.Listener) any);\n-//                result = handle;\n-//                handle.getAppId();\n-//                returns(null, null, appId);\n-//                handle.getState();\n-//                returns(SparkLoadAppHandle.State.CONNECTED, SparkLoadAppHandle.State.SUBMITTED, SparkLoadAppHandle.State.RUNNING);\n-//            }\n-//        };\n-//\n-//        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n-//        SparkResource resource = new SparkResource(resourceName);\n-//        new Expectations(resource) {\n-//            {\n-//                resource.prepareArchive();\n-//                result = archive;\n-//            }\n-//        };\n-//\n-//        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n-//        sparkConfigs.put(\"spark.master\", \"yarn\");\n-//        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n-//        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n-//        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n-//        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n-//        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n-//        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n-//\n-//        // check submit etl job success\n-//        Assert.assertEquals(appId, attachment.getAppId());\n-//        Assert.assertEquals(handle, attachment.getHandle());\n-//    }\n-\n-//    @Test(expected = LoadException.class)\n-//    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher,\n-//                                       @Injectable SparkAppHandle handle) throws IOException, LoadException {\n-//        new Expectations() {\n-//            {\n-//                launcher.startApplication((SparkAppHandle.Listener) any);\n-//                result = handle;\n-//                handle.getAppId();\n-//                result = null;\n-//                handle.getState();\n-//                returns(State.CONNECTED, State.SUBMITTED, State.FAILED);\n-//            }\n-//        };\n-//\n-//        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n-//        SparkResource resource = new SparkResource(resourceName);\n-//        new Expectations(resource) {\n-//            {\n-//                resource.prepareArchive();\n-//                result = archive;\n-//            }\n-//        };\n-//\n-//        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n-//        sparkConfigs.put(\"spark.master\", \"yarn\");\n-//        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n-//        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n-//        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n-//        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n-//        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n-//        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n-//    }\n+    @Test\n+    public void testSubmitEtlJob(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n+                                 @Mocked SparkLoadAppHandle handle ) throws IOException, LoadException {\n+        new Expectations() {\n+            {\n+                launcher.launch();\n+                result = process;\n+                handle.getAppId();\n+                result = appId;\n+                handle.getState();\n+                result = SparkLoadAppHandle.State.RUNNING;\n+            }\n+        };\n+\n+        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n+        SparkResource resource = new SparkResource(resourceName);\n+        new Expectations(resource) {\n+            {\n+                resource.prepareArchive();\n+                result = archive;\n+            }\n+        };\n+\n+        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n+        sparkConfigs.put(\"spark.master\", \"yarn\");\n+        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n+        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n+        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n+        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n+        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n+\n+        // check submit etl job success\n+        Assert.assertEquals(appId, attachment.getAppId());\n+    }\n+\n+    @Test(expected = LoadException.class)\n+    public void testSubmitEtlJobFailed(@Mocked BrokerUtil brokerUtil, @Mocked SparkLauncher launcher, @Injectable Process process,\n+                                       @Mocked SparkLoadAppHandle handle) throws IOException, LoadException {\n+        new Expectations() {\n+            {\n+                launcher.launch();\n+                result = process;\n+                handle.getAppId();\n+                result = appId;\n+                handle.getState();\n+                result = SparkLoadAppHandle.State.FAILED;\n+            }\n+        };\n+\n+        EtlJobConfig etlJobConfig = new EtlJobConfig(Maps.newHashMap(), etlOutputPath, label, null);\n+        SparkResource resource = new SparkResource(resourceName);\n+        new Expectations(resource) {\n+            {\n+                resource.prepareArchive();\n+                result = archive;\n+            }\n+        };\n+\n+        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n+        sparkConfigs.put(\"spark.master\", \"yarn\");\n+        sparkConfigs.put(\"spark.submit.deployMode\", \"cluster\");\n+        sparkConfigs.put(\"spark.hadoop.yarn.resourcemanager.address\", \"127.0.0.1:9999\");\n+        BrokerDesc brokerDesc = new BrokerDesc(broker, Maps.newHashMap());\n+        SparkPendingTaskAttachment attachment = new SparkPendingTaskAttachment(pendingTaskId);\n+        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        handler.submitEtlJob(loadJobId, label, etlJobConfig, resource, brokerDesc, attachment);\n+    }\n \n     @Test\n     public void testGetEtlJobStatus(@Mocked BrokerUtil brokerUtil, @Mocked Util util, @Mocked CommandResult commandResult,\n-                                    @Mocked SparkYarnConfigFiles sparkYarnConfigFiles)\n+                                    @Mocked SparkYarnConfigFiles sparkYarnConfigFiles, @Mocked SparkLoadAppHandle handle)\n             throws IOException, UserException {\n \n         new Expectations() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ0NjU0Mg==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476446542", "bodyText": "Remove unused code", "author": "morningman", "createdAt": "2020-08-25T13:26:38Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -140,93 +136,96 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n                 .setAppName(String.format(ETL_JOB_NAME, loadLabel))\n                 .setSparkHome(sparkHome)\n                 .addAppArgs(jobConfigHdfsPath)\n-                .redirectError()\n-                .redirectOutput(new File(Config.sys_log_dir + \"/spark-submitter.log\"));\n+                .redirectError();\n+                //.redirectOutput(new File(Config.sys_log_dir + \"/spark-submitter.log\"));", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\nindex 9e8dda788..52de11da6 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n\n@@ -137,7 +138,6 @@ public class SparkEtlJobHandler {\n                 .setSparkHome(sparkHome)\n                 .addAppArgs(jobConfigHdfsPath)\n                 .redirectError();\n-                //.redirectOutput(new File(Config.sys_log_dir + \"/spark-submitter.log\"));\n \n         // spark configs\n         for (Map.Entry<String, String> entry : resource.getSparkConfigs().entrySet()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ1NDU1OA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476454558", "bodyText": "How to make sure the process is still alive here?", "author": "morningman", "createdAt": "2020-08-25T13:37:45Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java", "diffHunk": "@@ -0,0 +1,216 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.base.Strings;\n+\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+public class SparkLauncherMonitors {\n+    private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n+    // 5min\n+    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n+\n+    private LogMonitor logMonitor;\n+\n+    public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n+        return new LogMonitor(handle);\n+    }\n+\n+    private static SparkLoadAppHandle.State fromYarnState(YarnApplicationState yarnState) {\n+        switch (yarnState) {\n+            case SUBMITTED:\n+            case ACCEPTED:\n+                return SparkLoadAppHandle.State.SUBMITTED;\n+            case RUNNING:\n+                return SparkLoadAppHandle.State.RUNNING;\n+            case FINISHED:\n+                return SparkLoadAppHandle.State.FINISHED;\n+            case FAILED:\n+                return SparkLoadAppHandle.State.FAILED;\n+            case KILLED:\n+                return SparkLoadAppHandle.State.KILLED;\n+            default:\n+                // NEW NEW_SAVING\n+                return SparkLoadAppHandle.State.UNKNOWN;\n+        }\n+    }\n+\n+    public static class LogMonitor extends Thread {\n+        private final Process process;\n+        private SparkLoadAppHandle handle;\n+        private long submitTimeoutMs;\n+        private boolean isStop;\n+\n+        private static final String STATE = \"state\";\n+        private static final String QUEUE = \"queue\";\n+        private static final String START_TIME = \"start time\";\n+        private static final String FINAL_STATUS = \"final status\";\n+        private static final String URL = \"tracking URL\";\n+        private static final String USER = \"user\";\n+\n+        public LogMonitor(SparkLoadAppHandle handle) {\n+            this.handle = handle;\n+            this.process = handle.getProcess();\n+            this.isStop = false;\n+        }\n+\n+        public void setSubmitTimeoutMs(long submitTimeoutMs) {\n+            this.submitTimeoutMs = submitTimeoutMs;\n+        }\n+\n+        // Monitor the process's output\n+        @Override\n+        public void run() {\n+            BufferedReader outReader = null;\n+            String line = null;\n+            long startTime = System.currentTimeMillis();\n+            try {\n+                Preconditions.checkState(process.isAlive());", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MjkwNw==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477152907", "bodyText": "No need to make sure the process is alive. We can get output even if the process is not alive.", "author": "xy720", "createdAt": "2020-08-26T09:11:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ1NDU1OA=="}], "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\nindex 83806f3bf..fc38d344e 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n\n@@ -35,10 +35,6 @@ import java.util.regex.Pattern;\n \n public class SparkLauncherMonitors {\n     private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n-    // 5min\n-    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n-\n-    private LogMonitor logMonitor;\n \n     public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n         return new LogMonitor(handle);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ1NjA3Ng==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476456076", "bodyText": "You can add an example output line here, so that the reviewer can know what the line looks like.", "author": "morningman", "createdAt": "2020-08-25T13:39:53Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java", "diffHunk": "@@ -0,0 +1,216 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.base.Strings;\n+\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+public class SparkLauncherMonitors {\n+    private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n+    // 5min\n+    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n+\n+    private LogMonitor logMonitor;\n+\n+    public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n+        return new LogMonitor(handle);\n+    }\n+\n+    private static SparkLoadAppHandle.State fromYarnState(YarnApplicationState yarnState) {\n+        switch (yarnState) {\n+            case SUBMITTED:\n+            case ACCEPTED:\n+                return SparkLoadAppHandle.State.SUBMITTED;\n+            case RUNNING:\n+                return SparkLoadAppHandle.State.RUNNING;\n+            case FINISHED:\n+                return SparkLoadAppHandle.State.FINISHED;\n+            case FAILED:\n+                return SparkLoadAppHandle.State.FAILED;\n+            case KILLED:\n+                return SparkLoadAppHandle.State.KILLED;\n+            default:\n+                // NEW NEW_SAVING\n+                return SparkLoadAppHandle.State.UNKNOWN;\n+        }\n+    }\n+\n+    public static class LogMonitor extends Thread {\n+        private final Process process;\n+        private SparkLoadAppHandle handle;\n+        private long submitTimeoutMs;\n+        private boolean isStop;\n+\n+        private static final String STATE = \"state\";\n+        private static final String QUEUE = \"queue\";\n+        private static final String START_TIME = \"start time\";\n+        private static final String FINAL_STATUS = \"final status\";\n+        private static final String URL = \"tracking URL\";\n+        private static final String USER = \"user\";\n+\n+        public LogMonitor(SparkLoadAppHandle handle) {\n+            this.handle = handle;\n+            this.process = handle.getProcess();\n+            this.isStop = false;\n+        }\n+\n+        public void setSubmitTimeoutMs(long submitTimeoutMs) {\n+            this.submitTimeoutMs = submitTimeoutMs;\n+        }\n+\n+        // Monitor the process's output\n+        @Override\n+        public void run() {\n+            BufferedReader outReader = null;\n+            String line = null;\n+            long startTime = System.currentTimeMillis();\n+            try {\n+                Preconditions.checkState(process.isAlive());\n+                outReader = new BufferedReader(new InputStreamReader(process.getInputStream()));\n+                while (!isStop && (line = outReader.readLine()) != null) {\n+                    LOG.info(\"Monitor Log: \" + line);\n+                    // parse state and appId\n+                    if (line.contains(STATE)) {", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MzE0OA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477153148", "bodyText": "done", "author": "xy720", "createdAt": "2020-08-26T09:11:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ1NjA3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\nindex 83806f3bf..fc38d344e 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n\n@@ -35,10 +35,6 @@ import java.util.regex.Pattern;\n \n public class SparkLauncherMonitors {\n     private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n-    // 5min\n-    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n-\n-    private LogMonitor logMonitor;\n \n     public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n         return new LogMonitor(handle);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ1ODU3NQ==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476458575", "bodyText": "Add comment to explain the function of this class", "author": "morningman", "createdAt": "2020-08-25T13:43:24Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java", "diffHunk": "@@ -0,0 +1,216 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.base.Strings;\n+\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+public class SparkLauncherMonitors {", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\nindex 83806f3bf..fc38d344e 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n\n@@ -35,10 +35,6 @@ import java.util.regex.Pattern;\n \n public class SparkLauncherMonitors {\n     private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n-    // 5min\n-    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n-\n-    private LogMonitor logMonitor;\n \n     public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n         return new LogMonitor(handle);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ2NTE0Mg==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r476465142", "bodyText": "if the line contains \"STATE\", the while loop may be broken. So how to guarantee that this else if block can be ran?", "author": "morningman", "createdAt": "2020-08-25T13:52:07Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java", "diffHunk": "@@ -0,0 +1,216 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.base.Strings;\n+\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+public class SparkLauncherMonitors {\n+    private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n+    // 5min\n+    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n+\n+    private LogMonitor logMonitor;\n+\n+    public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n+        return new LogMonitor(handle);\n+    }\n+\n+    private static SparkLoadAppHandle.State fromYarnState(YarnApplicationState yarnState) {\n+        switch (yarnState) {\n+            case SUBMITTED:\n+            case ACCEPTED:\n+                return SparkLoadAppHandle.State.SUBMITTED;\n+            case RUNNING:\n+                return SparkLoadAppHandle.State.RUNNING;\n+            case FINISHED:\n+                return SparkLoadAppHandle.State.FINISHED;\n+            case FAILED:\n+                return SparkLoadAppHandle.State.FAILED;\n+            case KILLED:\n+                return SparkLoadAppHandle.State.KILLED;\n+            default:\n+                // NEW NEW_SAVING\n+                return SparkLoadAppHandle.State.UNKNOWN;\n+        }\n+    }\n+\n+    public static class LogMonitor extends Thread {\n+        private final Process process;\n+        private SparkLoadAppHandle handle;\n+        private long submitTimeoutMs;\n+        private boolean isStop;\n+\n+        private static final String STATE = \"state\";\n+        private static final String QUEUE = \"queue\";\n+        private static final String START_TIME = \"start time\";\n+        private static final String FINAL_STATUS = \"final status\";\n+        private static final String URL = \"tracking URL\";\n+        private static final String USER = \"user\";\n+\n+        public LogMonitor(SparkLoadAppHandle handle) {\n+            this.handle = handle;\n+            this.process = handle.getProcess();\n+            this.isStop = false;\n+        }\n+\n+        public void setSubmitTimeoutMs(long submitTimeoutMs) {\n+            this.submitTimeoutMs = submitTimeoutMs;\n+        }\n+\n+        // Monitor the process's output\n+        @Override\n+        public void run() {\n+            BufferedReader outReader = null;\n+            String line = null;\n+            long startTime = System.currentTimeMillis();\n+            try {\n+                Preconditions.checkState(process.isAlive());\n+                outReader = new BufferedReader(new InputStreamReader(process.getInputStream()));\n+                while (!isStop && (line = outReader.readLine()) != null) {\n+                    LOG.info(\"Monitor Log: \" + line);\n+                    // parse state and appId\n+                    if (line.contains(STATE)) {\n+                        SparkLoadAppHandle.State oldState = handle.getState();\n+                        SparkLoadAppHandle.State newState = oldState;\n+                        // 1. state\n+                        String state = regexGetState(line);\n+                        if (state != null) {\n+                            YarnApplicationState yarnState = YarnApplicationState.valueOf(state);\n+                            newState = fromYarnState(yarnState);\n+                            if (newState != oldState) {\n+                                handle.setState(newState);\n+                            }\n+                        }\n+                        // 2. appId\n+                        String appId = regexGetAppId(line);\n+                        if (appId != null) {\n+                            if (!appId.equals(handle.getAppId())) {\n+                                handle.setAppId(appId);\n+                            }\n+                        }\n+\n+                        LOG.info(\"spark appId that handle get is {}, state: {}\", handle.getAppId(), handle.getState().toString());\n+                        switch (newState) {\n+                            case UNKNOWN:\n+                            case CONNECTED:\n+                            case SUBMITTED:\n+                                // If the app stays in the UNKNOWN/CONNECTED/SUBMITTED state for more than submitTimeoutMs\n+                                // stop monitoring and kill the process\n+                                if (System.currentTimeMillis() - startTime > submitTimeoutMs) {\n+                                    isStop = true;\n+                                    handle.kill();\n+                                }\n+                                break;\n+                            case RUNNING:\n+                            case FINISHED:\n+                                // There's no need to parse all logs of handle process to get all the information.\n+                                // As soon as the state changes to RUNNING/KILLED/FAILED/FINISHED/LOST,\n+                                // stop monitoring but keep the process alive.\n+                                isStop = true;\n+                                break;\n+                            case KILLED:\n+                            case FAILED:\n+                            case LOST:\n+                                // If the state changes to KILLED/FAILED/LOST,\n+                                // stop monitoring and kill the process\n+                                isStop = true;\n+                                handle.kill();\n+                                break;\n+                            default:\n+                                Preconditions.checkState(false, \"wrong spark app state\");\n+                        }\n+                    }\n+                    // parse other values\n+                    else if (line.contains(QUEUE) || line.contains(START_TIME) || line.contains(FINAL_STATUS) ||", "originalCommit": "f85f6926f33caf0b0297180466ec98b5ff8176e0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2NTIxMg==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477165212", "bodyText": "The state's changing follows the rule of\n\nsubmited > running > finished / failed\nsubmitted > killed\nsubmitted > running > killed\nNormally, the laucher will periodically print the queue\u3001start time\u3001final status\u3001tracking url\u3001user logs in state submitted/runnning. So in case 2, the else if block may still not be ran when the while loop be broken.\nBut it's not very terrible that this else if block not be ran, beacuse the necessary value we need only contains appId and state. In this case, queue\u3001start time\u3001final status\u3001tracking url\u3001user is just missing.", "author": "xy720", "createdAt": "2020-08-26T09:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ2NTE0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "77deaef42479f84d2b3ee4240e541036996455c3", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\nindex 83806f3bf..fc38d344e 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n\n@@ -35,10 +35,6 @@ import java.util.regex.Pattern;\n \n public class SparkLauncherMonitors {\n     private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n-    // 5min\n-    private static final long SUBMIT_APP_TIMEOUT_MS = 300 * 1000;\n-\n-    private LogMonitor logMonitor;\n \n     public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n         return new LogMonitor(handle);\n"}}, {"oid": "77deaef42479f84d2b3ee4240e541036996455c3", "url": "https://github.com/apache/incubator-doris/commit/77deaef42479f84d2b3ee4240e541036996455c3", "message": "add ut", "committedDate": "2020-08-26T09:08:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1NzIwOQ==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477157209", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<String, String> sparkConfigs = Maps.newHashMap();\n          \n          \n            \n                    Map<String, String> sparkConfig = Maps.newHashMap();", "author": "wuyunfeng", "createdAt": "2020-08-26T09:16:21Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -243,6 +264,16 @@ protected void setProperties(Map<String, String> properties) throws DdlException\n         return sparkConfigs;\n     }\n \n+    private Map<String, String> getSparkHadoopConfigs(Map<String, String> properties) {\n+        Map<String, String> sparkConfigs = Maps.newHashMap();", "originalCommit": "77deaef42479f84d2b3ee4240e541036996455c3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fefbf7c860ac3584f59cb7974e86641dcac1533b", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\nindex e3bb1cb8f..35822cb57 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\n\n@@ -265,13 +265,13 @@ public class SparkResource extends Resource {\n     }\n \n     private Map<String, String> getSparkHadoopConfigs(Map<String, String> properties) {\n-        Map<String, String> sparkConfigs = Maps.newHashMap();\n+        Map<String, String> sparkConfig = Maps.newHashMap();\n         for (Map.Entry<String, String> entry : properties.entrySet()) {\n             if (entry.getKey().startsWith(SPARK_HADOOP_CONFIG_PREFIX)) {\n-                sparkConfigs.put(entry.getKey(), entry.getValue());\n+                sparkConfig.put(entry.getKey(), entry.getValue());\n             }\n         }\n-        return sparkConfigs;\n+        return sparkConfig;\n     }\n \n     private Map<String, String> getBrokerProperties(Map<String, String> properties) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1OTE2MA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477159160", "bodyText": "Can you add some comment ?", "author": "wuyunfeng", "createdAt": "2020-08-26T09:17:58Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/ConfigFile.java", "diffHunk": "@@ -0,0 +1,25 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.common.LoadException;\n+\n+public interface ConfigFile {", "originalCommit": "77deaef42479f84d2b3ee4240e541036996455c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4MjIwNA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477182204", "bodyText": "ok", "author": "xy720", "createdAt": "2020-08-26T09:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1OTE2MA=="}], "type": "inlineReview", "revised_code": {"commit": "fefbf7c860ac3584f59cb7974e86641dcac1533b", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/ConfigFile.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/ConfigFile.java\nindex de2536f48..17384a72c 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/ConfigFile.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/ConfigFile.java\n\n@@ -19,6 +19,9 @@ package org.apache.doris.load.loadv2;\n \n import org.apache.doris.common.LoadException;\n \n+// The config file required to run the yarn command.\n+// Each time before running the yarn command, we need to check that the\n+// config file exists in the specified path, and if not, create them.\n public interface ConfigFile {\n     public void createFile() throws LoadException;\n     public String getFilePath();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2MDQ5Mw==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477160493", "bodyText": "Maybe put statement Preconditions.checkState  outer the if statement?", "author": "wuyunfeng", "createdAt": "2020-08-26T09:20:15Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -140,93 +137,98 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n                 .setAppName(String.format(ETL_JOB_NAME, loadLabel))\n                 .setSparkHome(sparkHome)\n                 .addAppArgs(jobConfigHdfsPath)\n-                .redirectError()\n-                .redirectOutput(new File(Config.sys_log_dir + \"/spark-submitter.log\"));\n+                .redirectError();\n \n         // spark configs\n         for (Map.Entry<String, String> entry : resource.getSparkConfigs().entrySet()) {\n             launcher.setConf(entry.getKey(), entry.getValue());\n         }\n \n         // start app\n-        SparkAppHandle handle = null;\n+        SparkLoadAppHandle handle = null;\n         State state = null;\n         String appId = null;\n-        int retry = 0;\n         String errMsg = \"start spark app failed. error: \";\n         try {\n-            handle = launcher.startApplication(new SparkAppListener());\n+            Process process = launcher.launch();\n+            handle = new SparkLoadAppHandle(process);\n+            handle.addListener(new SparkAppListener());\n+            if (!FeConstants.runningUnitTest) {\n+                SparkLauncherMonitors.LogMonitor logMonitor = SparkLauncherMonitors.createLogMonitor(handle);\n+                logMonitor.setSubmitTimeoutMs(GET_APPID_TIMEOUT_MS);\n+                logMonitor.start();\n+                try {\n+                    logMonitor.join();\n+                } catch (InterruptedException e) {\n+                    logMonitor.interrupt();\n+                    throw new LoadException(errMsg + e.getMessage());\n+                }\n+            }\n+            appId = handle.getAppId();\n+            state = handle.getState();\n         } catch (IOException e) {\n             LOG.warn(errMsg, e);\n             throw new LoadException(errMsg + e.getMessage());\n         }\n \n-        while (retry++ < GET_APPID_MAX_RETRY_TIMES) {\n-            appId = handle.getAppId();\n-            if (appId != null) {\n-                break;\n-            }\n-\n-            // check state and retry\n-            state = handle.getState();\n-            if (fromSparkState(state) == TEtlState.CANCELLED) {\n-                throw new LoadException(errMsg + \"spark app state: \" + state.toString());\n-            }\n-            if (retry >= GET_APPID_MAX_RETRY_TIMES) {\n-                throw new LoadException(errMsg + \"wait too much time for getting appid. spark app state: \"\n-                                                + state.toString());\n-            }\n+        if (fromSparkState(state) == TEtlState.CANCELLED) {\n+            throw new LoadException(errMsg + \"spark app state: \" + state.toString() + \", loadJobId:\" + loadJobId);\n+        }\n \n-            // log\n-            if (retry % 10 == 0) {\n-                LOG.info(\"spark appid that handle get is null. load job id: {}, state: {}, retry times: {}\",\n-                         loadJobId, state.toString(), retry);\n-            }\n-            try {\n-                Thread.sleep(GET_APPID_SLEEP_MS);\n-            } catch (InterruptedException e) {\n-                LOG.warn(e.getMessage());\n-            }\n+        if (appId == null) {\n+            throw new LoadException(errMsg + \"Failed to get appId from handle. spark app state: \"\n+                    + state.toString() + \", loadJobId:\" + loadJobId);\n         }\n \n         // success\n         attachment.setAppId(appId);\n         attachment.setHandle(handle);\n     }\n \n-    public EtlStatus getEtlJobStatus(SparkAppHandle handle, String appId, long loadJobId, String etlOutputPath,\n-                                     SparkResource resource, BrokerDesc brokerDesc) {\n+    public EtlStatus getEtlJobStatus(SparkLoadAppHandle handle, String appId, long loadJobId, String etlOutputPath,\n+                                     SparkResource resource, BrokerDesc brokerDesc) throws LoadException {\n         EtlStatus status = new EtlStatus();\n \n         if (resource.isYarnMaster()) {\n-            // state from yarn\n             Preconditions.checkState(appId != null && !appId.isEmpty());", "originalCommit": "77deaef42479f84d2b3ee4240e541036996455c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4MjQ0Ng==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477182446", "bodyText": "done", "author": "xy720", "createdAt": "2020-08-26T09:57:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2MDQ5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "fefbf7c860ac3584f59cb7974e86641dcac1533b", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\nindex 52de11da6..ed2dc2338 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n\n@@ -189,8 +189,8 @@ public class SparkEtlJobHandler {\n                                      SparkResource resource, BrokerDesc brokerDesc) throws LoadException {\n         EtlStatus status = new EtlStatus();\n \n+        Preconditions.checkState(appId != null && !appId.isEmpty());\n         if (resource.isYarnMaster()) {\n-            Preconditions.checkState(appId != null && !appId.isEmpty());\n             // prepare yarn config\n             String configDir = resource.prepareYarnConfig();\n             // yarn client path\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2MTY1OA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477161658", "bodyText": "I do not like the xxxxs", "author": "wuyunfeng", "createdAt": "2020-08-26T09:22:14Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java", "diffHunk": "@@ -0,0 +1,231 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.base.Strings;\n+\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+public class SparkLauncherMonitors {", "originalCommit": "77deaef42479f84d2b3ee4240e541036996455c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4NTkyOA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477185928", "bodyText": "How about SparkLauncherMonitor?", "author": "xy720", "createdAt": "2020-08-26T10:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2MTY1OA=="}], "type": "inlineReview", "revised_code": {"commit": "0b77bab05dd9fc99bca5f5d9c43136e9c5fe5983", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitor.java\nsimilarity index 99%\nrename from fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\nrename to fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitor.java\nindex fc38d344e..628037d68 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitors.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkLauncherMonitor.java\n\n@@ -33,8 +33,8 @@ import java.util.List;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n-public class SparkLauncherMonitors {\n-    private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitors.class);\n+public class SparkLauncherMonitor {\n+    private static final Logger LOG = LogManager.getLogger(SparkLauncherMonitor.class);\n \n     public static LogMonitor createLogMonitor(SparkLoadAppHandle handle) {\n         return new LogMonitor(handle);\n"}}, {"oid": "fefbf7c860ac3584f59cb7974e86641dcac1533b", "url": "https://github.com/apache/incubator-doris/commit/fefbf7c860ac3584f59cb7974e86641dcac1533b", "message": "add comments", "committedDate": "2020-08-26T09:56:04Z", "type": "commit"}, {"oid": "0b77bab05dd9fc99bca5f5d9c43136e9c5fe5983", "url": "https://github.com/apache/incubator-doris/commit/0b77bab05dd9fc99bca5f5d9c43136e9c5fe5983", "message": "remove s", "committedDate": "2020-08-26T10:02:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE5OTg2OA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477199868", "bodyText": "@ConfField", "author": "morningman", "createdAt": "2020-08-26T10:29:58Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -543,6 +543,14 @@\n     @ConfField\n     public static String spark_resource_path = \"\";\n \n+    /**\n+     * Default yarn client path\n+     */\n+    public static String yarn_client_path = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-client/hadoop/bin/yarn\";", "originalCommit": "0b77bab05dd9fc99bca5f5d9c43136e9c5fe5983", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/common/Config.java b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\nindex df94bf9c8..72c5ff855 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/common/Config.java\n\n@@ -548,6 +548,11 @@ public class Config extends ConfigBase {\n      */\n     public static String yarn_client_path = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-client/hadoop/bin/yarn\";\n \n+    /**\n+     * Default yarn config file directory\n+     * Each time before running the yarn command, we need to check that the\n+     * config file exists under this path, and if not, create them.\n+     */\n     @ConfField\n     public static String yarn_config_dir = PaloFe.DORIS_HOME_DIR + \"/lib/yarn-config\";\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIwMDE1Ng==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477200156", "bodyText": "Not used?", "author": "morningman", "createdAt": "2020-08-26T10:30:31Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -74,17 +67,21 @@\n     private static final String JOB_CONFIG_DIR = \"configs\";\n     private static final String ETL_JOB_NAME = \"doris__%s\";\n     // 5min\n-    private static final int GET_APPID_MAX_RETRY_TIMES = 300;\n-    private static final int GET_APPID_SLEEP_MS = 1000;\n-\n-    class SparkAppListener implements Listener {\n+    private static final long GET_APPID_TIMEOUT_MS = 300000L;\n+    // 30s\n+    private static final long EXEC_CMD_TIMEOUT_MS = 30000L;\n+    // yarn command\n+    private static final String YARN_STATUS_CMD = \"%s --config %s application -status %s\";\n+    private static final String YARN_KILL_CMD = \"%s --config %s application -kill %s\";\n+\n+    class SparkAppListener implements SparkLoadAppHandle.Listener {", "originalCommit": "0b77bab05dd9fc99bca5f5d9c43136e9c5fe5983", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\nindex 0749fbf8b..3b53851e9 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java\n\n@@ -74,18 +74,6 @@ public class SparkEtlJobHandler {\n     private static final String YARN_STATUS_CMD = \"%s --config %s application -status %s\";\n     private static final String YARN_KILL_CMD = \"%s --config %s application -kill %s\";\n \n-    class SparkAppListener implements SparkLoadAppHandle.Listener {\n-        @Override\n-        public void stateChanged(SparkLoadAppHandle sparkAppHandle) {\n-            LOG.info(\"get spark state changed: {}, app id: {}\", sparkAppHandle.getState(), sparkAppHandle.getAppId());\n-        }\n-\n-        @Override\n-        public void infoChanged(SparkLoadAppHandle sparkAppHandle) {\n-            LOG.info(\"get spark info changed: {}, app id: {}\", sparkAppHandle.getState(), sparkAppHandle.getAppId());\n-        }\n-    }\n-\n     public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobConfig, SparkResource resource,\n                              BrokerDesc brokerDesc, SparkPendingTaskAttachment attachment) throws LoadException {\n         // delete outputPath\n"}}, {"oid": "86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "url": "https://github.com/apache/incubator-doris/commit/86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "message": "save code", "committedDate": "2020-08-26T10:44:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIwOTE4MA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477209180", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return sparkConfigs;\n          \n          \n            \n                    return sparkConfig;\n          \n      \n    \n    \n  \n\nThe method name also need changed?", "author": "wuyunfeng", "createdAt": "2020-08-26T10:49:11Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -243,6 +264,16 @@ protected void setProperties(Map<String, String> properties) throws DdlException\n         return sparkConfigs;", "originalCommit": "86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f5fc44f863a70fc5040ee8c6438c6dd4af007ab2", "chunk": "diff --git a/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java b/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\nindex 35822cb57..0614bd34e 100644\n--- a/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\n+++ b/fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java\n\n@@ -254,17 +254,17 @@ public class SparkResource extends Resource {\n         brokerProperties = getBrokerProperties(properties);\n     }\n \n-    private Map<String, String> getSparkConfigs(Map<String, String> properties) {\n-        Map<String, String> sparkConfigs = Maps.newHashMap();\n+    private Map<String, String> getSparkConfig(Map<String, String> properties) {\n+        Map<String, String> sparkConfig = Maps.newHashMap();\n         for (Map.Entry<String, String> entry : properties.entrySet()) {\n             if (entry.getKey().startsWith(SPARK_CONFIG_PREFIX)) {\n-                sparkConfigs.put(entry.getKey(), entry.getValue());\n+                sparkConfig.put(entry.getKey(), entry.getValue());\n             }\n         }\n-        return sparkConfigs;\n+        return sparkConfig;\n     }\n \n-    private Map<String, String> getSparkHadoopConfigs(Map<String, String> properties) {\n+    private Map<String, String> getSparkHadoopConfig(Map<String, String> properties) {\n         Map<String, String> sparkConfig = Maps.newHashMap();\n         for (Map.Entry<String, String> entry : properties.entrySet()) {\n             if (entry.getKey().startsWith(SPARK_HADOOP_CONFIG_PREFIX)) {\n"}}, {"oid": "7f9651688bae81bcc0978b4c2148716322c6beb2", "url": "https://github.com/apache/incubator-doris/commit/7f9651688bae81bcc0978b4c2148716322c6beb2", "message": "add comments", "committedDate": "2020-08-26T11:01:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIxNjM1NA==", "url": "https://github.com/apache/incubator-doris/pull/4383#discussion_r477216354", "bodyText": "format seems to wrong?", "author": "wuyunfeng", "createdAt": "2020-08-26T11:03:48Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/YarnApplicationReport.java", "diffHunk": "@@ -0,0 +1,121 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.common.LoadException;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.Maps;\n+\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n+import org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl;\n+import org.apache.hadoop.yarn.util.ConverterUtils;\n+\n+import java.text.NumberFormat;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Covert output string of command `yarn application -status` to application report.\n+ * Input sample:\n+ * -------------------\n+ * Application Report :\n+ * \tApplication-Id : application_1573630236805_6763648\n+ * \tApplication-Name : doris_label_test\n+ * \tApplication-Type : SPARK-2.4.1\n+ * \tUser : test\n+ * \tQueue : test-queue\n+ * \tStart-Time : 1597654469958\n+ * \tFinish-Time : 1597654801939\n+ * \tProgress : 100%\n+ * \tState : FINISHED\n+ * \tFinal-State : SUCCEEDED\n+ * \tTracking-URL : 127.0.0.1:8004/history/application_1573630236805_6763648/1\n+ * \tRPC Port : 40236\n+ * \tAM Host : host-name\n+ * \t------------------\n+ *\n+ * \tOutput:\n+ * \tApplicationReport\n+ */\n+public class YarnApplicationReport {\n+    private static final String APPLICATION_ID = \"Application-Id\";\n+    private static final String APPLICATION_TYPE = \"Application-Type\";\n+    private static final String APPLICATION_NAME = \"Application-Name\";\n+    private static final String USER = \"User\";\n+    private static final String QUEUE = \"Queue\";\n+    private static final String START_TIME = \"Start-Time\";\n+    private static final String FINISH_TIME = \"Finish-Time\";\n+    private static final String PROGRESS = \"Progress\";\n+    private static final String STATE = \"State\";\n+    private static final String FINAL_STATE = \"Final-State\";\n+    private static final String TRACKING_URL = \"Tracking-URL\";\n+    private static final String RPC_PORT = \"RPC Port\";\n+    private static final String AM_HOST = \"AM Host\";\n+    private static final String DIAGNOSTICS = \"Diagnostics\";\n+\n+    private ApplicationReport report;\n+\n+    public YarnApplicationReport(String output) throws LoadException {\n+        this.report = new ApplicationReportPBImpl();\n+        parseFromOutput(output);\n+    }\n+\n+    public ApplicationReport getReport() {\n+        return report;\n+    }\n+\n+    private void parseFromOutput(String output) throws LoadException {\n+        Map<String, String> reportMap = Maps.newHashMap();\n+        List<String> lines = Splitter.onPattern(\"\\n\").trimResults().splitToList(output);\n+        // Application-Id : application_1573630236805_6763648 ==> (Application-Id, application_1573630236805_6763648)\n+        for (String line : lines) {\n+            List<String> entry = Splitter.onPattern(\":\").limit(2).trimResults().splitToList(line);\n+            Preconditions.checkState(entry.size() <= 2, line);\n+            if (entry.size() > 1) {\n+                reportMap.put(entry.get(0), entry.get(1));\n+            } else {\n+                reportMap.put(entry.get(0), \"\");\n+            }\n+        }\n+\n+        try {\n+            report.setApplicationId(ConverterUtils.toApplicationId(reportMap.get(APPLICATION_ID)));", "originalCommit": "86077e20b5056dd86b45f8e21c4b80e1dfa6063f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "f5fc44f863a70fc5040ee8c6438c6dd4af007ab2", "url": "https://github.com/apache/incubator-doris/commit/f5fc44f863a70fc5040ee8c6438c6dd4af007ab2", "message": "remove s", "committedDate": "2020-08-26T11:12:27Z", "type": "commit"}, {"oid": "f3b51caeaa2da9b19784769f2fc01518e33ca397", "url": "https://github.com/apache/incubator-doris/commit/f3b51caeaa2da9b19784769f2fc01518e33ca397", "message": "save code", "committedDate": "2020-08-26T11:45:21Z", "type": "commit"}]}