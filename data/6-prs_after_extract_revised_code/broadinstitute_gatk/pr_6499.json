{"pr_number": 6499, "pr_title": "Enabled multisample segmentation in ModelSegments.", "pr_createdAt": "2020-03-12T17:53:00Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6499", "timeline": [{"oid": "b0239f1654466c78699e9642a2695f05fcac9685", "url": "https://github.com/broadinstitute/gatk/commit/b0239f1654466c78699e9642a2695f05fcac9685", "message": "comment out local tests", "committedDate": "2020-03-12T20:01:44Z", "type": "forcePushed"}, {"oid": "692d67847688a633c42a1f0c4d7651c7dc643c82", "url": "https://github.com/broadinstitute/gatk/commit/692d67847688a633c42a1f0c4d7651c7dc643c82", "message": "optimize imports", "committedDate": "2020-03-30T17:57:33Z", "type": "forcePushed"}, {"oid": "354121a0108751ea6fd6fd5420c23c9c544e671f", "url": "https://github.com/broadinstitute/gatk/commit/354121a0108751ea6fd6fd5420c23c9c544e671f", "message": "work", "committedDate": "2020-03-30T18:05:27Z", "type": "forcePushed"}, {"oid": "c2d34ae7be035bd985b8fd986748503ec3472465", "url": "https://github.com/broadinstitute/gatk/commit/c2d34ae7be035bd985b8fd986748503ec3472465", "message": "work", "committedDate": "2020-03-30T18:09:57Z", "type": "forcePushed"}, {"oid": "6f78819197ae0abd202d41e86f6e49960e410d85", "url": "https://github.com/broadinstitute/gatk/commit/6f78819197ae0abd202d41e86f6e49960e410d85", "message": "work", "committedDate": "2020-03-30T19:20:05Z", "type": "forcePushed"}, {"oid": "02a5c27baac36552a1ae1d701deaa14494012788", "url": "https://github.com/broadinstitute/gatk/commit/02a5c27baac36552a1ae1d701deaa14494012788", "message": "consolidated segmenters", "committedDate": "2020-04-03T17:02:52Z", "type": "forcePushed"}, {"oid": "812fd88ba02eda88e0c22cb7364db85035799f07", "url": "https://github.com/broadinstitute/gatk/commit/812fd88ba02eda88e0c22cb7364db85035799f07", "message": "more", "committedDate": "2020-04-07T18:48:30Z", "type": "forcePushed"}, {"oid": "985c3c2d0cafeea11e1783294b38d7704dd9dc4b", "url": "https://github.com/broadinstitute/gatk/commit/985c3c2d0cafeea11e1783294b38d7704dd9dc4b", "message": "sjs", "committedDate": "2020-04-08T03:02:23Z", "type": "forcePushed"}, {"oid": "3c01bfdeed7e3c16e7caefc77e3151707feba24e", "url": "https://github.com/broadinstitute/gatk/commit/3c01bfdeed7e3c16e7caefc77e3151707feba24e", "message": "more", "committedDate": "2020-04-09T02:18:41Z", "type": "forcePushed"}, {"oid": "9b6685d3fbec6d7f03ee0596c88aacb9fcdea6fb", "url": "https://github.com/broadinstitute/gatk/commit/9b6685d3fbec6d7f03ee0596c88aacb9fcdea6fb", "message": "work", "committedDate": "2020-04-09T02:25:29Z", "type": "forcePushed"}, {"oid": "aba8ff04663fb86bcbe89c9f7d2d0286feef783f", "url": "https://github.com/broadinstitute/gatk/commit/aba8ff04663fb86bcbe89c9f7d2d0286feef783f", "message": "more", "committedDate": "2020-04-09T18:26:40Z", "type": "forcePushed"}, {"oid": "a4fc769bc43252bc8d9b48f98e1b3ab018e0f2c8", "url": "https://github.com/broadinstitute/gatk/commit/a4fc769bc43252bc8d9b48f98e1b3ab018e0f2c8", "message": "more", "committedDate": "2020-04-09T18:29:09Z", "type": "forcePushed"}, {"oid": "5316a6ff3b0d666f2c34404d7120fb0dc37f66fe", "url": "https://github.com/broadinstitute/gatk/commit/5316a6ff3b0d666f2c34404d7120fb0dc37f66fe", "message": "more", "committedDate": "2020-04-09T18:31:47Z", "type": "forcePushed"}, {"oid": "28a6d21b30cbbfa614f4c1f31420a1eb89e7e500", "url": "https://github.com/broadinstitute/gatk/commit/28a6d21b30cbbfa614f4c1f31420a1eb89e7e500", "message": "more", "committedDate": "2020-04-09T19:03:36Z", "type": "forcePushed"}, {"oid": "4784001ccafb322a2d161162459929351111345f", "url": "https://github.com/broadinstitute/gatk/commit/4784001ccafb322a2d161162459929351111345f", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-10T18:58:53Z", "type": "forcePushed"}, {"oid": "dd905c508fdfe4f4f6e94383d81ef5c426ea0861", "url": "https://github.com/broadinstitute/gatk/commit/dd905c508fdfe4f4f6e94383d81ef5c426ea0861", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-10T19:23:17Z", "type": "forcePushed"}, {"oid": "940aaf92ff37c52efd9d7fa57f468a9b309d9b5c", "url": "https://github.com/broadinstitute/gatk/commit/940aaf92ff37c52efd9d7fa57f468a9b309d9b5c", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T15:52:38Z", "type": "forcePushed"}, {"oid": "797f0e207702fe01d7338ae07cd8c2e577e6c7d4", "url": "https://github.com/broadinstitute/gatk/commit/797f0e207702fe01d7338ae07cd8c2e577e6c7d4", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T16:00:22Z", "type": "forcePushed"}, {"oid": "e5631eae73a47eedd6892f1bdd0679b6fad9b376", "url": "https://github.com/broadinstitute/gatk/commit/e5631eae73a47eedd6892f1bdd0679b6fad9b376", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T16:46:59Z", "type": "forcePushed"}, {"oid": "8b3b691ecafa2108712d5bab0ec595b9caaa3bd5", "url": "https://github.com/broadinstitute/gatk/commit/8b3b691ecafa2108712d5bab0ec595b9caaa3bd5", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T19:00:57Z", "type": "forcePushed"}, {"oid": "8b69097cb5f91de4806aa4c87a2976d5e8878e22", "url": "https://github.com/broadinstitute/gatk/commit/8b69097cb5f91de4806aa4c87a2976d5e8878e22", "message": "debug", "committedDate": "2020-04-13T19:06:42Z", "type": "forcePushed"}, {"oid": "413586e29cbf65de738eef9135230d3684b17946", "url": "https://github.com/broadinstitute/gatk/commit/413586e29cbf65de738eef9135230d3684b17946", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T21:32:40Z", "type": "forcePushed"}, {"oid": "72a0e77bfa3167cfd1618c6152045e8eb769b7a7", "url": "https://github.com/broadinstitute/gatk/commit/72a0e77bfa3167cfd1618c6152045e8eb769b7a7", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-13T21:34:06Z", "type": "forcePushed"}, {"oid": "9e15031d43b35ef74fd47d15826b89eda19ab010", "url": "https://github.com/broadinstitute/gatk/commit/9e15031d43b35ef74fd47d15826b89eda19ab010", "message": "more", "committedDate": "2020-04-15T19:35:59Z", "type": "forcePushed"}, {"oid": "b2af1a1b0d5063617ad3a94eefe5aeb83e70738f", "url": "https://github.com/broadinstitute/gatk/commit/b2af1a1b0d5063617ad3a94eefe5aeb83e70738f", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:38:42Z", "type": "forcePushed"}, {"oid": "f4214bee86d9091f5655103452f616ffc6576a1c", "url": "https://github.com/broadinstitute/gatk/commit/f4214bee86d9091f5655103452f616ffc6576a1c", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:40:14Z", "type": "forcePushed"}, {"oid": "521b812144e79760269663d55d67bd1f10bc693c", "url": "https://github.com/broadinstitute/gatk/commit/521b812144e79760269663d55d67bd1f10bc693c", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:41:13Z", "type": "forcePushed"}, {"oid": "25d935016e6cb6ba9d420af37a63df2cb1db4689", "url": "https://github.com/broadinstitute/gatk/commit/25d935016e6cb6ba9d420af37a63df2cb1db4689", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:43:11Z", "type": "forcePushed"}, {"oid": "0a5bf73b15acdc1b50a8da95425bb5dbfc809b73", "url": "https://github.com/broadinstitute/gatk/commit/0a5bf73b15acdc1b50a8da95425bb5dbfc809b73", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:47:06Z", "type": "forcePushed"}, {"oid": "15a4a8dd2493d5e33a6dc0639242eebadaa163e5", "url": "https://github.com/broadinstitute/gatk/commit/15a4a8dd2493d5e33a6dc0639242eebadaa163e5", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-15T19:48:45Z", "type": "forcePushed"}, {"oid": "21bc4236b54f55b4a19f2c7ab8db11852d573a8d", "url": "https://github.com/broadinstitute/gatk/commit/21bc4236b54f55b4a19f2c7ab8db11852d573a8d", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-16T15:38:28Z", "type": "forcePushed"}, {"oid": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "url": "https://github.com/broadinstitute/gatk/commit/1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2020-04-16T15:51:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3OTg3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r441079879", "bodyText": "Let's use explicit imports.", "author": "fleharty", "createdAt": "2020-06-16T19:06:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/ModelSegments.java", "diffHunk": "@@ -1,46 +1,56 @@\n package org.broadinstitute.hellbender.tools.copynumber;\n \n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Interval;\n+import htsjdk.samtools.util.IntervalList;\n import htsjdk.samtools.util.OverlapDetector;\n-import org.apache.commons.math3.special.Beta;\n-import org.apache.commons.math3.util.FastMath;\n+import org.broadinstitute.barclay.argparser.Advanced;\n import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n import org.broadinstitute.barclay.help.DocumentedFeature;\n import org.broadinstitute.hellbender.cmdline.CommandLineProgram;\n import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n import org.broadinstitute.hellbender.cmdline.programgroups.CopyNumberProgramGroup;\n-import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n-import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberStandardArgument;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.copynumber.arguments.*;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQwMDY3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648400678", "bodyText": "OK, optimized with explicit imports across source/test code in the copynumber package.", "author": "samuelklee", "createdAt": "2021-06-09T15:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3OTg3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/ModelSegments.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/ModelSegments.java\nindex 924ec4db9..476338929 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/ModelSegments.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/ModelSegments.java\n\n@@ -1,56 +1,46 @@\n package org.broadinstitute.hellbender.tools.copynumber;\n \n-import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n-import htsjdk.samtools.SAMSequenceDictionary;\n-import htsjdk.samtools.util.Interval;\n-import htsjdk.samtools.util.IntervalList;\n import htsjdk.samtools.util.OverlapDetector;\n-import org.broadinstitute.barclay.argparser.Advanced;\n+import org.apache.commons.math3.special.Beta;\n+import org.apache.commons.math3.util.FastMath;\n import org.broadinstitute.barclay.argparser.Argument;\n-import org.broadinstitute.barclay.argparser.ArgumentCollection;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n import org.broadinstitute.barclay.help.DocumentedFeature;\n import org.broadinstitute.hellbender.cmdline.CommandLineProgram;\n import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n import org.broadinstitute.hellbender.cmdline.programgroups.CopyNumberProgramGroup;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n-import org.broadinstitute.hellbender.tools.copynumber.arguments.*;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n+import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberStandardArgument;\n import org.broadinstitute.hellbender.tools.copynumber.formats.collections.*;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SampleLocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SimpleLocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.CopyRatio;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.CopyRatioSegment;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.LegacySegment;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.records.*;\n import org.broadinstitute.hellbender.tools.copynumber.models.AlleleFractionModeller;\n import org.broadinstitute.hellbender.tools.copynumber.models.AlleleFractionPrior;\n import org.broadinstitute.hellbender.tools.copynumber.models.CopyRatioModeller;\n import org.broadinstitute.hellbender.tools.copynumber.models.MultidimensionalModeller;\n-import org.broadinstitute.hellbender.tools.copynumber.segmentation.MultisampleMultidimensionalKernelSegmenter;\n-import org.broadinstitute.hellbender.tools.copynumber.utils.genotyping.NaiveHeterozygousPileupGenotypingUtils;\n+import org.broadinstitute.hellbender.tools.copynumber.segmentation.AlleleFractionKernelSegmenter;\n+import org.broadinstitute.hellbender.tools.copynumber.segmentation.CopyRatioKernelSegmenter;\n+import org.broadinstitute.hellbender.tools.copynumber.segmentation.MultidimensionalKernelSegmenter;\n import org.broadinstitute.hellbender.tools.copynumber.utils.segmentation.KernelSegmenter;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n import org.broadinstitute.hellbender.utils.Utils;\n-import org.broadinstitute.hellbender.utils.param.ParamUtils;\n \n import java.io.File;\n import java.util.*;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n /**\n- * Models segmented copy ratios from denoised copy ratios and segmented minor-allele fractions from allelic counts.\n+ * Models segmented copy ratios from denoised read counts and segmented minor-allele fractions from allelic counts.\n  *\n  * <p>\n- *     Possible data inputs are: 1) denoised copy ratios for the case sample, 2) allelic counts for the case sample,\n+ *     Possible inputs are: 1) denoised copy ratios for the case sample, 2) allelic counts for the case sample,\n  *     and 3) allelic counts for a matched-normal sample.  All available inputs will be used to to perform\n  *     segmentation and model inference.\n  * </p>\n  *\n- * <h4>Genotyping step</h4>\n- *\n  * <p>\n  *     If allelic counts are available, the first step in the inference process is to genotype heterozygous sites,\n  *     as the allelic counts at these sites will subsequently be modeled to infer segmented minor-allele fraction.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ0NDQ0Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r534444443", "bodyText": "Explicit imports.", "author": "fleharty", "createdAt": "2020-12-02T19:59:58Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -3,16 +3,15 @@\n import com.google.common.collect.Ordering;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.util.Locatable;\n+import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection;\n import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy;\n import org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AnnotatedIntervalCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.*;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\nindex a52c109fd..dbee943f5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n\n@@ -3,15 +3,16 @@ package org.broadinstitute.hellbender.tools.copynumber.arguments;\n import com.google.common.collect.Ordering;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.util.Locatable;\n-import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumentCollection;\n import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy;\n import org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.*;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AnnotatedIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.LocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.Metadata;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SimpleLocatableMetadata;\n import org.broadinstitute.hellbender.tools.copynumber.formats.records.AnnotatedInterval;\n import org.broadinstitute.hellbender.utils.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ0NDU1MA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r534444550", "bodyText": "Explicit imports", "author": "fleharty", "createdAt": "2020-12-02T20:00:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -22,10 +21,7 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.OptionalInt;\n-import java.util.Set;\n+import java.util.*;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "12b003b3e5f566b9c147c1a022a3db6ee8f3fbab", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\nindex a52c109fd..1c15d0bb6 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n\n@@ -9,19 +9,32 @@ import org.broadinstitute.hellbender.cmdline.argumentcollections.IntervalArgumen\n import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy;\n import org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.*;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractRecordCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AnnotatedIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleCountCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.LocatableMetadata;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.Metadata;\n import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SimpleLocatableMetadata;\n import org.broadinstitute.hellbender.tools.copynumber.formats.records.AnnotatedInterval;\n-import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.IntervalMergingRule;\n+import org.broadinstitute.hellbender.utils.IntervalSetRule;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SequenceDictionaryUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n import org.broadinstitute.hellbender.utils.io.IOUtils;\n import org.broadinstitute.hellbender.utils.python.PythonScriptExecutor;\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.*;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4MjMxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r534482313", "bodyText": "This seems more like an error to me.  Is there a situation where you might like to allow sequence dictionaries to not match?", "author": "fleharty", "createdAt": "2020-12-02T21:08:22Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -121,6 +120,41 @@ public static SimpleIntervalCollection resolveIntervals(final String readCountPa\n         return new SimpleIntervalCollection(metadata, intervals);\n     }\n \n+    /**\n+     * For all non-null inputs, validate that all metadata are identical and return the metadata.\n+     */\n+    @SafeVarargs\n+    @SuppressWarnings({\"varargs\"})\n+    public static <METADATA extends Metadata> METADATA getValidatedMetadata(final AbstractRecordCollection<METADATA, ?> ... recordCollections) {\n+        Utils.nonNull(recordCollections);\n+        final Set<METADATA> metadataSet = Stream.of(recordCollections)\n+                .filter(Objects::nonNull)\n+                .map(AbstractRecordCollection::getMetadata)\n+                .collect(Collectors.toSet());\n+        Utils.nonEmpty(metadataSet, \"At least one collection must be non-null.\");\n+        Utils.validateArg(metadataSet.size() == 1, \"Metadata do not match.\");\n+        return metadataSet.stream().findFirst().get();\n+    }\n+\n+    /**\n+     * For all non-null inputs, validate that all sequence dictionaries match (using {@link #isSameDictionary})\n+     * and return the sequence dictionary; otherwise, emit a warning.\n+     */\n+    public static SAMSequenceDictionary getValidatedSequenceDictionary(final AbstractLocatableCollection<?, ?> ... locatableCollections) {\n+        Utils.nonNull(locatableCollections);\n+        final List<SAMSequenceDictionary> sequenceDictionaries = Stream.of(locatableCollections)\n+                .filter(Objects::nonNull)\n+                .map(AbstractLocatableCollection::getMetadata)\n+                .map(LocatableMetadata::getSequenceDictionary)\n+                .collect(Collectors.toList());\n+        Utils.nonEmpty(sequenceDictionaries, \"At least one collection must be non-null.\");\n+        if (!IntStream.range(0, sequenceDictionaries.size() - 1).\n+                allMatch(i -> CopyNumberArgumentValidationUtils.isSameDictionary(sequenceDictionaries.get(i), sequenceDictionaries.get(i + 1)))) {\n+            logger.warn(\"Sequence dictionaries do not match across all inputs.\");", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODM5Njc2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648396766", "bodyText": "I think requiring sequence dictionaries to match even caused issues in TCGA data at some point. I think there are some discussions from long ago in the dsde-methods Slack about how strict we should be about this sort of thing, generally.", "author": "samuelklee", "createdAt": "2021-06-09T14:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4MjMxMw=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\nindex a52c109fd..dbee943f5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n\n@@ -120,41 +118,6 @@ public final class CopyNumberArgumentValidationUtils {\n         return new SimpleIntervalCollection(metadata, intervals);\n     }\n \n-    /**\n-     * For all non-null inputs, validate that all metadata are identical and return the metadata.\n-     */\n-    @SafeVarargs\n-    @SuppressWarnings({\"varargs\"})\n-    public static <METADATA extends Metadata> METADATA getValidatedMetadata(final AbstractRecordCollection<METADATA, ?> ... recordCollections) {\n-        Utils.nonNull(recordCollections);\n-        final Set<METADATA> metadataSet = Stream.of(recordCollections)\n-                .filter(Objects::nonNull)\n-                .map(AbstractRecordCollection::getMetadata)\n-                .collect(Collectors.toSet());\n-        Utils.nonEmpty(metadataSet, \"At least one collection must be non-null.\");\n-        Utils.validateArg(metadataSet.size() == 1, \"Metadata do not match.\");\n-        return metadataSet.stream().findFirst().get();\n-    }\n-\n-    /**\n-     * For all non-null inputs, validate that all sequence dictionaries match (using {@link #isSameDictionary})\n-     * and return the sequence dictionary; otherwise, emit a warning.\n-     */\n-    public static SAMSequenceDictionary getValidatedSequenceDictionary(final AbstractLocatableCollection<?, ?> ... locatableCollections) {\n-        Utils.nonNull(locatableCollections);\n-        final List<SAMSequenceDictionary> sequenceDictionaries = Stream.of(locatableCollections)\n-                .filter(Objects::nonNull)\n-                .map(AbstractLocatableCollection::getMetadata)\n-                .map(LocatableMetadata::getSequenceDictionary)\n-                .collect(Collectors.toList());\n-        Utils.nonEmpty(sequenceDictionaries, \"At least one collection must be non-null.\");\n-        if (!IntStream.range(0, sequenceDictionaries.size() - 1).\n-                allMatch(i -> CopyNumberArgumentValidationUtils.isSameDictionary(sequenceDictionaries.get(i), sequenceDictionaries.get(i + 1)))) {\n-            logger.warn(\"Sequence dictionaries do not match across all inputs.\");\n-        }\n-        return sequenceDictionaries.get(0);\n-    }\n-\n     /**\n      * Common method for subsetting and validating read counts in both {@link DetermineGermlineContigPloidy}\n      * and {@link GermlineCNVCaller}.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5MjkwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r534492900", "bodyText": "Could you add  a simple test for this.", "author": "fleharty", "createdAt": "2020-12-02T21:28:22Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -121,6 +120,41 @@ public static SimpleIntervalCollection resolveIntervals(final String readCountPa\n         return new SimpleIntervalCollection(metadata, intervals);\n     }\n \n+    /**\n+     * For all non-null inputs, validate that all metadata are identical and return the metadata.\n+     */\n+    @SafeVarargs\n+    @SuppressWarnings({\"varargs\"})\n+    public static <METADATA extends Metadata> METADATA getValidatedMetadata(final AbstractRecordCollection<METADATA, ?> ... recordCollections) {\n+        Utils.nonNull(recordCollections);\n+        final Set<METADATA> metadataSet = Stream.of(recordCollections)\n+                .filter(Objects::nonNull)\n+                .map(AbstractRecordCollection::getMetadata)\n+                .collect(Collectors.toSet());\n+        Utils.nonEmpty(metadataSet, \"At least one collection must be non-null.\");\n+        Utils.validateArg(metadataSet.size() == 1, \"Metadata do not match.\");\n+        return metadataSet.stream().findFirst().get();\n+    }\n+\n+    /**\n+     * For all non-null inputs, validate that all sequence dictionaries match (using {@link #isSameDictionary})\n+     * and return the sequence dictionary; otherwise, emit a warning.\n+     */\n+    public static SAMSequenceDictionary getValidatedSequenceDictionary(final AbstractLocatableCollection<?, ?> ... locatableCollections) {", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQyMzI0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648423245", "bodyText": "A bit of extra work to check for warnings, rather than errors. I don't think we care too closely about the exact behavior here (see above comment), so punting as well!", "author": "samuelklee", "createdAt": "2021-06-09T15:25:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5MjkwMA=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\nindex a52c109fd..dbee943f5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n\n@@ -120,41 +118,6 @@ public final class CopyNumberArgumentValidationUtils {\n         return new SimpleIntervalCollection(metadata, intervals);\n     }\n \n-    /**\n-     * For all non-null inputs, validate that all metadata are identical and return the metadata.\n-     */\n-    @SafeVarargs\n-    @SuppressWarnings({\"varargs\"})\n-    public static <METADATA extends Metadata> METADATA getValidatedMetadata(final AbstractRecordCollection<METADATA, ?> ... recordCollections) {\n-        Utils.nonNull(recordCollections);\n-        final Set<METADATA> metadataSet = Stream.of(recordCollections)\n-                .filter(Objects::nonNull)\n-                .map(AbstractRecordCollection::getMetadata)\n-                .collect(Collectors.toSet());\n-        Utils.nonEmpty(metadataSet, \"At least one collection must be non-null.\");\n-        Utils.validateArg(metadataSet.size() == 1, \"Metadata do not match.\");\n-        return metadataSet.stream().findFirst().get();\n-    }\n-\n-    /**\n-     * For all non-null inputs, validate that all sequence dictionaries match (using {@link #isSameDictionary})\n-     * and return the sequence dictionary; otherwise, emit a warning.\n-     */\n-    public static SAMSequenceDictionary getValidatedSequenceDictionary(final AbstractLocatableCollection<?, ?> ... locatableCollections) {\n-        Utils.nonNull(locatableCollections);\n-        final List<SAMSequenceDictionary> sequenceDictionaries = Stream.of(locatableCollections)\n-                .filter(Objects::nonNull)\n-                .map(AbstractLocatableCollection::getMetadata)\n-                .map(LocatableMetadata::getSequenceDictionary)\n-                .collect(Collectors.toList());\n-        Utils.nonEmpty(sequenceDictionaries, \"At least one collection must be non-null.\");\n-        if (!IntStream.range(0, sequenceDictionaries.size() - 1).\n-                allMatch(i -> CopyNumberArgumentValidationUtils.isSameDictionary(sequenceDictionaries.get(i), sequenceDictionaries.get(i + 1)))) {\n-            logger.warn(\"Sequence dictionaries do not match across all inputs.\");\n-        }\n-        return sequenceDictionaries.get(0);\n-    }\n-\n     /**\n      * Common method for subsetting and validating read counts in both {@link DetermineGermlineContigPloidy}\n      * and {@link GermlineCNVCaller}.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5NjE2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r534496164", "bodyText": "Since base qualities are typically stored in phred scale, would it make sense to use phred scale here too?", "author": "fleharty", "createdAt": "2020-12-02T21:34:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticGenotypingArgumentCollection.java", "diffHunk": "@@ -0,0 +1,53 @@\n+package org.broadinstitute.hellbender.tools.copynumber.arguments;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+\n+import java.io.Serializable;\n+\n+public class SomaticGenotypingArgumentCollection implements Serializable {\n+    public static final long serialVersionUID = 1L;\n+\n+    //het genotyping argument names\n+    public static final String MINIMUM_TOTAL_ALLELE_COUNT_CASE_LONG_NAME = \"minimum-total-allele-count-case\";\n+    public static final String MINIMUM_TOTAL_ALLELE_COUNT_NORMAL_LONG_NAME = \"minimum-total-allele-count-normal\";\n+    public static final String GENOTYPING_HOMOZYGOUS_LOG_RATIO_THRESHOLD_LONG_NAME = \"genotyping-homozygous-log-ratio-threshold\";\n+    public static final String GENOTYPING_BASE_ERROR_RATE_LONG_NAME = \"genotyping-base-error-rate\";\n+\n+    @Argument(\n+            doc = \"Minimum total count for filtering allelic counts in the case sample, if available.  \" +\n+                    \"The default value of zero is appropriate for matched-normal mode; \" +\n+                    \"increase to an appropriate value for case-only mode.\",\n+            fullName = MINIMUM_TOTAL_ALLELE_COUNT_CASE_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    public int minTotalAlleleCountCase = 0;\n+\n+    @Argument(\n+            doc = \"Minimum total count for filtering allelic counts in the matched-normal sample, if available.\",\n+            fullName = MINIMUM_TOTAL_ALLELE_COUNT_NORMAL_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    public int minTotalAlleleCountNormal = 30;\n+\n+    @Argument(\n+            doc = \"Log-ratio threshold for genotyping and filtering homozygous allelic counts, if available.  \" +\n+                    \"Increasing this value will increase the number of sites assumed to be heterozygous for modeling.\",\n+            fullName = GENOTYPING_HOMOZYGOUS_LOG_RATIO_THRESHOLD_LONG_NAME,\n+            optional = true\n+    )\n+    public double genotypingHomozygousLogRatioThreshold = -10.;\n+\n+    @Argument(\n+            doc = \"Maximum base-error rate for genotyping and filtering homozygous allelic counts, if available.  \" +", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQyNDg2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648424864", "bodyText": "This base-error rate is intended to model stray bases in the pileups, which could arise for reasons other than the sequencing errors corresponding to base qualities. The maximum value parameterized here is probably typically on the order of a few percent, so it doesn't seem natural to express this in phred scale.", "author": "samuelklee", "createdAt": "2021-06-09T15:27:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5NjE2NA=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticGenotypingArgumentCollection.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticGenotypingArgumentCollection.java\ndeleted file mode 100644\nindex ebb5c0200..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticGenotypingArgumentCollection.java\n+++ /dev/null\n\n@@ -1,53 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.arguments;\n-\n-import org.broadinstitute.barclay.argparser.Argument;\n-\n-import java.io.Serializable;\n-\n-public class SomaticGenotypingArgumentCollection implements Serializable {\n-    public static final long serialVersionUID = 1L;\n-\n-    //het genotyping argument names\n-    public static final String MINIMUM_TOTAL_ALLELE_COUNT_CASE_LONG_NAME = \"minimum-total-allele-count-case\";\n-    public static final String MINIMUM_TOTAL_ALLELE_COUNT_NORMAL_LONG_NAME = \"minimum-total-allele-count-normal\";\n-    public static final String GENOTYPING_HOMOZYGOUS_LOG_RATIO_THRESHOLD_LONG_NAME = \"genotyping-homozygous-log-ratio-threshold\";\n-    public static final String GENOTYPING_BASE_ERROR_RATE_LONG_NAME = \"genotyping-base-error-rate\";\n-\n-    @Argument(\n-            doc = \"Minimum total count for filtering allelic counts in the case sample, if available.  \" +\n-                    \"The default value of zero is appropriate for matched-normal mode; \" +\n-                    \"increase to an appropriate value for case-only mode.\",\n-            fullName = MINIMUM_TOTAL_ALLELE_COUNT_CASE_LONG_NAME,\n-            minValue = 0,\n-            optional = true\n-    )\n-    public int minTotalAlleleCountCase = 0;\n-\n-    @Argument(\n-            doc = \"Minimum total count for filtering allelic counts in the matched-normal sample, if available.\",\n-            fullName = MINIMUM_TOTAL_ALLELE_COUNT_NORMAL_LONG_NAME,\n-            minValue = 0,\n-            optional = true\n-    )\n-    public int minTotalAlleleCountNormal = 30;\n-\n-    @Argument(\n-            doc = \"Log-ratio threshold for genotyping and filtering homozygous allelic counts, if available.  \" +\n-                    \"Increasing this value will increase the number of sites assumed to be heterozygous for modeling.\",\n-            fullName = GENOTYPING_HOMOZYGOUS_LOG_RATIO_THRESHOLD_LONG_NAME,\n-            optional = true\n-    )\n-    public double genotypingHomozygousLogRatioThreshold = -10.;\n-\n-    @Argument(\n-            doc = \"Maximum base-error rate for genotyping and filtering homozygous allelic counts, if available.  \" +\n-                    \"The likelihood for an allelic count to be generated from a homozygous site will be integrated \" +\n-                    \"from zero base-error rate up to this value.  Decreasing this value will increase \" +\n-                    \"the number of sites assumed to be heterozygous for modeling.\",\n-            fullName = GENOTYPING_BASE_ERROR_RATE_LONG_NAME,\n-            minValue = 0.,\n-            maxValue = 1.,\n-            optional = true\n-    )\n-    public double genotypingBaseErrorRate = 5E-2;\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjIzNTg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r536235881", "bodyText": "I'm confused about why the default parameters are alpha, 1, 0, 1/2.  Why is beta = 1, and why are you scaling a = 0, and c = 1/2?  Is my interpretation correct here?", "author": "fleharty", "createdAt": "2020-12-04T16:49:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticModelingArgumentCollection.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package org.broadinstitute.hellbender.tools.copynumber.arguments;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.io.Serializable;\n+\n+public class SomaticModelingArgumentCollection implements Serializable {\n+    public static final long serialVersionUID = 1L;\n+\n+    //MCMC argument names\n+    public static final String MINOR_ALLELE_FRACTION_PRIOR_ALPHA_LONG_NAME = \"minor-allele-fraction-prior-alpha\";\n+    public static final String NUMBER_OF_SAMPLES_COPY_RATIO_LONG_NAME = \"number-of-samples-copy-ratio\";\n+    public static final String NUMBER_OF_BURN_IN_SAMPLES_COPY_RATIO_LONG_NAME = \"number-of-burn-in-samples-copy-ratio\";\n+    public static final String NUMBER_OF_SAMPLES_ALLELE_FRACTION_LONG_NAME = \"number-of-samples-allele-fraction\";\n+    public static final String NUMBER_OF_BURN_IN_SAMPLES_ALLELE_FRACTION_LONG_NAME = \"number-of-burn-in-samples-allele-fraction\";\n+\n+    //smoothing argument names\n+    public static final String SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_COPY_RATIO_LONG_NAME = \"smoothing-credible-interval-threshold-copy-ratio\";\n+    public static final String SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_ALLELE_FRACTION_LONG_NAME = \"smoothing-credible-interval-threshold-allele-fraction\";\n+    public static final String MAXIMUM_NUMBER_OF_SMOOTHING_ITERATIONS_LONG_NAME = \"maximum-number-of-smoothing-iterations\";\n+    public static final String NUMBER_OF_SMOOTHING_ITERATIONS_PER_FIT_LONG_NAME = \"number-of-smoothing-iterations-per-fit\";\n+\n+    @Argument(\n+            doc = \"Alpha hyperparameter for the 4-parameter beta-distribution prior on segment minor-allele fraction. \" +\n+                    \"The prior for the minor-allele fraction f in each segment is assumed to be Beta(alpha, 1, 0, 1/2). \" +", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQwNDQzMA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648404430", "bodyText": "Minor-allele fraction is supported on [0, 1/2), so we set a and c accordingly. We fix beta = 1 just to remove a degree of freedom; it seems unlikely that anyone would want to specify a prior peaked anywhere else besides 0.5, and the prior hopefully doesn't make too much of a difference with good data anyway.", "author": "samuelklee", "createdAt": "2021-06-09T15:06:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjIzNTg4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticModelingArgumentCollection.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticModelingArgumentCollection.java\ndeleted file mode 100644\nindex 5163e5028..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticModelingArgumentCollection.java\n+++ /dev/null\n\n@@ -1,106 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.arguments;\n-\n-import org.broadinstitute.barclay.argparser.Argument;\n-import org.broadinstitute.hellbender.utils.Utils;\n-\n-import java.io.Serializable;\n-\n-public class SomaticModelingArgumentCollection implements Serializable {\n-    public static final long serialVersionUID = 1L;\n-\n-    //MCMC argument names\n-    public static final String MINOR_ALLELE_FRACTION_PRIOR_ALPHA_LONG_NAME = \"minor-allele-fraction-prior-alpha\";\n-    public static final String NUMBER_OF_SAMPLES_COPY_RATIO_LONG_NAME = \"number-of-samples-copy-ratio\";\n-    public static final String NUMBER_OF_BURN_IN_SAMPLES_COPY_RATIO_LONG_NAME = \"number-of-burn-in-samples-copy-ratio\";\n-    public static final String NUMBER_OF_SAMPLES_ALLELE_FRACTION_LONG_NAME = \"number-of-samples-allele-fraction\";\n-    public static final String NUMBER_OF_BURN_IN_SAMPLES_ALLELE_FRACTION_LONG_NAME = \"number-of-burn-in-samples-allele-fraction\";\n-\n-    //smoothing argument names\n-    public static final String SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_COPY_RATIO_LONG_NAME = \"smoothing-credible-interval-threshold-copy-ratio\";\n-    public static final String SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_ALLELE_FRACTION_LONG_NAME = \"smoothing-credible-interval-threshold-allele-fraction\";\n-    public static final String MAXIMUM_NUMBER_OF_SMOOTHING_ITERATIONS_LONG_NAME = \"maximum-number-of-smoothing-iterations\";\n-    public static final String NUMBER_OF_SMOOTHING_ITERATIONS_PER_FIT_LONG_NAME = \"number-of-smoothing-iterations-per-fit\";\n-\n-    @Argument(\n-            doc = \"Alpha hyperparameter for the 4-parameter beta-distribution prior on segment minor-allele fraction. \" +\n-                    \"The prior for the minor-allele fraction f in each segment is assumed to be Beta(alpha, 1, 0, 1/2). \" +\n-                    \"Increasing this hyperparameter will reduce the effect of reference bias at the expense of sensitivity.\",\n-            fullName = MINOR_ALLELE_FRACTION_PRIOR_ALPHA_LONG_NAME,\n-            optional = true,\n-            minValue = 1\n-    )\n-    public double minorAlleleFractionPriorAlpha = 25.;\n-\n-    @Argument(\n-            doc = \"Total number of MCMC samples for copy-ratio model.\",\n-            fullName = NUMBER_OF_SAMPLES_COPY_RATIO_LONG_NAME,\n-            optional = true,\n-            minValue = 1\n-    )\n-    public int numSamplesCopyRatio = 100;\n-\n-    @Argument(\n-            doc = \"Number of burn-in samples to discard for copy-ratio model.\",\n-            fullName = NUMBER_OF_BURN_IN_SAMPLES_COPY_RATIO_LONG_NAME,\n-            optional = true,\n-            minValue = 0\n-    )\n-    public int numBurnInCopyRatio = 50;\n-\n-    @Argument(\n-            doc = \"Total number of MCMC samples for allele-fraction model.\",\n-            fullName = NUMBER_OF_SAMPLES_ALLELE_FRACTION_LONG_NAME,\n-            optional = true,\n-            minValue = 1\n-    )\n-    public int numSamplesAlleleFraction = 100;\n-\n-    @Argument(\n-            doc = \"Number of burn-in samples to discard for allele-fraction model.\",\n-            fullName = NUMBER_OF_BURN_IN_SAMPLES_ALLELE_FRACTION_LONG_NAME,\n-            optional = true,\n-            minValue = 0\n-    )\n-    public int numBurnInAlleleFraction = 50;\n-\n-    @Argument(\n-            doc = \"Number of 10% equal-tailed credible-interval widths to use for copy-ratio segmentation smoothing.\",\n-            fullName = SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_COPY_RATIO_LONG_NAME,\n-            optional = true,\n-            minValue = 0.\n-    )\n-    public double smoothingCredibleIntervalThresholdCopyRatio = 2.;\n-\n-    @Argument(\n-            doc = \"Number of 10% equal-tailed credible-interval widths to use for allele-fraction segmentation smoothing.\",\n-            fullName = SMOOTHING_CREDIBLE_INTERVAL_THRESHOLD_ALLELE_FRACTION_LONG_NAME,\n-            optional = true,\n-            minValue = 0.\n-    )\n-    public double smoothingCredibleIntervalThresholdAlleleFraction = 2.;\n-\n-    @Argument(\n-            doc = \"Maximum number of iterations allowed for segmentation smoothing.\",\n-            fullName = MAXIMUM_NUMBER_OF_SMOOTHING_ITERATIONS_LONG_NAME,\n-            optional = true,\n-            minValue = 0\n-    )\n-    public int maxNumSmoothingIterations = 25;\n-\n-    @Argument(\n-            doc = \"Number of segmentation-smoothing iterations per MCMC model refit. \" +\n-                    \"(Increasing this will decrease runtime, but the final number of segments may be higher. \" +\n-                    \"Setting this to 0 will completely disable model refitting between iterations.)\",\n-            fullName = NUMBER_OF_SMOOTHING_ITERATIONS_PER_FIT_LONG_NAME,\n-            optional = true,\n-            minValue = 0\n-    )\n-    public int numSmoothingIterationsPerFit = 0;\n-\n-    public void validateArguments() {\n-        Utils.validateArg(numSamplesCopyRatio > numBurnInCopyRatio,\n-                \"Number of copy-ratio samples must be greater than number of copy-ratio burn-in samples.\");\n-        Utils.validateArg(numSamplesAlleleFraction > numBurnInAlleleFraction,\n-                \"Number of allele-fraction samples must be greater than number of allele-fraction burn-in samples.\");\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI0OTk3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r536249971", "bodyText": "What's the justification for the change from 0.1 to 0.025?\nThe test no longer passes with the value 0.1.", "author": "fleharty", "createdAt": "2020-12-04T17:13:44Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java", "diffHunk": "@@ -36,19 +35,19 @@\n     public Object[][] dataAlleleFractionKernelSegmenter() {\n         final int numPoints = 10000;\n         final double noiseLevel = 0.001;\n-        final double homFraction = 0.1;     //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints\n+        final double homFraction = 0.025;   //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQwNTE3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648405173", "bodyText": "See comment in #6499 (comment)", "author": "samuelklee", "createdAt": "2021-06-09T15:07:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI0OTk3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java b/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\nindex dece5289d..7519faf81 100644\n--- a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\n\n@@ -35,9 +36,10 @@ public final class AlleleFractionKernelSegmenterUnitTest extends GATKBaseTest {\n     public Object[][] dataAlleleFractionKernelSegmenter() {\n         final int numPoints = 10000;\n         final double noiseLevel = 0.001;\n-        final double homFraction = 0.025;   //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints\n+        final double homFraction = 0.1;     //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints\n \n         final Random rng = new Random(RANDOM_SEED);\n+        rng.setSeed(RANDOM_SEED);\n         final List<Double> minorAlleleFractions = Arrays.asList(0.45, 0.05, 0.25, 0.45, 0.05, 0.25, 0.45, 0.05, 0.25, 0.45, 0.05, 0.25);\n         final List<Double> alternateAlleleFractions = IntStream.range(0, numPoints).boxed()\n                 .map(i -> rng.nextFloat() < homFraction\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1MzUzNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r536253535", "bodyText": "Test also fails if I change the kernelVariance from 0.05 to its previous value 0.01.", "author": "fleharty", "createdAt": "2020-12-04T17:19:53Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java", "diffHunk": "@@ -99,15 +98,15 @@\n \n     @Test(dataProvider = \"dataAlleleFractionKernelSegmenter\")\n     public void testAlleleFractionKernelSegmenter(final AllelicCountCollection allelicCounts,\n-                                                  final AlleleFractionSegmentCollection segmentsExpected) {\n+                                                  final SimpleIntervalCollection segmentsExpected) {\n         final int maxNumChangepointsPerChromosome = 25;\n-        final double kernelVariance = 0.01;\n+        final double kernelVariance = 0.05;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQwNTM5NA==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648405394", "bodyText": "Again, see comment above.", "author": "samuelklee", "createdAt": "2021-06-09T15:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1MzUzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java b/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\nindex dece5289d..7519faf81 100644\n--- a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/AlleleFractionKernelSegmenterUnitTest.java\n\n@@ -98,15 +99,15 @@ public final class AlleleFractionKernelSegmenterUnitTest extends GATKBaseTest {\n \n     @Test(dataProvider = \"dataAlleleFractionKernelSegmenter\")\n     public void testAlleleFractionKernelSegmenter(final AllelicCountCollection allelicCounts,\n-                                                  final SimpleIntervalCollection segmentsExpected) {\n+                                                  final AlleleFractionSegmentCollection segmentsExpected) {\n         final int maxNumChangepointsPerChromosome = 25;\n-        final double kernelVariance = 0.05;\n+        final double kernelVariance = 0.01;\n         final int kernelApproximationDimension = 20;\n         final List<Integer> windowSizes = Arrays.asList(8, 16, 32, 64);\n         final double numChangepointsPenaltyLinearFactor = 1.;\n         final double numChangepointsPenaltyLogLinearFactor = 1.;\n \n-        final SimpleIntervalCollection segments = new AlleleFractionKernelSegmenter(allelicCounts)\n+        final AlleleFractionSegmentCollection segments = new AlleleFractionKernelSegmenter(allelicCounts)\n                 .findSegmentation(maxNumChangepointsPerChromosome, kernelVariance, kernelApproximationDimension,\n                         windowSizes, numChangepointsPenaltyLinearFactor, numChangepointsPenaltyLogLinearFactor);\n         Assert.assertEquals(segments, segmentsExpected);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1NDg3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r536254879", "bodyText": "Could you set variables for 250, and 10.\nIt's not clear if this is equivalent to...\n(i % x) * y + z\nor\n(i % x) * y + y", "author": "fleharty", "createdAt": "2020-12-04T17:22:17Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenterUnitTest.java", "diffHunk": "@@ -0,0 +1,179 @@\n+package org.broadinstitute.hellbender.tools.copynumber.segmentation;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import org.broadinstitute.hellbender.GATKBaseTest;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.CopyRatioCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SampleLocatableMetadata;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SimpleSampleLocatableMetadata;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.records.CopyRatio;\n+import org.broadinstitute.hellbender.tools.copynumber.utils.segmentation.KernelSegmenterUnitTest;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.testng.Assert;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * @author Samuel Lee &lt;slee@broadinstitute.org&gt;\n+ */\n+public final class MultisampleMultidimensionalKernelSegmenterUnitTest extends GATKBaseTest {\n+    private static final int RANDOM_SEED = 1;   //reset seed before each simulated test case\n+\n+    /**\n+     * Generates same Gaussian and test data as {@link KernelSegmenterUnitTest#dataKernelSegmenter()}\n+     * and alternate-allele-fraction-like data (similar to zero-mean multimodal test data\n+     * in {@link KernelSegmenterUnitTest#dataKernelSegmenter()}),\n+     * but introduces further segments by placing data on different chromosomes.\n+     * This is just a combination of the test data from\n+     * {@link CopyRatioKernelSegmenterUnitTest} and {@link AlleleFractionKernelSegmenterUnitTest},\n+     * but for multiple samples and at lower signal-to-noise ratio.\n+     */\n+    @DataProvider(name = \"dataMultisampleMultidimensionalKernelSegmenter\")\n+    public Object[][] dataMultisampleMultidimensionalKernelSegmenter() {\n+        final Random rng = new Random(RANDOM_SEED);\n+\n+        final int numSamples = 20;\n+        final List<CopyRatioCollection> denoisedCopyRatiosPerSample = new ArrayList<>(numSamples);\n+        final List<AllelicCountCollection> allelicCountsPerSample = new ArrayList<>(numSamples);\n+\n+        final int numIntervals = 1000;\n+        final int numAllSites = 1000;\n+        final double noiseLevelCopyRatio = 1.0;\n+        final double noiseLevelAlleleFraction = 0.25;\n+        final double homFraction = 0.025;   //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints\n+\n+        //generate intervals for copy-ratio data\n+        final List<SimpleInterval> intervals = IntStream.range(0, numIntervals).boxed()\n+                .map(i -> new SimpleInterval(\n+                        Integer.toString(i / 250 + 1),  //start a new chromosome every 250 points, which adds additional changepoints\n+                        (i % 250) * 10 + 1,\n+                        (i % 250) * 10 + 10))           //intervals for copy-ratio data points have length = 10", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQyMjM4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648422386", "bodyText": "This bit of code has already been copied to the other versions of this test, unfortunately. However, I think the inline comments are sufficient for test code/parameters that are unlikely to be changed. Punting!", "author": "samuelklee", "createdAt": "2021-06-09T15:24:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1NDg3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenterUnitTest.java b/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenterUnitTest.java\ndeleted file mode 100644\nindex f44a36517..000000000\n--- a/src/test/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenterUnitTest.java\n+++ /dev/null\n\n@@ -1,179 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.segmentation;\n-\n-import htsjdk.samtools.SAMSequenceDictionary;\n-import htsjdk.samtools.SAMSequenceRecord;\n-import org.broadinstitute.hellbender.GATKBaseTest;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.CopyRatioCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SampleLocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SimpleSampleLocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.CopyRatio;\n-import org.broadinstitute.hellbender.tools.copynumber.utils.segmentation.KernelSegmenterUnitTest;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.testng.Assert;\n-import org.testng.annotations.DataProvider;\n-import org.testng.annotations.Test;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Random;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n-\n-/**\n- * @author Samuel Lee &lt;slee@broadinstitute.org&gt;\n- */\n-public final class MultisampleMultidimensionalKernelSegmenterUnitTest extends GATKBaseTest {\n-    private static final int RANDOM_SEED = 1;   //reset seed before each simulated test case\n-\n-    /**\n-     * Generates same Gaussian and test data as {@link KernelSegmenterUnitTest#dataKernelSegmenter()}\n-     * and alternate-allele-fraction-like data (similar to zero-mean multimodal test data\n-     * in {@link KernelSegmenterUnitTest#dataKernelSegmenter()}),\n-     * but introduces further segments by placing data on different chromosomes.\n-     * This is just a combination of the test data from\n-     * {@link CopyRatioKernelSegmenterUnitTest} and {@link AlleleFractionKernelSegmenterUnitTest},\n-     * but for multiple samples and at lower signal-to-noise ratio.\n-     */\n-    @DataProvider(name = \"dataMultisampleMultidimensionalKernelSegmenter\")\n-    public Object[][] dataMultisampleMultidimensionalKernelSegmenter() {\n-        final Random rng = new Random(RANDOM_SEED);\n-\n-        final int numSamples = 20;\n-        final List<CopyRatioCollection> denoisedCopyRatiosPerSample = new ArrayList<>(numSamples);\n-        final List<AllelicCountCollection> allelicCountsPerSample = new ArrayList<>(numSamples);\n-\n-        final int numIntervals = 1000;\n-        final int numAllSites = 1000;\n-        final double noiseLevelCopyRatio = 1.0;\n-        final double noiseLevelAlleleFraction = 0.25;\n-        final double homFraction = 0.025;   //low hom fraction minimizes uncertainty in the changepoints coming from runs of adjacent homs near the changepoints\n-\n-        //generate intervals for copy-ratio data\n-        final List<SimpleInterval> intervals = IntStream.range(0, numIntervals).boxed()\n-                .map(i -> new SimpleInterval(\n-                        Integer.toString(i / 250 + 1),  //start a new chromosome every 250 points, which adds additional changepoints\n-                        (i % 250) * 10 + 1,\n-                        (i % 250) * 10 + 10))           //intervals for copy-ratio data points have length = 10\n-                .collect(Collectors.toList());\n-\n-        //generate sites for allele-fraction data\n-        final List<SimpleInterval> allSites = IntStream.range(0, numAllSites).boxed()\n-                .map(i -> new SimpleInterval(\n-                        Integer.toString(i / 250 + 1),  //start a new chromosome every 250 points, which adds additional changepoints\n-                        (i % 250) * 10 + 1,\n-                        (i % 250) * 10 + 1))            //one site per copy-ratio interval, sites have length = 1\n-                .collect(Collectors.toList());\n-\n-        //in all samples, drop half of the sites at random to give some copy-ratio intervals with no allele-fraction sites (to test imputation of allele fraction at 0.5)\n-        final List<Boolean> isNotDropped = IntStream.range(0, numAllSites).boxed()\n-                .map(i -> rng.nextBoolean())\n-                .collect(Collectors.toList());\n-\n-        final SAMSequenceDictionary sequenceDictionary = new SAMSequenceDictionary(intervals.stream()\n-                .map(SimpleInterval::getContig)\n-                .distinct()\n-                .map(c -> new SAMSequenceRecord(c, 10000))\n-                .collect(Collectors.toList()));\n-\n-        for (int sampleIndex = 0; sampleIndex < numSamples; sampleIndex++) {\n-            //generate numIntervals copy ratios\n-            final List<Double> dataGaussian = IntStream.range(0, numIntervals).boxed()\n-                    .map(i -> Math.abs(i / 100 - 5) + noiseLevelCopyRatio * rng.nextGaussian())\n-                    .collect(Collectors.toList());             //changepoints at 99, 199, 299, 399, 499, 599, 699, 799, 899\n-\n-            //generate numAllSites alternate-allele fractions\n-            final List<Double> allMinorAlleleFractions = Arrays.asList(0.45, 0.05, 0.25, 0.45, 0.05, 0.25, 0.45, 0.05, 0.25, 0.45, 0.05, 0.25);\n-            final List<Double> allAlternateAlleleFractions = IntStream.range(0, numAllSites).boxed()\n-                    .map(i -> rng.nextFloat() < homFraction\n-                            ? rng.nextBoolean()\n-                                ? 0. + noiseLevelAlleleFraction * Math.abs(rng.nextGaussian())                                                //hom ref\n-                                : 1. - noiseLevelAlleleFraction * Math.abs(rng.nextGaussian())                                                //hom alt\n-                            : rng.nextBoolean()\n-                                ? Math.max(allMinorAlleleFractions.get(i / 1000) + noiseLevelAlleleFraction * rng.nextGaussian(), 0.)          //het alt minor\n-                                : Math.min(1. - allMinorAlleleFractions.get(i / 1000) + noiseLevelAlleleFraction * rng.nextGaussian(), 1.))    //het ref minor\n-                    .map(f -> Math.min(Math.max(f, 0.), 1.))\n-                    .collect(Collectors.toList());             //changepoints at 99, 199, 299, 399, 499, 599, 699, 799, 899\n-\n-            final List<Double> alternateAlleleFractions = IntStream.range(0, numAllSites).boxed()\n-                    .filter(isNotDropped::get)\n-                    .map(allAlternateAlleleFractions::get)\n-                    .collect(Collectors.toList());\n-            final List<SimpleInterval> sites = IntStream.range(0, numAllSites).boxed()\n-                    .filter(isNotDropped::get)\n-                    .map(allSites::get)\n-                    .collect(Collectors.toList());\n-\n-            final SampleLocatableMetadata metadata = new SimpleSampleLocatableMetadata(\n-                    String.format(\"test-sample-%d\", sampleIndex),\n-                    sequenceDictionary);\n-\n-            final CopyRatioCollection denoisedCopyRatios = new CopyRatioCollection(\n-                    metadata,\n-                    IntStream.range(0, intervals.size()).boxed()\n-                            .map(i -> new CopyRatio(intervals.get(i), dataGaussian.get(i)))\n-                            .collect(Collectors.toList()));\n-            denoisedCopyRatiosPerSample.add(denoisedCopyRatios);\n-\n-            final int globalDepth = 100;\n-            final List<AllelicCount> allelicCountsList = IntStream.range(0, sites.size()).boxed()\n-                    .map(i -> new AllelicCount(\n-                            sites.get(i),\n-                            (int) ((1 - alternateAlleleFractions.get(i)) * globalDepth),\n-                            (int) (alternateAlleleFractions.get(i) * globalDepth))\n-            )\n-                    .collect(Collectors.toList());\n-            final AllelicCountCollection allelicCounts = new AllelicCountCollection(metadata, allelicCountsList);\n-            allelicCountsPerSample.add(allelicCounts);\n-        }\n-\n-        final SimpleIntervalCollection segmentsExpected =\n-                new SimpleIntervalCollection(\n-                        denoisedCopyRatiosPerSample.get(0).getMetadata(),\n-                        Arrays.asList(\n-                                new SimpleInterval(\"1\", 1, 1000),\n-                                new SimpleInterval(\"1\", 1001, 2000),\n-                                new SimpleInterval(\"1\", 2001, 2500),\n-                                new SimpleInterval(\"2\", 1, 500),\n-                                new SimpleInterval(\"2\", 501, 1500),\n-                                new SimpleInterval(\"2\", 1501, 2500),\n-                                new SimpleInterval(\"3\", 1, 1000),\n-                                new SimpleInterval(\"3\", 1001, 2000),\n-                                new SimpleInterval(\"3\", 2001, 2500),\n-                                new SimpleInterval(\"4\", 1, 500),\n-                                new SimpleInterval(\"4\", 501, 1500),\n-                                new SimpleInterval(\"4\", 1501, 2500)));\n-\n-        //we do not expect to find all breakpoints in subsets of the generated data (i.e., subsets with fewer samples)\n-        return new Object[][]{\n-                {denoisedCopyRatiosPerSample.subList(0, 1), allelicCountsPerSample.subList(0, 1), segmentsExpected, false},\n-                {denoisedCopyRatiosPerSample.subList(0, 5), allelicCountsPerSample.subList(0, 5), segmentsExpected, false},\n-                {denoisedCopyRatiosPerSample, allelicCountsPerSample, segmentsExpected, true}\n-        };\n-    }\n-\n-    @Test(dataProvider = \"dataMultisampleMultidimensionalKernelSegmenter\")\n-    public void testMultisampleMultidimensionalKernelSegmenter(final List<CopyRatioCollection> denoisedCopyRatiosPerSample,\n-                                                               final List<AllelicCountCollection> allelicCountsPerSample,\n-                                                               final SimpleIntervalCollection segmentsExpected,\n-                                                               final boolean isPassing) {\n-        final int maxNumChangepointsPerChromosome = 25;\n-        final double kernelVarianceCopyRatio = 0.;\n-        final double kernelVarianceAlleleFraction = 0.05;\n-        final double kernelScalingAlleleFraction = 1.;\n-        final int kernelApproximationDimension = 20;\n-        final List<Integer> windowSizes = Arrays.asList(8, 16, 32, 64);\n-        final double numChangepointsPenaltyLinearFactor = 10.;\n-        final double numChangepointsPenaltyLogLinearFactor = 10.;\n-\n-        final SimpleIntervalCollection segments = new MultisampleMultidimensionalKernelSegmenter(denoisedCopyRatiosPerSample, allelicCountsPerSample)\n-                .findSegmentation(maxNumChangepointsPerChromosome, kernelVarianceCopyRatio, kernelVarianceAlleleFraction,\n-                        kernelScalingAlleleFraction, kernelApproximationDimension,\n-                        windowSizes, numChangepointsPenaltyLinearFactor, numChangepointsPenaltyLogLinearFactor);\n-\n-        Assert.assertEquals(segments.equals(segmentsExpected), isPassing);\n-    }\n-}\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mzc5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r536263792", "bodyText": "This number seems pretty magical, is there any intuition for why this is a good default value?", "author": "fleharty", "createdAt": "2020-12-04T17:35:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java", "diffHunk": "@@ -0,0 +1,90 @@\n+package org.broadinstitute.hellbender.tools.copynumber.arguments;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SomaticSegmentationArgumentCollection implements Serializable {\n+    public static final long serialVersionUID = 1L;\n+\n+    //segmentation argument names\n+    public static final String MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME = \"maximum-number-of-segments-per-chromosome\";\n+    public static final String KERNEL_VARIANCE_COPY_RATIO_LONG_NAME = \"kernel-variance-copy-ratio\";\n+    public static final String KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME = \"kernel-variance-allele-fraction\";\n+    public static final String KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME = \"kernel-scaling-allele-fraction\";\n+    public static final String KERNEL_APPROXIMATION_DIMENSION_LONG_NAME = \"kernel-approximation-dimension\";\n+    public static final String WINDOW_SIZE_LONG_NAME = \"window-size\";\n+    public static final String NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME = \"number-of-changepoints-penalty-factor\";\n+\n+    @Argument(\n+            doc = \"Maximum number of segments allowed per chromosome.\",\n+            fullName = MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME,\n+            minValue = 1,\n+            optional = true\n+    )\n+    public int maxNumSegmentsPerChromosome = 1000;\n+\n+    @Argument(\n+            doc = \"Variance of Gaussian kernel for copy-ratio segmentation, if performed.  If zero, a linear kernel will be used.\",\n+            fullName = KERNEL_VARIANCE_COPY_RATIO_LONG_NAME,\n+            minValue = 0.,\n+            optional = true\n+    )\n+    public double kernelVarianceCopyRatio = 0.;\n+\n+    @Argument(\n+            doc = \"Variance of Gaussian kernel for allele-fraction segmentation, if performed.  If zero, a linear kernel will be used.\",\n+            fullName = KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME,\n+            minValue = 0.,\n+            optional = true\n+    )\n+    public double kernelVarianceAlleleFraction = 0.025;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQxMTA1Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648411056", "bodyText": "Not really. Seems to produce reasonable results on typical data. This is the sort of parameter that would need good, representative truth data and some hyperparameter optimization to tune rigorously, as it doesn't have a natural, universal scale.", "author": "samuelklee", "createdAt": "2021-06-09T15:14:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mzc5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java\ndeleted file mode 100644\nindex fe79b8676..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java\n+++ /dev/null\n\n@@ -1,90 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.arguments;\n-\n-import org.broadinstitute.barclay.argparser.Argument;\n-\n-import java.io.Serializable;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-\n-public class SomaticSegmentationArgumentCollection implements Serializable {\n-    public static final long serialVersionUID = 1L;\n-\n-    //segmentation argument names\n-    public static final String MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME = \"maximum-number-of-segments-per-chromosome\";\n-    public static final String KERNEL_VARIANCE_COPY_RATIO_LONG_NAME = \"kernel-variance-copy-ratio\";\n-    public static final String KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME = \"kernel-variance-allele-fraction\";\n-    public static final String KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME = \"kernel-scaling-allele-fraction\";\n-    public static final String KERNEL_APPROXIMATION_DIMENSION_LONG_NAME = \"kernel-approximation-dimension\";\n-    public static final String WINDOW_SIZE_LONG_NAME = \"window-size\";\n-    public static final String NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME = \"number-of-changepoints-penalty-factor\";\n-\n-    @Argument(\n-            doc = \"Maximum number of segments allowed per chromosome.\",\n-            fullName = MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public int maxNumSegmentsPerChromosome = 1000;\n-\n-    @Argument(\n-            doc = \"Variance of Gaussian kernel for copy-ratio segmentation, if performed.  If zero, a linear kernel will be used.\",\n-            fullName = KERNEL_VARIANCE_COPY_RATIO_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelVarianceCopyRatio = 0.;\n-\n-    @Argument(\n-            doc = \"Variance of Gaussian kernel for allele-fraction segmentation, if performed.  If zero, a linear kernel will be used.\",\n-            fullName = KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelVarianceAlleleFraction = 0.025;\n-\n-    @Argument(\n-            doc = \"Relative scaling S of the kernel K_AF for allele-fraction segmentation to the kernel K_CR for copy-ratio segmentation.  \" +\n-                    \"If multidimensional segmentation is performed, the total kernel used will be K_CR + S * K_AF.\",\n-            fullName = KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelScalingAlleleFraction = 1.0;\n-\n-    @Argument(\n-            doc = \"Dimension of the kernel approximation.  A subsample containing this number of data points \" +\n-                    \"will be used to construct the approximation for each chromosome.  \" +\n-                    \"If the total number of data points in a chromosome is greater \" +\n-                    \"than this number, then all data points in the chromosome will be used.  \" +\n-                    \"Time complexity scales quadratically and space complexity scales linearly with this parameter.\",\n-            fullName = KERNEL_APPROXIMATION_DIMENSION_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public int kernelApproximationDimension = 100;\n-\n-    @Argument(\n-            doc = \"Window sizes to use for calculating local changepoint costs.  \" +\n-                    \"For each window size, the cost for each data point to be a changepoint will be calculated \" +\n-                    \"assuming that the point demarcates two adjacent segments of that size.  \" +\n-                    \"Including small (large) window sizes will increase sensitivity to small (large) events.  \" +\n-                    \"Duplicate values will be ignored.\",\n-            fullName = WINDOW_SIZE_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public List<Integer> windowSizes = new ArrayList<>(Arrays.asList(8, 16, 32, 64, 128, 256));\n-\n-    @Argument(\n-            doc = \"Factor A for the penalty on the number of changepoints per chromosome for segmentation.  \" +\n-                    \"Adds a penalty of the form A * C * [1 + log (N / C)], \" +\n-                    \"where C is the number of changepoints in the chromosome, \" +\n-                    \"to the cost function for each chromosome.  \" +\n-                    \"Must be non-negative.\",\n-            fullName = NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double numChangepointsPenaltyFactor = 1.;\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5NzI3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r540597279", "bodyText": "What is N?", "author": "fleharty", "createdAt": "2020-12-11T00:19:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java", "diffHunk": "@@ -0,0 +1,90 @@\n+package org.broadinstitute.hellbender.tools.copynumber.arguments;\n+\n+import org.broadinstitute.barclay.argparser.Argument;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class SomaticSegmentationArgumentCollection implements Serializable {\n+    public static final long serialVersionUID = 1L;\n+\n+    //segmentation argument names\n+    public static final String MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME = \"maximum-number-of-segments-per-chromosome\";\n+    public static final String KERNEL_VARIANCE_COPY_RATIO_LONG_NAME = \"kernel-variance-copy-ratio\";\n+    public static final String KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME = \"kernel-variance-allele-fraction\";\n+    public static final String KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME = \"kernel-scaling-allele-fraction\";\n+    public static final String KERNEL_APPROXIMATION_DIMENSION_LONG_NAME = \"kernel-approximation-dimension\";\n+    public static final String WINDOW_SIZE_LONG_NAME = \"window-size\";\n+    public static final String NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME = \"number-of-changepoints-penalty-factor\";\n+\n+    @Argument(\n+            doc = \"Maximum number of segments allowed per chromosome.\",\n+            fullName = MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME,\n+            minValue = 1,\n+            optional = true\n+    )\n+    public int maxNumSegmentsPerChromosome = 1000;\n+\n+    @Argument(\n+            doc = \"Variance of Gaussian kernel for copy-ratio segmentation, if performed.  If zero, a linear kernel will be used.\",\n+            fullName = KERNEL_VARIANCE_COPY_RATIO_LONG_NAME,\n+            minValue = 0.,\n+            optional = true\n+    )\n+    public double kernelVarianceCopyRatio = 0.;\n+\n+    @Argument(\n+            doc = \"Variance of Gaussian kernel for allele-fraction segmentation, if performed.  If zero, a linear kernel will be used.\",\n+            fullName = KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME,\n+            minValue = 0.,\n+            optional = true\n+    )\n+    public double kernelVarianceAlleleFraction = 0.025;\n+\n+    @Argument(\n+            doc = \"Relative scaling S of the kernel K_AF for allele-fraction segmentation to the kernel K_CR for copy-ratio segmentation.  \" +\n+                    \"If multidimensional segmentation is performed, the total kernel used will be K_CR + S * K_AF.\",\n+            fullName = KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME,\n+            minValue = 0.,\n+            optional = true\n+    )\n+    public double kernelScalingAlleleFraction = 1.0;\n+\n+    @Argument(\n+            doc = \"Dimension of the kernel approximation.  A subsample containing this number of data points \" +\n+                    \"will be used to construct the approximation for each chromosome.  \" +\n+                    \"If the total number of data points in a chromosome is greater \" +\n+                    \"than this number, then all data points in the chromosome will be used.  \" +\n+                    \"Time complexity scales quadratically and space complexity scales linearly with this parameter.\",\n+            fullName = KERNEL_APPROXIMATION_DIMENSION_LONG_NAME,\n+            minValue = 1,\n+            optional = true\n+    )\n+    public int kernelApproximationDimension = 100;\n+\n+    @Argument(\n+            doc = \"Window sizes to use for calculating local changepoint costs.  \" +\n+                    \"For each window size, the cost for each data point to be a changepoint will be calculated \" +\n+                    \"assuming that the point demarcates two adjacent segments of that size.  \" +\n+                    \"Including small (large) window sizes will increase sensitivity to small (large) events.  \" +\n+                    \"Duplicate values will be ignored.\",\n+            fullName = WINDOW_SIZE_LONG_NAME,\n+            minValue = 1,\n+            optional = true\n+    )\n+    public List<Integer> windowSizes = new ArrayList<>(Arrays.asList(8, 16, 32, 64, 128, 256));\n+\n+    @Argument(\n+            doc = \"Factor A for the penalty on the number of changepoints per chromosome for segmentation.  \" +\n+                    \"Adds a penalty of the form A * C * [1 + log (N / C)], \" +", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQyODg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648428881", "bodyText": "Fixed, thanks for catching this! N is the number of data points in the chromosome.", "author": "samuelklee", "createdAt": "2021-06-09T15:32:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5NzI3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java\ndeleted file mode 100644\nindex fe79b8676..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/SomaticSegmentationArgumentCollection.java\n+++ /dev/null\n\n@@ -1,90 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.arguments;\n-\n-import org.broadinstitute.barclay.argparser.Argument;\n-\n-import java.io.Serializable;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-\n-public class SomaticSegmentationArgumentCollection implements Serializable {\n-    public static final long serialVersionUID = 1L;\n-\n-    //segmentation argument names\n-    public static final String MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME = \"maximum-number-of-segments-per-chromosome\";\n-    public static final String KERNEL_VARIANCE_COPY_RATIO_LONG_NAME = \"kernel-variance-copy-ratio\";\n-    public static final String KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME = \"kernel-variance-allele-fraction\";\n-    public static final String KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME = \"kernel-scaling-allele-fraction\";\n-    public static final String KERNEL_APPROXIMATION_DIMENSION_LONG_NAME = \"kernel-approximation-dimension\";\n-    public static final String WINDOW_SIZE_LONG_NAME = \"window-size\";\n-    public static final String NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME = \"number-of-changepoints-penalty-factor\";\n-\n-    @Argument(\n-            doc = \"Maximum number of segments allowed per chromosome.\",\n-            fullName = MAXIMUM_NUMBER_OF_SEGMENTS_PER_CHROMOSOME_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public int maxNumSegmentsPerChromosome = 1000;\n-\n-    @Argument(\n-            doc = \"Variance of Gaussian kernel for copy-ratio segmentation, if performed.  If zero, a linear kernel will be used.\",\n-            fullName = KERNEL_VARIANCE_COPY_RATIO_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelVarianceCopyRatio = 0.;\n-\n-    @Argument(\n-            doc = \"Variance of Gaussian kernel for allele-fraction segmentation, if performed.  If zero, a linear kernel will be used.\",\n-            fullName = KERNEL_VARIANCE_ALLELE_FRACTION_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelVarianceAlleleFraction = 0.025;\n-\n-    @Argument(\n-            doc = \"Relative scaling S of the kernel K_AF for allele-fraction segmentation to the kernel K_CR for copy-ratio segmentation.  \" +\n-                    \"If multidimensional segmentation is performed, the total kernel used will be K_CR + S * K_AF.\",\n-            fullName = KERNEL_SCALING_ALLELE_FRACTION_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double kernelScalingAlleleFraction = 1.0;\n-\n-    @Argument(\n-            doc = \"Dimension of the kernel approximation.  A subsample containing this number of data points \" +\n-                    \"will be used to construct the approximation for each chromosome.  \" +\n-                    \"If the total number of data points in a chromosome is greater \" +\n-                    \"than this number, then all data points in the chromosome will be used.  \" +\n-                    \"Time complexity scales quadratically and space complexity scales linearly with this parameter.\",\n-            fullName = KERNEL_APPROXIMATION_DIMENSION_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public int kernelApproximationDimension = 100;\n-\n-    @Argument(\n-            doc = \"Window sizes to use for calculating local changepoint costs.  \" +\n-                    \"For each window size, the cost for each data point to be a changepoint will be calculated \" +\n-                    \"assuming that the point demarcates two adjacent segments of that size.  \" +\n-                    \"Including small (large) window sizes will increase sensitivity to small (large) events.  \" +\n-                    \"Duplicate values will be ignored.\",\n-            fullName = WINDOW_SIZE_LONG_NAME,\n-            minValue = 1,\n-            optional = true\n-    )\n-    public List<Integer> windowSizes = new ArrayList<>(Arrays.asList(8, 16, 32, 64, 128, 256));\n-\n-    @Argument(\n-            doc = \"Factor A for the penalty on the number of changepoints per chromosome for segmentation.  \" +\n-                    \"Adds a penalty of the form A * C * [1 + log (N / C)], \" +\n-                    \"where C is the number of changepoints in the chromosome, \" +\n-                    \"to the cost function for each chromosome.  \" +\n-                    \"Must be non-negative.\",\n-            fullName = NUMBER_OF_CHANGEPOINTS_PENALTY_FACTOR_LONG_NAME,\n-            minValue = 0.,\n-            optional = true\n-    )\n-    public double numChangepointsPenaltyFactor = 1.;\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5ODExNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r540598115", "bodyText": "Because this is public, I think this should have a simple java doc.", "author": "fleharty", "createdAt": "2020-12-11T00:21:16Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/formats/metadata/SimpleLocatableMetadata.java", "diffHunk": "@@ -16,6 +16,10 @@ public SimpleLocatableMetadata(final SAMSequenceDictionary sequenceDictionary) {\n         this.sequenceDictionary = Utils.nonNull(sequenceDictionary);\n     }\n \n+    public SimpleLocatableMetadata(final LocatableMetadata metadata) {", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0ODQxNjMwNg==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r648416306", "bodyText": "These are one-liner constructors, which seem pretty self documenting to me? These metadata classes are also relatively internal to the copynumber package, as they're solely used for making sure file-format conventions can be followed. Punting, if you don't mind!", "author": "samuelklee", "createdAt": "2021-06-09T15:19:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5ODExNQ=="}], "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/formats/metadata/SimpleLocatableMetadata.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/formats/metadata/SimpleLocatableMetadata.java\nindex 9201347f1..f7c8fe7f6 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/formats/metadata/SimpleLocatableMetadata.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/formats/metadata/SimpleLocatableMetadata.java\n\n@@ -16,10 +16,6 @@ public class SimpleLocatableMetadata implements LocatableMetadata {\n         this.sequenceDictionary = Utils.nonNull(sequenceDictionary);\n     }\n \n-    public SimpleLocatableMetadata(final LocatableMetadata metadata) {\n-        this.sequenceDictionary = Utils.nonNull(metadata).getSequenceDictionary();\n-    }\n-\n     @Override\n     public SAMSequenceDictionary getSequenceDictionary() {\n         return sequenceDictionary;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAyMDYzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r541020633", "bodyText": "explicit imports", "author": "fleharty", "createdAt": "2020-12-11T15:18:16Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/genotyping/NaiveHeterozygousPileupGenotypingUtils.java", "diffHunk": "@@ -0,0 +1,254 @@\n+package org.broadinstitute.hellbender.tools.copynumber.utils.genotyping;\n+\n+import com.google.common.collect.HashMultiset;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Multiset;\n+import htsjdk.samtools.util.Locatable;\n+import org.apache.commons.math3.special.Beta;\n+import org.apache.commons.math3.util.FastMath;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SampleLocatableMetadata;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.util.*;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/genotyping/NaiveHeterozygousPileupGenotypingUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/genotyping/NaiveHeterozygousPileupGenotypingUtils.java\ndeleted file mode 100644\nindex 79675f031..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/utils/genotyping/NaiveHeterozygousPileupGenotypingUtils.java\n+++ /dev/null\n\n@@ -1,254 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.utils.genotyping;\n-\n-import com.google.common.collect.HashMultiset;\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Multiset;\n-import htsjdk.samtools.util.Locatable;\n-import org.apache.commons.math3.special.Beta;\n-import org.apache.commons.math3.util.FastMath;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.SampleLocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n-import org.broadinstitute.hellbender.utils.param.ParamUtils;\n-\n-import java.util.*;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n-import java.util.stream.Stream;\n-\n-/**\n- * Naive methods for binomial genotyping of heterozygous sites from pileup allele counts.\n- * Filters for total count and overlap with copy-ratio intervals are also implemented.\n- */\n-public final class NaiveHeterozygousPileupGenotypingUtils {\n-    private static final Logger logger = LogManager.getLogger(NaiveHeterozygousPileupGenotypingUtils.class);\n-\n-    private NaiveHeterozygousPileupGenotypingUtils() {\n-    }\n-\n-    public static final class NaiveHeterozygousPileupGenotypingResult {\n-        private final ImmutableList<AllelicCountCollection> hetAllelicCountsPerSample;\n-        private final AllelicCountCollection hetNormalAllelicCounts;\n-\n-        /**\n-         * @param hetNormalAllelicCounts    {@code null}, if result generated in case-only mode\n-         */\n-        private NaiveHeterozygousPileupGenotypingResult(final List<AllelicCountCollection> hetAllelicCountsPerSample,\n-                                                        final AllelicCountCollection hetNormalAllelicCounts) {\n-            Utils.nonEmpty(hetAllelicCountsPerSample);\n-            Utils.validateArg((int) Stream.of(hetAllelicCountsPerSample, Collections.singletonList(hetNormalAllelicCounts))\n-                            .flatMap(Collection::stream)\n-                            .filter(Objects::nonNull)\n-                            .map(AllelicCountCollection::getIntervals)\n-                            .distinct()\n-                            .count() == 1,\n-                    \"Allelic-count sites must be identical across all samples.\");\n-            CopyNumberArgumentValidationUtils.getValidatedSequenceDictionary(\n-                    Stream.of(hetAllelicCountsPerSample, Collections.singletonList(hetNormalAllelicCounts))\n-                            .flatMap(Collection::stream)\n-                            .toArray(AllelicCountCollection[]::new));\n-            this.hetAllelicCountsPerSample = ImmutableList.copyOf(hetAllelicCountsPerSample);\n-            this.hetNormalAllelicCounts = hetNormalAllelicCounts;\n-        }\n-\n-        public List<AllelicCountCollection> getHetAllelicCountsPerSample() {\n-            return hetAllelicCountsPerSample;\n-        }\n-\n-        /**\n-         * @return {@code null}, if result generated in case-only mode\n-         */\n-        public AllelicCountCollection getHetNormalAllelicCounts() {\n-            return hetNormalAllelicCounts;\n-        }\n-    }\n-\n-    /**\n-     * Filters allelic counts based on total count, overlap with copy-ratio intervals, and a naive test for heterozygosity.\n-     * This method can be called either in matched-normal or case-only mode.\n-     * In the former, the test for heterozygosity is only performed on the normal and the corresponding sites are filtered\n-     * out of the case samples; this ensures that the set of genotyped sites is identical across all samples.\n-     * In the latter (activated when {@code normalAllelicCounts} is {@code null}), the test for heterozygosity is performed\n-     * individually on each case sample, and then the intersection of all genotyped sites is taken to ensure an identical\n-     * set across all samples.\n-     * Validation of allelic-count sites and sequence dictionaries will be performed.\n-     * @param allelicCountsPerSample    non-empty\n-     * @param normalAllelicCounts       if not {@code null}, sites that are homozygous in the normal will be filtered out;\n-     *                                  if {@code null} (i.e., case-only),\n-     * @param copyRatioIntervals        never {@code null} (but may be empty), sites not overlapping with copy-ratio intervals will be filtered out\n-     * @return {@link NaiveHeterozygousPileupGenotypingResult}, with hetNormalAllelicCounts set to {@code null} for case-only mode\n-     */\n-    public static NaiveHeterozygousPileupGenotypingResult genotypeHets(final List<AllelicCountCollection> allelicCountsPerSample,\n-                                                                       final AllelicCountCollection normalAllelicCounts,\n-                                                                       final SimpleIntervalCollection copyRatioIntervals,\n-                                                                       final int minTotalAlleleCountCase,\n-                                                                       final int minTotalAlleleCountNormal,\n-                                                                       final double genotypingHomozygousLogRatioThreshold,\n-                                                                       final double genotypingBaseErrorRate) {\n-        Utils.nonEmpty(allelicCountsPerSample);\n-        Utils.nonNull(copyRatioIntervals);\n-        ParamUtils.isPositiveOrZero(minTotalAlleleCountCase, \"Minimum total allele count for the case sample must be positive or zero.\");\n-        ParamUtils.isPositiveOrZero(minTotalAlleleCountNormal, \"Minimum total allele count for the normal sample must be positive or zero.\");\n-        ParamUtils.isFinite(genotypingHomozygousLogRatioThreshold, \"Genotyping homozygous log-ratio threshold must be finite.\");\n-        ParamUtils.inRange(genotypingBaseErrorRate, 0., 1., \"Genotyping base-error rate must be in [0, 1].\");\n-        Utils.validateArg((int) Stream.of(allelicCountsPerSample, Collections.singletonList(normalAllelicCounts))\n-                        .flatMap(Collection::stream)\n-                        .filter(Objects::nonNull)\n-                        .map(AllelicCountCollection::getIntervals)\n-                        .distinct()\n-                        .count() == 1,\n-                \"Allelic-count sites must be identical across all samples.\");\n-        CopyNumberArgumentValidationUtils.getValidatedSequenceDictionary(\n-                Stream.of(allelicCountsPerSample, Arrays.asList(normalAllelicCounts, copyRatioIntervals))\n-                        .flatMap(Collection::stream)\n-                        .toArray(AbstractLocatableCollection[]::new));\n-\n-        logger.info(\"Genotyping heterozygous sites from available allelic counts...\");\n-\n-        final List<AllelicCountCollection> hetAllelicCountsPerSample = new ArrayList<>(allelicCountsPerSample.size());\n-        final AllelicCountCollection hetNormalAllelicCounts;\n-\n-        if (normalAllelicCounts != null) {\n-            logger.info(\"Matched normal was provided, running in matched-normal mode...\");\n-\n-            final SampleLocatableMetadata normalMetadata = normalAllelicCounts.getMetadata();\n-            final String normalSampleName = normalMetadata.getSampleName();\n-\n-            //filter on total count in matched normal\n-            AllelicCountCollection filteredNormalAllelicCounts = filterByTotalCount(normalAllelicCounts, minTotalAlleleCountNormal);\n-            logger.info(String.format(\"Retained %d / %d sites after filtering allelic counts with total count less than %d in matched-normal sample %s...\",\n-                    filteredNormalAllelicCounts.size(), normalAllelicCounts.size(), minTotalAlleleCountNormal, normalSampleName));\n-\n-            //filter matched normal on overlap with copy-ratio intervals, if available\n-            if (copyRatioIntervals.size() > 0) {\n-                filteredNormalAllelicCounts = filterByOverlap(filteredNormalAllelicCounts, copyRatioIntervals);\n-                logger.info(String.format(\"Retained %d / %d sites after filtering on overlap with copy-ratio intervals in matched-normal sample %s...\",\n-                        filteredNormalAllelicCounts.size(), normalAllelicCounts.size(), normalSampleName));\n-            }\n-\n-            //filter on heterozygosity in matched normal\n-            hetNormalAllelicCounts = filterByHeterozygosity(filteredNormalAllelicCounts, genotypingHomozygousLogRatioThreshold, genotypingBaseErrorRate);\n-            logger.info(String.format(\"Retained %d / %d sites after filtering on heterozygosity in matched-normal sample %s...\",\n-                    hetNormalAllelicCounts.size(), normalAllelicCounts.size(), normalSampleName));\n-        } else {\n-            logger.info(\"No matched normal was provided, not running in matched-normal mode...\");\n-            hetNormalAllelicCounts = null;\n-        }\n-\n-        //filter and genotype all case samples\n-        for (final AllelicCountCollection allelicCounts : allelicCountsPerSample) {\n-            final String sampleName = allelicCounts.getMetadata().getSampleName();\n-\n-            //filter on total count in case sample\n-            AllelicCountCollection filteredAllelicCounts = filterByTotalCount(allelicCounts, minTotalAlleleCountCase);\n-            logger.info(String.format(\"Retained %d / %d sites after filtering allelic counts with total count less than %d in case sample %s...\",\n-                    filteredAllelicCounts.size(), allelicCounts.size(), minTotalAlleleCountCase, sampleName));\n-\n-            //filter on overlap with copy-ratio intervals, if available\n-            if (copyRatioIntervals.size() > 0) {\n-                filteredAllelicCounts = filterByOverlap(filteredAllelicCounts, copyRatioIntervals);\n-                logger.info(String.format(\"Retained %d / %d sites after filtering on overlap with copy-ratio intervals in case sample %s...\",\n-                        filteredAllelicCounts.size(), allelicCounts.size(), sampleName));\n-            }\n-\n-            final AllelicCountCollection hetAllelicCounts;\n-            if (hetNormalAllelicCounts != null) {\n-                //retrieve sites that were heterozygous in normal from the case sample\n-                logger.info(String.format(\"Retaining allelic counts for case sample %s at heterozygous sites in matched-normal sample %s...\",\n-                        hetNormalAllelicCounts.getMetadata().getSampleName(), sampleName));\n-                hetAllelicCounts = filterByOverlap(filteredAllelicCounts, hetNormalAllelicCounts);\n-            } else {\n-                hetAllelicCounts = filterByHeterozygosity(filteredAllelicCounts, genotypingHomozygousLogRatioThreshold, genotypingBaseErrorRate);\n-                logger.info(String.format(\"Retained %d / %d sites after filtering on heterozygosity in case sample %s...\",\n-                        hetAllelicCounts.size(), allelicCounts.size(), sampleName));\n-            }\n-            logger.info(String.format(\"Retained %d / %d sites after applying all filters to case sample %s.\",\n-                    hetAllelicCounts.size(), allelicCounts.size(), sampleName));\n-            hetAllelicCountsPerSample.add(hetAllelicCounts);\n-        }\n-\n-        if (hetAllelicCountsPerSample.size() > 1) {\n-            //filter by intersection of heterozygous sites in all case samples\n-            final int maxNumHetSites = hetAllelicCountsPerSample.stream()\n-                    .mapToInt(AllelicCountCollection::size)\n-                    .max().getAsInt();\n-            final Multiset<SimpleInterval> hetSitesMultiset = HashMultiset.create(maxNumHetSites);\n-            hetAllelicCountsPerSample.stream()\n-                    .map(AllelicCountCollection::getIntervals)\n-                    .forEach(hetSitesMultiset::addAll);\n-            final List<SimpleInterval> hetSitesIntersectionList = hetSitesMultiset.entrySet().stream()\n-                    .filter(e -> e.getCount() == hetAllelicCountsPerSample.size())\n-                    .map(Multiset.Entry::getElement)\n-                    .collect(Collectors.toList());\n-            final SimpleIntervalCollection hetSitesIntersection = new SimpleIntervalCollection(\n-                    hetAllelicCountsPerSample.get(0).getMetadata(),\n-                    hetSitesIntersectionList);\n-            IntStream.range(0, hetAllelicCountsPerSample.size())\n-                    .forEach(i -> hetAllelicCountsPerSample.set(i, filterByOverlap(hetAllelicCountsPerSample.get(i), hetSitesIntersection)));\n-            logger.info(String.format(\"Retained %d / %d sites after taking intersection of sites in all case samples.\",\n-                    hetSitesIntersection.size(), allelicCountsPerSample.get(0).size()));\n-        }\n-\n-        return new NaiveHeterozygousPileupGenotypingResult(hetAllelicCountsPerSample, hetNormalAllelicCounts);\n-    }\n-\n-    private static AllelicCountCollection filterByTotalCount(final AllelicCountCollection allelicCounts,\n-                                                             final int minTotalAlleleCount) {\n-        return minTotalAlleleCount == 0\n-                ? allelicCounts\n-                : new AllelicCountCollection(\n-                        allelicCounts.getMetadata(),\n-                        allelicCounts.getRecords().stream()\n-                                .filter(ac -> ac.getTotalReadCount() >= minTotalAlleleCount)\n-                                .collect(Collectors.toList()));\n-    }\n-\n-    private static <T extends Locatable> AllelicCountCollection filterByOverlap(final AllelicCountCollection allelicCounts,\n-                                                                                final AbstractLocatableCollection<?, T> locatableCollection) {\n-        return locatableCollection.size() == 0\n-                ? new AllelicCountCollection(\n-                        allelicCounts.getMetadata(),\n-                        Collections.emptyList())\n-                : new AllelicCountCollection(\n-                        allelicCounts.getMetadata(),\n-                        allelicCounts.getRecords().stream()\n-                            .filter(ac -> locatableCollection.getOverlapDetector().overlapsAny(ac))\n-                                .collect(Collectors.toList()));\n-    }\n-\n-    private static AllelicCountCollection filterByHeterozygosity(final AllelicCountCollection allelicCounts,\n-                                                                 final double genotypingHomozygousLogRatioThreshold,\n-                                                                 final double genotypingBaseErrorRate) {\n-        return new AllelicCountCollection(\n-                allelicCounts.getMetadata(),\n-                allelicCounts.getRecords().stream()\n-                        .filter(ac -> calculateHomozygousLogRatio(ac, genotypingBaseErrorRate) < genotypingHomozygousLogRatioThreshold)\n-                        .collect(Collectors.toList()));\n-    }\n-\n-    /**\n-     * Calculates a log likelihood ratio to perform a relatively naive test for heterozygosity.\n-     * For a binomial likelihood, returns the log of the ratio of\n-     *  1) the likelihood of the success probability being in [0, {@code genotypingBaseErrorRate}] or [1 - {@code genotypingBaseErrorRate}, 1], and\n-     *  2) the likelihood of the success probability being in [{@code genotypingBaseErrorRate}, 1 - {@code genotypingBaseErrorRate}],\n-     * given the count data represented by {@code allelicCount}.\n-     */\n-    private static double calculateHomozygousLogRatio(final AllelicCount allelicCount,\n-                                                      final double genotypingBaseErrorRate) {\n-        final int r = allelicCount.getRefReadCount();\n-        final int n = allelicCount.getTotalReadCount();\n-        final double betaAll = Beta.regularizedBeta(1, r + 1, n - r + 1);\n-        final double betaError = Beta.regularizedBeta(genotypingBaseErrorRate, r + 1, n - r + 1);\n-        final double betaOneMinusError = Beta.regularizedBeta(1 - genotypingBaseErrorRate, r + 1, n - r + 1);\n-        final double betaHom = betaError + betaAll - betaOneMinusError;\n-        final double betaHet = betaOneMinusError - betaError;\n-        return FastMath.log(betaHom) - FastMath.log(betaHet);\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA0NjA2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6499#discussion_r541046062", "bodyText": "explicit imports", "author": "fleharty", "createdAt": "2020-12-11T15:53:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenter.java", "diffHunk": "@@ -0,0 +1,323 @@\n+package org.broadinstitute.hellbender.tools.copynumber.segmentation;\n+\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.samtools.util.OverlapDetector;\n+import org.apache.commons.math3.distribution.NormalDistribution;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.CopyRatioCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.LocatableMetadata;\n+import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n+import org.broadinstitute.hellbender.tools.copynumber.utils.segmentation.KernelSegmenter;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.param.ParamUtils;\n+\n+import java.util.*;", "originalCommit": "1dba22a70a8207ae438f75f3d3092e3e78f9b3bf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89944c82241e8e182a20596dac21e0106b85eee6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenter.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenter.java\ndeleted file mode 100644\nindex 541fb3c46..000000000\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/segmentation/MultisampleMultidimensionalKernelSegmenter.java\n+++ /dev/null\n\n@@ -1,323 +0,0 @@\n-package org.broadinstitute.hellbender.tools.copynumber.segmentation;\n-\n-import htsjdk.samtools.util.Locatable;\n-import htsjdk.samtools.util.OverlapDetector;\n-import org.apache.commons.math3.distribution.NormalDistribution;\n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n-import org.broadinstitute.hellbender.tools.copynumber.arguments.CopyNumberArgumentValidationUtils;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AbstractLocatableCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.AllelicCountCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.CopyRatioCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.collections.SimpleIntervalCollection;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.metadata.LocatableMetadata;\n-import org.broadinstitute.hellbender.tools.copynumber.formats.records.AllelicCount;\n-import org.broadinstitute.hellbender.tools.copynumber.utils.segmentation.KernelSegmenter;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n-import org.broadinstitute.hellbender.utils.param.ParamUtils;\n-\n-import java.util.*;\n-import java.util.function.BiFunction;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n-import java.util.stream.Stream;\n-\n-/**\n- * Segments copy-ratio data and/or alternate-allele-fraction data from one or more samples using kernel segmentation.\n- * Copy-ratio intervals and/or allele-fraction sites must be identical in all samples.  Segments do not span chromosomes.\n- * If both types of data are provided, only the first allele-fraction site in each copy-ratio interval is used, and\n- * the alternate-allele fraction in copy-ratio intervals that do not contain any sites is imputed to be balanced at 0.5.\n- *\n- * @author Samuel Lee &lt;slee@broadinstitute.org&gt;\n- */\n-public final class MultisampleMultidimensionalKernelSegmenter {\n-    private static final Logger logger = LogManager.getLogger(MultisampleMultidimensionalKernelSegmenter.class);\n-\n-    private enum Mode {\n-        COPY_RATIO_ONLY, ALLELE_FRACTION_ONLY, COPY_RATIO_AND_ALLELE_FRACTION\n-    }\n-\n-    private static final int MIN_NUM_POINTS_REQUIRED_PER_CHROMOSOME = 10;\n-\n-    //assume alternate-allele fraction is 0.5 for missing data\n-    private static final SimpleInterval DUMMY_INTERVAL = new SimpleInterval(\"DUMMY\", 1, 1);\n-    private static final AllelicCount BALANCED_ALLELIC_COUNT = new AllelicCount(DUMMY_INTERVAL, 1, 1);\n-\n-    //Gaussian kernel for a specified variance; if variance is zero, use a linear kernel\n-    private static final Function<Double, BiFunction<Double, Double, Double>> KERNEL =\n-            standardDeviation -> standardDeviation == 0.\n-                    ? (x, y) -> x * y\n-                    : (x, y) -> new NormalDistribution(null, x, standardDeviation).density(y);\n-\n-    private static final class MultidimensionalPoint implements Locatable {\n-        private final SimpleInterval interval;\n-        private final double[] log2CopyRatios;\n-        private final double[] alternateAlleleFractions;\n-\n-        MultidimensionalPoint(final SimpleInterval interval,\n-                              final double[] log2CopyRatios,\n-                              final double[] alternateAlleleFractions) {\n-            this.interval = interval;\n-            this.log2CopyRatios = log2CopyRatios;\n-            this.alternateAlleleFractions = alternateAlleleFractions;\n-        }\n-\n-        @Override\n-        public String getContig() {\n-            return interval.getContig();\n-        }\n-\n-        @Override\n-        public int getStart() {\n-            return interval.getStart();\n-        }\n-\n-        @Override\n-        public int getEnd() {\n-            return interval.getEnd();\n-        }\n-    }\n-\n-    private final Mode mode;\n-    private final int numSamples;\n-    private final int numPointsCopyRatio;\n-    private final int numPointsAlleleFraction;\n-    private final LocatableMetadata metadata;\n-    private final Map<String, List<MultidimensionalPoint>> multidimensionalPointsPerChromosome;\n-\n-    /**\n-     * @param denoisedCopyRatiosPerSample   non-empty; all copy-ratio intervals identical across samples;\n-     *                                      number of samples and order identical to that in {@code allelicCountsPerSample};\n-     *                                      pass list of empty {@link CopyRatioCollection}s for {@code ALLELE_FRACTION_ONLY} mode\n-     * @param allelicCountsPerSample        non-empty; all allele-fraction sites identical across samples;\n-     *                                      number of samples and order identical to that in {@code denoisedCopyRatiosPerSample};\n-     *                                      pass list of empty {@link AllelicCountCollection}s for {@code COPY_RATIO_ONLY} mode\n-     */\n-    public MultisampleMultidimensionalKernelSegmenter(final List<CopyRatioCollection> denoisedCopyRatiosPerSample,\n-                                                      final List<AllelicCountCollection> allelicCountsPerSample) {\n-        validateInputs(denoisedCopyRatiosPerSample, allelicCountsPerSample);\n-        numSamples = denoisedCopyRatiosPerSample.size();\n-        final CopyRatioCollection denoisedCopyRatiosFirstSample = denoisedCopyRatiosPerSample.get(0);\n-        final AllelicCountCollection allelicCountsFirstSample = allelicCountsPerSample.get(0);\n-        metadata = denoisedCopyRatiosFirstSample.getMetadata();\n-        numPointsCopyRatio = denoisedCopyRatiosFirstSample.size();\n-        numPointsAlleleFraction = allelicCountsFirstSample.size();\n-\n-        if (numPointsAlleleFraction == 0) {\n-            mode = Mode.COPY_RATIO_ONLY;\n-            multidimensionalPointsPerChromosome = IntStream.range(0, numPointsCopyRatio).boxed()\n-                    .map(i -> new MultidimensionalPoint(\n-                            denoisedCopyRatiosFirstSample.getRecords().get(i).getInterval(),\n-                            denoisedCopyRatiosPerSample.stream()\n-                                    .mapToDouble(cr -> cr.getRecords().get(i).getLog2CopyRatioValue())\n-                                    .toArray(),\n-                            null))\n-                    .collect(Collectors.groupingBy(\n-                            MultidimensionalPoint::getContig,\n-                            LinkedHashMap::new,\n-                            Collectors.toList()));\n-        } else if (numPointsCopyRatio == 0) {\n-            mode = Mode.ALLELE_FRACTION_ONLY;\n-            multidimensionalPointsPerChromosome = IntStream.range(0, numPointsAlleleFraction).boxed()\n-                    .map(i -> new MultidimensionalPoint(\n-                            allelicCountsFirstSample.getRecords().get(i).getInterval(),\n-                            null,\n-                            allelicCountsPerSample.stream()\n-                                    .mapToDouble(ac -> ac.getRecords().get(i).getAlternateAlleleFraction())\n-                                    .toArray()))\n-                    .collect(Collectors.groupingBy(\n-                            MultidimensionalPoint::getContig,\n-                            LinkedHashMap::new,\n-                            Collectors.toList()));\n-        } else {\n-            mode = Mode.COPY_RATIO_AND_ALLELE_FRACTION;\n-            final OverlapDetector<AllelicCount> allelicCountOverlapDetector = allelicCountsFirstSample.getOverlapDetector();\n-            final Comparator<Locatable> comparator = denoisedCopyRatiosFirstSample.getComparator();\n-            final Map<SimpleInterval, Integer> allelicSiteToIndexMap = IntStream.range(0, numPointsAlleleFraction).boxed()\n-                    .collect(Collectors.toMap(\n-                            i -> allelicCountsFirstSample.getRecords().get(i).getInterval(),\n-                            Function.identity(),\n-                            (u, v) -> {\n-                                throw new GATKException.ShouldNeverReachHereException(\"Cannot have duplicate sites.\");\n-                            },   //sites should already be distinct\n-                            LinkedHashMap::new));\n-            final Map<Integer, Integer> intervalIndexToSiteIndexMap = IntStream.range(0, numPointsCopyRatio).boxed()\n-                    .collect(Collectors.toMap(\n-                            Function.identity(),\n-                            i -> allelicCountOverlapDetector.getOverlaps(denoisedCopyRatiosFirstSample.getRecords().get(i)).stream()\n-                                    .map(AllelicCount::getInterval)\n-                                    .min(comparator::compare)\n-                                    .map(allelicSiteToIndexMap::get)\n-                                    .orElse(-1),\n-                            (u, v) -> {\n-                                throw new GATKException.ShouldNeverReachHereException(\"Cannot have duplicate indices.\");\n-                            },\n-                            LinkedHashMap::new));\n-            final int numAllelicCountsToUse = (int) intervalIndexToSiteIndexMap.values().stream()\n-                    .filter(i -> i != -1)\n-                    .count();\n-            logger.info(String.format(\"Using first allelic-count site in each copy-ratio interval (%d / %d) for multidimensional segmentation...\",\n-                    numAllelicCountsToUse, numPointsAlleleFraction));\n-            multidimensionalPointsPerChromosome = IntStream.range(0, numPointsCopyRatio).boxed()\n-                    .map(i -> new MultidimensionalPoint(\n-                            denoisedCopyRatiosFirstSample.getRecords().get(i).getInterval(),\n-                            denoisedCopyRatiosPerSample.stream()\n-                                    .mapToDouble(denoisedCopyRatios -> denoisedCopyRatios.getRecords().get(i).getLog2CopyRatioValue())\n-                                    .toArray(),\n-                            allelicCountsPerSample.stream()\n-                                    .map(allelicCounts -> intervalIndexToSiteIndexMap.get(i) != -1\n-                                            ? allelicCounts.getRecords().get(intervalIndexToSiteIndexMap.get(i))\n-                                            : BALANCED_ALLELIC_COUNT)\n-                                    .mapToDouble(AllelicCount::getAlternateAlleleFraction)\n-                                    .toArray()))\n-                    .collect(Collectors.groupingBy(\n-                            MultidimensionalPoint::getContig,\n-                            LinkedHashMap::new,\n-                            Collectors.toList()));\n-        }\n-    }\n-\n-    private static void validateInputs(final List<CopyRatioCollection> denoisedCopyRatiosPerSample,\n-                                       final List<AllelicCountCollection> allelicCountsPerSample) {\n-        Utils.nonEmpty(denoisedCopyRatiosPerSample);\n-        Utils.nonEmpty(allelicCountsPerSample);\n-        Utils.validateArg(denoisedCopyRatiosPerSample.size() == allelicCountsPerSample.size(),\n-                \"Number of copy-ratio and allelic-count collections must be equal.\");\n-\n-        Utils.validateArg(IntStream.range(0, denoisedCopyRatiosPerSample.size())\n-                        .allMatch(i -> denoisedCopyRatiosPerSample.get(i).getMetadata().equals(allelicCountsPerSample.get(i).getMetadata())),\n-                \"Metadata do not match across copy-ratio and allelic-count collections for the samples.  \" +\n-                        \"Check that the sample orders for the corresponding inputs are identical.\");\n-\n-        CopyNumberArgumentValidationUtils.getValidatedSequenceDictionary(\n-                Stream.of(denoisedCopyRatiosPerSample, allelicCountsPerSample)\n-                        .flatMap(Collection::stream)\n-                        .toArray(AbstractLocatableCollection[]::new));\n-\n-        Utils.validateArg((int) denoisedCopyRatiosPerSample.stream()\n-                        .map(CopyRatioCollection::getIntervals)\n-                        .distinct()\n-                        .count() == 1,\n-                \"Copy-ratio intervals must be identical across all samples.\");\n-\n-        Utils.validateArg((int) allelicCountsPerSample.stream()\n-                        .map(AllelicCountCollection::getIntervals)\n-                        .distinct()\n-                        .count() == 1,\n-                \"Allelic-count sites must be identical across all samples.\");\n-    }\n-\n-    /**\n-     * Segments the internally held {@link CopyRatioCollection} and {@link AllelicCountCollection}\n-     * using a separate {@link KernelSegmenter} for each chromosome.\n-     * @param kernelVarianceCopyRatio       variance of the Gaussian kernel used for copy-ratio data;\n-     *                                      if zero, a linear kernel is used instead\n-     * @param kernelVarianceAlleleFraction  variance of the Gaussian kernel used for allele-fraction data;\n-     *                                      if zero, a linear kernel is used instead\n-     * @param kernelScalingAlleleFraction   relative scaling S of the kernel K_AF for allele-fraction data\n-     *                                      to the kernel K_CR for copy-ratio data;\n-     *                                      the total kernel is K_CR + S * K_AF\n-     */\n-    public SimpleIntervalCollection findSegmentation(final int maxNumSegmentsPerChromosome,\n-                                                     final double kernelVarianceCopyRatio,\n-                                                     final double kernelVarianceAlleleFraction,\n-                                                     final double kernelScalingAlleleFraction,\n-                                                     final int kernelApproximationDimension,\n-                                                     final List<Integer> windowSizes,\n-                                                     final double numChangepointsPenaltyLinearFactor,\n-                                                     final double numChangepointsPenaltyLogLinearFactor) {\n-        ParamUtils.isPositive(maxNumSegmentsPerChromosome, \"Maximum number of segments must be positive.\");\n-        ParamUtils.isPositiveOrZero(kernelVarianceCopyRatio, \"Variance of copy-ratio Gaussian kernel must be non-negative (if zero, a linear kernel will be used).\");\n-        ParamUtils.isPositiveOrZero(kernelVarianceAlleleFraction, \"Variance of allele-fraction Gaussian kernel must be non-negative (if zero, a linear kernel will be used).\");\n-        ParamUtils.isPositiveOrZero(kernelScalingAlleleFraction, \"Scaling of allele-fraction Gaussian kernel must be non-negative.\");\n-        ParamUtils.isPositive(kernelApproximationDimension, \"Dimension of kernel approximation must be positive.\");\n-        Utils.validateArg(windowSizes.stream().allMatch(ws -> ws > 0), \"Window sizes must all be positive.\");\n-        Utils.validateArg(new HashSet<>(windowSizes).size() == windowSizes.size(), \"Window sizes must all be unique.\");\n-        ParamUtils.isPositiveOrZero(numChangepointsPenaltyLinearFactor,\n-                \"Linear factor for the penalty on the number of changepoints per chromosome must be non-negative.\");\n-        ParamUtils.isPositiveOrZero(numChangepointsPenaltyLogLinearFactor,\n-                \"Log-linear factor for the penalty on the number of changepoints per chromosome must be non-negative.\");\n-\n-        final BiFunction<MultidimensionalPoint, MultidimensionalPoint, Double> kernel = constructKernel(\n-                kernelVarianceCopyRatio, kernelVarianceAlleleFraction, kernelScalingAlleleFraction);\n-\n-        final int maxNumChangepointsPerChromosome = maxNumSegmentsPerChromosome - 1;\n-\n-        logger.info(String.format(\"Finding changepoints in (%d, %d) data points and %d chromosomes across %d sample(s)...\",\n-                numPointsCopyRatio, numPointsAlleleFraction, multidimensionalPointsPerChromosome.size(), numSamples));\n-\n-        //loop over chromosomes, find changepoints, and create segments\n-        final List<SimpleInterval> segments = new ArrayList<>();\n-        for (final String chromosome : multidimensionalPointsPerChromosome.keySet()) {\n-            final List<MultidimensionalPoint> multidimensionalPointsInChromosome = multidimensionalPointsPerChromosome.get(chromosome);\n-            final int numMultidimensionalPointsInChromosome = multidimensionalPointsInChromosome.size();\n-            logger.info(String.format(\"Finding changepoints in %d data points in chromosome %s...\",\n-                    numMultidimensionalPointsInChromosome, chromosome));\n-\n-            if (numMultidimensionalPointsInChromosome < MIN_NUM_POINTS_REQUIRED_PER_CHROMOSOME) {\n-                logger.warn(String.format(\"Number of points in chromosome %s (%d) is less than that required (%d), skipping segmentation...\",\n-                        chromosome, numMultidimensionalPointsInChromosome, MIN_NUM_POINTS_REQUIRED_PER_CHROMOSOME));\n-                final int start = multidimensionalPointsInChromosome.get(0).getStart();\n-                final int end = multidimensionalPointsInChromosome.get(numMultidimensionalPointsInChromosome - 1).getEnd();\n-                segments.add(new SimpleInterval(chromosome, start, end));\n-                continue;\n-            }\n-\n-            final List<Integer> changepoints = new ArrayList<>(new KernelSegmenter<>(multidimensionalPointsInChromosome)\n-                .findChangepoints(maxNumChangepointsPerChromosome, kernel, kernelApproximationDimension,\n-                        windowSizes, numChangepointsPenaltyLinearFactor, numChangepointsPenaltyLogLinearFactor, KernelSegmenter.ChangepointSortOrder.INDEX));\n-\n-            if (!changepoints.contains(numMultidimensionalPointsInChromosome)) {\n-                changepoints.add(numMultidimensionalPointsInChromosome - 1);\n-            }\n-            int previousChangepoint = -1;\n-            for (final int changepoint : changepoints) {\n-                final int start = multidimensionalPointsPerChromosome.get(chromosome).get(previousChangepoint + 1).getStart();\n-                final int end = multidimensionalPointsPerChromosome.get(chromosome).get(changepoint).getEnd();\n-                segments.add(new SimpleInterval(chromosome, start, end));\n-                previousChangepoint = changepoint;\n-            }\n-        }\n-        logger.info(String.format(\"Found %d segments in %d chromosomes across %d sample(s).\", segments.size(), multidimensionalPointsPerChromosome.size(), numSamples));\n-        return new SimpleIntervalCollection(metadata, segments);\n-    }\n-\n-    private BiFunction<MultidimensionalPoint, MultidimensionalPoint, Double> constructKernel(final double kernelVarianceCopyRatio,\n-                                                                                             final double kernelVarianceAlleleFraction,\n-                                                                                             final double kernelScalingAlleleFraction) {\n-        final double standardDeviationCopyRatio = Math.sqrt(kernelVarianceCopyRatio);\n-        final double standardDeviationAlleleFraction = Math.sqrt(kernelVarianceAlleleFraction);\n-        switch (mode) {\n-            case COPY_RATIO_ONLY:\n-                return (p1, p2) -> {\n-                    double sum = 0.;\n-                    for (int sampleIndex = 0; sampleIndex < numSamples; sampleIndex++) {\n-                        sum += KERNEL.apply(standardDeviationCopyRatio).apply(p1.log2CopyRatios[sampleIndex], p2.log2CopyRatios[sampleIndex]);\n-                    }\n-                    return sum;\n-                };\n-            case ALLELE_FRACTION_ONLY:\n-                return (p1, p2) -> {\n-                    double sum = 0.;\n-                    for (int sampleIndex = 0; sampleIndex < numSamples; sampleIndex++) {\n-                        sum += KERNEL.apply(standardDeviationAlleleFraction).apply(p1.alternateAlleleFractions[sampleIndex], p2.alternateAlleleFractions[sampleIndex]);\n-                    }\n-                    return sum;\n-                };\n-            case COPY_RATIO_AND_ALLELE_FRACTION:\n-                return (p1, p2) -> {\n-                    double sum = 0.;\n-                    for (int sampleIndex = 0; sampleIndex < numSamples; sampleIndex++) {\n-                        sum += KERNEL.apply(standardDeviationCopyRatio).apply(p1.log2CopyRatios[sampleIndex], p2.log2CopyRatios[sampleIndex]) +\n-                                kernelScalingAlleleFraction * KERNEL.apply(standardDeviationAlleleFraction).apply(p1.alternateAlleleFractions[sampleIndex], p2.alternateAlleleFractions[sampleIndex]);\n-                    }\n-                    return sum;\n-                };\n-            default:\n-                throw new GATKException.ShouldNeverReachHereException(\"Encountered unknown Mode.\");\n-        }\n-    }\n-}\n"}}, {"oid": "89944c82241e8e182a20596dac21e0106b85eee6", "url": "https://github.com/broadinstitute/gatk/commit/89944c82241e8e182a20596dac21e0106b85eee6", "message": "Fixed minor documentation and miscellaneous issues.", "committedDate": "2021-06-09T14:56:59Z", "type": "commit"}, {"oid": "38612e35f2b1ff36fc875dca16198aefd1a8f0c5", "url": "https://github.com/broadinstitute/gatk/commit/38612e35f2b1ff36fc875dca16198aefd1a8f0c5", "message": "Extracted somatic CNV argument collections.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "3b75111c598eac9a3017f505ff67fe98b40d5159", "url": "https://github.com/broadinstitute/gatk/commit/3b75111c598eac9a3017f505ff67fe98b40d5159", "message": "Extracted genotyping utility method and added unit tests.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "5419e2e0e9b213197e00cd51669c9a3604e16855", "url": "https://github.com/broadinstitute/gatk/commit/5419e2e0e9b213197e00cd51669c9a3604e16855", "message": "Refactored KernelSegmenters to use MultisampleMultidimensionalKernelSegmenter, removed unnecessary segment classes, and changed bounds of SimpleIntervalCollection and AnnotatedIntervalCollection.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "6ab8b740164eddd9f17f0263367dbf5544c9bebf", "url": "https://github.com/broadinstitute/gatk/commit/6ab8b740164eddd9f17f0263367dbf5544c9bebf", "message": "Added test resources for ModelSegmentsIntegrationTest.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "98cd553d0e391897575c7c255ab866d864cae221", "url": "https://github.com/broadinstitute/gatk/commit/98cd553d0e391897575c7c255ab866d864cae221", "message": "Added utility methods and minor fixes for validation.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "b62fa3de3c9cd22ad610e6e0f7850b041e222122", "url": "https://github.com/broadinstitute/gatk/commit/b62fa3de3c9cd22ad610e6e0f7850b041e222122", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2021-06-09T14:57:01Z", "type": "commit"}, {"oid": "b62fa3de3c9cd22ad610e6e0f7850b041e222122", "url": "https://github.com/broadinstitute/gatk/commit/b62fa3de3c9cd22ad610e6e0f7850b041e222122", "message": "Enabled multisample segmentation in ModelSegments.", "committedDate": "2021-06-09T14:57:01Z", "type": "forcePushed"}, {"oid": "12b003b3e5f566b9c147c1a022a3db6ee8f3fbab", "url": "https://github.com/broadinstitute/gatk/commit/12b003b3e5f566b9c147c1a022a3db6ee8f3fbab", "message": "Optimized imports in copynumber packages using explicit imports.", "committedDate": "2021-06-09T15:00:12Z", "type": "commit"}, {"oid": "eedf857168196622d40322134c94d5f7f7903fd8", "url": "https://github.com/broadinstitute/gatk/commit/eedf857168196622d40322134c94d5f7f7903fd8", "message": "Addressed review comments.", "committedDate": "2021-06-09T15:32:32Z", "type": "commit"}]}