{"pr_number": 6652, "pr_title": "de-sparkify SV discovery", "pr_createdAt": "2020-06-09T17:22:51Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6652", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQzOTY0MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r454439640", "bodyText": "I think someone from the engine team should probably review this part of PR (This interface and its uses).", "author": "cwhelan", "createdAt": "2020-07-14T15:23:03Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/BasicReference.java", "diffHunk": "@@ -0,0 +1,15 @@\n+package org.broadinstitute.hellbender.engine;\n+\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+/**\n+ * A source of reference base calls.\n+ */\n+public interface BasicReference {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1MTkwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460151907", "bodyText": "This seems OK, but it's a shame to add yet another reference interface to the mess of reference interfaces / classes that already exist.  It seems like the basic need is to be able to unify ReferenceContext and the mess of ReferenceSparkSources?  I don't see an easy way to fix it without adding something new or doing more substantial rework to the spark reference classes.  (I wouldn't be opposed to substantially reworking those in the future but it's probably out of scope here.)\nI would like to clarify all of them into a smaller set of classes / interfaces that can be used between spark/nonspark but we never get around to it.\nSeems ok unless @droazen has comments.", "author": "lbergelson", "createdAt": "2020-07-24T16:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQzOTY0MA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNzc3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459007774", "bodyText": "Could you rename simpleMap and complexMap to something like evidenceForSimpleNovelAdjacencies and contigsForComplexVariants, or something similar? I found these names to be too generic for me to understand their uses in the code below.", "author": "cwhelan", "createdAt": "2020-07-22T18:46:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMTQ0MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459011440", "bodyText": "Maybe call redoMap novelAdjacenciesToReinterpret?", "author": "cwhelan", "createdAt": "2020-07-22T18:52:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMjAyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459012023", "bodyText": "mapVal -> chimerasForNovelAdjacency?", "author": "cwhelan", "createdAt": "2020-07-22T18:53:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxNDUyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459014528", "bodyText": "filterMergedVariantList would be a more accurate name", "author": "cwhelan", "createdAt": "2020-07-22T18:57:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxNjc2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459016762", "bodyText": "I know this wasn't your method name, but could you rename this method, notDiscardForBadMQ, to hasGoodMQ? Double negatives made my head hurt here.", "author": "cwhelan", "createdAt": "2020-07-22T19:01:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {\n+        final List<AlignmentInterval> alignmentIntervals = new ArrayList<>(contigAlignments.size());\n+        String contigName = null;\n+        byte[] contigSequence = null;\n+        for ( final GATKRead read : contigAlignments ) {\n+            contigName = read.getName();\n+            if ( !read.isSupplementaryAlignment() ) {\n+                contigSequence = read.getBasesNoCopy();\n+                if ( read.isReverseStrand() ) {\n+                    contigSequence = BaseUtils.simpleReverseComplement(contigSequence);\n+                }\n+            }\n+            alignmentIntervals.add(new AlignmentInterval(read));\n+        }\n+        if ( contigSequence == null ) {\n+            throw new UserException(\"No primary line for \" + contigName);\n+        }\n+        final AlignedContig alignedContig = new AlignedContig(contigName, contigSequence, alignmentIntervals);\n+        if ( !alignedContig.notDiscardForBadMQ() ) return;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyMDI2Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459020263", "bodyText": "Since it looks like you moved pickBestConfigurations into this method (reConstructContigFromPickedConfiguration) could you call this reconstructContigFromBestConfiguration (assuming that we picked the best one)?", "author": "cwhelan", "createdAt": "2020-07-22T19:07:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {\n+        final List<AlignmentInterval> alignmentIntervals = new ArrayList<>(contigAlignments.size());\n+        String contigName = null;\n+        byte[] contigSequence = null;\n+        for ( final GATKRead read : contigAlignments ) {\n+            contigName = read.getName();\n+            if ( !read.isSupplementaryAlignment() ) {\n+                contigSequence = read.getBasesNoCopy();\n+                if ( read.isReverseStrand() ) {\n+                    contigSequence = BaseUtils.simpleReverseComplement(contigSequence);\n+                }\n+            }\n+            alignmentIntervals.add(new AlignmentInterval(read));\n+        }\n+        if ( contigSequence == null ) {\n+            throw new UserException(\"No primary line for \" + contigName);\n+        }\n+        final AlignedContig alignedContig = new AlignedContig(contigName, contigSequence, alignmentIntervals);\n+        if ( !alignedContig.notDiscardForBadMQ() ) return;\n+\n+        final List<AssemblyContigWithFineTunedAlignments> fineTunedAlignmentsList =\n+            alignedContig.reConstructContigFromPickedConfiguration(canonicalChromosomes, SCORE_DIFF_TOLERANCE);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNDM0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459024344", "bodyText": "It's not clear to me from reading this code what these two values are \"more relaxed\" than. Could you just call them SPLIT_PAIR_EVIDENCE_MQ_THRESHOLD and SPLIT_PAIR_EVIDENCE_ALIGNMENT_LENGTH_THRESHOLD, unless you have a better name?", "author": "cwhelan", "createdAt": "2020-07-22T19:15:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java", "diffHunk": "@@ -25,6 +25,9 @@\n @DefaultSerializer(SimpleChimera.Serializer.class)\n public class SimpleChimera {\n \n+    static final int MORE_RELAXED_ALIGNMENT_MIN_LENGTH = 30;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\nindex 84635a391..6512dd898 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\n\n@@ -25,8 +25,8 @@ import java.util.List;\n @DefaultSerializer(SimpleChimera.Serializer.class)\n public class SimpleChimera {\n \n-    static final int MORE_RELAXED_ALIGNMENT_MIN_LENGTH = 30;\n-    static final int MORE_RELAXED_ALIGNMENT_MIN_MQ = 20;\n+    static final int SPLIT_PAIR_MIN_ALIGNMENT_LENGTH = 30;\n+    static final int SPLIT_PAIR_MIN_ALIGNMENT_MQ = 20;\n \n     public final String sourceContigName;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNTc1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459025758", "bodyText": "The comment for this method refers to DiscoverVariantsFromContigAlignmentsSAMSpark#nextAlignmentMayBeInsertion but I think that should actually be ContigChimericAlignmentIterativeInterpreter#nextAlignmentMayBeInsertion. Could you update?", "author": "cwhelan", "createdAt": "2020-07-22T19:17:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java", "diffHunk": "@@ -102,18 +105,16 @@ public SimpleChimera(final AlignmentInterval intervalWithLowerCoordOnContig, fin\n      *  2) either alignment may consume only a \"short\" part of the contig, or if assuming that the alignment consumes\n      *     roughly the same amount of ref bases and read bases, has isAlignment that is too short\n      */\n-    static boolean splitPairStrongEnoughEvidenceForCA(final AlignmentInterval intervalOne,\n-                                                      final AlignmentInterval intervalTwo,\n-                                                      final int mapQThresholdInclusive,\n-                                                      final int alignmentLengthThresholdInclusive) {\n+    public static boolean splitPairStrongEnoughEvidenceForCA(final AlignmentInterval intervalOne,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNzMxOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468017319", "bodyText": "How in the world did you catch that.  Impressive!", "author": "tedsharpe", "createdAt": "2020-08-10T16:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNTc1OA=="}], "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\nindex 84635a391..6512dd898 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java\n\n@@ -100,7 +100,7 @@ public class SimpleChimera {\n \n     /**\n      * Roughly similar to\n-     * DiscoverVariantsFromContigAlignmentsSAMSpark#nextAlignmentMayBeInsertion(AlignmentInterval, AlignmentInterval, Integer, Integer, boolean):\n+     * ContigChimericAlignmentIterativeInterpreter#nextAlignmentMayBeInsertion(AlignmentInterval, AlignmentInterval, Integer, Integer, boolean):\n      *  1) either alignment may have very low mapping quality (a more relaxed mapping quality threshold);\n      *  2) either alignment may consume only a \"short\" part of the contig, or if assuming that the alignment consumes\n      *     roughly the same amount of ref bases and read bases, has isAlignment that is too short\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1NzU2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459057560", "bodyText": "As mentioned above, change this to filterMergedLVariantList", "author": "cwhelan", "createdAt": "2020-07-22T20:17:22Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\nindex c36dc53d9..cc32952d1 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n\n@@ -107,16 +107,17 @@ public class AnnotatedVariantProducer implements Serializable {\n     }\n \n     /**\n-     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * Apply filters (that implements {@link StructuralVariantFilter}) given list of variants,\n      * and write the variants to a single VCF file.\n      * @param variants variants to which filters are to be applied and written to file\n      */\n-    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n-                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+    public static List<VariantContext> filterMergedVariantList(\n+                                final List<VariantContext> variants,\n+                                final DiscoverVariantsFromContigAlignmentsArgumentCollection discoveryArgs ) {\n         final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n-        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        final List<StructuralVariantFilter> filters = Arrays.asList(\n+                new SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n         for (final VariantContext variant : variants) {\n             String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n             if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1ODkwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459058908", "bodyText": "Maybe it would be worth moving this interface up to not be an inner class anymore.", "author": "cwhelan", "createdAt": "2020-07-22T20:19:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+        final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n+        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\nindex c36dc53d9..cc32952d1 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n\n@@ -107,16 +107,17 @@ public class AnnotatedVariantProducer implements Serializable {\n     }\n \n     /**\n-     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * Apply filters (that implements {@link StructuralVariantFilter}) given list of variants,\n      * and write the variants to a single VCF file.\n      * @param variants variants to which filters are to be applied and written to file\n      */\n-    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n-                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+    public static List<VariantContext> filterMergedVariantList(\n+                                final List<VariantContext> variants,\n+                                final DiscoverVariantsFromContigAlignmentsArgumentCollection discoveryArgs ) {\n         final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n-        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        final List<StructuralVariantFilter> filters = Arrays.asList(\n+                new SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n         for (final VariantContext variant : variants) {\n             String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n             if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTM0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459059346", "bodyText": "What do you think about renaming this argument collection to remove Spark from the name, if it's going to be used in non-spark code paths?", "author": "cwhelan", "createdAt": "2020-07-22T20:20:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\nindex c36dc53d9..cc32952d1 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n\n@@ -107,16 +107,17 @@ public class AnnotatedVariantProducer implements Serializable {\n     }\n \n     /**\n-     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * Apply filters (that implements {@link StructuralVariantFilter}) given list of variants,\n      * and write the variants to a single VCF file.\n      * @param variants variants to which filters are to be applied and written to file\n      */\n-    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n-                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+    public static List<VariantContext> filterMergedVariantList(\n+                                final List<VariantContext> variants,\n+                                final DiscoverVariantsFromContigAlignmentsArgumentCollection discoveryArgs ) {\n         final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n-        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        final List<StructuralVariantFilter> filters = Arrays.asList(\n+                new SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n         for (final VariantContext variant : variants) {\n             String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n             if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA2MDk1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459060959", "bodyText": "Maybe worth pushing this up to top level too?", "author": "cwhelan", "createdAt": "2020-07-22T20:23:45Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+        final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n+        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n+                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        for (final VariantContext variant : variants) {\n+            String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n+            if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n+                if (Math.abs(variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0)) < StructuralVariationDiscoveryArgumentCollection.STRUCTURAL_VARIANT_SIZE_LOWER_BOUND )\n+                    continue;\n+            }\n+            variantsWithFilterApplied.add(applyFilters(variant, filters));\n+        }\n+        return variantsWithFilterApplied;\n+    }\n+\n+    /**\n+     * Filters out variants by testing against provided\n+     * filter key, threshold.\n+     *\n+     * Variants with value below specified threshold (or null value)\n+     * are filtered out citing given reason.\n+     *\n+     * @throws ClassCastException if the value corresponding to provided key cannot be casted as a {@link Double}\n+     */\n+    private static VariantContext applyFilters(final VariantContext variantContext,\n+                                               final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters) {\n+\n+        final Set<String> appliedFilters = new HashSet<>();\n+        for (final SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter filter : filters) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\nindex c36dc53d9..cc32952d1 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java\n\n@@ -107,16 +107,17 @@ public class AnnotatedVariantProducer implements Serializable {\n     }\n \n     /**\n-     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * Apply filters (that implements {@link StructuralVariantFilter}) given list of variants,\n      * and write the variants to a single VCF file.\n      * @param variants variants to which filters are to be applied and written to file\n      */\n-    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n-                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+    public static List<VariantContext> filterMergedVariantList(\n+                                final List<VariantContext> variants,\n+                                final DiscoverVariantsFromContigAlignmentsArgumentCollection discoveryArgs ) {\n         final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n-        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n-                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        final List<StructuralVariantFilter> filters = Arrays.asList(\n+                new SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n         for (final VariantContext variant : variants) {\n             String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n             if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUxMjkyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459512926", "bodyText": "This parameter doesn't exist any more, can you update the comment, ie explain that you're now doing the pickAndFilterConfigurations call inside this method?", "author": "cwhelan", "createdAt": "2020-07-23T14:55:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java", "diffHunk": "@@ -93,28 +134,688 @@ public boolean isUnmapped() {\n         return alignmentIntervals;\n     }\n \n-    @Override\n-    public String toString() {\n-        return formatContigInfo(\n-                new Tuple2<>(contigName, alignmentIntervals.stream().map(AlignmentInterval::toPackedString).collect(Collectors.toList())));\n+\n+    /**\n+     * Idea is to keep mapped contig that\n+     * either has at least two alignments over {@link #ALIGNMENT_MQ_THRESHOLD},\n+     * or in the case of a single alignment, it must be MQ > {@link #ALIGNMENT_MQ_THRESHOLD}.\n+     * Note that we are not simply filtering out contigs with only 1 alignment because\n+     * they might contain large (> 50) gaps hence should be kept.\n+     * <p>\n+     * a point that could use improvements:\n+     * the current implementation exhaustively checks the power set of all possible alignments of each assembly contig,\n+     * which is computationally impossible for contigs having many-but-barely-any-good alignments, yet bringing in no value,\n+     * hence this primitive filtering step to get rid of these bad assembly contigs.\n+     */\n+    public boolean notDiscardForBadMQ() {\n+        if ( alignmentIntervals.size() < 2 ) {\n+            return (!alignmentIntervals.isEmpty()) && alignmentIntervals.get(0).mapQual > ALIGNMENT_MQ_THRESHOLD;\n+        } else {\n+            int notBadMappingsCount = 0; // either more than 1 non-bad mappings, or at least 1 non-bad mapping containing large gap\n+            for ( final AlignmentInterval alignment : alignmentIntervals ) {\n+                if ( alignment.mapQual > ALIGNMENT_MQ_THRESHOLD ) {\n+                    if ( alignment.containsGapOfEqualOrLargerSize(GAPPED_ALIGNMENT_BREAK_DEFAULT_SENSITIVITY) ) {\n+                        return true;// early return when a not-bad one contains a large gap\n+                    } else {\n+                        ++notBadMappingsCount;\n+                    }\n+                }\n+            }\n+            return notBadMappingsCount > 1;\n+        }\n+    }\n+\n+    /**\n+     * Reconstructs (possibly more than one) {@link AssemblyContigWithFineTunedAlignments} based on\n+     * the given best-scored configuration(s) in {@code nameSeqAndBestConfigurationsOfOneAssemblyContig}.", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java\nindex cb8d97a4f..04a4c673a 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java\n\n@@ -147,7 +194,7 @@ public final class AlignedContig {\n      * which is computationally impossible for contigs having many-but-barely-any-good alignments, yet bringing in no value,\n      * hence this primitive filtering step to get rid of these bad assembly contigs.\n      */\n-    public boolean notDiscardForBadMQ() {\n+    public boolean hasGoodMQ() {\n         if ( alignmentIntervals.size() < 2 ) {\n             return (!alignmentIntervals.isEmpty()) && alignmentIntervals.get(0).mapQual > ALIGNMENT_MQ_THRESHOLD;\n         } else {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUzMjg1NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459532855", "bodyText": "All that seems to be left in this class is a couple of Spark-specific RDD methods and the SAMFormattedContigAlignmentParser, which has some RDD processing stuff and one method that does non-RDD specific logic (parseReadsAndOptionallySplitGappedAlignments). I might suggest moving the latter method to a non-spark-specific class, and renaming this class to something like AssemblyContigAlignmentsRDDProcessor to indicate that it's full of spark specific stuff.", "author": "cwhelan", "createdAt": "2020-07-23T15:22:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsConfigPicker.java", "diffHunk": "@@ -32,49 +31,13 @@\n  */\n public class AssemblyContigAlignmentsConfigPicker {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsConfigPicker.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsRDDProcessor.java\nsimilarity index 65%\nrename from src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsConfigPicker.java\nrename to src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsRDDProcessor.java\nindex ae9fecbc9..8c1411e56 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsConfigPicker.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsRDDProcessor.java\n\n@@ -29,7 +21,7 @@ import static org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDi\n  * assembly contig.\n  * Future improvements are definitely welcome and could benefit the whole pipeline.\n  */\n-public class AssemblyContigAlignmentsConfigPicker {\n+public class AssemblyContigAlignmentsRDDProcessor {\n \n     /**\n      * Filters input alignments of single-ended long reads, e.g. local assembly contigs,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459544812", "bodyText": "I may have just been bogged down in the number of classes that have moved here somehow, but the old code path (in, for example, StructuralVariationDiscoveryPipelineSpark.filterAndConvertToAlignedContigViaSAM) used to go through SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SAMFormattedContigAlignmentParserand particularly the methodparseReadsAndOptionallySplitGappedAlignments` which does some logic processing on the reads. Do you still need to send the reads through that, or is there some replacement for that logic here that I'm missing?", "author": "cwhelan", "createdAt": "2020-07-23T15:40:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE0NjgyNw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468146827", "bodyText": "The existing code was dealing with SAMRecords, but in a ReadWalker I really needed to be dealing with GATKReads.  So I rewrote parseReadsAndOptionallySplitGappedAlignments, and that's inlined here.  It's the first 17 lines, up to the place where I create a new AlignedContig.  The default read filters have already peeled off unmapped and secondary alignments, and the option to split gaps was always being passed as \"false\"--that actually happens downstream--so the logic is quite a bit simpler in this method than in the original.", "author": "tedsharpe", "createdAt": "2020-08-10T19:57:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODYzNzUyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468637526", "bodyText": "Got it, thanks for the explanation.", "author": "cwhelan", "createdAt": "2020-08-11T14:45:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg=="}], "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTA0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459645041", "bodyText": "Do you need to call convertTerminalInsertionToSoftClip as in the old code's splitGappedAlignment method?", "author": "cwhelan", "createdAt": "2020-07-23T18:26:42Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;\n+        int alignmentStartRef = NOT_SET;\n+        int alignmentEndContig = NOT_SET;\n+        int alignmentEndIdx = NOT_SET;\n+        int alignmentEndRef = NOT_SET;\n+\n+        final AlignmentInterval oneRegion;\n+        final int unclippedContigLen;\n+\n+        public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n+            this.oneRegion = oneRegion;\n+            this.unclippedContigLen = unclippedContigLen;\n+        }\n+\n+        public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n+            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE4OTI5NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468189295", "bodyText": "No, this is completely rewritten to circumvent the exception that was being thrown on cigars with adjacent insertion and deletion operators.  In the new code, leading and trailing indels are never included in the cigar pieces because the starting and ending positions are updated only on aligned bits (M operators), never on indels.", "author": "tedsharpe", "createdAt": "2020-08-10T21:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTA0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\nindex e13c54194..30c158691 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n\n@@ -240,36 +240,38 @@ public final class ContigAlignmentsModifier {\n     private static class GapSplitter {\n         private static final int NOT_SET = -1;\n \n-        int alignmentStartContig = NOT_SET;\n-        int alignmentStartIdx = NOT_SET;\n-        int alignmentStartRef = NOT_SET;\n-        int alignmentEndContig = NOT_SET;\n-        int alignmentEndIdx = NOT_SET;\n-        int alignmentEndRef = NOT_SET;\n+        // boundaries of each ungapped alignment chunk\n+        int alignmentStartContig = NOT_SET; // contig starting position\n+        int alignmentStartIdx = NOT_SET; // cigar elements starting position\n+        int alignmentStartRef = NOT_SET; // reference starting position\n+        int alignmentEndContig = NOT_SET; // contig ending position\n+        int alignmentEndIdx = NOT_SET; // cigar elements ending position\n+        int alignmentEndRef = NOT_SET; // reference ending position\n \n         final AlignmentInterval oneRegion;\n         final int unclippedContigLen;\n+        final List<CigarElement> cigarElements;\n \n         public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n             this.oneRegion = oneRegion;\n             this.unclippedContigLen = unclippedContigLen;\n+            cigarElements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n         }\n \n         public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n-            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n-            int nElements = elements.size();\n+            int nElements = cigarElements.size();\n             if ( nElements <= 1 ) {\n                 return new ArrayList<>( Collections.singletonList(oneRegion) );\n             }\n \n             int contigOffset = 0;\n             int elementIdx = 0;\n-            if ( elements.get(0).getOperator() == CigarOperator.H ) {\n-                contigOffset += elements.get(0).getLength();\n+            if ( cigarElements.get(0).getOperator() == CigarOperator.H ) {\n+                contigOffset += cigarElements.get(0).getLength();\n                 elementIdx += 1;\n             }\n \n-            if ( elements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n+            if ( cigarElements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n                 nElements -= 1;\n             }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2Nzg0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459667841", "bodyText": "Could you add some little comments here to say what these are? It took me a while to figure out that this was an index into the cigar element list, etc.", "author": "cwhelan", "createdAt": "2020-07-23T19:08:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\nindex e13c54194..30c158691 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n\n@@ -240,36 +240,38 @@ public final class ContigAlignmentsModifier {\n     private static class GapSplitter {\n         private static final int NOT_SET = -1;\n \n-        int alignmentStartContig = NOT_SET;\n-        int alignmentStartIdx = NOT_SET;\n-        int alignmentStartRef = NOT_SET;\n-        int alignmentEndContig = NOT_SET;\n-        int alignmentEndIdx = NOT_SET;\n-        int alignmentEndRef = NOT_SET;\n+        // boundaries of each ungapped alignment chunk\n+        int alignmentStartContig = NOT_SET; // contig starting position\n+        int alignmentStartIdx = NOT_SET; // cigar elements starting position\n+        int alignmentStartRef = NOT_SET; // reference starting position\n+        int alignmentEndContig = NOT_SET; // contig ending position\n+        int alignmentEndIdx = NOT_SET; // cigar elements ending position\n+        int alignmentEndRef = NOT_SET; // reference ending position\n \n         final AlignmentInterval oneRegion;\n         final int unclippedContigLen;\n+        final List<CigarElement> cigarElements;\n \n         public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n             this.oneRegion = oneRegion;\n             this.unclippedContigLen = unclippedContigLen;\n+            cigarElements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n         }\n \n         public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n-            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n-            int nElements = elements.size();\n+            int nElements = cigarElements.size();\n             if ( nElements <= 1 ) {\n                 return new ArrayList<>( Collections.singletonList(oneRegion) );\n             }\n \n             int contigOffset = 0;\n             int elementIdx = 0;\n-            if ( elements.get(0).getOperator() == CigarOperator.H ) {\n-                contigOffset += elements.get(0).getLength();\n+            if ( cigarElements.get(0).getOperator() == CigarOperator.H ) {\n+                contigOffset += cigarElements.get(0).getLength();\n                 elementIdx += 1;\n             }\n \n-            if ( elements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n+            if ( cigarElements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n                 nElements -= 1;\n             }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTMyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459671323", "bodyText": "Stylistically I think it would make it a bit easier to understand this method if you passed the alignment indices (alignmentStartIdx, etc.) to this method as parameters. Conversely, it feels to me like elements is the thing that should be accessed via an instance variable since it's immutable over the lifespan of this GapSplitter object. But that might be just my own style preference, if you like it better this way that's fine.", "author": "cwhelan", "createdAt": "2020-07-23T19:14:51Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;\n+        int alignmentStartRef = NOT_SET;\n+        int alignmentEndContig = NOT_SET;\n+        int alignmentEndIdx = NOT_SET;\n+        int alignmentEndRef = NOT_SET;\n+\n+        final AlignmentInterval oneRegion;\n+        final int unclippedContigLen;\n+\n+        public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n+            this.oneRegion = oneRegion;\n+            this.unclippedContigLen = unclippedContigLen;\n+        }\n+\n+        public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n+            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n+            int nElements = elements.size();\n+            if ( nElements <= 1 ) {\n+                return new ArrayList<>( Collections.singletonList(oneRegion) );\n+            }\n+\n+            int contigOffset = 0;\n+            int elementIdx = 0;\n+            if ( elements.get(0).getOperator() == CigarOperator.H ) {\n+                contigOffset += elements.get(0).getLength();\n+                elementIdx += 1;\n+            }\n+\n+            if ( elements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n+                nElements -= 1;\n+            }\n+\n+            int refOffset = oneRegion.referenceSpan.getStart();\n+            if ( !oneRegion.forwardStrand ) refOffset = -oneRegion.referenceSpan.getEnd();\n+\n+            final List<AlignmentInterval> result = new ArrayList<>();\n+            while ( elementIdx < nElements ) {\n+                final CigarElement element = elements.get(elementIdx);\n+                final CigarOperator op = element.getOperator();\n+                final int len = element.getLength();\n+                if ( op.isAlignment() ) {\n+                    if ( alignmentStartContig == NOT_SET ) {\n+                        alignmentStartContig = contigOffset;\n+                        alignmentStartIdx = elementIdx;\n+                        alignmentStartRef = refOffset;\n+                    }\n+                    alignmentEndContig = contigOffset + len;\n+                    alignmentEndIdx = elementIdx + 1;\n+                    alignmentEndRef = refOffset + len;\n+                } else if ( op.isIndel() && len >= sensitivity && alignmentStartContig != -1 ) {\n+                    result.add(createInterval(elements));\n+                    alignmentStartContig = NOT_SET;\n+                }\n+                if ( op.consumesReadBases() ) {\n+                    contigOffset += len;\n+                }\n+                if ( op.consumesReferenceBases() ) {\n+                    refOffset += len;\n+                }\n+                elementIdx += 1;\n+            }\n+\n+            if ( alignmentStartContig != -1 ) {\n+                result.add(createInterval(elements));\n+            }\n+\n+            if ( result.size() < 2 ) {\n+                return new ArrayList<>(Collections.singletonList(oneRegion));\n+            }\n+            return result;\n+        }\n+\n+        private AlignmentInterval createInterval( final List<CigarElement> elements ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE5NzY4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468197684", "bodyText": "I agree it was counter-intuitive to pass elements as an argument.  (It was a silly performance hack that really shouldn't matter--getting the elements from a Cigar creates a copy.)  I've made the cigar elements an instance variable so they're still only retrieved once, but don't have to be passed.  And I renamed the createInterval method grabCurrentInterval to make it clearer that it's using object state.", "author": "tedsharpe", "createdAt": "2020-08-10T21:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTMyMw=="}], "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\nindex e13c54194..30c158691 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java\n\n@@ -240,36 +240,38 @@ public final class ContigAlignmentsModifier {\n     private static class GapSplitter {\n         private static final int NOT_SET = -1;\n \n-        int alignmentStartContig = NOT_SET;\n-        int alignmentStartIdx = NOT_SET;\n-        int alignmentStartRef = NOT_SET;\n-        int alignmentEndContig = NOT_SET;\n-        int alignmentEndIdx = NOT_SET;\n-        int alignmentEndRef = NOT_SET;\n+        // boundaries of each ungapped alignment chunk\n+        int alignmentStartContig = NOT_SET; // contig starting position\n+        int alignmentStartIdx = NOT_SET; // cigar elements starting position\n+        int alignmentStartRef = NOT_SET; // reference starting position\n+        int alignmentEndContig = NOT_SET; // contig ending position\n+        int alignmentEndIdx = NOT_SET; // cigar elements ending position\n+        int alignmentEndRef = NOT_SET; // reference ending position\n \n         final AlignmentInterval oneRegion;\n         final int unclippedContigLen;\n+        final List<CigarElement> cigarElements;\n \n         public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n             this.oneRegion = oneRegion;\n             this.unclippedContigLen = unclippedContigLen;\n+            cigarElements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n         }\n \n         public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n-            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n-            int nElements = elements.size();\n+            int nElements = cigarElements.size();\n             if ( nElements <= 1 ) {\n                 return new ArrayList<>( Collections.singletonList(oneRegion) );\n             }\n \n             int contigOffset = 0;\n             int elementIdx = 0;\n-            if ( elements.get(0).getOperator() == CigarOperator.H ) {\n-                contigOffset += elements.get(0).getLength();\n+            if ( cigarElements.get(0).getOperator() == CigarOperator.H ) {\n+                contigOffset += cigarElements.get(0).getLength();\n                 elementIdx += 1;\n             }\n \n-            if ( elements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n+            if ( cigarElements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n                 nElements -= 1;\n             }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA5ODAxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460098013", "bodyText": "While you're moving this, want to rename it to something with one fewer word like createVariantContext or even just toVariantContext?", "author": "cwhelan", "createdAt": "2020-07-24T14:44:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java", "diffHunk": "@@ -269,16 +271,14 @@ static boolean yieldOverlapToAlignmentTwo(final AlignmentInterval one, final Ali\n \n     // =================================================================================================================\n \n-    @VisibleForTesting\n-    static VariantContext turnIntoVariantContext(final Tuple2<CpxVariantCanonicalRepresentation, Iterable<CpxVariantInducingAssemblyContig>> pair,\n-                                                 final ReferenceMultiSparkSource reference)\n-            throws IOException {\n-\n-        final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation = pair._1;\n-        final byte[] refBases = getRefBases(reference, cpxVariantCanonicalRepresentation);\n+    public static VariantContext turnIntoVariantContext( final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java\nindex a2ee044e6..00322f942 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java\n\n@@ -271,9 +270,9 @@ public final class CpxVariantInterpreter {\n \n     // =================================================================================================================\n \n-    public static VariantContext turnIntoVariantContext( final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation,\n-                                                         final Iterable<CpxVariantInducingAssemblyContig> evidenceContigs,\n-                                                         final BasicReference reference ) {\n+    public static VariantContext toVariantContext( final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation,\n+                                                   final Iterable<CpxVariantInducingAssemblyContig> evidenceContigs,\n+                                                   final BasicReference reference ) {\n         final SimpleInterval refRegion = cpxVariantCanonicalRepresentation.getAffectedRefRegion();\n         final byte[] refBases =\n                 reference.getBases(new SimpleInterval(refRegion.getContig(), refRegion.getStart(), refRegion.getStart()));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEwMDQyMA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460100420", "bodyText": "Again, what about renaming to createVariantContexts or toVariantContexts while we're moving stuff around?", "author": "cwhelan", "createdAt": "2020-07-24T14:48:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java", "diffHunk": "@@ -48,22 +65,163 @@ private SimpleNovelAdjacencyAndChimericAlignmentEvidence(final Kryo kryo, final\n         }\n     }\n \n-    private void serialize(final Kryo kryo, final Output output) {\n-        narlSerializer.write(kryo, output, novelAdjacencyAndAltHaplotype);\n-        output.writeInt(alignmentEvidence.size());\n-        alignmentEvidence.forEach(ev -> alignmentEvidenceSerializer.write(kryo, output, ev));\n+    /**\n+     * This implementation is the 1st step going towards allowing re-interpretation,\n+     * below we simply take the inferred type and turn it to a VC,\n+     * future implementation may integrate other types of evidence and re-interpret if necessary\n+     */\n+    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\nindex f16404ea8..f3456b3a0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\n\n@@ -70,10 +70,10 @@ public final class SimpleNovelAdjacencyAndChimericAlignmentEvidence {\n      * below we simply take the inferred type and turn it to a VC,\n      * future implementation may integrate other types of evidence and re-interpret if necessary\n      */\n-    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,\n-                                                         final String sampleId,\n-                                                         final SAMSequenceDictionary refDict,\n-                                                         final SVIntervalTree<VariantContext> cnvCalls) {\n+    public List<VariantContext> toVariantContexts( final List<SvType> svTypes,\n+                                                   final String sampleId,\n+                                                   final SAMSequenceDictionary refDict,\n+                                                   final SVIntervalTree<VariantContext> cnvCalls) {\n         if( svTypes.isEmpty() || svTypes.size() > 2 ) {\n             throw new GATKException(\"Wrong number of variants sent for analysis: \" + svTypes.toString() +\n                     \"\\nWe currently only support 1 (symbolic simple or CPX) or 2 (BND mate pairs) variants for producing annotated variants.\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEwNTYyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460105622", "bodyText": "Maybe make this method static?", "author": "cwhelan", "createdAt": "2020-07-24T14:56:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java", "diffHunk": "@@ -48,22 +65,163 @@ private SimpleNovelAdjacencyAndChimericAlignmentEvidence(final Kryo kryo, final\n         }\n     }\n \n-    private void serialize(final Kryo kryo, final Output output) {\n-        narlSerializer.write(kryo, output, novelAdjacencyAndAltHaplotype);\n-        output.writeInt(alignmentEvidence.size());\n-        alignmentEvidence.forEach(ev -> alignmentEvidenceSerializer.write(kryo, output, ev));\n+    /**\n+     * This implementation is the 1st step going towards allowing re-interpretation,\n+     * below we simply take the inferred type and turn it to a VC,\n+     * future implementation may integrate other types of evidence and re-interpret if necessary\n+     */\n+    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,\n+                                                         final String sampleId,\n+                                                         final SAMSequenceDictionary refDict,\n+                                                         final SVIntervalTree<VariantContext> cnvCalls) {\n+        if( svTypes.isEmpty() || svTypes.size() > 2 ) {\n+            throw new GATKException(\"Wrong number of variants sent for analysis: \" + svTypes.toString() +\n+                    \"\\nWe currently only support 1 (symbolic simple or CPX) or 2 (BND mate pairs) variants for producing annotated variants.\");\n+        }\n+        if (svTypes.size() == 2) {\n+            final SvType firstVar = svTypes.get(0);\n+            final SvType secondVar = svTypes.get(1);\n+            final String linkKey = firstVar instanceof BreakEndVariantType ? GATKSVVCFConstants.BND_MATEID_STR : GATKSVVCFConstants.LINK;\n+            return produceLinkedAssemblyBasedVariants(firstVar, secondVar, refDict, cnvCalls, sampleId, linkKey);\n+        } else {\n+            final VariantContext variantContext =\n+                    produceAnnotatedVcFromAssemblyEvidence(svTypes.get(0), refDict, cnvCalls, sampleId).make();\n+            return Collections.singletonList(variantContext);\n+        }\n     }\n \n-    public static final class Serializer extends com.esotericsoftware.kryo.Serializer<SimpleNovelAdjacencyAndChimericAlignmentEvidence> {\n-        @Override\n-        public void write(final Kryo kryo, final Output output, final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyReferenceLocations ) {\n-            novelAdjacencyReferenceLocations.serialize(kryo, output);\n+    /**\n+     * Produces a VC from a {@link NovelAdjacencyAndAltHaplotype}\n+     * (consensus among different assemblies if they all point to the same breakpoint).\n+     * @param inferredType                      inferred type of variant\n+     * @param sequenceDictionary                reference sequence dictionary\n+     * @param cnvCalls                          external CNV calls, if available, will be used for annotating the assembly based calls\n+     * @param sampleId                          sample identifier of the current sample\n+     */\n+    @VisibleForTesting\n+    public VariantContextBuilder produceAnnotatedVcFromAssemblyEvidence( final SvType inferredType,\n+                                                                         final SAMSequenceDictionary sequenceDictionary,\n+                                                                         final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                         final String sampleId) {\n+\n+        // basic information and attributes\n+        final VariantContextBuilder vcBuilder = inferredType.getBasicInformation();\n+\n+        // attributes from complications\n+        novelAdjacencyAndAltHaplotype.getComplication().toVariantAttributes().forEach(vcBuilder::attribute);\n+\n+        // evidence used for producing the novel adjacency\n+        getAssemblyEvidenceRelatedAnnotations(alignmentEvidence).forEach(vcBuilder::attribute);\n+\n+        // alt seq for non-BND variants, and if available or not empty\n+        final byte[] altHaplotypeSequence = novelAdjacencyAndAltHaplotype.getAltHaplotypeSequence();\n+        if (inferredType instanceof BreakEndVariantType) {\n+            return annotateWithExternalCNVCalls(inferredType.getVariantChromosome(), inferredType.getVariantStart(), inferredType.getVariantStop(),\n+                    vcBuilder, sequenceDictionary, cnvCalls, sampleId);\n         }\n \n-        @Override\n-        public SimpleNovelAdjacencyAndChimericAlignmentEvidence read(final Kryo kryo, final Input input, final Class<SimpleNovelAdjacencyAndChimericAlignmentEvidence> klass ) {\n-            return new SimpleNovelAdjacencyAndChimericAlignmentEvidence(kryo, input);\n+        if (altHaplotypeSequence != null && altHaplotypeSequence.length != 0)\n+            vcBuilder.attribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE, StringUtil.bytesToString(altHaplotypeSequence));\n+\n+        return annotateWithExternalCNVCalls(inferredType.getVariantChromosome(), inferredType.getVariantStart(), inferredType.getVariantStop(),\n+                vcBuilder, sequenceDictionary, cnvCalls, sampleId);\n+    }\n+\n+    private static VariantContextBuilder annotateWithExternalCNVCalls(final String recordContig,\n+                                                                      final int pos,\n+                                                                      final int end,\n+                                                                      final VariantContextBuilder inputBuilder,\n+                                                                      final SAMSequenceDictionary sequenceDictionary,\n+                                                                      final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                      final String sampleId) {\n+        if (cnvCalls == null)\n+            return inputBuilder;\n+\n+        final SVInterval variantInterval = new SVInterval(sequenceDictionary.getSequenceIndex(recordContig), pos, end);\n+        final String cnvCallAnnotation =\n+                Utils.stream(cnvCalls.overlappers(variantInterval))\n+                        .map(overlapper -> formatExternalCNVCallAnnotation(overlapper.getValue(), sampleId))\n+                        .collect(Collectors.joining(VCFConstants.INFO_FIELD_ARRAY_SEPARATOR));\n+        if (!cnvCallAnnotation.isEmpty()) {\n+            return inputBuilder.attribute(GATKSVVCFConstants.EXTERNAL_CNV_CALLS, cnvCallAnnotation);\n+        } else\n+            return inputBuilder;\n+    }\n+\n+    private static String formatExternalCNVCallAnnotation(final VariantContext externalCNVCall, String sampleId) {\n+        return externalCNVCall.getID() + \":\"\n+                + externalCNVCall.getGenotype(sampleId).getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) + \":\"\n+                + externalCNVCall.getGenotype(sampleId).getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_QUALITY_FORMAT);\n+    }\n+\n+    /**\n+     * Given novel adjacency and inferred variant types that should be linked together,\n+     * produce annotated, and linked VCF records.\n+     */\n+    public List<VariantContext> produceLinkedAssemblyBasedVariants(final SvType svType1,\n+                                                                   final SvType svType2,\n+                                                                   final SAMSequenceDictionary sequenceDictionary,\n+                                                                   final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                   final String sampleId,\n+                                                                   final String linkKey) {\n+\n+        final VariantContext firstVar = produceAnnotatedVcFromAssemblyEvidence(\n+                    svType1, sequenceDictionary, cnvCalls, sampleId).make();\n+        final VariantContext secondVar = produceAnnotatedVcFromAssemblyEvidence(\n+                    svType2, sequenceDictionary, cnvCalls, sampleId).make();\n+\n+        final VariantContextBuilder builder1 = new VariantContextBuilder(firstVar);\n+        builder1.attribute(linkKey, secondVar.getID());\n+\n+        final VariantContextBuilder builder2 = new VariantContextBuilder(secondVar);\n+        builder2.attribute(linkKey, firstVar.getID());\n+\n+        // manually remove inserted sequence information from RPL event-produced DEL, when it can be linked with an INS\n+        if (svType1 instanceof SimpleSVType.Deletion)\n+            return Arrays.asList(builder1.rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE)\n+                            .rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE_LENGTH)\n+                            .rmAttribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE)\n+                            .make(),\n+                    builder2.make());\n+        else if (svType2 instanceof SimpleSVType.Deletion) {\n+            return Arrays.asList(builder1.make(),\n+                    builder2.rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE)\n+                            .rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE_LENGTH)\n+                            .rmAttribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE)\n+                            .make());\n+        } else\n+            return Arrays.asList(builder1.make(), builder2.make());\n+    }\n+\n+    private Map<String, Object> getAssemblyEvidenceRelatedAnnotations( final List<SimpleChimera> splitAlignmentEvidence ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\nindex f16404ea8..f3456b3a0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java\n\n@@ -70,10 +70,10 @@ public final class SimpleNovelAdjacencyAndChimericAlignmentEvidence {\n      * below we simply take the inferred type and turn it to a VC,\n      * future implementation may integrate other types of evidence and re-interpret if necessary\n      */\n-    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,\n-                                                         final String sampleId,\n-                                                         final SAMSequenceDictionary refDict,\n-                                                         final SVIntervalTree<VariantContext> cnvCalls) {\n+    public List<VariantContext> toVariantContexts( final List<SvType> svTypes,\n+                                                   final String sampleId,\n+                                                   final SAMSequenceDictionary refDict,\n+                                                   final SVIntervalTree<VariantContext> cnvCalls) {\n         if( svTypes.isEmpty() || svTypes.size() > 2 ) {\n             throw new GATKException(\"Wrong number of variants sent for analysis: \" + svTypes.toString() +\n                     \"\\nWe currently only support 1 (symbolic simple or CPX) or 2 (BND mate pairs) variants for producing annotated variants.\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460119852", "bodyText": "Great to see this test changed to passing.", "author": "cwhelan", "createdAt": "2020-07-24T15:19:06Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4ODExOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460188118", "bodyText": "\ud83d\udc4d Did you run into this in the wild?", "author": "lbergelson", "createdAt": "2020-07-24T17:23:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIwMTE3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468201171", "bodyText": "@lbergelson Yes, I did see the empty partition issue in the wild.  Situation was a very small BAM containing large contigs from a local assembly with lots of supplemental alignments.", "author": "tedsharpe", "createdAt": "2020-08-10T21:39:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\nindex cb17bad88..169953609 100644\n--- a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n\n@@ -143,15 +144,15 @@ public class SparkUtilsUnitTest extends GATKBaseTest {\n \n     @Test\n     public void testReadsPairsSpanningMultiplePartitions() {\n-        JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n-        SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n+        final JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n+        final SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n-        List<GATKRead> reads = createPairedReads(header, 40, 2);\n+        final List<GATKRead> reads = createPairedReads(header, 40, 2);\n         reads.addAll(createPairedReads(header, 1, 30));\n-        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n+        reads.sort(Comparator.comparing(GATKRead::getName));\n \n-        JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        JavaRDD<GATKRead> fixedReads =\n+        final JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n+        final JavaRDD<GATKRead> fixedReads =\n                 SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n         final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n                 .mapPartitionsWithIndex(( idx, itr ) -> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEyNDUxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460124516", "bodyText": "It might be nice to also assert that each read name you expect is still there, since the fix you implemented involves removing some groups and re-adding them to different partitions.", "author": "cwhelan", "createdAt": "2020-07-24T15:26:34Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {\n+                    int count = 0;\n+                    for ( final int idx : t._2() ) ++count;\n+                    return count > 1;\n+                })\n+                .collect();\n+        Assert.assertEquals(xPartitionPairs.size(), 0);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\nindex cb17bad88..169953609 100644\n--- a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n\n@@ -143,15 +144,15 @@ public class SparkUtilsUnitTest extends GATKBaseTest {\n \n     @Test\n     public void testReadsPairsSpanningMultiplePartitions() {\n-        JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n-        SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n+        final JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n+        final SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n-        List<GATKRead> reads = createPairedReads(header, 40, 2);\n+        final List<GATKRead> reads = createPairedReads(header, 40, 2);\n         reads.addAll(createPairedReads(header, 1, 30));\n-        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n+        reads.sort(Comparator.comparing(GATKRead::getName));\n \n-        JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        JavaRDD<GATKRead> fixedReads =\n+        final JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n+        final JavaRDD<GATKRead> fixedReads =\n                 SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n         final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n                 .mapPartitionsWithIndex(( idx, itr ) -> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1Mjk4MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460152980", "bodyText": "It's really weird to make arguments static.  That seems like a mistake to me.", "author": "lbergelson", "createdAt": "2020-07-24T16:15:32Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\nindex 30736f121..289568bb5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java\n\n@@ -15,9 +15,8 @@ import org.broadinstitute.hellbender.engine.ReadWalker;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.exceptions.GATKException;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsArgumentCollection;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1NjIyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460156226", "bodyText": "Pulling this up seems strange because now we have to serialize the empty list.", "author": "lbergelson", "createdAt": "2020-07-24T16:21:43Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\nindex 404daefa2..3c2f37680 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n\n@@ -184,11 +184,10 @@ public final class SparkUtils {\n                 \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n \n         // Find the first group in each partition\n-        final List<GATKRead> empty = Collections.emptyList();\n         final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n                 .mapPartitions(it -> {\n                     if ( !it.hasNext() ) {\n-                        return Iterators.singletonIterator(empty);\n+                        return Iterators.singletonIterator(Collections.<GATKRead>emptyList());\n                     }\n                     final List<GATKRead> firstGroup = new ArrayList<>(2);\n                     final GATKRead firstRead = it.next();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4NTUxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460185511", "bodyText": "You know how I feel about {}", "author": "lbergelson", "createdAt": "2020-07-24T17:17:46Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();\n+        final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n+                .mapPartitions(it -> {\n+                    if ( !it.hasNext() ) {\n+                        return Iterators.singletonIterator(empty);\n+                    }\n+                    final List<GATKRead> firstGroup = new ArrayList<>(2);\n+                    final GATKRead firstRead = it.next();\n+                    firstGroup.add(firstRead);\n+                    final String groupName = firstRead.getName();\n+                    while ( it.hasNext() ) {\n+                        final GATKRead read = it.next();\n+                        if ( !groupName.equals(read.getName()) ) break;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\nindex 404daefa2..3c2f37680 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n\n@@ -184,11 +184,10 @@ public final class SparkUtils {\n                 \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n \n         // Find the first group in each partition\n-        final List<GATKRead> empty = Collections.emptyList();\n         final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n                 .mapPartitions(it -> {\n                     if ( !it.hasNext() ) {\n-                        return Iterators.singletonIterator(empty);\n+                        return Iterators.singletonIterator(Collections.<GATKRead>emptyList());\n                     }\n                     final List<GATKRead> firstGroup = new ArrayList<>(2);\n                     final GATKRead firstRead = it.next();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4NzYyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460187626", "bodyText": "brackets brackets everywhere", "author": "lbergelson", "createdAt": "2020-07-24T17:22:04Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();\n+        final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n+                .mapPartitions(it -> {\n+                    if ( !it.hasNext() ) {\n+                        return Iterators.singletonIterator(empty);\n+                    }\n+                    final List<GATKRead> firstGroup = new ArrayList<>(2);\n+                    final GATKRead firstRead = it.next();\n+                    firstGroup.add(firstRead);\n+                    final String groupName = firstRead.getName();\n+                    while ( it.hasNext() ) {\n+                        final GATKRead read = it.next();\n+                        if ( !groupName.equals(read.getName()) ) break;\n+                        firstGroup.add(read);\n+                    }\n+                    return Iterators.singletonIterator(firstGroup);\n+                })\n                 .collect();\n \n-        // Checking for pathological cases (read name groups that span more than 2 partitions)\n-        String groupName = null;\n-        for (List<GATKRead> group : firstReadNameGroupInEachPartition) {\n-            if (group!=null && !group.isEmpty()) {\n-                // If a read spans multiple partitions we expect its name to show up multiple times and we don't expect this to work properly\n-                if (groupName != null && group.get(0).getName().equals(groupName)) {\n-                    throw new GATKException(String.format(\"The read name '%s' appeared across multiple partitions this could indicate there was a problem \" +\n-                            \"with the sorting or that the rdd has too many partitions, check that the file is queryname sorted and consider decreasing the number of partitions\", groupName));\n+        // Shift left, so that each partition will be zipped with the first read group from the _next_ partition\n+        final int numPartitions = reads.getNumPartitions();\n+        final List<List<GATKRead>> firstGroupFromNextPartition =\n+                new ArrayList<>(firstReadNameGroupInEachPartition.subList(1, numPartitions));\n+        firstGroupFromNextPartition.add(Collections.emptyList()); // the last partition does not have any reads to add to it\n+\n+        // Take care of the situation where an entire partition contains reads with the same name\n+        // (unlikely, but could happen with very long reads, or very small partitions).\n+        for ( int idx = numPartitions - 1; idx >= 1; --idx ) {\n+            final List<GATKRead> curGroup = firstGroupFromNextPartition.get(idx);\n+            if ( !curGroup.isEmpty() ) {\n+                final String groupName = curGroup.get(0).getName();\n+                int idx2 = idx;\n+                while ( --idx2 >= 0 ) {\n+                    final List<GATKRead> prevGroup = firstGroupFromNextPartition.get(idx2);\n+                    if ( !prevGroup.isEmpty() ) {\n+                        if ( groupName.equals(prevGroup.get(0).getName()) ) {\n+                            prevGroup.addAll(curGroup);\n+                            curGroup.clear();\n+                        }\n+                        break;\n+                    }\n                 }\n-                groupName =  group.get(0).getName();\n             }\n         }\n \n-        // Shift left, so that each partition will be joined with the first read group from the _next_ partition\n-        List<List<GATKRead>> firstReadInNextPartition = new ArrayList<>(firstReadNameGroupInEachPartition.subList(1, numPartitions));\n-        firstReadInNextPartition.add(null); // the last partition does not have any reads to add to it\n+        // Peel off the first group in each partition\n+        final int[] firstGroupSizes = firstReadNameGroupInEachPartition.stream().mapToInt(List::size).toArray();\n+        firstGroupSizes[0] = 0; // first partition has no predecessor to handle its first group of reads\n+        JavaRDD<GATKRead> readsSansFirstGroup = reads.mapPartitionsWithIndex( (idx, itr) ->\n+            { int groupSize = firstGroupSizes[idx];\n+              while ( itr.hasNext() && groupSize-- > 0 ) itr.next();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\nindex 404daefa2..3c2f37680 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java\n\n@@ -184,11 +184,10 @@ public final class SparkUtils {\n                 \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n \n         // Find the first group in each partition\n-        final List<GATKRead> empty = Collections.emptyList();\n         final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n                 .mapPartitions(it -> {\n                     if ( !it.hasNext() ) {\n-                        return Iterators.singletonIterator(empty);\n+                        return Iterators.singletonIterator(Collections.<GATKRead>emptyList());\n                     }\n                     final List<GATKRead> firstGroup = new ArrayList<>(2);\n                     final GATKRead firstRead = it.next();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4OTE1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460189158", "bodyText": "I like Comparator.comparing(GATKRead::getName) but the lambda is fine too.", "author": "lbergelson", "createdAt": "2020-07-24T17:25:14Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\nindex cb17bad88..169953609 100644\n--- a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n\n@@ -143,15 +144,15 @@ public class SparkUtilsUnitTest extends GATKBaseTest {\n \n     @Test\n     public void testReadsPairsSpanningMultiplePartitions() {\n-        JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n-        SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n+        final JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n+        final SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n-        List<GATKRead> reads = createPairedReads(header, 40, 2);\n+        final List<GATKRead> reads = createPairedReads(header, 40, 2);\n         reads.addAll(createPairedReads(header, 1, 30));\n-        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n+        reads.sort(Comparator.comparing(GATKRead::getName));\n \n-        JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        JavaRDD<GATKRead> fixedReads =\n+        final JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n+        final JavaRDD<GATKRead> fixedReads =\n                 SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n         final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n                 .mapPartitionsWithIndex(( idx, itr ) -> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MTQ3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460191474", "bodyText": "I always think it's much more readable to split these operations on multiple lines\n.mapToPari()\n.groupByKey()\n.filter(...)", "author": "lbergelson", "createdAt": "2020-07-24T17:29:53Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\nindex cb17bad88..169953609 100644\n--- a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n\n@@ -143,15 +144,15 @@ public class SparkUtilsUnitTest extends GATKBaseTest {\n \n     @Test\n     public void testReadsPairsSpanningMultiplePartitions() {\n-        JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n-        SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n+        final JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n+        final SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n-        List<GATKRead> reads = createPairedReads(header, 40, 2);\n+        final List<GATKRead> reads = createPairedReads(header, 40, 2);\n         reads.addAll(createPairedReads(header, 1, 30));\n-        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n+        reads.sort(Comparator.comparing(GATKRead::getName));\n \n-        JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        JavaRDD<GATKRead> fixedReads =\n+        final JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n+        final JavaRDD<GATKRead> fixedReads =\n                 SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n         final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n                 .mapPartitionsWithIndex(( idx, itr ) -> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MjMyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460192322", "bodyText": "why not Iterators.size(t._2())", "author": "lbergelson", "createdAt": "2020-07-24T17:31:37Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {\n+                    int count = 0;\n+                    for ( final int idx : t._2() ) ++count;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "chunk": "diff --git a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\nindex cb17bad88..169953609 100644\n--- a/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n+++ b/src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java\n\n@@ -143,15 +144,15 @@ public class SparkUtilsUnitTest extends GATKBaseTest {\n \n     @Test\n     public void testReadsPairsSpanningMultiplePartitions() {\n-        JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n-        SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n+        final JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n+        final SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n-        List<GATKRead> reads = createPairedReads(header, 40, 2);\n+        final List<GATKRead> reads = createPairedReads(header, 40, 2);\n         reads.addAll(createPairedReads(header, 1, 30));\n-        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n+        reads.sort(Comparator.comparing(GATKRead::getName));\n \n-        JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        JavaRDD<GATKRead> fixedReads =\n+        final JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n+        final JavaRDD<GATKRead> fixedReads =\n                 SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n         final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n                 .mapPartitionsWithIndex(( idx, itr ) -> {\n"}}, {"oid": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "url": "https://github.com/broadinstitute/gatk/commit/ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "message": "de-sparkify SV discovery", "committedDate": "2020-08-11T19:08:42Z", "type": "commit"}, {"oid": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "url": "https://github.com/broadinstitute/gatk/commit/ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "message": "de-sparkify SV discovery", "committedDate": "2020-08-11T19:08:42Z", "type": "forcePushed"}]}