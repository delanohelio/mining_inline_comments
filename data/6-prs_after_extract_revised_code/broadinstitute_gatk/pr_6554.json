{"pr_number": 6554, "pr_title": "(all of) gCNV exome joint calling", "pr_createdAt": "2020-04-17T20:54:42Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6554", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466633", "bodyText": "This doesn't go so well with just two events that have the minimal overlap.", "author": "ldgauthier", "createdAt": "2020-04-17T20:58:44Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMzNTE4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422335184", "bodyText": "Fair point, although I think there are downsides to any simple rule. The solution may be to incorporate evidence to make a more intelligent decision. For depth-only calls where the breakpoints are imprecise, it seems reasonable to use a mean value. Other options would be min start / max end or max start / min end. It would be great if all these could be chosen by the user with another commandline arg.", "author": "mwalker174", "createdAt": "2020-05-08T19:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgzMTYzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r434831632", "bodyText": "Done.", "author": "ldgauthier", "createdAt": "2020-06-03T20:23:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex b2c0fef34..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466855", "bodyText": "I'm not sure what role the evidence plays in the WGS pipeline.", "author": "ldgauthier", "createdAt": "2020-04-17T20:59:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDQzNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424664435", "bodyText": "Don't need it for gCNV but I think you can leave it in", "author": "mwalker174", "createdAt": "2020-05-13T19:00:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex b2c0fef34..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467041", "bodyText": "Same here -- are they identical if the evidence doesn't match?", "author": "ldgauthier", "createdAt": "2020-04-17T20:59:35Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1NDQzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427454438", "bodyText": "Yes I would like to keep the flexibility to be able to compare items without using equals() for the purposes of deduplication. (I'll comment elsewhere on whether we should be using SVCallRecordWithEvidence).", "author": "mwalker174", "createdAt": "2020-05-19T16:55:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex b2c0fef34..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467583", "bodyText": "I modified the math here to better reflect my idea of how we compute reciprocal overlap, but I'm open to discussion.", "author": "ldgauthier", "createdAt": "2020-04-17T21:00:47Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0MTQzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424741434", "bodyText": "Hm this looks correct! Not sure what I was doing before...", "author": "mwalker174", "createdAt": "2020-05-13T21:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex b2c0fef34..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468109", "bodyText": "Should we be checking the copy number here?  I don't think I want to merge a CN1 deletion with a CN0 deletion, for example.", "author": "ldgauthier", "createdAt": "2020-04-17T21:02:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTk2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427541962", "bodyText": "Thanks for pointing that out - we should check genotypes. I think easiest way would be to add a genotype to SVCallRecord.", "author": "mwalker174", "createdAt": "2020-05-19T19:18:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\nindex 76458cd5b..ace095085 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n\n@@ -3,10 +3,10 @@ package org.broadinstitute.hellbender.tools.sv;\n import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n-import org.broadinstitute.hellbender.utils.IntervalUtils;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.*;\n \n import java.util.*;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468633", "bodyText": "Is the length inclusive?  That's typically the GATK convention, in which case this needs a +1.  Unless it comes as the above compensatory +1?", "author": "ldgauthier", "createdAt": "2020-04-17T21:03:18Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyNjE2OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422326169", "bodyText": "Let's go with the GATK convention", "author": "mwalker174", "createdAt": "2020-05-08T19:20:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 3d5affcb7..50994f6b4 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -74,7 +74,7 @@ public class SVCallRecord implements Feature {\n \n         //TODO : use new vcfs to get actual allele\n         final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n-        if (copyNumber == 2) return null;\n+        if (copyNumber == 2) { return null; }\n         final boolean isDel = copyNumber < 2;\n         final boolean startStrand = isDel ? true : false;\n         final boolean endStrand = isDel ? false : true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTU2MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469561", "bodyText": "This is a more strict check compared with itemsAreIdentical -- I use this in the tests", "author": "ldgauthier", "createdAt": "2020-04-17T21:05:27Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java", "diffHunk": "@@ -0,0 +1,82 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+\n+public class SVCallRecordWithEvidence extends SVCallRecord {\n+\n+    private final List<SplitReadSite> startSplitReadSites;\n+    private final List<SplitReadSite> endSplitReadSites;\n+    private final List<DiscordantPairEvidence> discordantPairs;\n+\n+    public SVCallRecordWithEvidence(final SVCallRecord record) {\n+        super(record.getContig(), record.getStart(), record.getStartStrand(), record.getEndContig(), record.getEnd(),\n+                record.getEndStrand(), record.getType(), record.getLength(), record.getAlgorithms(), record.getGenotypes());\n+        this.startSplitReadSites = Collections.emptyList();\n+        this.endSplitReadSites = Collections.emptyList();\n+        this.discordantPairs = Collections.emptyList();\n+    }\n+\n+    public SVCallRecordWithEvidence(final String startContig,\n+                                    final int start,\n+                                    final boolean startStrand,\n+                                    final String endContig,\n+                                    final int end,\n+                                    final boolean endStrand,\n+                                    final StructuralVariantType type,\n+                                    final int length,\n+                                    final List<String> algorithms,\n+                                    final List<Genotype> genotypes,\n+                                    final List<SplitReadSite> startSplitReadSites,\n+                                    final List<SplitReadSite> endSplitReadSites,\n+                                    final List<DiscordantPairEvidence> discordantPairs) {\n+        super(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, genotypes);\n+        Utils.nonNull(startSplitReadSites);\n+        Utils.nonNull(endSplitReadSites);\n+        Utils.nonNull(discordantPairs);\n+        Utils.containsNoNull(startSplitReadSites, \"Encountered null in start split reads\");\n+        Utils.containsNoNull(endSplitReadSites, \"Encountered null in end split reads\");\n+        Utils.containsNoNull(discordantPairs, \"Encountered null in discordant pairs\");\n+        this.startSplitReadSites = startSplitReadSites;\n+        this.endSplitReadSites = endSplitReadSites;\n+        this.discordantPairs = discordantPairs;\n+    }\n+\n+    public List<DiscordantPairEvidence> getDiscordantPairs() {\n+        return discordantPairs;\n+    }\n+\n+    public List<SplitReadSite> getStartSplitReadSites() {\n+        return startSplitReadSites;\n+    }\n+\n+    public List<SplitReadSite> getEndSplitReadSites() {\n+        return endSplitReadSites;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java\nindex 7090d7784..2c114ffa7 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java\n\n@@ -79,4 +79,8 @@ public class SVCallRecordWithEvidence extends SVCallRecord {\n     public int hashCode() {\n         return Objects.hash(super.hashCode(), discordantPairs, endSplitReadSites, startSplitReadSites);\n     }\n+\n+    public String prettyPrint() {\n+        return getContig() + \":\" + getStart() + \"-\" + getEnd();\n+    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTc3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469770", "bodyText": "I don't have any insertion tests yet.", "author": "ldgauthier", "createdAt": "2020-04-17T21:05:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex b2c0fef34..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410470128", "bodyText": "Would it be better to throw an IllegalStateException if we shouldn't reach here?", "author": "ldgauthier", "createdAt": "2020-04-17T21:06:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)\n+                .overlaps(getClusteringInterval(b, null));\n+    }\n+\n+\n+    /**\n+     * Determine an overlap interval for clustering using {@value #PADDING_FRACTION} padding\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param currentClusterInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval currentClusterInterval) {\n+        Utils.nonNull(call);\n+        final SimpleInterval callInterval = getCallInterval(call);\n+        final int paddedCallStart = (int) (callInterval.getStart() - PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final int paddedCallEnd = (int) (callInterval.getEnd() + PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final String currentContig = getCurrentContig();\n+        final int contigLength = dictionary.getSequence(currentContig).getSequenceLength();\n+        if (currentClusterInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, paddedCallStart, paddedCallEnd, contigLength);\n+        }\n+        //NOTE: this is an approximation -- padding should be based on the length of the call plus currentClusterIntervals\n+        final int newMinStart = Math.min(paddedCallStart, currentClusterInterval.getStart());\n+        final int newMaxEnd = Math.max(paddedCallEnd, currentClusterInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxEnd, contigLength);\n+    }\n+\n+    // Not used for single-linkage clustering\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return false;", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMDM1Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424820353", "bodyText": "Actually, as it's written now, it appears that this code does get called but will always be false for single-linkage clustering. The code path is getOutput() -> deduplicateItems() -> itemsAreIdentical(), so I think the better solution would be to check if the class algorithm type is single-linkage or max-clique and skip deduplicateItems() if it's single-linkage. In that case, we could throw a NeverReachHere exception.", "author": "mwalker174", "createdAt": "2020-05-14T01:27:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\nindex 76458cd5b..ace095085 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n\n@@ -3,10 +3,10 @@ package org.broadinstitute.hellbender.tools.sv;\n import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n-import org.broadinstitute.hellbender.utils.IntervalUtils;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.*;\n \n import java.util.*;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0OTk5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422249990", "bodyText": "Unfortunate that it's come to this, but I suppose this is our best option at this point given all of the gCNV vcfs that have been generated already. Alternatively we could implement a task to detect missing sequence dictionaries and fix the headers. @droazen Any concerns?", "author": "mwalker174", "createdAt": "2020-05-08T16:47:32Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private CloseableIterator<VariantContext> currentIterator;\n     private SortedSet<String> mergedSamples;\n \n+    private boolean skipDictionaryValidation = false;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE1NjUwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515156500", "bodyText": "Not sure I have all the context here, but if there are a large amount of gCNV VCFs that have already been generated with inconsistent and/or missing dictionaries, can we just run UpdateVCFSequenceDictionary on them once and pretend like it never happened? I think it would be good to enforce correct behavior with this pipeline (and/or with this engine-level class) going forward.", "author": "samuelklee", "createdAt": "2020-10-30T14:53:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0OTk5MA=="}], "type": "inlineReview", "revised_code": {"commit": "874848454bbd012d995a7c879927ff4491174b34", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\nindex 0919ab6b2..eb4f5b2f0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n+++ b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n\n@@ -60,8 +60,6 @@ public final class MultiVariantDataSource implements GATKDataSource<VariantConte\n     private CloseableIterator<VariantContext> currentIterator;\n     private SortedSet<String> mergedSamples;\n \n-    private boolean skipDictionaryValidation = false;\n-\n     /**\n      * Creates a MultiVariantDataSource backed by the provided FeatureInputs. We will look ahead the specified number of bases\n      * during queries that produce cache misses.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI1NTMwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422255308", "bodyText": "These aren't needed are they?", "author": "mwalker174", "createdAt": "2020-05-08T16:57:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java", "diffHunk": "@@ -74,6 +74,14 @@\n     public static final String DUP_TAN_EXPANSION_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-TANDEM-EXPANSION\";\n     public static final String DUP_INV_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-INVERTED-EXPANSION\";\n \n+    // for breakpoint segmentation\n+    public static final String ALGORITHMS_ATTRIBUTE = \"ALGORITHMS\";\n+    public static final String STRANDS_ATTRIBUTE = \"STRANDS\";\n+    public static final String DEPTH_ALGORITHM = \"depth\";\n+    public static final String END_CONTIG_ATTRIBUTE = \"CHR2\";\n+    public static String START_SPLIT_READ_COUNT_ATTRIBUTE = \"SSR\";\n+    public static String END_SPLIT_READ_COUNT_ATTRIBUTE = \"ESR\";\n+    public static String DISCORDANT_PAIR_COUNT_ATTRIBUTE = \"PE\";", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java\nindex f4e2e0e96..efbf10a6d 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java\n\n@@ -79,9 +79,6 @@ public final class GATKSVVCFConstants {\n     public static final String STRANDS_ATTRIBUTE = \"STRANDS\";\n     public static final String DEPTH_ALGORITHM = \"depth\";\n     public static final String END_CONTIG_ATTRIBUTE = \"CHR2\";\n-    public static String START_SPLIT_READ_COUNT_ATTRIBUTE = \"SSR\";\n-    public static String END_SPLIT_READ_COUNT_ATTRIBUTE = \"ESR\";\n-    public static String DISCORDANT_PAIR_COUNT_ATTRIBUTE = \"PE\";\n \n     // format block\n     public static final String COPY_NUMBER_FORMAT = \"CN\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMxNDYyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422314623", "bodyText": "This seems to do a lot of unneeded work if the compared objects are not equal. I think you should short-circuit the checks like this:\nif (!this.getContig().equals(b.getContig())) return false;\nif (this.getStart() != b.getStart()) return false;\n...\n\nIt's a little less readable but IMO the efficiency costs are worth it.", "author": "mwalker174", "createdAt": "2020-05-08T18:56:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (obj == null) {\n+            return false;\n+        }\n+        if (this.getClass() != obj.getClass()) {\n+            return false;\n+        }\n+        final SVCallRecord b = (SVCallRecord) obj;\n+        boolean areEqual = this.getContig().equals(b.getContig());\n+        areEqual &= this.getStart() == b.getStart();\n+        areEqual &= this.getStartStrand() == b.getStartStrand();\n+\n+        areEqual &= this.getEndContig() == b.getEndContig();\n+        areEqual &= this.getEnd() == b.getEnd();\n+        areEqual &= this.getEndStrand() == b.getEndStrand();\n+\n+        areEqual &= this.getType() == b.getType();\n+        areEqual &= this.getLength() == b.getLength();\n+\n+        areEqual &= this.getAlgorithms().containsAll(b.getAlgorithms());\n+        areEqual &= b.getAlgorithms().containsAll(this.getAlgorithms());\n+\n+        areEqual &= this.getSamples().containsAll(b.getSamples());\n+        areEqual &= b.getSamples().containsAll(this.getSamples());\n+\n+        areEqual &= this.getGenotypes().containsAll(b.getGenotypes());\n+        areEqual &= b.getGenotypes().containsAll(this.getGenotypes());\n+\n+        return areEqual;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 3d5affcb7..50994f6b4 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -74,7 +74,7 @@ public class SVCallRecord implements Feature {\n \n         //TODO : use new vcfs to get actual allele\n         final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n-        if (copyNumber == 2) return null;\n+        if (copyNumber == 2) { return null; }\n         final boolean isDel = copyNumber < 2;\n         final boolean startStrand = isDel ? true : false;\n         final boolean endStrand = isDel ? false : true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422328059", "bodyText": "I'm fine with merging this as is, but we should open a ticket to keep track of this", "author": "mwalker174", "createdAt": "2020-05-08T19:24:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1MTcyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424651728", "bodyText": "Have you been excluding allosomes when testing?", "author": "mwalker174", "createdAt": "2020-05-13T18:39:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 3d5affcb7..50994f6b4 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -74,7 +74,7 @@ public class SVCallRecord implements Feature {\n \n         //TODO : use new vcfs to get actual allele\n         final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n-        if (copyNumber == 2) return null;\n+        if (copyNumber == 2) { return null; }\n         final boolean isDel = copyNumber < 2;\n         final boolean startStrand = isDel ? true : false;\n         final boolean endStrand = isDel ? false : true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyOTcwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422329709", "bodyText": "Remove + 1's", "author": "mwalker174", "createdAt": "2020-05-08T19:27:49Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 3d5affcb7..50994f6b4 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -74,7 +74,7 @@ public class SVCallRecord implements Feature {\n \n         //TODO : use new vcfs to get actual allele\n         final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n-        if (copyNumber == 2) return null;\n+        if (copyNumber == 2) { return null; }\n         final boolean isDel = copyNumber < 2;\n         final boolean startStrand = isDel ? true : false;\n         final boolean endStrand = isDel ? false : true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0OTI1MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424649251", "bodyText": "By GATK convention, should be just return start;", "author": "mwalker174", "createdAt": "2020-05-13T18:34:43Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+\n+import htsjdk.tribble.Feature;\n+\n+public final class DiscordantPairEvidence implements Feature {\n+\n+    final String sample;\n+    final String startContig;\n+    final String endContig;\n+    final int start;\n+    final int end;\n+    final boolean startStrand;\n+    final boolean endStrand;\n+\n+    public DiscordantPairEvidence(final String sample, final String startContig, final int start, final boolean startStrand,\n+                                  final String endContig, final int end, final boolean endStrand) {\n+        this.sample = sample;\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+    }\n+\n+    public String getSample() {\n+        return sample;\n+    }\n+\n+    // For purposes of indexing, we will return the start position\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        if (startContig.equals(endContig)) {\n+            return end;\n+        } else {\n+            return start + 1;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java\nindex 73f9e0f00..07cdd5bc9 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java\n\n@@ -52,7 +52,7 @@ public final class DiscordantPairEvidence implements Feature {\n         if (startContig.equals(endContig)) {\n             return end;\n         } else {\n-            return start + 1;\n+            return start;\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0ODI5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424748290", "bodyText": "I think it should be 1 + validClusterIds.size()", "author": "mwalker174", "createdAt": "2020-05-13T21:43:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {\n+                    final int n = (int) clusterItemIds.stream().filter(linkedItemIds::contains).count();\n+                    if (n == clusterItemIds.size()) {\n+                        clustersToAdd.add(clusterIndex);\n+                    } else if (n > 0) {\n+                        clustersToSeedWith.add(clusterIndex);\n+                    }\n+                } else if (clusteringType.equals(CLUSTERING_TYPE.MAX_CLIQUE)) {\n+                    final boolean matchesCluster = clusterItemIds.stream().anyMatch(linkedItemIds::contains);\n+                    if (matchesCluster) {\n+                        clustersToAdd.add(clusterIndex);\n+                    }\n+                } else {\n+                    throw new IllegalArgumentException(\"Clustering algorithm for type \" + clusteringType.name() + \" not implemented\");\n+                }\n+            }\n+            clusterIndex++;\n+        }\n+\n+        // Add to item clusters\n+        for (final int index : clustersToAdd) {\n+            addToCluster(index, currentItemId);\n+        }\n+        // Create new clusters/cliques\n+        for (final int index : clustersToSeedWith) {\n+            seedWithExistingCluster(currentItemId, index, linkedItemIds);\n+        }\n+        // If there weren't any matches, create a new singleton cluster\n+        if (clustersToAdd.isEmpty() && clustersToSeedWith.isEmpty()) {\n+            seedCluster(currentItemId);\n+        }\n+        return clusterIdsToProcess;\n+    }\n+\n+    private void processCluster(final int clusterIndex) {\n+        final Tuple2<SimpleInterval, List<Long>> cluster = validateClusterIndex(clusterIndex);\n+        final List<Long> clusterItemIds = cluster._2;\n+        currentClusters.remove(clusterIndex);\n+        final List<T> clusterItems = clusterItemIds.stream().map(idToItemMap::get).collect(Collectors.toList());\n+        outputBuffer.add(flattenCluster(clusterItems));\n+    }\n+\n+    private void processFinalizedClusters(final List<Integer> clusterIdsToProcess) {\n+        final Set<Integer> activeClusterIds = IntStream.range(0, currentClusters.size()).boxed().collect(Collectors.toSet());\n+        activeClusterIds.removeAll(clusterIdsToProcess);\n+        final Set<Long> activeClusterItemIds = activeClusterIds.stream().flatMap(i -> currentClusters.get(i)._2.stream()).collect(Collectors.toSet());\n+        final Set<Long> finalizedItemIds = clusterIdsToProcess.stream()\n+                .flatMap(i -> currentClusters.get(i)._2.stream())\n+                .filter(i -> !activeClusterItemIds.contains(i))\n+                .collect(Collectors.toSet());\n+        for (int i = clusterIdsToProcess.size() - 1; i >= 0; i--) {\n+            processCluster(clusterIdsToProcess.get(i));\n+        }\n+        finalizedItemIds.stream().forEach(idToItemMap::remove);\n+    }\n+\n+    private void deleteRedundantClusters() {\n+        final Set<Integer> redundantClusterSet = new HashSet<>();\n+        for (int i = 0; i < currentClusters.size(); i++) {\n+            final Set<Long> clusterSetA = new HashSet<>(currentClusters.get(i)._2);\n+            for (int j = 0; j < i; j++) {\n+                final Set<Long> clusterSetB = new HashSet<>(currentClusters.get(j)._2);\n+                if (clusterSetA.containsAll(clusterSetB)) {\n+                    redundantClusterSet.add(j);\n+                } else if (clusterSetA.size() != clusterSetB.size() && clusterSetB.containsAll(clusterSetA)) {\n+                    redundantClusterSet.add(i);\n+                }\n+            }\n+        }\n+        final List<Integer> redundantClustersList = new ArrayList<>(redundantClusterSet);\n+        redundantClustersList.sort(Comparator.naturalOrder());\n+        for (int i = redundantClustersList.size() - 1; i >= 0; i--) {\n+            currentClusters.remove((int)redundantClustersList.get(i));\n+        }\n+    }\n+\n+    private void flushClusters() {\n+        while (!currentClusters.isEmpty()) {\n+            processCluster(0);\n+        }\n+        resetItemIds();\n+    }\n+\n+    private void seedCluster(final long seedId) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> newCluster = new ArrayList<>(1);\n+        newCluster.add(seedId);\n+        currentClusters.add(new Tuple2<>(getClusteringInterval(seed, null), newCluster));\n+    }\n+\n+    /**\n+     * Create a new cluster\n+     * @param seedId    itemId\n+     * @param existingClusterIndex\n+     * @param clusteringIds\n+     */\n+    private void seedWithExistingCluster(final Long seedId, final int existingClusterIndex, final Set<Long> clusteringIds) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> existingCluster = currentClusters.get(existingClusterIndex)._2;\n+        final List<Long> validClusterIds = existingCluster.stream().filter(clusteringIds::contains).collect(Collectors.toList());\n+        final List<Long> newCluster = new ArrayList<>(1 + existingCluster.size());", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\nindex d4f59f955..991a231bb 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\n\n@@ -2,9 +2,7 @@ package org.broadinstitute.hellbender.tools.sv;\n \n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.util.Locatable;\n-import org.broadinstitute.hellbender.utils.IntervalUtils;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.*;\n import scala.Tuple2;\n \n import java.util.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc5OTg4NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424799885", "bodyText": "Uh ohhhh, this should be CLUSTERING_TYPE.MAX_CLIQUE and vice versa below. This is my mistake.", "author": "mwalker174", "createdAt": "2020-05-14T00:09:08Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\nindex d4f59f955..991a231bb 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java\n\n@@ -2,9 +2,7 @@ package org.broadinstitute.hellbender.tools.sv;\n \n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.util.Locatable;\n-import org.broadinstitute.hellbender.utils.IntervalUtils;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.*;\n import scala.Tuple2;\n \n import java.util.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0Njc3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427446773", "bodyText": "Thanks it looks like you fixed this from my version where I had min/max switched, but I'm not sure I follow how this is an approximation. The purpose of the cluster interval is to determine whether a given item is \"past\" the cluster, ie the item's start position guarantees that it cannot be linked to any members of the cluster. To guarantee this, don't we just take the largest end position of all the items in the cluster?", "author": "mwalker174", "createdAt": "2020-05-19T16:43:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex f8323199a..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1Njk5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427456990", "bodyText": "I'm seeing this again and don't like it. Can you add a TODO here stating that we need to put more thought into this?", "author": "mwalker174", "createdAt": "2020-05-19T16:59:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return a.getContig().equals(b.getContig())\n+                && a.getStart() == b.getStart()\n+                && a.getEndContig().equals(b.getEndContig())\n+                && a.getEnd() == b.getEnd()\n+                && a.getType().equals(b.getType())\n+                && a.getStartStrand() == b.getStartStrand()\n+                && a.getEndStrand() == b.getEndStrand();\n+    }\n+\n+    /**\n+     *  Merge genotypes and algorithms for multiple calls describing the same event\n+     * @param items all entries are assumed to describe the same event, i.e. satisfy {@link #itemsAreIdentical}\n+     * @return  a single representative call\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence deduplicateIdenticalItems(final Collection<SVCallRecordWithEvidence> items) {\n+        if (items.isEmpty()) {\n+            return null;\n+        }\n+        final List<Genotype> genotypes = items.stream()\n+                .map(SVCallRecordWithEvidence::getGenotypes)\n+                .flatMap(Collection::stream)\n+                .collect(Collectors.toList());\n+        final List<String> algorithms = items.stream()\n+                .map(SVCallRecordWithEvidence::getAlgorithms)\n+                .flatMap(Collection::stream)\n+                .distinct()\n+                .collect(Collectors.toList());\n+        final SVCallRecordWithEvidence example = items.iterator().next();\n+        return new SVCallRecordWithEvidence(\n+                example.getContig(),\n+                example.getStart(),\n+                example.getStartStrand(),\n+                example.getEndContig(),\n+                example.getEnd(),\n+                example.getEndStrand(),\n+                example.getType(),\n+                example.getLength(),\n+                algorithms,\n+                genotypes,\n+                example.getStartSplitReadSites(),\n+                example.getEndSplitReadSites(),\n+                example.getDiscordantPairs());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\nindex f8323199a..3b1d756c0 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java\n\n@@ -1,10 +1,13 @@\n package org.broadinstitute.hellbender.tools.sv;\n \n+import com.google.common.primitives.Doubles;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.apache.commons.math3.stat.descriptive.moment.Mean;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n \n import java.util.Collection;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUzODU3Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427538572", "bodyText": "I'm looking back at my code and I believe I used SVCallRecordWithEvidence rather than SVCallRecord out of convenience for my clustering tool. However, I think it would be better to refactor using the latter so we can avoid issues like this one here.", "author": "mwalker174", "createdAt": "2020-05-19T19:12:35Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\nindex 76458cd5b..ace095085 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java\n\n@@ -3,10 +3,10 @@ package org.broadinstitute.hellbender.tools.sv;\n import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n-import org.broadinstitute.hellbender.utils.IntervalUtils;\n-import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.*;\n \n import java.util.*;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MjcyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427542723", "bodyText": "I think this and DiscordantPairEvidence can be omitted if you stick to SVCallRecord", "author": "mwalker174", "createdAt": "2020-05-19T19:20:07Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadSite.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+final class SplitReadSite {", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzQwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427543405", "bodyText": "20 seems low to me. Did you consult with jfu on this?", "author": "mwalker174", "createdAt": "2020-05-19T19:21:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1MzcxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r518353718", "bodyText": "That's still what Isaac has in the latest version.  Higher would be more efficient (i.e. merging fewer calls), but this is safer in case someone reduces the QS filter threshold downstream.", "author": "ldgauthier", "createdAt": "2020-11-05T20:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzQwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\nindex 52460e5bf..e875653d8 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n\n@@ -7,7 +7,8 @@ import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n import htsjdk.variant.vcf.VCFConstants;\n import htsjdk.variant.vcf.VCFHeader;\n import htsjdk.variant.vcf.VCFHeaderLine;\n-import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n import org.broadinstitute.barclay.argparser.Argument;\n import org.broadinstitute.barclay.argparser.BetaFeature;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0NTgxNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427545815", "bodyText": "I feel like this should get built into the engine with a requiresDictionary() method", "author": "mwalker174", "createdAt": "2020-05-19T19:25:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\nindex 52460e5bf..e875653d8 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n\n@@ -7,7 +7,8 @@ import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n import htsjdk.variant.vcf.VCFConstants;\n import htsjdk.variant.vcf.VCFHeader;\n import htsjdk.variant.vcf.VCFHeaderLine;\n-import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n import org.broadinstitute.barclay.argparser.Argument;\n import org.broadinstitute.barclay.argparser.BetaFeature;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwNzc2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427607768", "bodyText": "I think the end will now be at call.getEnd() + 1", "author": "mwalker174", "createdAt": "2020-05-19T21:22:16Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.0);\n+        clusterEngine = new SVClusterEngine(dictionary, true);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                defragmenter.add(new SVCallRecordWithEvidence(record));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();  //Don't forget to do the last cluster!!!\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+        defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        //defragmented calls may still be overlapping, so run the clustering engine to combine based on reciprocal overlap\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        calls.stream()\n+                .sorted(Comparator.comparing(c -> c.getStartAsInterval(), IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(this::buildVariantContext)\n+                .forEachOrdered(vcfWriter::add);\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call) {\n+        Utils.nonNull(call);\n+        final Allele altAllele = Allele.create(\"<\" + call.getType().name() + \">\", false);\n+        final Allele refAllele = Allele.REF_N;\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                Lists.newArrayList(refAllele, altAllele));\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\nindex 52460e5bf..e875653d8 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java\n\n@@ -7,7 +7,8 @@ import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n import htsjdk.variant.vcf.VCFConstants;\n import htsjdk.variant.vcf.VCFHeader;\n import htsjdk.variant.vcf.VCFHeaderLine;\n-import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n import org.broadinstitute.barclay.argparser.Argument;\n import org.broadinstitute.barclay.argparser.BetaFeature;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDU3Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427610572", "bodyText": "What was the impetus for these changes?", "author": "mwalker174", "createdAt": "2020-05-19T21:27:39Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java", "diffHunk": "@@ -26,16 +31,17 @@ private IndexUtils(){}\n      * Load a Tribble .idx index from disk, checking for out of date indexes and old versions\n      * @return an Index, or null if we're unable to load\n      */\n-    public static Index loadTribbleIndex(final File featureFile) {\n+    public static Index loadTribbleIndex(final Path featureFile) {", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODg0MTQwMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r428841401", "bodyText": "This is necessary to get the sequence dictionary out of the tabix index over NIO because the results I was working with were from an older version.  I've since added the sequence dictionary to the header, but this bug should be fixed.", "author": "ldgauthier", "createdAt": "2020-05-21T18:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDU3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "b7175045562c85450dedbc8ecb909edf4bb1b4c6", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java\nindex eed296283..d6814ee96 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java\n\n@@ -31,17 +26,16 @@ public final class IndexUtils {\n      * Load a Tribble .idx index from disk, checking for out of date indexes and old versions\n      * @return an Index, or null if we're unable to load\n      */\n-    public static Index loadTribbleIndex(final Path featureFile) {\n+    public static Index loadTribbleIndex(final File featureFile) {\n         Utils.nonNull(featureFile);\n-        final String indexFile = Tribble.indexFile(featureFile.toString());\n-        final Path indexPath = IOUtils.getPath(indexFile);\n-        if (! Files.isReadable(indexPath)) {\n+        final File indexFile = Tribble.indexFile(featureFile);\n+        if (! indexFile.canRead()) {\n             return null;\n         }\n         logger.debug(\"Loading Tribble index from disk for file \" + featureFile);\n         try {\n-            final Index index = IndexFactory.loadIndex(indexFile);\n-            checkIndexVersionAndModificationTime(featureFile, indexPath, index);\n+            final Index index = IndexFactory.loadIndex(indexFile.getAbsolutePath());\n+            checkIndexVersionAndModificationTime(featureFile, indexFile, index);\n             return index;\n         } catch (final RuntimeException e){\n             return null;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMTczMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427611732", "bodyText": "This class will probably evolve later, but it would be best to change this one line now. Let's make the suffix \".sv_calls.tsv.gz\" otherwise we could create trouble with feature codec collisions.", "author": "mwalker174", "createdAt": "2020-05-19T21:30:12Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java", "diffHunk": "@@ -0,0 +1,76 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import com.google.common.base.Splitter;\n+import htsjdk.tribble.AsciiFeatureCodec;\n+import htsjdk.tribble.index.tabix.TabixFormat;\n+import htsjdk.tribble.readers.LineIterator;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class SVCallRecordCodec extends AsciiFeatureCodec<SVCallRecord> {\n+\n+    public static final String FORMAT_SUFFIX = \".tsv.gz\";", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java b/src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java\nindex 5e4d1b331..50d7c2277 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java\n\n@@ -14,7 +14,7 @@ import java.util.List;\n \n public class SVCallRecordCodec extends AsciiFeatureCodec<SVCallRecord> {\n \n-    public static final String FORMAT_SUFFIX = \".tsv.gz\";\n+    public static final String FORMAT_SUFFIX = \".sv_calls.tsv.gz\";\n     public static final String COL_DELIMITER = \"\\t\";\n     public static String STRAND_PLUS = \"+\";\n     public static String STRAND_MINUS = \"-\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r434789098", "bodyText": "Now that I'm looking at this again, I don't understand this logic.  Inversions, sure, but dupes?", "author": "ldgauthier", "createdAt": "2020-06-03T19:02:52Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0MjQxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527142411", "bodyText": "DELs are +/- (true/false) and DUPs and -/+. Inversions are -/- and +/+.", "author": "mwalker174", "createdAt": "2020-11-19T19:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA=="}], "type": "inlineReview", "revised_code": {"commit": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 3d5affcb7..50994f6b4 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -74,7 +74,7 @@ public class SVCallRecord implements Feature {\n \n         //TODO : use new vcfs to get actual allele\n         final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n-        if (copyNumber == 2) return null;\n+        if (copyNumber == 2) { return null; }\n         final boolean isDel = copyNumber < 2;\n         final boolean startStrand = isDel ? true : false;\n         final boolean endStrand = isDel ? false : true;\n"}}, {"oid": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "url": "https://github.com/broadinstitute/gatk/commit/4f8b4b099475adf375c72df9e7f1096c194a1bed", "message": "More tests to check qual score calculation", "committedDate": "2020-06-15T20:32:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r447263502", "bodyText": "Note that use of non-overlapping intervals for all CNV locatable collections was enforced by using the base AbstractLocatableCollection class---see documentation of that class and subclasses. In practice, PreprocessIntervals creates valid non-overlapping bins, which remain valid as they move through the pipelines.\nIt might be kind of a nightmare to go through and change all the relevant documentation---and even more of a nightmare to define behavior for algorithms/procedures (such as segmentation, segment merging, identifying unique site/bin overlaps, etc.), which rely on this rather fundamental property (part of why we enforce this in the first place).\nI haven't looked closely at the code to see why you absolutely need to use the CNV collections classes to represent overlapping intervals, but I'd strongly suggest refactoring to avoid this if possible. Perhaps instead convert to VCF, IntervalList, etc.?", "author": "samuelklee", "createdAt": "2020-06-29T21:25:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -48,17 +48,24 @@ public static void validateIntervalArgumentCollection(final IntervalArgumentColl\n     }\n \n     /**\n-     * Validate that a list of locatables is valid and sorted according to a sequence dictionary and contains no duplicates or overlaps.\n+     * Validate that a list of locatables is valid and sorted according to a sequence dictionary", "originalCommit": "6cd8471380a722a009cf80fd81446c8000ebd115", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NjM2NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r447276365", "bodyText": "Took a quick look---you only need to validate the sample name (which might be a redundant check, haven't looked closely) and read in a list of IntegerCopyNumberSegments from copyNumberSegmentsFile. So unless you need the other machinery that the CNV collection class buys you, you might get away with doing something more one-off for this procedure---perhaps a private method that uses one of the other more generic TSV reading utility methods.\nIn any case, I'd add some comments/documentation that indicates that this business with overlapping intervals is the exception, not the norm.", "author": "samuelklee", "createdAt": "2020-06-29T21:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM3NjM1Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r458376357", "bodyText": "I was hoping to be able to use the encoder and decoder to get my data into a format compatible with FuncotateSegments without writing more IO methods, but that didn't work out so well because FuncotateSegments expects fields that the Python segment generation code doesn't write out.  I'm still struggling with an elegant way to turn the VCF into something that can be annotated, so I may end up ripping out the CopyNumberSegmentCollection-related changes anyway.", "author": "ldgauthier", "createdAt": "2020-07-21T20:44:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM4NzMzOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r458387339", "bodyText": "I thought FuncotateSegments was only intended to be used on somatic seg files, not the VCFs produced by the gCNV pipeline? Is there any additional functionality in FuncotateSegments besides being able to read seg files---can you not use Funcotator instead?\nI was always in favor of having the somatic pipeline output VCFs rather than seg files (there are probably still a few related issues filed), but not sure what the plan going forward should be.", "author": "samuelklee", "createdAt": "2020-07-21T21:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE0OTYxMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515149610", "bodyText": "Was reminded of this while looking at #6924. Not sure if you were planning on going back and addressing it, but I would make sure that all concomitant changes are reverted before merging. Happy to help review or discuss more!", "author": "samuelklee", "createdAt": "2020-10-30T14:45:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE5MTA0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r516191045", "bodyText": "Jon says Funcotator doesn't understand the END key.  It's on the list, but it's a long list. He says converting the VCF to a .set and using FuncotateSegments is the best bet for now.", "author": "ldgauthier", "createdAt": "2020-11-02T19:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE5NDAwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r516194005", "bodyText": "Sorry, just to be clear, I meant the changes to AbstractLocatableCollection, etc. to allow overlapping intervals should be rolled back, since most of the existing methods and code very strongly depend on the assumption of non-overlapping intervals. I think it will be difficult to update code and documentation if you want to keep those changes. I don\u2019t think I\u2019m up to date on the development plans for Funcotator/FuncotateSegments, so I don\u2019t have strong opinions there.", "author": "samuelklee", "createdAt": "2020-11-02T19:05:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}], "type": "inlineReview", "revised_code": {"commit": "874848454bbd012d995a7c879927ff4491174b34", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\nindex 5c604124c..dbee943f5 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java\n\n@@ -48,7 +48,7 @@ public final class CopyNumberArgumentValidationUtils {\n     }\n \n     /**\n-     * Validate that a list of locatables is valid and sorted according to a sequence dictionary\n+     * Validate that a list of locatables is valid and sorted according to a sequence dictionary and contains no duplicates or overlaps.\n      */\n     public static <T extends Locatable> void validateIntervals(final List<T> intervals,\n                                                                final SAMSequenceDictionary sequenceDictionary) {\n"}}, {"oid": "854619b80a1516622859192909557b831afe7fc9", "url": "https://github.com/broadinstitute/gatk/commit/854619b80a1516622859192909557b831afe7fc9", "message": "Use call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up", "committedDate": "2020-10-19T17:15:55Z", "type": "forcePushed"}, {"oid": "0d6bb314543637a02636795e07be1ba7345f023c", "url": "https://github.com/broadinstitute/gatk/commit/0d6bb314543637a02636795e07be1ba7345f023c", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up", "committedDate": "2020-10-19T20:31:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2ODU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515168568", "bodyText": "Looks like this isn't used yet, but if it's something you'll add, perhaps change the argument name to input-denoised-copy-ratios to be consistent with output-denoised-copy-ratios.", "author": "samuelklee", "createdAt": "2020-10-30T15:09:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -128,6 +129,15 @@\n     public static final String OUTPUT_DENOISED_COPY_RATIOS_LONG_NAME = \"output-denoised-copy-ratios\";\n     public static final String AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME = \"autosomal-ref-copy-number\";\n     public static final String ALLOSOMAL_CONTIG_LONG_NAME = \"allosomal-contig\";\n+    public static final String INPUT_INTERVALS_LONG_NAME = \"input-intervals-vcf\";\n+    public static final String INPUT_DENOISED_COPY_RATIO_LONG_NAME = \"input-denoised-copy-ratio\";", "originalCommit": "0d6bb314543637a02636795e07be1ba7345f023c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "874848454bbd012d995a7c879927ff4491174b34", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\nindex 4510dd8ba..78eb971be 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n\n@@ -130,14 +131,11 @@ public final class PostprocessGermlineCNVCalls extends GATKTool {\n     public static final String AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME = \"autosomal-ref-copy-number\";\n     public static final String ALLOSOMAL_CONTIG_LONG_NAME = \"allosomal-contig\";\n     public static final String INPUT_INTERVALS_LONG_NAME = \"input-intervals-vcf\";\n-    public static final String INPUT_DENOISED_COPY_RATIO_LONG_NAME = \"input-denoised-copy-ratio\";\n     public static final String CLUSTERED_FILE_LONG_NAME = \"clustered-breakpoints\";\n     public static final String DUPLICATION_QS_THRESHOLD_LONG_NAME = \"duplication-qs-threshold\";\n     public static final String HET_DEL_QS_THRESHOLD_LONG_NAME = \"het-deletion-qs-threshold\";\n     public static final String HOM_DEL_QS_THRESHOLD_LONG_NAME = \"hom-deletion-qs-threshold\";\n     public static final String SITE_FREQUENCY_THRESHOLD_LONG_NAME = \"site-frequency-threshold\";\n-    public static final String SAMPLE_FILTERED_CALL_COUNT_THRESHOLD = \"sample-filtered-call-count-threshold\";\n-    public static final String SAMPLE_UNFILTERED_CALL_COUNT_THRESHOLD = \"sample-unfiltered-call-count-threshold\";\n \n     @Argument(\n             doc = \"List of paths to GermlineCNVCaller call directories.\",\n"}}, {"oid": "874848454bbd012d995a7c879927ff4491174b34", "url": "https://github.com/broadinstitute/gatk/commit/874848454bbd012d995a7c879927ff4491174b34", "message": "Python unit test runner looks good\nAddress overlapping interval collection refactor", "committedDate": "2020-11-17T15:44:45Z", "type": "forcePushed"}, {"oid": "62b655c720bdea7621392a194861c8366a641fae", "url": "https://github.com/broadinstitute/gatk/commit/62b655c720bdea7621392a194861c8366a641fae", "message": "I thought we would need this....", "committedDate": "2020-11-19T17:03:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjM2Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112363", "bodyText": "optional=true", "author": "mwalker174", "createdAt": "2020-11-19T18:37:26Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112546", "bodyText": "This was a little confusing to me, and I would think that WGS would need this as well. Suggestion: gCNV model intervals created with the FilterIntervals tool.", "author": "mwalker174", "createdAt": "2020-11-19T18:37:47Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMzcwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527113703", "bodyText": "Also if this is truly optional, add optional = true", "author": "mwalker174", "createdAt": "2020-11-19T18:39:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2ODQ4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532868489", "bodyText": "WGS would only need this if the bin sizes were uneven -- do we ever expect gCNV to be run like that?", "author": "ldgauthier", "createdAt": "2020-11-30T20:01:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg4NTQ3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532885473", "bodyText": "I think the current CNV tools themselves are WGS/WES/etc. agnostic, for the most part if not entirely\u2014they just have enough exposed generic parameters to handle both cases.\nNot sure I have the entire context here, but I would generally expect FilterIntervals to be run regardless of sequencing type, and it\u2019s conceivable that arbitrary bins could be used on WGS data for certain use cases.", "author": "samuelklee", "createdAt": "2020-11-30T20:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxMzU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534413568", "bodyText": "Yes sometimes the bin sizes are uneven because gaps are always omitted, among other possibilities.", "author": "mwalker174", "createdAt": "2020-12-02T19:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527116879", "bodyText": "optional = true. I didn't realize barclay supported enum type inputs. Should you list the options here or do they pop out somewhere in the docs?", "author": "mwalker174", "createdAt": "2020-11-19T18:45:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2Nzg4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532867889", "bodyText": "Pretty sure they pop out in the docs, like for --emit-reference-confidence:\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360050814612-HaplotypeCaller", "author": "ldgauthier", "createdAt": "2020-11-30T19:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxMzk2NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534413965", "bodyText": "Very cool. I like this.", "author": "mwalker174", "createdAt": "2020-12-02T19:07:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNzE4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527117189", "bodyText": "Clarify that this is a vcf. Also can remove optional=false since that is default.", "author": "mwalker174", "createdAt": "2020-11-19T18:45:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExODAxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527118018", "bodyText": "Can you make them all end in _LONG_NAME or just _NAME?", "author": "mwalker174", "createdAt": "2020-11-19T18:47:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExOTAzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527119033", "bodyText": "What's the reasoning for this? Maybe we should make disabling this an option.", "author": "mwalker174", "createdAt": "2020-11-19T18:48:41Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NDg4MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534484880", "bodyText": "This is to avoid the inefficient dictionary checks mentioned above.  The tool requires a reference, though, so each input VCF will be checked against the reference, which is just O(N).", "author": "ldgauthier", "createdAt": "2020-12-02T21:13:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExOTAzMw=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMTMzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527121332", "bodyText": "I've written some machinery to read in gCNV contig ploidy calls that we can use to deal with inferring ploidy. We can put a pin in that for now.\nAlso, I do think it's nice to have the ped file option although my understanding is the sex assignments tend to be unreliable in practice.", "author": "mwalker174", "createdAt": "2020-11-19T18:52:19Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTg0Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534485842", "bodyText": "In theory one could pass in a pedigree constructed in another way, e.g. one that's higher confidence than the gCNV X calls stapled together.", "author": "ldgauthier", "createdAt": "2020-12-02T21:15:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMTMzMg=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjAwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122009", "bodyText": "For readability, can you put these validation blocks into a separate function?", "author": "mwalker174", "createdAt": "2020-11-19T18:53:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjEwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122105", "bodyText": "Extra newline here", "author": "mwalker174", "createdAt": "2020-11-19T18:53:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjMyMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122320", "bodyText": "Missing indent", "author": "mwalker174", "createdAt": "2020-11-19T18:53:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzEwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123107", "bodyText": "Should make 0.8 a tool parameter", "author": "mwalker174", "createdAt": "2020-11-19T18:55:14Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzk4Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123983", "bodyText": "callIntervals is null by default already, can reduce code here:\nif (modelCallIntervalList != null) {\n    final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n    final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n    callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n}", "author": "mwalker174", "createdAt": "2020-11-19T18:56:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNjc0OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527126748", "bodyText": "There is probably something I don't understand here, but why not just final VCFHeader vcfHeader = new VCFHeader(headerLines, samples);?", "author": "mwalker174", "createdAt": "2020-11-19T19:01:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3ODE2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535378166", "bodyText": "I copied this from a tool that copied this from a tool that copied this from CombineGVCFs where it has @jamesemery 's name on it.  It looks like these hijinx are to prevent the writer's samples from being modified?", "author": "ldgauthier", "createdAt": "2020-12-03T16:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNjc0OA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNzU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527127568", "bodyText": "Can make all parameters final", "author": "mwalker174", "createdAt": "2020-11-19T19:02:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyODIwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527128207", "bodyText": "This is very neat", "author": "mwalker174", "createdAt": "2020-11-19T19:03:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4Nzg1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534487859", "bodyText": "I believe that was James's invention.  Very handy.", "author": "ldgauthier", "createdAt": "2020-12-02T21:18:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyODIwNw=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyOTQxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527129418", "bodyText": "Should add some minimal documentation here", "author": "mwalker174", "createdAt": "2020-11-19T19:05:41Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNTE0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r538805145", "bodyText": "done", "author": "ldgauthier", "createdAt": "2020-12-08T21:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyOTQxOA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMDc1Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527130753", "bodyText": "vc.getNSamples() would be better", "author": "mwalker174", "createdAt": "2020-11-19T19:08:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMjE4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527132181", "bodyText": "Is this behavior documented somewhere?", "author": "mwalker174", "createdAt": "2020-11-19T19:10:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzNTQ3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527135470", "bodyText": "This variable was a little confusing at first. I think this should be called isMultiSampleInput and assigned using the header in onTraversalStart, rather than checking the number of samples for every variant.", "author": "mwalker174", "createdAt": "2020-11-19T19:16:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MTc5Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535381797", "bodyText": "That would be too easy, though, wouldn't it?\nOn the plus side, I got to use the \"Invert Boolean\" refactor for the first time!", "author": "ldgauthier", "createdAt": "2020-12-03T16:23:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzNTQ3MA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0Mzc4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527143786", "bodyText": "Someday we will be able to just check the alt alleles.", "author": "mwalker174", "createdAt": "2020-11-19T19:29:27Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality\n+                    || isNullCall(g)) {\n+                return null;\n+            }\n+        }\n+\n+\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        boolean isDel = false;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 71092b52d..7bb355fbf 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -70,7 +70,7 @@ public class SVCallRecord implements Feature {\n      *\n      * @param variant single-sample variant from a gCNV segments VCF\n      * @param minQuality drop events with quality lower than this\n-     * @return\n+     * @return a new SVCallRecord, may be null\n      */\n     public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n         Utils.nonNull(variant);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NzMwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527147308", "bodyText": "Should make this a class variable and initialize this in onTraversalStart()", "author": "mwalker174", "createdAt": "2020-11-19T19:35:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODE2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527148168", "bodyText": "Can be private", "author": "mwalker174", "createdAt": "2020-11-19T19:36:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MDIxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527150213", "bodyText": "Since this is a high-use function, we should assign this list its final size here", "author": "mwalker174", "createdAt": "2020-11-19T19:40:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527151744", "bodyText": "Avoid running this twice and make it a variable, boolean isCnv = call.getType().equals(StructuralVariantType.CNV). This will also help initializing the alleles list size.", "author": "mwalker174", "createdAt": "2020-11-19T19:42:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MzE2MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535383161", "bodyText": "Will do, but is the compiler not smart enough to do this internally?", "author": "ldgauthier", "createdAt": "2020-12-03T16:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NTQzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r543545432", "bodyText": "Possibly, but I think it helps with readability too.", "author": "mwalker174", "createdAt": "2020-12-15T17:33:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MjQ3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527152471", "bodyText": "We've moved away from MCNV in gatk-sv, is there a specific reason here? This should probably be defined as a static String somewhere.", "author": "mwalker174", "createdAt": "2020-11-19T19:44:06Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5Mzk1MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534493950", "bodyText": "This is for compatibility with the svtk annotate tool.  I will add a comment to that extent.", "author": "ldgauthier", "createdAt": "2020-12-02T21:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MjQ3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MzA4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527153086", "bodyText": "final and initial size g.getAlleles().size()", "author": "mwalker174", "createdAt": "2020-11-19T19:45:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();\n+        for (final Genotype g : call.getGenotypes()) {\n+            final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(g);\n+            //update reference alleles\n+            List<Allele> newGenotypeAlleles = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1OTg0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527159846", "bodyText": "Initial size call.getGenotypes().size()", "author": "mwalker174", "createdAt": "2020-11-19T19:56:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2MjY3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527162673", "bodyText": "final", "author": "mwalker174", "createdAt": "2020-11-19T20:01:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NTYxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527165611", "bodyText": "Suggestion: resolvedVCs.forEach(vcfWriter::add);", "author": "mwalker174", "createdAt": "2020-11-19T20:07:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NzgwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527167800", "bodyText": "I think you can simplify here by just having this:\nint clusterEnd = 0;\nString clusterContig = null;\n\nand then maybe have a check at the beginning of resolveVariantContexts() for empty input.", "author": "mwalker174", "createdAt": "2020-11-19T20:11:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2OTQyMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527169421", "bodyText": "Give initial size", "author": "mwalker174", "createdAt": "2020-11-19T20:14:21Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MTQwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527171409", "bodyText": "Avoid parsing g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) twice by assigning to a variable, and make sure it exists first.", "author": "mwalker174", "createdAt": "2020-11-19T20:17:49Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MzI4Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527173287", "bodyText": "Can this be combined with the previous loop to be more efficient?", "author": "mwalker174", "createdAt": "2020-11-19T20:21:08Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174408", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "author": "mwalker174", "createdAt": "2020-11-19T20:23:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTEzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527205138", "bodyText": "Can you wrap this into a nested class? The getLeft()'s and getRights()'s can be hard to follow", "author": "mwalker174", "createdAt": "2020-11-19T21:18:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQ3Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174477", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "author": "mwalker174", "createdAt": "2020-11-19T20:23:21Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NTU5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527175590", "bodyText": "When is this the case?", "author": "mwalker174", "createdAt": "2020-11-19T20:25:24Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5NjY4Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534496682", "bodyText": "As noted in the javadoc, the VC just has variant genotypes.  I added some clarification.", "author": "ldgauthier", "createdAt": "2020-12-02T21:35:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NTU5MA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDc3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184779", "bodyText": "Should make this into a Set", "author": "mwalker174", "createdAt": "2020-11-19T20:41:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDkzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184938", "bodyText": "Extra newline", "author": "mwalker174", "createdAt": "2020-11-19T20:41:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTUwMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527185501", "bodyText": "The if clause is redundant, and can combine with previous line -> } else {", "author": "mwalker174", "createdAt": "2020-11-19T20:43:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NzQ2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527187460", "bodyText": "Should this be Samples missing from pedigree and without VCF genotypes assumed to have ploidy 1 on allosomes. ?", "author": "mwalker174", "createdAt": "2020-11-19T20:46:36Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYxNzM3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535617370", "bodyText": "I went through a couple different iterations of trying to avoid the ped file, which is now required.  The better solution is just to require all samples to be in the pedigree and check on startup, which I will do.  I don't love that you need a pedigree even if you're just calling over the autosomes, though.  Although maybe we should just go with @samuelklee's philosophy that these inputs are easy if the tool is run in the WDL.", "author": "ldgauthier", "createdAt": "2020-12-03T21:05:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NzQ2MA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4ODY2Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527188667", "bodyText": "I think you could eliminate the samplePloidy variable and just have return statements wherever it is assigned and at the default-case warning message.", "author": "mwalker174", "createdAt": "2020-11-19T20:49:01Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527189964", "bodyText": "It seems confusing to allow users to specify a list of allosomal contigs but hard-code X and Y. Could we instead have separate \"X\" and \"Y\" tool input lists and use them here?", "author": "mwalker174", "createdAt": "2020-11-19T20:51:17Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2NzQyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535667428", "bodyText": "I just came up out of a deep Wikipedia rabbit hole about Bryophytes and platyfish, but honestly the only non-human species I can imagine this being run on are canine and mouse, which both have XX/XY sex determination.  The issue at hand is that we can't trust the old PostprocessGermlineCNVCalls genotypes because they were all haploid.  The new ones are fixed, so they shouldn't need a pedigree file, but I was still working with old data.  Maybe this is a lot of effort for backward compatibility.  Another option is to output a schema version in the Postprocess VCF that we can parse here and then trust those genotype ploidies if it's the new version.", "author": "ldgauthier", "createdAt": "2020-12-03T21:54:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNDUyNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r538804524", "bodyText": "I took out the allosome arg, so the tool only supports XX/XY, and I also put the schema version into the segments VCF header.  The intervals VCFs haven't been updated, but when they are I'll change the header there too.", "author": "ldgauthier", "createdAt": "2020-12-08T21:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5MjU1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527192554", "bodyText": "Suggest rewriting as: final int copyNumber = g != null && g.hasAnyAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()) : samplePloidy;", "author": "mwalker174", "createdAt": "2020-11-19T20:55:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYwMjUyNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535602524", "bodyText": "You and David Benjamin can be on Team Totally Ternary, but if its multiple lines I prefer regular ifs.", "author": "ldgauthier", "createdAt": "2020-12-03T20:55:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5MjU1NA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527194941", "bodyText": "Can you add a doc section for this?", "author": "mwalker174", "createdAt": "2020-11-19T21:00:14Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NjY3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527196670", "bodyText": "Also maybe rename to make it clear that this is for DEL/DUP/CNV only", "author": "mwalker174", "createdAt": "2020-11-19T21:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\nindex ce5ff3ec7..c528bc51a 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n\n@@ -5,10 +5,20 @@ import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n \n public class GATKSVVariantContextUtils {\n-    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+\n+    /**\n+     * Build the list of called alleles based on reference and called copy numbers\n+     * For CNVs only, i.e. will only assign reference, DEL, and DUP alleles\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele reference allele representation for the position of interest\n+     * @return a list of alleles appropriate to pass to a GenotypeBuilder\n+     */\n+    public static List<Allele> makeGenotypeAllelesFromCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n         final List<Allele> returnAlleles = new ArrayList<>();\n         final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n         //some allosomes like Y can have ref copy number zero, in which case we just no-call\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NTYzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527195634", "bodyText": "Can avoid allele variable by replacing assignments with return statements", "author": "mwalker174", "createdAt": "2020-11-19T21:01:32Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+            return GATKVariantContextUtils.noCallAlleles(refCopyNumber);\n+        //for homDels, hetDels or homRefs\n+        } else if (refCopyNumber == 2) {\n+            if (copyNumberCall == 0) {\n+                returnAlleles.add(genotypeAllele);\n+            } else {\n+                returnAlleles.add(refAllele);\n+            }\n+            returnAlleles.add(genotypeAllele);\n+            return returnAlleles;\n+        //multiploid dels\n+        } else {\n+            for (int i = 0; i < copyNumberCall; i++) {\n+                returnAlleles.add(refAllele);\n+            }\n+            for (int i = copyNumberCall; i < refCopyNumber; i++) {\n+                returnAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            }\n+            return returnAlleles;\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele\n+     * @return variant allele if copyNumberCall != refCopyNumber, else refAllele\n+     */\n+    public static Allele getAlleleForCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final Allele allele;\n+        if (copyNumberCall > refCopyNumber) {\n+            allele = GATKSVVCFConstants.DUP_ALLELE;\n+        } else if (copyNumberCall < refCopyNumber) {\n+            allele = GATKSVVCFConstants.DEL_ALLELE;\n+        } else {\n+            allele = refAllele;\n+        }\n+        return allele;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\nindex ce5ff3ec7..c528bc51a 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n\n@@ -5,10 +5,20 @@ import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n \n public class GATKSVVariantContextUtils {\n-    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+\n+    /**\n+     * Build the list of called alleles based on reference and called copy numbers\n+     * For CNVs only, i.e. will only assign reference, DEL, and DUP alleles\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele reference allele representation for the position of interest\n+     * @return a list of alleles appropriate to pass to a GenotypeBuilder\n+     */\n+    public static List<Allele> makeGenotypeAllelesFromCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n         final List<Allele> returnAlleles = new ArrayList<>();\n         final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n         //some allosomes like Y can have ref copy number zero, in which case we just no-call\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NzQzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197432", "bodyText": "return Collections.singletonList(genotypeAllele);", "author": "mwalker174", "createdAt": "2020-11-19T21:04:50Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\nindex ce5ff3ec7..c528bc51a 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n\n@@ -5,10 +5,20 @@ import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n \n public class GATKSVVariantContextUtils {\n-    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+\n+    /**\n+     * Build the list of called alleles based on reference and called copy numbers\n+     * For CNVs only, i.e. will only assign reference, DEL, and DUP alleles\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele reference allele representation for the position of interest\n+     * @return a list of alleles appropriate to pass to a GenotypeBuilder\n+     */\n+    public static List<Allele> makeGenotypeAllelesFromCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n         final List<Allele> returnAlleles = new ArrayList<>();\n         final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n         //some allosomes like Y can have ref copy number zero, in which case we just no-call\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5Nzk0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197945", "bodyText": "All of these else's are unnecessary (only need if)", "author": "mwalker174", "createdAt": "2020-11-19T21:05:41Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\nindex ce5ff3ec7..c528bc51a 100644\n--- a/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n+++ b/src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java\n\n@@ -5,10 +5,20 @@ import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n \n public class GATKSVVariantContextUtils {\n-    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+\n+    /**\n+     * Build the list of called alleles based on reference and called copy numbers\n+     * For CNVs only, i.e. will only assign reference, DEL, and DUP alleles\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele reference allele representation for the position of interest\n+     * @return a list of alleles appropriate to pass to a GenotypeBuilder\n+     */\n+    public static List<Allele> makeGenotypeAllelesFromCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n         final List<Allele> returnAlleles = new ArrayList<>();\n         final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n         //some allosomes like Y can have ref copy number zero, in which case we just no-call\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527201334", "bodyText": "Could avoid potentially computing vc.hasGenotype(sample) twice using nested if's", "author": "mwalker174", "createdAt": "2020-11-19T21:11:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203489", "bodyText": "(copyNumber > samplePloidy) -> copyNumber > samplePloidy", "author": "mwalker174", "createdAt": "2020-11-19T21:15:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE1Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203152", "bodyText": "temp -> count (and below)", "author": "mwalker174", "createdAt": "2020-11-19T21:15:11Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNjkyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527206922", "bodyText": "Seems like this should be built into the initialization of copyNumber above. It's a bit confusing this way.", "author": "mwalker174", "createdAt": "2020-11-19T21:21:58Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNDU1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527214559", "bodyText": "I think this could be consolidated into a one-liner: final long AC = alleleCountMap.get(vc.getAlternateAllele(0));", "author": "mwalker174", "createdAt": "2020-11-19T21:35:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNTI0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527215241", "bodyText": "Give initial sizes", "author": "mwalker174", "createdAt": "2020-11-19T21:35:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUwNjMxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534506312", "bodyText": "I promise I'll do better next time:", "author": "ldgauthier", "createdAt": "2020-12-02T21:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNTI0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNzE2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527217160", "bodyText": "When would we expect a non-ref allele to not be in the count map? We should probably throw an error", "author": "mwalker174", "createdAt": "2020-11-19T21:38:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyMTA5OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527221099", "bodyText": "What happened here? Are the proper alleles set elsewhere?", "author": "mwalker174", "createdAt": "2020-11-19T21:42:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUxMzExOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534513118", "bodyText": "buildVariantContext takes care of this now.  I'll remove the comments.", "author": "ldgauthier", "createdAt": "2020-12-02T22:05:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyMTA5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233271", "bodyText": "Mention this can be generated with the JointGermlineCNVSegmentation tool", "author": "mwalker174", "createdAt": "2020-11-19T22:05:11Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;\n+\n+    @Argument(\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzc0Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233747", "bodyText": "Also the functionality you've added should be added to the tool documentation", "author": "mwalker174", "createdAt": "2020-11-19T22:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\nindex 78eb971be..82ea60a26 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n\n@@ -187,7 +213,7 @@ public final class PostprocessGermlineCNVCalls extends GATKTool {\n     private File combinedIntervalsVCFFile = null;\n \n     @Argument(\n-            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples, can be generated with GATK JointGermlineCNVSegmentation tool\",\n             fullName = CLUSTERED_FILE_LONG_NAME,\n             optional = true\n     )\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MTg1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527241854", "bodyText": "What differentiates GATKSVVCFConstants.COPY_NUMBER_FORMAT and GermlineCNVSegmentVariantComposer.CN? Maybe we should only have one", "author": "mwalker174", "createdAt": "2020-11-19T22:18:55Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5MzU3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535393578", "bodyText": "Will do. There needs to be a serious constant roundup and refactor in the GCNV tools, but that's a task for another day.", "author": "ldgauthier", "createdAt": "2020-12-03T16:38:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MTg1NA=="}], "type": "inlineReview", "revised_code": {"commit": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 71092b52d..7bb355fbf 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -70,7 +70,7 @@ public class SVCallRecord implements Feature {\n      *\n      * @param variant single-sample variant from a gCNV segments VCF\n      * @param minQuality drop events with quality lower than this\n-     * @return\n+     * @return a new SVCallRecord, may be null\n      */\n     public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n         Utils.nonNull(variant);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MzI3NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527243275", "bodyText": "Switch to using the VariantContextGetters utils throughout for getting genotype and variant attributes.", "author": "mwalker174", "createdAt": "2020-11-19T22:21:45Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\nindex 71092b52d..7bb355fbf 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java\n\n@@ -70,7 +70,7 @@ public class SVCallRecord implements Feature {\n      *\n      * @param variant single-sample variant from a gCNV segments VCF\n      * @param minQuality drop events with quality lower than this\n-     * @return\n+     * @return a new SVCallRecord, may be null\n      */\n     public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n         Utils.nonNull(variant);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0Njg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527246881", "bodyText": "It seems like setRight() and setLeft() are switched with respect to the use of getLeft() and getRight():\ngenotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n\nif (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {", "author": "mwalker174", "createdAt": "2020-11-19T22:29:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NDA3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534484074", "bodyText": "You're totally right, but that map doesn't get used until these values are overwritten by some almost redundant but correct code.  I took this block out.", "author": "ldgauthier", "createdAt": "2020-12-02T21:11:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0Njg4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\nindex 47abbd39d..d69f5fc5b 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java\n\n@@ -1,5 +1,6 @@\n package org.broadinstitute.hellbender.tools.walkers.sv;\n \n+import com.google.common.annotations.VisibleForTesting;\n import htsjdk.samtools.SAMSequenceDictionary;\n import htsjdk.samtools.reference.ReferenceSequenceFile;\n import htsjdk.variant.variantcontext.*;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1MjI3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527252278", "bodyText": "Suggested edit:\n        //if we supply a breakpoints file, then allow overlapping segments\n        final AbstractRecordCollection integerCopyNumberSegmentCollection;\n        final String sampleNameFromSegmentCollection;\n        if (clusteredBreakpointsVCFFile == null) {\n            final IntegerCopyNumberSegmentCollection collection = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        } else {\n            final OverlappingIntegerCopyNumberSegmentCollection collection = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        }\n        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n                String.format(\"Sample name found in the header of copy-number segments file is \" +\n                                \"different from the expected sample name (found: %s, expected: %s).\",\n                        sampleNameFromSegmentCollection, sampleName));\n...\n        germlineCNVSegmentVariantComposer.writeAll(integerCopyNumberSegmentCollection.getRecords());", "author": "mwalker174", "createdAt": "2020-11-19T22:40:43Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -387,36 +442,56 @@ private void generateIntervalsVCFFileFromAllShards() {\n     }\n \n     private void generateSegmentsVCFFileFromAllShards() {\n-        logger.info(\"Generating segments VCF file...\");\n+        logger.info(\"Generating segments...\");\n \n         /* perform segmentation */\n         final File pythonScriptOutputPath = IOUtils.createTempDir(\"gcnv-segmented-calls\");\n         final boolean pythonScriptSucceeded = executeSegmentGermlineCNVCallsPythonScript(\n                 sampleIndex, inputContigPloidyCallsPath, sortedCallsShardPaths, sortedModelShardPaths,\n-                pythonScriptOutputPath);\n+                combinedIntervalsVCFFile, clusteredBreakpointsVCFFile, pythonScriptOutputPath);\n         if (!pythonScriptSucceeded) {\n             throw new UserException(\"Python return code was non-zero.\");\n         }\n \n         /* parse segments */\n+        logger.info(\"Parsing Python output...\");\n         final File copyNumberSegmentsFile = getCopyNumberSegmentsFile(pythonScriptOutputPath, sampleIndex);\n-        final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection =\n-                new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n-        final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n-                .getMetadata().getSampleName();\n-        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n-                String.format(\"Sample name found in the header of copy-number segments file is \" +\n-                                \"different from the expected sample name (found: %s, expected: %s).\",\n-                        sampleNameFromSegmentCollection, sampleName));\n+\n+        final List<IntegerCopyNumberSegment> records;\n+        //if we supply a breakpoints file, then allow overlapping segments\n+        if (clusteredBreakpointsVCFFile == null) {\n+            final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",\n+                            sampleNameFromSegmentCollection, sampleName));\n+            records = integerCopyNumberSegmentCollection.getRecords();\n+        } else {\n+            final OverlappingIntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\nindex 78eb971be..82ea60a26 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n\n@@ -459,27 +485,26 @@ public final class PostprocessGermlineCNVCalls extends GATKTool {\n \n         final List<IntegerCopyNumberSegment> records;\n         //if we supply a breakpoints file, then allow overlapping segments\n+        final AbstractRecordCollection<SampleLocatableMetadata, IntegerCopyNumberSegment> integerCopyNumberSegmentCollection;\n+        final String sampleNameFromSegmentCollection;\n         if (clusteredBreakpointsVCFFile == null) {\n-            final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+            integerCopyNumberSegmentCollection\n                     = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n-            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+            sampleNameFromSegmentCollection= integerCopyNumberSegmentCollection\n                     .getMetadata().getSampleName();\n-            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n-                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n-                                    \"different from the expected sample name (found: %s, expected: %s).\",\n-                            sampleNameFromSegmentCollection, sampleName));\n-            records = integerCopyNumberSegmentCollection.getRecords();\n+\n         } else {\n-            final OverlappingIntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+            integerCopyNumberSegmentCollection\n                     = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n-            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+            sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n                     .getMetadata().getSampleName();\n-            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n-                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n-                                    \"different from the expected sample name (found: %s, expected: %s).\",\n-                            sampleNameFromSegmentCollection, sampleName));\n-            records = integerCopyNumberSegmentCollection.getRecords();\n         }\n+        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                \"different from the expected sample name (found: %s, expected: %s).\",\n+                        sampleNameFromSegmentCollection, sampleName));\n+        records = integerCopyNumberSegmentCollection.getRecords();\n+\n \n         /* write variants */\n         logger.info(String.format(\"Writing segments VCF file to %s...\", outputSegmentsVCFFile.getAbsolutePath()));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1NDU4OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527254588", "bodyText": "Can you break both lambdas out into their own functions?", "author": "mwalker174", "createdAt": "2020-11-19T22:45:37Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts\n                     dictionary.getSequences().forEach(\n-                        sourceSequence -> {\n-                            final String sourceSequenceName = sourceSequence.getSequenceName();\n-                            final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n-                            if (previousDataSource != null) {\n-                                final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n-                                final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n-                                validateSequenceDictionaryRecords(\n-                                        ds.getName(), dictionary, sourceSequence,\n-                                        previousDataSource.getName(), previousDictionary, previousSequence);\n-                            } else {\n-                                contigMap.put(sourceSequenceName, ds);\n+                            sourceSequence -> {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\nindex eb4f5b2f0..efbb77ce2 100644\n--- a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n+++ b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n\n@@ -283,35 +283,44 @@ public final class MultiVariantDataSource implements GATKDataSource<VariantConte\n     private void validateAllSequenceDictionaries() {\n         final Map<String, FeatureDataSource<VariantContext>> contigMap = new HashMap<>();\n         featureDataSources.forEach(\n-            ds -> {\n-                final SAMSequenceDictionary dictionary = ds.getSequenceDictionary();\n-                if (dictionary == null) {\n-                    logger.warn(\n-                            \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                                    \" not be obtained for feature input: \" + ds.getName() +\n-                                    \". The input may not exist or may not have a valid header\");\n-                } else {\n-                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts\n-                    dictionary.getSequences().forEach(\n-                            sourceSequence -> {\n-                                final String sourceSequenceName = sourceSequence.getSequenceName();\n-                                final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n-                                if (previousDataSource != null) {\n-                                    final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n-                                    final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n-                                    validateSequenceDictionaryRecords(\n-                                            ds.getName(), dictionary, sourceSequence,\n-                                            previousDataSource.getName(), previousDictionary, previousSequence);\n-                                } else {\n-                                    contigMap.put(sourceSequenceName, ds);\n-                                }\n-                            }\n-                    );\n-                }\n-            }\n+            ds -> getDataSourceDictionaryAndValidate(ds, contigMap)\n         );\n     }\n \n+    private void getDataSourceDictionaryAndValidate(final FeatureDataSource<VariantContext> ds,\n+                                                    final Map<String, FeatureDataSource<VariantContext>> contigMap) {\n+        final SAMSequenceDictionary dictionary = ds.getSequenceDictionary();\n+        if (dictionary == null) {\n+            logger.warn(\n+                    \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n+                            \" not be obtained for feature input: \" + ds.getName() +\n+                            \". The input may not exist or may not have a valid header\");\n+        } else {\n+            //This is HORRIFICALLY inefficient -- for tools with many inputs instead skip cross-validation by\n+            // overloading doDictionaryCrossValidation as false and require a reference\n+            dictionary.getSequences().forEach(\n+                    sourceSequence -> validateContigAgainstPreviousDataSource(sourceSequence, dictionary, contigMap, ds)\n+            );\n+        }\n+    }\n+\n+    private void validateContigAgainstPreviousDataSource(final SAMSequenceRecord sourceSequence,\n+                  final SAMSequenceDictionary dictionary,\n+                  final Map<String, FeatureDataSource<VariantContext>> contigMap,\n+                  final FeatureDataSource<VariantContext> ds){\n+        final String sourceSequenceName = sourceSequence.getSequenceName();\n+        final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n+        if (previousDataSource != null) {\n+            final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n+            final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n+            validateSequenceDictionaryRecords(\n+                    ds.getName(), dictionary, sourceSequence,\n+                    previousDataSource.getName(), previousDictionary, previousSequence);\n+        } else {\n+            contigMap.put(sourceSequenceName, ds);\n+        }\n+    }\n+\n     // Cross validate the length and md5 for a pair of sequence records.\n     private void validateSequenceDictionaryRecords(\n             final String sourceDataSourceName,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527260392", "bodyText": "It does seem complicated. Do you know how it affects runtime now?", "author": "mwalker174", "createdAt": "2020-11-19T22:58:14Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg3MTA4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532871084", "bodyText": "200 samples takes like 20 minutes to validate the sequence dictionaries.  If we require a reference and skip cross-validation (i.e. just validate each against the reference), then it's negligible.", "author": "ldgauthier", "createdAt": "2020-11-30T20:05:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxNTA0Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534415043", "bodyText": "Yikes! This is okay as long as the ref is provided in our WDLs.", "author": "mwalker174", "createdAt": "2020-12-02T19:09:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\nindex eb4f5b2f0..efbb77ce2 100644\n--- a/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n+++ b/src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java\n\n@@ -283,35 +283,44 @@ public final class MultiVariantDataSource implements GATKDataSource<VariantConte\n     private void validateAllSequenceDictionaries() {\n         final Map<String, FeatureDataSource<VariantContext>> contigMap = new HashMap<>();\n         featureDataSources.forEach(\n-            ds -> {\n-                final SAMSequenceDictionary dictionary = ds.getSequenceDictionary();\n-                if (dictionary == null) {\n-                    logger.warn(\n-                            \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                                    \" not be obtained for feature input: \" + ds.getName() +\n-                                    \". The input may not exist or may not have a valid header\");\n-                } else {\n-                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts\n-                    dictionary.getSequences().forEach(\n-                            sourceSequence -> {\n-                                final String sourceSequenceName = sourceSequence.getSequenceName();\n-                                final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n-                                if (previousDataSource != null) {\n-                                    final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n-                                    final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n-                                    validateSequenceDictionaryRecords(\n-                                            ds.getName(), dictionary, sourceSequence,\n-                                            previousDataSource.getName(), previousDictionary, previousSequence);\n-                                } else {\n-                                    contigMap.put(sourceSequenceName, ds);\n-                                }\n-                            }\n-                    );\n-                }\n-            }\n+            ds -> getDataSourceDictionaryAndValidate(ds, contigMap)\n         );\n     }\n \n+    private void getDataSourceDictionaryAndValidate(final FeatureDataSource<VariantContext> ds,\n+                                                    final Map<String, FeatureDataSource<VariantContext>> contigMap) {\n+        final SAMSequenceDictionary dictionary = ds.getSequenceDictionary();\n+        if (dictionary == null) {\n+            logger.warn(\n+                    \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n+                            \" not be obtained for feature input: \" + ds.getName() +\n+                            \". The input may not exist or may not have a valid header\");\n+        } else {\n+            //This is HORRIFICALLY inefficient -- for tools with many inputs instead skip cross-validation by\n+            // overloading doDictionaryCrossValidation as false and require a reference\n+            dictionary.getSequences().forEach(\n+                    sourceSequence -> validateContigAgainstPreviousDataSource(sourceSequence, dictionary, contigMap, ds)\n+            );\n+        }\n+    }\n+\n+    private void validateContigAgainstPreviousDataSource(final SAMSequenceRecord sourceSequence,\n+                  final SAMSequenceDictionary dictionary,\n+                  final Map<String, FeatureDataSource<VariantContext>> contigMap,\n+                  final FeatureDataSource<VariantContext> ds){\n+        final String sourceSequenceName = sourceSequence.getSequenceName();\n+        final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n+        if (previousDataSource != null) {\n+            final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n+            final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n+            validateSequenceDictionaryRecords(\n+                    ds.getName(), dictionary, sourceSequence,\n+                    previousDataSource.getName(), previousDictionary, previousSequence);\n+        } else {\n+            contigMap.put(sourceSequenceName, ds);\n+        }\n+    }\n+\n     // Cross validate the length and md5 for a pair of sequence records.\n     private void validateSequenceDictionaryRecords(\n             final String sourceDataSourceName,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MzkwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527263903", "bodyText": "File inputs should be GATKPaths (also in JointGermlineCNVSegmentation)", "author": "mwalker174", "createdAt": "2020-11-19T23:05:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU5MDQ2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535590468", "bodyText": "Since this is going to go to the underlying Python script as-is, I think it has to stay a File right?  I suspect it will crash and burn if it gets a GCS path.", "author": "ldgauthier", "createdAt": "2020-12-03T20:45:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MzkwMw=="}], "type": "inlineReview", "revised_code": {"commit": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "chunk": "diff --git a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\nindex 78eb971be..82ea60a26 100644\n--- a/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n+++ b/src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java\n\n@@ -187,7 +213,7 @@ public final class PostprocessGermlineCNVCalls extends GATKTool {\n     private File combinedIntervalsVCFFile = null;\n \n     @Argument(\n-            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples, can be generated with GATK JointGermlineCNVSegmentation tool\",\n             fullName = CLUSTERED_FILE_LONG_NAME,\n             optional = true\n     )\n"}}, {"oid": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "url": "https://github.com/broadinstitute/gatk/commit/61759e542a67b74ae02f728f6ca1cbac3ba0a857", "message": "More integration tests and other review responses", "committedDate": "2020-12-08T21:13:29Z", "type": "forcePushed"}, {"oid": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "url": "https://github.com/broadinstitute/gatk/commit/c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling\ngCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up\nNew Python unit test runner", "committedDate": "2020-12-22T18:58:48Z", "type": "forcePushed"}, {"oid": "b7175045562c85450dedbc8ecb909edf4bb1b4c6", "url": "https://github.com/broadinstitute/gatk/commit/b7175045562c85450dedbc8ecb909edf4bb1b4c6", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling", "committedDate": "2020-12-22T20:54:33Z", "type": "commit"}, {"oid": "c41239987b118546daa78533256216fcee3378c0", "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner", "committedDate": "2020-12-22T20:54:36Z", "type": "commit"}, {"oid": "c41239987b118546daa78533256216fcee3378c0", "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner", "committedDate": "2020-12-22T20:54:36Z", "type": "forcePushed"}]}