{"pr_number": 1798, "pr_title": "HIVE-22944: Upgrade to Kryo5", "pr_createdAt": "2020-12-21T17:10:55Z", "pr_url": "https://github.com/apache/hive/pull/1798", "timeline": [{"oid": "6567cef52592faecaa99055b40a5549376d95f23", "url": "https://github.com/apache/hive/commit/6567cef52592faecaa99055b40a5549376d95f23", "message": "HIVE-22944: Upgrade to Kryo5", "committedDate": "2020-12-21T18:50:08Z", "type": "forcePushed"}, {"oid": "aacc386571d9a573cb1a3d67ba3783e3783f6e5a", "url": "https://github.com/apache/hive/commit/aacc386571d9a573cb1a3d67ba3783e3783f6e5a", "message": "HIVE-22944: Upgrade to Kryo5 - benchmark", "committedDate": "2020-12-27T16:12:23Z", "type": "forcePushed"}, {"oid": "f569fd31f5ad25998258ed3f6c477dc47adcb109", "url": "https://github.com/apache/hive/commit/f569fd31f5ad25998258ed3f6c477dc47adcb109", "message": "HIVE-22944: Upgrade to Kryo5 - benchmark", "committedDate": "2020-12-28T09:17:54Z", "type": "forcePushed"}, {"oid": "d0452ecefbd73f88cea61f61deb47b573a671079", "url": "https://github.com/apache/hive/commit/d0452ecefbd73f88cea61f61deb47b573a671079", "message": "HIVE-22944: Upgrade to Kryo5 - benchmark", "committedDate": "2021-01-07T14:40:54Z", "type": "forcePushed"}, {"oid": "0cebb875cdf3f830425a7c1c4c62f81fbfc07408", "url": "https://github.com/apache/hive/commit/0cebb875cdf3f830425a7c1c4c62f81fbfc07408", "message": "@Ignore", "committedDate": "2021-01-08T06:42:55Z", "type": "forcePushed"}, {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec", "url": "https://github.com/apache/hive/commit/a51fee9c4b5f8137ad83949d54858240dab221ec", "message": "@Ignore", "committedDate": "2021-01-14T08:37:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3Mjk0Mg==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562072942", "bodyText": "nit: maybe move Sarg  Expr creation to separate method for consistency?", "author": "pgaref", "createdAt": "2021-01-21T17:39:54Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestConvertAstToSearchArg.java", "diffHunk": "@@ -2855,16 +2836,26 @@ public void TestBigintSarg() throws Exception {\n   }\n \n   @Test\n-  public void TestBooleanSarg() throws Exception {\n-    String serialAst =\n-        \"AQEAamF2YS51dGlsLkFycmF5TGlz9AECAQFvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnFsLnBsYW4uRXh\" +\n-            \"wck5vZGVHZW5lcmljRnVuY0Rlc+MBAQABAgECb3JnLmFwYWNoZS5oYWRvb3AuaGl2ZS5xbC5wbGFuLk\" +\n-            \"V4cHJOb2RlQ29sdW1uRGVz4wEBYrEAAAFib29sb3LjAQNvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnNlc\" +\n-            \"mRlMi50eXBlaW5mby5QcmltaXRpdmVUeXBlSW5m7wEBYm9vbGVh7gEEb3JnLmFwYWNoZS5oYWRvb3Au\" +\n-            \"aGl2ZS5xbC5wbGFuLkV4cHJOb2RlQ29uc3RhbnREZXPjAQEDCQUBAQVvcmcuYXBhY2hlLmhhZG9vcC5\" +\n-            \"oaXZlLnFsLnVkZi5nZW5lcmljLkdlbmVyaWNVREZPUEVxdWHsAQAAAYI9AUVRVUHMAQZvcmcuYXBhY2\" +\n-            \"hlLmhhZG9vcC5pby5Cb29sZWFuV3JpdGFibOUBAAABAwkBAgEBYrIAAAgBAwkBB29yZy5hcGFjaGUua\" +\n-            \"GFkb29wLmhpdmUucWwudWRmLmdlbmVyaWMuR2VuZXJpY1VERk9QQW7kAQEGAQAAAQMJ\";\n+  public void testBooleanSarg() throws Exception {\n+    ExprNodeDesc column1 =", "originalCommit": "a51fee9c4b5f8137ad83949d54858240dab221ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMzI4Mw==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562103283", "bodyText": "the reason I haven't do that is this case differs from others in the class as we have two columns, and this is the only case...however, I can do a utility method", "author": "abstractdog", "createdAt": "2021-01-21T18:25:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3Mjk0Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NDIwMQ==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562074201", "bodyText": "Does this mean this is a breaking change for Spark?", "author": "pgaref", "createdAt": "2021-01-21T17:41:45Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java", "diffHunk": "@@ -30,11 +30,13 @@\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n import org.junit.Assert;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import java.io.File;\n import java.io.IOException;\n \n+@Ignore(\"HIVE-22944: Kryo 5 upgrade conflicts with Spark, which is not supported anymore\")", "originalCommit": "a51fee9c4b5f8137ad83949d54858240dab221ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA5NTk5NA==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562095994", "bodyText": "yes, it is\nmaybe I could hack with that, but it would be painful: using kryo5 for tez execution and kryo4 for spark execution", "author": "abstractdog", "createdAt": "2021-01-21T18:14:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NDIwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "73d60e09bb97d7eac74afd5e528116962e4327c0", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java\nindex 8d7010dc96..08891218b9 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java\n\n@@ -30,13 +30,11 @@\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n import org.junit.Assert;\n-import org.junit.Ignore;\n import org.junit.Test;\n \n import java.io.File;\n import java.io.IOException;\n \n-@Ignore(\"HIVE-22944: Kryo 5 upgrade conflicts with Spark, which is not supported anymore\")\n public class TestSparkInvalidFileFormat {\n \n   @Test\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NTI1NQ==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562075255", "bodyText": "Maybe rename the method itself here as well?  something like obtainKryo() ?", "author": "pgaref", "createdAt": "2021-01-21T17:43:27Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -278,7 +284,7 @@ public Kryo create() {\n    * @return kryo instance\n    */\n   public static Kryo borrowKryo() {\n-    Kryo kryo = kryoPool.borrow();\n+    Kryo kryo = kryoPool.obtain();", "originalCommit": "a51fee9c4b5f8137ad83949d54858240dab221ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMDcxMg==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562100712", "bodyText": "it's very unfortunate that kryo changed this method name without any reasons (or just I don't understand that :) )...we might want a method name that reflects the behavior of \"getting a kryo instance from the pool\", I don't have a strong opinion about that, but I'm not sure if we need to change a public method name because kryo changed theirs", "author": "abstractdog", "createdAt": "2021-01-21T18:21:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NTI1NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562079574", "bodyText": "This is cool! Shall we add a more complex Map for bench here and a perf number for reference?\nMaybe previous kryo4 vs kryo5?", "author": "pgaref", "createdAt": "2021-01-21T17:49:52Z", "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.ql.exec;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n+import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n+import org.apache.hadoop.hive.ql.plan.MapWork;\n+import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import com.google.common.io.ByteStreams;\n+\n+@State(Scope.Benchmark)\n+public class KryoBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private MapWork mapWork;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testSerializeMapWork() {\n+      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {", "originalCommit": "a51fee9c4b5f8137ad83949d54858240dab221ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA5NzY0Ng==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562097646", "bodyText": "there is a benchmark result kryo4 vs kryo5: https://issues.apache.org/jira/secure/attachment/13018768/kryo4_vs_5_benchmark.log\nnot an over-complicated MapWork, but contains 1000 partitions, which is the most weight of mapworks (+ a few columns)", "author": "abstractdog", "createdAt": "2021-01-21T18:17:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEzMDA5Ng==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562130096", "bodyText": "Nice, I missed that -- the numbers are only for 5 though right?", "author": "pgaref", "createdAt": "2021-01-21T19:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzgzMDYxNA==", "url": "https://github.com/apache/hive/pull/1798#discussion_r563830614", "bodyText": "it's an edited file, scroll down in it for kryo4 results", "author": "abstractdog", "createdAt": "2021-01-25T15:51:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}], "type": "inlineReview", "revised_code": {"commit": "73d60e09bb97d7eac74afd5e528116962e4327c0", "chunk": "diff --git a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java b/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java\ndeleted file mode 100644\nindex 26f2d6bde5..0000000000\n--- a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java\n+++ /dev/null\n\n@@ -1,154 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hive.benchmark.ql.exec;\n-\n-import java.util.ArrayList;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.TimeUnit;\n-\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n-import org.apache.hadoop.hive.ql.exec.Utilities;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n-import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n-import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n-import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n-import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n-import org.apache.hadoop.hive.ql.plan.MapWork;\n-import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n-import org.apache.hadoop.hive.ql.plan.TableDesc;\n-import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n-import org.apache.hadoop.hive.serde.serdeConstants;\n-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n-import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n-import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.openjdk.jmh.annotations.Benchmark;\n-import org.openjdk.jmh.annotations.BenchmarkMode;\n-import org.openjdk.jmh.annotations.Fork;\n-import org.openjdk.jmh.annotations.Measurement;\n-import org.openjdk.jmh.annotations.Mode;\n-import org.openjdk.jmh.annotations.OutputTimeUnit;\n-import org.openjdk.jmh.annotations.Scope;\n-import org.openjdk.jmh.annotations.Setup;\n-import org.openjdk.jmh.annotations.State;\n-import org.openjdk.jmh.annotations.Warmup;\n-import org.openjdk.jmh.runner.Runner;\n-import org.openjdk.jmh.runner.RunnerException;\n-import org.openjdk.jmh.runner.options.Options;\n-import org.openjdk.jmh.runner.options.OptionsBuilder;\n-\n-import com.google.common.io.ByteStreams;\n-\n-@State(Scope.Benchmark)\n-public class KryoBench {\n-\n-  @BenchmarkMode(Mode.AverageTime)\n-  @Fork(1)\n-  @State(Scope.Thread)\n-  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n-  public static class BaseBench {\n-\n-    private MapWork mapWork;\n-\n-    @Setup\n-    public void setup() throws Exception {\n-      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n-    }\n-\n-    @Benchmark\n-    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n-    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n-    public void testSerializeMapWork() {\n-      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n-    }\n-  }\n-\n-  public static void main(String[] args) throws RunnerException {\n-    Options opt =\n-        new OptionsBuilder().include(\".*\" + KryoBench.class.getSimpleName() + \".*\").build();\n-    new Runner(opt).run();\n-  }\n-\n-  public static MapWork mockMapWork(String tableName, int partitions,\n-      ObjectInspector objectInspector) throws Exception {\n-    Path root = new Path(\"/warehouse\", tableName);\n-\n-    String[] partPath = new String[partitions];\n-    StringBuilder buffer = new StringBuilder();\n-    for (int p = 0; p < partitions; ++p) {\n-      partPath[p] = new Path(root, \"p=\" + p).toString();\n-      if (p != 0) {\n-        buffer.append(',');\n-      }\n-      buffer.append(partPath[p]);\n-    }\n-    StringBuilder columnIds = new StringBuilder();\n-    StringBuilder columnNames = new StringBuilder();\n-    StringBuilder columnTypes = new StringBuilder();\n-    StructObjectInspector structOI = (StructObjectInspector) objectInspector;\n-    List<? extends StructField> fields = structOI.getAllStructFieldRefs();\n-    int numCols = fields.size();\n-    for (int i = 0; i < numCols; ++i) {\n-      if (i != 0) {\n-        columnIds.append(',');\n-        columnNames.append(',');\n-        columnTypes.append(',');\n-      }\n-      columnIds.append(i);\n-      columnNames.append(fields.get(i).getFieldName());\n-      columnTypes.append(fields.get(i).getFieldObjectInspector().getTypeName());\n-    }\n-\n-    Properties tblProps = new Properties();\n-    tblProps.put(\"name\", tableName);\n-    tblProps.put(\"serialization.lib\", OrcSerde.class.getName());\n-    tblProps.put(\"columns\", columnNames.toString());\n-    tblProps.put(\"columns.types\", columnTypes.toString());\n-    TableDesc tbl = new TableDesc(OrcInputFormat.class, OrcOutputFormat.class, tblProps);\n-\n-    MapWork mapWork = new MapWork();\n-    mapWork.setVectorMode(true);\n-\n-    VectorizedRowBatchCtx vectorizedRowBatchCtx = new VectorizedRowBatchCtx();\n-    vectorizedRowBatchCtx.init(structOI, new String[0]);\n-    mapWork.setVectorizedRowBatchCtx(vectorizedRowBatchCtx);\n-\n-    mapWork.setUseBucketizedHiveInputFormat(false);\n-    Map<Path, List<String>> aliasMap = new LinkedHashMap<>();\n-    List<String> aliases = new ArrayList<String>();\n-    aliases.add(tableName);\n-    LinkedHashMap<Path, PartitionDesc> partMap = new LinkedHashMap<>();\n-    for (int p = 0; p < partitions; ++p) {\n-      Path path = new Path(partPath[p]);\n-      aliasMap.put(path, aliases);\n-      LinkedHashMap<String, String> partSpec = new LinkedHashMap<String, String>();\n-      PartitionDesc part = new PartitionDesc(tbl, partSpec);\n-      part.setVectorPartitionDesc(VectorPartitionDesc\n-          .createVectorizedInputFileFormat(\"MockInputFileFormatClassName\", false, null));\n-      partMap.put(path, part);\n-    }\n-    mapWork.setPathToAliases(aliasMap);\n-    mapWork.setPathToPartitionInfo(partMap);\n-\n-    return mapWork;\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562080218", "bodyText": "Maybe add a comment why the above 3 lines are needed?", "author": "pgaref", "createdAt": "2021-01-21T17:50:50Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -224,51 +224,57 @@ public Registration readClass(Input input) {\n \n   private static final Object FAKE_REFERENCE = new Object();\n \n-  private static KryoFactory factory = new KryoFactory() {\n-    @Override\n-    public Kryo create() {\n-      KryoWithHooks kryo = new KryoWithHooks();\n-      kryo.register(java.sql.Date.class, new SqlDateSerializer());\n-      kryo.register(java.sql.Timestamp.class, new TimestampSerializer());\n-      kryo.register(TimestampTZ.class, new TimestampTZSerializer());\n-      kryo.register(Path.class, new PathSerializer());\n-      kryo.register(Arrays.asList(\"\").getClass(), new ArraysAsListSerializer());\n-      kryo.register(new java.util.ArrayList().subList(0,0).getClass(), new ArrayListSubListSerializer());\n-      kryo.register(CopyOnFirstWriteProperties.class, new CopyOnFirstWritePropertiesSerializer());\n-      kryo.register(PartitionDesc.class, new PartitionDescSerializer(kryo, PartitionDesc.class));\n-\n-      ((Kryo.DefaultInstantiatorStrategy) kryo.getInstantiatorStrategy())\n-          .setFallbackInstantiatorStrategy(\n-              new StdInstantiatorStrategy());\n-      removeField(kryo, AbstractOperatorDesc.class, \"colExprMap\");\n-      removeField(kryo, AbstractOperatorDesc.class, \"statistics\");\n-      kryo.register(ReduceWork.class);\n-      kryo.register(TableDesc.class);\n-      kryo.register(UnionOperator.class);\n-      kryo.register(FileSinkOperator.class);\n-      kryo.register(VectorFileSinkOperator.class);\n-      kryo.register(HiveIgnoreKeyTextOutputFormat.class);\n-      kryo.register(StandardConstantListObjectInspector.class);\n-      kryo.register(StandardConstantMapObjectInspector.class);\n-      kryo.register(StandardConstantStructObjectInspector.class);\n-      kryo.register(SequenceFileInputFormat.class);\n-      kryo.register(RCFileInputFormat.class);\n-      kryo.register(HiveSequenceFileOutputFormat.class);\n-      kryo.register(LlapOutputFormat.class);\n-      kryo.register(SparkEdgeProperty.class);\n-      kryo.register(SparkWork.class);\n-      kryo.register(Pair.class);\n-      kryo.register(MemoryMonitorInfo.class);\n-\n-      // This must be called after all the explicit register calls.\n-      return kryo.processHooks(kryoTypeHooks, globalHook);\n-    }\n-  };\n-\n   // Bounded queue could be specified here but that will lead to blocking.\n   // ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under\n   // memory pressure.\n-  private static KryoPool kryoPool = new KryoPool.Builder(factory).softReferences().build();\n+  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, false, 8) {\n+    protected Kryo create() {\n+      return createNewKryo();\n+    }\n+  };\n+\n+  public static Kryo createNewKryo() {\n+    KryoWithHooks kryo = new KryoWithHooks();\n+\n+    kryo.setReferences(true);\n+    kryo.setCopyReferences(true);\n+    kryo.setRegistrationRequired(false);", "originalCommit": "a51fee9c4b5f8137ad83949d54858240dab221ec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMTQzNg==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562101436", "bodyText": "to be honest, because it works this way :) worst case I'll comment about what issues can come without these options set", "author": "abstractdog", "createdAt": "2021-01-21T18:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEzMDMwMw==", "url": "https://github.com/apache/hive/pull/1798#discussion_r562130303", "bodyText": "Sure, makes sense!", "author": "pgaref", "createdAt": "2021-01-21T19:09:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA=="}], "type": "inlineReview", "revised_code": {"commit": "2a1ecd8393228885b66ccd2cab802b2706d3fa22", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java\nindex fe84620438..d4e8407ca6 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java\n\n@@ -227,7 +213,7 @@ public Registration readClass(Input input) {\n   // Bounded queue could be specified here but that will lead to blocking.\n   // ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under\n   // memory pressure.\n-  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, false, 8) {\n+  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, true, 32) {\n     protected Kryo create() {\n       return createNewKryo();\n     }\n"}}, {"oid": "73d60e09bb97d7eac74afd5e528116962e4327c0", "url": "https://github.com/apache/hive/commit/73d60e09bb97d7eac74afd5e528116962e4327c0", "message": "HIVE-22944: Upgrade to Kryo5", "committedDate": "2021-01-25T15:49:05Z", "type": "commit"}, {"oid": "69392d8adcdb6c53df90180b1bf79ba1bec63b27", "url": "https://github.com/apache/hive/commit/69392d8adcdb6c53df90180b1bf79ba1bec63b27", "message": "HIVE-22944: Upgrade to Kryo5 - benchmark", "committedDate": "2021-01-25T15:49:05Z", "type": "commit"}, {"oid": "3d41336fe51f64d00ce1b3a59bb30c9c9bf7c8b7", "url": "https://github.com/apache/hive/commit/3d41336fe51f64d00ce1b3a59bb30c9c9bf7c8b7", "message": "@Ignore", "committedDate": "2021-01-25T15:49:05Z", "type": "commit"}, {"oid": "2a1ecd8393228885b66ccd2cab802b2706d3fa22", "url": "https://github.com/apache/hive/commit/2a1ecd8393228885b66ccd2cab802b2706d3fa22", "message": "some changes after PR comments", "committedDate": "2021-01-25T16:34:40Z", "type": "forcePushed"}, {"oid": "cf5921b071b23928d4df1075b4d01ee1aad04854", "url": "https://github.com/apache/hive/commit/cf5921b071b23928d4df1075b4d01ee1aad04854", "message": "some changes after PR comments", "committedDate": "2021-01-28T17:57:04Z", "type": "forcePushed"}, {"oid": "ec63776394ff2438e6d510566aa9820d1da7fa29", "url": "https://github.com/apache/hive/commit/ec63776394ff2438e6d510566aa9820d1da7fa29", "message": "some changes after PR comments", "committedDate": "2021-01-29T13:54:21Z", "type": "forcePushed"}, {"oid": "4accc489515366632ca84a77b3a99a99f2de19bb", "url": "https://github.com/apache/hive/commit/4accc489515366632ca84a77b3a99a99f2de19bb", "message": "some changes after PR comments", "committedDate": "2021-02-01T06:09:43Z", "type": "commit"}, {"oid": "4accc489515366632ca84a77b3a99a99f2de19bb", "url": "https://github.com/apache/hive/commit/4accc489515366632ca84a77b3a99a99f2de19bb", "message": "some changes after PR comments", "committedDate": "2021-02-01T06:09:43Z", "type": "forcePushed"}]}