{"pr_number": 1542, "pr_title": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "pr_createdAt": "2020-09-30T16:38:01Z", "pr_url": "https://github.com/apache/hive/pull/1542", "timeline": [{"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c", "url": "https://github.com/apache/hive/commit/adf8d6509cb9003bf08671ac95d5913e12afb80c", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-01T09:00:05Z", "type": "forcePushed"}, {"oid": "5addba4f2167c91874b835595a3aebcbe567b5f9", "url": "https://github.com/apache/hive/commit/5addba4f2167c91874b835595a3aebcbe567b5f9", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-14T12:34:09Z", "type": "forcePushed"}, {"oid": "1508f30aa7dc42372df67066a4dd464930cfe6b5", "url": "https://github.com/apache/hive/commit/1508f30aa7dc42372df67066a4dd464930cfe6b5", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-26T08:18:33Z", "type": "commit"}, {"oid": "b019108bc9efcc32c4a6d97eacca539af8eb22e1", "url": "https://github.com/apache/hive/commit/b019108bc9efcc32c4a6d97eacca539af8eb22e1", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-26T08:18:56Z", "type": "commit"}, {"oid": "0587963150a06530cd292e3556e1b3047ebdf3c3", "url": "https://github.com/apache/hive/commit/0587963150a06530cd292e3556e1b3047ebdf3c3", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-26T08:19:29Z", "type": "commit"}, {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "url": "https://github.com/apache/hive/commit/11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "message": " HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-26T08:30:35Z", "type": "commit"}, {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "url": "https://github.com/apache/hive/commit/11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "message": " HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-26T08:30:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4OTM0OQ==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511989349", "bodyText": "these lines start with * *", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:11:17Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MDQ1NA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512590454", "bodyText": "fixed", "author": "zeroflag", "createdAt": "2020-10-27T10:53:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4OTM0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "chunk": "diff --git a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\nindex 2e05709a43..abf7a92566 100644\n--- a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n+++ b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n\n@@ -1,21 +1,19 @@\n /*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  *\n- *  *     http://www.apache.org/licenses/LICENSE-2.0\n- *  *\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDA1NA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990054", "bodyText": "what does this returned boolean mean?", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:12:16Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODAyNA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058024", "bodyText": "true: function was found and executed, false if it was not found. The return value was already used and expected by the existing code so I kept it. There are some issues with how the existing implementation works, for example calling an undefined function evaluates to null, instead of throwing an error. We might want to revisit these things later, but this wasn't the scope of this patch.", "author": "zeroflag", "createdAt": "2020-10-26T15:38:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDA1NA=="}], "type": "inlineReview", "revised_code": {"commit": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "chunk": "diff --git a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\nindex 2e05709a43..abf7a92566 100644\n--- a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n+++ b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n\n@@ -1,21 +1,19 @@\n /*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  *\n- *  *     http://www.apache.org/licenses/LICENSE-2.0\n- *  *\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDgxMQ==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990811", "bodyText": "what does exists mean? an implementation of this interface is a Function - which should have a name ; so a getName would probably fit better", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:13:21Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);\n+  void addUserFunction(HplsqlParser.Create_function_stmtContext ctx);\n+  void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx);\n+  boolean exists(String name);", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODI5MQ==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058291", "bodyText": "The terminology came from the existing code, it doesn't represent a particular function but it was already called this way (it was a class not an interface but worked the same way). It's more like a registry for functions but I didn't want to change it as part of this patch. Fixing this right now is out of scope.", "author": "zeroflag", "createdAt": "2020-10-26T15:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDgxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "chunk": "diff --git a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\nindex 2e05709a43..abf7a92566 100644\n--- a/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n+++ b/hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java\n\n@@ -1,21 +1,19 @@\n /*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  *\n- *  *     http://www.apache.org/licenses/LICENSE-2.0\n- *  *\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NDI4Mg==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511994282", "bodyText": "do we really need to define these function differently than others; I've taken a look at MIN_PART_STRING and it seenms like its an ordinary function...so it could probably use the registry way approach", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:18:05Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/BuiltinFunctions.java", "diffHunk": "@@ -0,0 +1,435 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import java.sql.Date;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Calendar;\n+import java.util.HashMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Query;\n+import org.apache.hive.hplsql.Utils;\n+import org.apache.hive.hplsql.Var;\n+\n+public class BuiltinFunctions {", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODU3Ng==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058576", "bodyText": "This was just extracted out from the existing code, it's a not a new thing, I think this is out of scope now.", "author": "zeroflag", "createdAt": "2020-10-26T15:38:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NDI4Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzMzNg==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997336", "bodyText": "I think this class should implement the Function interface and not extends a class which has a name which suggest that it's a \"container of functions\"", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:22:06Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -27,7 +27,7 @@\n import org.apache.commons.lang3.StringUtils;\n import org.apache.hive.hplsql.*;\n \n-public class FunctionDatetime extends Function {\n+public class FunctionDatetime extends BuiltinFunctions {", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODg0Ng==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058846", "bodyText": "It used to extend from Function which was a class before. I didn't really change how it used to work, it's still using implementation inheritance, which I personally don't like but didn't want to change it as part of this patch. We might want to move builtin functions into the DB later on, making this class unnecessary in the future.", "author": "zeroflag", "createdAt": "2020-10-26T15:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzMzNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzY4Mg==", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997682", "bodyText": "note: registrator exposes it's internals", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:22:35Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -36,20 +36,20 @@ public FunctionDatetime(Exec e) {\n    * Register functions\n    */\n   @Override\n-  public void register(Function f) {\n-    f.map.put(\"DATE\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { date(ctx); }});\n-    f.map.put(\"FROM_UNIXTIME\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { fromUnixtime(ctx); }});\n-    f.map.put(\"NOW\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { now(ctx); }});\n-    f.map.put(\"TIMESTAMP_ISO\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { timestampIso(ctx); }});\n-    f.map.put(\"TO_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { toTimestamp(ctx); }});\n-    f.map.put(\"UNIX_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { unixTimestamp(ctx); }});\n+  public void register(BuiltinFunctions f) {\n+    f.map.put(\"DATE\", this::date);", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDM3Mg==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060372", "bodyText": "This is a general issue, many things are package private. We can refactor that little by little.\n.", "author": "zeroflag", "createdAt": "2020-10-26T15:40:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzY4Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwMzk4NA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512003984", "bodyText": "I think Function-s should be independent from where they are defined (builtin or hms) - and there should be some kind of registry which knows about all the functions.\nI feel that the Function interface is not really intuituve - its more like a cointainer of functions or something like that.\nRight now this class seem to contain some parts of a registry; which could also do chaining - plus  the registration and execution logic for functions defined in the metastore.\nIt seems to me that there is no real registry right now - if there would be one then I think this class could be some kind of \"factory of functions\" ?", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:30:24Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import static org.apache.hive.hplsql.functions.InMemoryFunction.setCallParameters;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import org.antlr.v4.runtime.ANTLRInputStream;\n+import org.antlr.v4.runtime.CommonTokenStream;\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedure;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedureRequest;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlBaseVisitor;\n+import org.apache.hive.hplsql.HplsqlLexer;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Scope;\n+import org.apache.hive.hplsql.Var;\n+import org.apache.thrift.TException;\n+\n+public class HmsFunction implements Function {", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDU0MQ==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060541", "bodyText": "I agree, but this is how it used to work, and I didn't want to address these things as part of this patch.", "author": "zeroflag", "createdAt": "2020-10-26T15:41:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwMzk4NA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMDU2Mg==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512010562", "bodyText": "is this an unused class?", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:38:31Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hive.metastore.model;\n+\n+public class MPosParam {", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MDM4Ng==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512590386", "bodyText": "removed", "author": "zeroflag", "createdAt": "2020-10-27T10:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMDU2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java\ndeleted file mode 100644\nindex 45208fd0a4..0000000000\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java\n+++ /dev/null\n\n@@ -1,69 +0,0 @@\n-/*\n- *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  *\n- *  *     http://www.apache.org/licenses/LICENSE-2.0\n- *  *\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n- *\n- */\n-\n-package org.apache.hadoop.hive.metastore.model;\n-\n-public class MPosParam {\n-  private String name;\n-  private String type;\n-  private Integer length;\n-  private Integer scale;\n-  private boolean isOut;\n-\n-  public MPosParam(String name, String type, boolean isOut, Integer length, Integer scale) {\n-    this.name = name;\n-    this.type = type;\n-    this.length = length;\n-    this.scale = scale;\n-    this.isOut = isOut;\n-  }\n-\n-  public String getName() {\n-    return name;\n-  }\n-\n-  public String getType() {\n-    return type;\n-  }\n-\n-  public boolean isOut() {\n-    return isOut;\n-  }\n-\n-  public Integer getLength() {\n-    return length;\n-  }\n-\n-  public Integer getScale() {\n-    return scale;\n-  }\n-\n-  @Override\n-  public String toString() {\n-    final StringBuilder sb = new StringBuilder(\"PosParam{\");\n-    sb.append(\"name='\").append(name).append('\\'');\n-    sb.append(\", type='\").append(type).append('\\'');\n-    sb.append(\", isOut=\").append(isOut);\n-    sb.append(\", length=\").append(length);\n-    sb.append(\", scale=\").append(scale);\n-    sb.append('}');\n-    return sb.toString();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMzg4OQ==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512023889", "bodyText": "will this escaping be enough in all cases?", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:55:07Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -1659,13 +1665,70 @@ public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n     }\n     return 0;\n   }\n-  \n+\n+  /**\n+   * User-defined function in a SQL query\n+   */\n+  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n+    if (execUserSql(ctx, name)) {\n+      return;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(name);\n+    sql.append(\"(\");\n+    if (ctx != null) {\n+      int cnt = ctx.func_param().size();\n+      for (int i = 0; i < cnt; i++) {\n+        sql.append(evalPop(ctx.func_param(i).expr()));\n+        if (i + 1 < cnt) {\n+          sql.append(\", \");\n+        }\n+      }\n+    }\n+    sql.append(\")\");\n+    exec.stackPush(sql);\n+  }\n+\n+  /**\n+   * Execute a HPL/SQL user-defined function in a query\n+   */\n+  private boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n+    if (!function.exists(name.toUpperCase())) {\n+      return false;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(\"hplsql('\");", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDk4Ng==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060986", "bodyText": "I didn't check since this is not new code, and fixing every issue in the existing code was not the scope of this issue.", "author": "zeroflag", "createdAt": "2020-10-26T15:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMzg4OQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNzcyOA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512027728", "bodyText": "typo: hplsq\nnote: this getMsc() method could be placed closer to the Hms related stuff", "author": "kgyrtkirk", "createdAt": "2020-10-26T14:59:37Z", "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -799,30 +801,35 @@ Integer init(String[] args) throws Exception {\n     select = new Select(this);\n     stmt = new Stmt(this);\n     converter = new Converter(this);\n-        \n-    function = new Function(this);\n-    new FunctionDatetime(this).register(function);\n-    new FunctionMisc(this).register(function);\n-    new FunctionString(this).register(function);\n-    new FunctionOra(this).register(function);\n+\n+    builtinFunctions = new BuiltinFunctions(this);\n+    new FunctionDatetime(this).register(builtinFunctions);\n+    new FunctionMisc(this).register(builtinFunctions);\n+    new FunctionString(this).register(builtinFunctions);\n+    new FunctionOra(this).register(builtinFunctions);\n+    if (\"hms\".equalsIgnoreCase(System.getProperty(\"hplsql.storage\"))) {\n+      function = new HmsFunction(this, getMsc(System.getProperty(\"hplsq.metastore.uris\", \"thrift://localhost:9083\")), builtinFunctions);", "originalCommit": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MTE4NA==", "url": "https://github.com/apache/hive/pull/1542#discussion_r512061184", "bodyText": "This part is removed from subsequent patch.", "author": "zeroflag", "createdAt": "2020-10-26T15:41:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNzcyOA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "url": "https://github.com/apache/hive/commit/e8719c0ab3431625706dd9e902f11d87a26a7f0a", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "committedDate": "2020-10-27T10:52:29Z", "type": "commit"}]}