{"pr_number": 1151, "pr_title": "HIVE-23725: ValidTxnManager snapshot outdating causing partial reads", "pr_createdAt": "2020-06-19T12:33:16Z", "pr_url": "https://github.com/apache/hive/pull/1151", "timeline": [{"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d", "url": "https://github.com/apache/hive/commit/f703ff9139d9ce9f9fd4128e9b9663d71809840d", "message": "HIVE-23725: ValidTxnManager snapshot outdating causing partial reads in merge insert", "committedDate": "2020-06-19T12:28:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyNjY1Ng==", "url": "https://github.com/apache/hive/pull/1151#discussion_r442826656", "bodyText": "I think retryLock would be a proper name.", "author": "deniskuzZ", "createdAt": "2020-06-19T13:01:04Z", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,lockacquisition\",", "originalCommit": "f703ff9139d9ce9f9fd4128e9b9663d71809840d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "chunk": "diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\nindex fa79f68046..22b69b856a 100644\n--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n\n@@ -4979,11 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,lockacquisition\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,retrylock\",\n         \"comma separated list of plugin can be used:\\n\"\n             + \"  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\\n\"\n             + \"  reoptimize: collects operator statistics during execution and recompile the query after a failure\\n\"\n-            + \"  lockacquisition: recompiles the query if snapshot becomes outdated before lock acquisition\"),\n+            + \"  retrylock: recompiles the query if snapshot becomes outdated before lock acquisition\"),\n     HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE(\"hive.query.reexecution.stats.persist.scope\", \"metastore\",\n         new StringSet(\"query\", \"hiveserver\", \"metastore\"),\n         \"Sets the persistence scope of runtime statistics\\n\"\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyODczMA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r442828730", "bodyText": "wrap releaseLocksAndCommitOrRollback(false) with try and throw exception in finally, instead of rollback null", "author": "deniskuzZ", "createdAt": "2020-06-19T13:05:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,14 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n-          }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          rollback(null);", "originalCommit": "f703ff9139d9ce9f9fd4128e9b9663d71809840d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex 2fbdeb7fca..e6b4c67236 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n\n@@ -680,8 +680,12 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n         if (!validTxnManager.isValidTxnListState()) {\n           LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          rollback(null);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n+          }\n           throw handleHiveException(\n               new HiveException(\n                   \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\"),\n"}}, {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "url": "https://github.com/apache/hive/commit/b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "message": "Fix failing tests and review comments", "committedDate": "2020-06-21T20:52:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTc4NQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269785", "bodyText": "Do we really want retrylock to be driven by a config? Shouldn't it just be enabled?", "author": "jcamachor", "createdAt": "2020-06-21T23:59:21Z", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,retrylock\",", "originalCommit": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba8c29266ee5ca8bd25b764f795f903505bda8a4", "chunk": "diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\nindex 22b69b856a..c79c4d8a8b 100644\n--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n\n@@ -4979,11 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,retrylock\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n         \"comma separated list of plugin can be used:\\n\"\n             + \"  overlay: hiveconf subtree 'reexec.overlay' is used as an overlay in case of an execution errors out\\n\"\n             + \"  reoptimize: collects operator statistics during execution and recompile the query after a failure\\n\"\n-            + \"  retrylock: recompiles the query if snapshot becomes outdated before lock acquisition\"),\n+            + \"  The retrylock strategy is always enabled: recompiles the query if snapshot becomes outdated before lock acquisition\"),\n     HIVE_QUERY_REEXECUTION_STATS_PERSISTENCE(\"hive.query.reexecution.stats.persist.scope\", \"metastore\",\n         new StringSet(\"query\", \"hiveserver\", \"metastore\"),\n         \"Sets the persistence scope of runtime statistics\\n\"\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTk3MA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269970", "bodyText": "equals ?", "author": "jcamachor", "createdAt": "2020-06-22T00:01:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "diffHunk": "@@ -64,6 +65,9 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if (name.equals(\"reoptimize\")) {\n       return new ReOptimizePlugin();\n     }\n+    if (name.endsWith(\"retrylock\")) {", "originalCommit": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba8c29266ee5ca8bd25b764f795f903505bda8a4", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java\nindex 774d60c839..8b62f11c3c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java\n\n@@ -65,9 +67,6 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if (name.equals(\"reoptimize\")) {\n       return new ReOptimizePlugin();\n     }\n-    if (name.endsWith(\"retrylock\")) {\n-      return new ReExecutionRetryLockPlugin();\n-    }\n     throw new RuntimeException(\n         \"Unknown re-execution plugin: \" + name + \" (\" + ConfVars.HIVE_QUERY_REEXECUTION_STRATEGIES.varname + \")\");\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r443270688", "bodyText": "I think this makes behavior wrt original logic slightly different. For instance, if another transaction obtains the locks in between the moment that this transaction releases them and is going to acquire them again, does this mean the transaction would fail for a second time? If that is the case, should we have a specific configuration for the number of retries in this case? It seems for the default re-execution the number of retries is 1, but in this case, we could retry several times before failing the query.", "author": "jcamachor", "createdAt": "2020-06-22T00:09:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "originalCommit": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYzOTEyNg==", "url": "https://github.com/apache/hive/pull/1151#discussion_r443639126", "bodyText": "This is an interesting question. In the original logic, if an other commit invalidated the snaphsot a second time, the query also failed with HiveExection. The main difference is, we do more work in this case (compile and acquire the locks again), so the chance is probably higher that the snapshot gets invalidated a second time, but I don't know if it is high enough that we should consider it. The ReexecDriver uses one global config for the number of retries, it would take some refactoring to make it independently configurable for the different plugins.", "author": "pvargacl", "createdAt": "2020-06-22T15:22:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk2OTQyMA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r443969420", "bodyText": "In the original logic, if another commit invalidated the snaphsot a second time, the query also failed with HiveExection.\n\nIiuc that should not happen because we were holding the locks that we had already acquired; however, now we are releasing them. Hence, the logic is slightly different?\nIn any case, it is straightforward to add a config property such as HIVE_QUERY_MAX_REEXECUTION_COUNT for this specific retry, then retrieve it in shouldReExecute method in ReExecutionRetryLockPlugin: You have both the number of retries as well as the conf (getConf method) to retrieve the max number of retries for the config. The check on HIVE_QUERY_MAX_REEXECUTION_COUNT for the rest of the plugins will need to be moved into shouldReExecute method in those plugins too (currently it is done within the run method in the ReExecDriver itself).", "author": "jcamachor", "createdAt": "2020-06-23T05:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0OTU0Nw==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444249547", "bodyText": "Added the new config.", "author": "pvargacl", "createdAt": "2020-06-23T14:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}], "type": "inlineReview", "revised_code": {"commit": "0427fe54a19bbe77a04feb9ef7a84ba9520563de", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex e6b4c67236..9fbd53794c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n\n@@ -678,7 +678,7 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n+          LOG.warn(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry (see ReExecutionRetryLockPlugin)\n           try {\n"}}, {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4", "url": "https://github.com/apache/hive/commit/ba8c29266ee5ca8bd25b764f795f903505bda8a4", "message": "Add new reexecution count config for retrylock", "committedDate": "2020-06-23T09:37:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444292814", "bodyText": "Is this still needed in ReExecDriver? Shouldn't it be driven completely by the plugins implementation now, i.e., shouldReExecute will return false after the number of retries exceeds the max?", "author": "jcamachor", "createdAt": "2020-06-23T15:00:06Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "originalCommit": "ba8c29266ee5ca8bd25b764f795f903505bda8a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NTUxMg==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444295512", "bodyText": "I would like to avoid, that some new plugin forgets to check the max in its shouldReExecute and we go in an infinite loop.", "author": "pvargacl", "createdAt": "2020-06-23T15:03:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NzcwMA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444297700", "bodyText": "Makes sense. Can we leave a comment mentioning that? Thanks", "author": "jcamachor", "createdAt": "2020-06-23T15:06:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5ODk4OQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444698989", "bodyText": "I also do not like this approach as you are aggregating all the conditions from underlying plugins here (when adding new plugin you should incorporate it's config here as well). What you could do is to define default shouldReExecute method under IReExecutionPlugin, in this case new plugin would use that or has to override it.", "author": "deniskuzZ", "createdAt": "2020-06-24T07:30:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcwMjg0OA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444702848", "bodyText": "That would not solve the problem, every plugin will override the shouldReExecute method, that is the main goal of a plugin. Still they can ignore the max execution property. This is here for a failsafe, to not have infinite loops.", "author": "pvargacl", "createdAt": "2020-06-24T07:38:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcyODkxMA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444728910", "bodyText": "in getMaxReExecutions could we then just stick with some default hardcoded value?", "author": "deniskuzZ", "createdAt": "2020-06-24T08:26:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}], "type": "inlineReview", "revised_code": {"commit": "4d421609193669b5a46c2f530f6c570de69ad4e7", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java\nindex 4677b583d3..b7c4bc1359 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java\n\n@@ -148,6 +148,8 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n+    // The plugins should check for execution limit in shouldReexecute\n+    // But just in case, we don't want an infinite loop\n     int maxExecutions = getMaxExecutions();\n \n     while (true) {\n"}}, {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7", "url": "https://github.com/apache/hive/commit/4d421609193669b5a46c2f530f6c570de69ad4e7", "message": "review comment fix", "committedDate": "2020-06-23T15:21:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4MzI0MQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683241", "bodyText": "should we use WARN here?", "author": "deniskuzZ", "createdAt": "2020-06-24T06:56:36Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");", "originalCommit": "4d421609193669b5a46c2f530f6c570de69ad4e7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0427fe54a19bbe77a04feb9ef7a84ba9520563de", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex e6b4c67236..9fbd53794c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n\n@@ -678,7 +678,7 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n+          LOG.warn(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry (see ReExecutionRetryLockPlugin)\n           try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4Mzc4Mg==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683782", "bodyText": "what is this magic number 12? do we have enum for error codes?", "author": "deniskuzZ", "createdAt": "2020-06-24T06:57:52Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);", "originalCommit": "4d421609193669b5a46c2f530f6c570de69ad4e7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NjExNQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444696115", "bodyText": "I do not really know where these response codes come from, it seems like to me there are different codes, for different error types. I can not see any enum or anything that would explain the different codes. 12 seems like the code for lockexception during rollback/commit.", "author": "pvargacl", "createdAt": "2020-06-24T07:25:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4Mzc4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "0427fe54a19bbe77a04feb9ef7a84ba9520563de", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex e6b4c67236..9fbd53794c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n\n@@ -678,7 +678,7 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n+          LOG.warn(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry (see ReExecutionRetryLockPlugin)\n           try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTk4NQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444689985", "bodyText": "why do we need this? there is clean up at the beginning. same in other tests", "author": "deniskuzZ", "createdAt": "2020-06-24T07:12:03Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2329,6 +2338,142 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+        \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});", "originalCommit": "4d421609193669b5a46c2f530f6c570de69ad4e7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDA2MA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444694060", "bodyText": "This just makes the local reexecution easier. If the test finishes it leaves some data in the warehouse folder and even if the next run starts with drop table if exists, it won't clean the folder since the table is not existing the hms", "author": "pvargacl", "createdAt": "2020-06-24T07:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTk4NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444694401", "bodyText": "should we re-execute when here?", "author": "deniskuzZ", "createdAt": "2020-06-24T07:21:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "originalCommit": "4d421609193669b5a46c2f530f6c570de69ad4e7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5OTAxMw==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444699013", "bodyText": "The ReExecutePluginInterface is not really straightforward. This method is called after the query is recompiled from the reExecDriver shouldReExecuteAfterCompile . Basicly it asks after the recompilation do you want to continue the execution. It is only used by the reoptimize plugin, which tries to recompile the query with different statistics (if I understand correctly) and only reexecutes if the plan did change.", "author": "pvargacl", "createdAt": "2020-06-24T07:30:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcwOTAxNQ==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444709015", "bodyText": "if it's only used by reoptimize plugin shouldn't we throw UnsupportedOperationException?", "author": "deniskuzZ", "createdAt": "2020-06-24T07:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcxMDI3Mg==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444710272", "bodyText": "No, you must return true, saying you do not care with the result of the recompile, you want the query to reexecute.", "author": "pvargacl", "createdAt": "2020-06-24T07:53:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDgxMjc1MA==", "url": "https://github.com/apache/hive/pull/1151#discussion_r444812750", "bodyText": "ok :)", "author": "deniskuzZ", "createdAt": "2020-06-24T10:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "0427fe54a19bbe77a04feb9ef7a84ba9520563de", "url": "https://github.com/apache/hive/commit/0427fe54a19bbe77a04feb9ef7a84ba9520563de", "message": "Address review comments", "committedDate": "2020-06-24T12:30:00Z", "type": "commit"}, {"oid": "314be512d84e5bf1aa9563139f3c38ff8974854b", "url": "https://github.com/apache/hive/commit/314be512d84e5bf1aa9563139f3c38ff8974854b", "message": "Merge remote-tracking branch 'origin/master' into HIVE-23725-validtxnmanager\n\n# Conflicts:\n#\tcommon/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n#\tql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "committedDate": "2020-06-26T08:55:34Z", "type": "commit"}]}