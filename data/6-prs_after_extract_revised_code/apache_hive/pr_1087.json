{"pr_number": 1087, "pr_title": "HIVE-23671: MSCK repair should handle transactional tables", "pr_createdAt": "2020-06-10T11:06:53Z", "pr_url": "https://github.com/apache/hive/pull/1087", "timeline": [{"oid": "7feb722e47769c63fb1829597863731f76bbc314", "url": "https://github.com/apache/hive/commit/7feb722e47769c63fb1829597863731f76bbc314", "message": "HIVE-23671: MSCK repair should handle transactional tables", "committedDate": "2020-06-10T10:07:45Z", "type": "commit"}, {"oid": "4a4b87fb864fa67534b0216617cbf654a24cb2f6", "url": "https://github.com/apache/hive/commit/4a4b87fb864fa67534b0216617cbf654a24cb2f6", "message": "Fix TestMsckRepairOnAcid test for jenkins", "committedDate": "2020-06-12T11:58:56Z", "type": "commit"}, {"oid": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "url": "https://github.com/apache/hive/commit/2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "message": "Fix TestNsckRepairOnAcid lowercase folder name", "committedDate": "2020-06-12T19:37:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMzkwNw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446123907", "bodyText": "redundant check (sessionTxnMgr != null), see if condition above", "author": "deniskuzZ", "createdAt": "2020-06-26T11:20:23Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2392,33 +2392,29 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n     long writeId = -1;\n     ValidWriteIdList validWriteIdList = null;\n \n-    HiveTxnManager sessionTxnMgr = SessionState.get().getTxnMgr();\n-    String fullTableName = getFullTableName(dbName, tblName);\n-    if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n-      validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n-      if (isStatsUpdater) {\n-        writeId = SessionState.get().getTxnMgr() != null ?\n-                SessionState.get().getTxnMgr().getAllocatedTableWriteId(\n-                  dbName, tblName) : -1;\n-        if (writeId < 1) {\n-          // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n-          //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n-          LOG.debug(\"Stats updater for {}.{} doesn't have a write ID ({})\",\n-              dbName, tblName, writeId);\n+    if (SessionState.get() != null) {\n+      HiveTxnManager sessionTxnMgr = SessionState.get().getTxnMgr();\n+      String fullTableName = getFullTableName(dbName, tblName);\n+      if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n+        validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n+        if (isStatsUpdater) {\n+          writeId = sessionTxnMgr != null ? sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName) : -1;", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxODg2MQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450318861", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T15:53:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMzkwNw=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex 0809c85ab2..00f955f164 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2398,7 +2343,7 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n       if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n         validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n         if (isStatsUpdater) {\n-          writeId = sessionTxnMgr != null ? sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName) : -1;\n+          writeId = sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName);\n           if (writeId < 1) {\n             // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n             //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyNTYzOQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446125639", "bodyText": "i think it can't be null, it returns ThreadLocal variable", "author": "deniskuzZ", "createdAt": "2020-06-26T11:24:24Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2392,33 +2392,29 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n     long writeId = -1;\n     ValidWriteIdList validWriteIdList = null;\n \n-    HiveTxnManager sessionTxnMgr = SessionState.get().getTxnMgr();\n-    String fullTableName = getFullTableName(dbName, tblName);\n-    if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n-      validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n-      if (isStatsUpdater) {\n-        writeId = SessionState.get().getTxnMgr() != null ?\n-                SessionState.get().getTxnMgr().getAllocatedTableWriteId(\n-                  dbName, tblName) : -1;\n-        if (writeId < 1) {\n-          // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n-          //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n-          LOG.debug(\"Stats updater for {}.{} doesn't have a write ID ({})\",\n-              dbName, tblName, writeId);\n+    if (SessionState.get() != null) {", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMwMTM0OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450301348", "bodyText": "It can be null, if the sessionstate was not set properly, it was failing one of the tests, i can't remember which.", "author": "pvargacl", "createdAt": "2020-07-06T15:27:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyNTYzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex 0809c85ab2..00f955f164 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2398,7 +2343,7 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n       if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n         validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n         if (isStatsUpdater) {\n-          writeId = sessionTxnMgr != null ? sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName) : -1;\n+          writeId = sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName);\n           if (writeId < 1) {\n             // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n             //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyNjkwNA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446126904", "bodyText": "is it ever a valid condition?", "author": "deniskuzZ", "createdAt": "2020-06-26T11:27:20Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2392,33 +2392,29 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n     long writeId = -1;\n     ValidWriteIdList validWriteIdList = null;\n \n-    HiveTxnManager sessionTxnMgr = SessionState.get().getTxnMgr();\n-    String fullTableName = getFullTableName(dbName, tblName);\n-    if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n-      validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n-      if (isStatsUpdater) {\n-        writeId = SessionState.get().getTxnMgr() != null ?\n-                SessionState.get().getTxnMgr().getAllocatedTableWriteId(\n-                  dbName, tblName) : -1;\n-        if (writeId < 1) {\n-          // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n-          //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n-          LOG.debug(\"Stats updater for {}.{} doesn't have a write ID ({})\",\n-              dbName, tblName, writeId);\n+    if (SessionState.get() != null) {\n+      HiveTxnManager sessionTxnMgr = SessionState.get().getTxnMgr();\n+      String fullTableName = getFullTableName(dbName, tblName);\n+      if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n+        validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n+        if (isStatsUpdater) {\n+          writeId = sessionTxnMgr != null ? sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName) : -1;\n+          if (writeId < 1) {", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMwMjAzMw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450302033", "bodyText": "The comment said so: \"stats updater that doesn't have write ID is currently \"create table\"; writeId would be 0/-1 here.\"", "author": "pvargacl", "createdAt": "2020-07-06T15:28:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyNjkwNA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex 0809c85ab2..00f955f164 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2398,7 +2343,7 @@ public static TableSnapshot getTableSnapshot(Configuration conf,\n       if (sessionTxnMgr != null && sessionTxnMgr.getCurrentTxnId() > 0) {\n         validWriteIdList = getTableValidWriteIdList(conf, fullTableName);\n         if (isStatsUpdater) {\n-          writeId = sessionTxnMgr != null ? sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName) : -1;\n+          writeId = sessionTxnMgr.getAllocatedTableWriteId(dbName, tblName);\n           if (writeId < 1) {\n             // TODO: this is not ideal... stats updater that doesn't have write ID is currently\n             //       \"create table\"; writeId would be 0/-1 here. No need to call this w/true.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzMTk4OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446131988", "bodyText": "could you extend TestTxnCommands2 from TxnCommandsBaseForTests and remove static?", "author": "deniskuzZ", "createdAt": "2020-06-26T11:40:02Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2209,20 +2209,7 @@ public void testAcidOrcWritePreservesFieldNames() throws Exception {\n    * sorts rows in dictionary order\n    */\n   static List<String> stringifyValues(int[][] rowsIn) {\n-    assert rowsIn.length > 0;\n-    int[][] rows = rowsIn.clone();\n-    Arrays.sort(rows, new RowComp());\n-    List<String> rs = new ArrayList<String>();\n-    for(int[] row : rows) {\n-      assert row.length > 0;\n-      StringBuilder sb = new StringBuilder();\n-      for(int value : row) {\n-        sb.append(value).append(\"\\t\");\n-      }\n-      sb.setLength(sb.length() - 1);\n-      rs.add(sb.toString());\n-    }\n-    return rs;\n+    return TxnCommandsBaseForTests.stringifyValues(rowsIn);", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxNTM5Nw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450315397", "bodyText": "I will do this in a separate Jira, started it, but it requires more change.", "author": "pvargacl", "createdAt": "2020-07-06T15:48:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzMTk4OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzMzUxOQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446133519", "bodyText": "move RowComp from TestTxnCommands2 to TxnCommandsBaseForTests", "author": "deniskuzZ", "createdAt": "2020-06-26T11:43:46Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java", "diffHunk": "@@ -162,9 +163,23 @@ protected String getWarehouseDir() {\n    * takes raw data and turns it into a string as if from Driver.getResults()\n    * sorts rows in dictionary order\n    */\n-  List<String> stringifyValues(int[][] rowsIn) {\n-    return TestTxnCommands2.stringifyValues(rowsIn);\n+  public static List<String> stringifyValues(int[][] rowsIn) {\n+    assert rowsIn.length > 0;\n+    int[][] rows = rowsIn.clone();\n+    Arrays.sort(rows, new TestTxnCommands2.RowComp());", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxODk3MA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450318970", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T15:54:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzMzUxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java b/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java\nindex c593cfeead..a59b364922 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/TxnCommandsBaseForTests.java\n\n@@ -166,8 +167,8 @@ protected String getWarehouseDir() {\n   public static List<String> stringifyValues(int[][] rowsIn) {\n     assert rowsIn.length > 0;\n     int[][] rows = rowsIn.clone();\n-    Arrays.sort(rows, new TestTxnCommands2.RowComp());\n-    List<String> rs = new ArrayList<String>();\n+    Arrays.sort(rows, new RowComp());\n+    List<String> rs = new ArrayList<>();\n     for(int[] row : rows) {\n       assert row.length > 0;\n       StringBuilder sb = new StringBuilder();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzNDUwNg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446134506", "bodyText": "why not setIntVar?", "author": "deniskuzZ", "createdAt": "2020-06-26T11:46:09Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java", "diffHunk": "@@ -74,21 +76,21 @@\n   @Before\n   public void setUp() throws Exception {\n     hive = Hive.get();\n-    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 15);\n-    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"throw\");\n+    hive.getConf().set(MetastoreConf.ConfVars.FS_HANDLER_THREADS_COUNT.getVarname(), \"15\");", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxNzQ1MQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450317451", "bodyText": "It does not work with MetasoreConf.ConfVars", "author": "pvargacl", "createdAt": "2020-07-06T15:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzNDUwNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE0MzYwOA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r446143608", "bodyText": "could you please move helper methods at the bottom of the Test class after all tests", "author": "deniskuzZ", "createdAt": "2020-06-26T12:07:24Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java", "diffHunk": "@@ -252,37 +241,165 @@ public void testInvalidPartitionKeyName()\n   @Test\n   public void testSkipInvalidPartitionKeyName()\n     throws HiveException, AlreadyExistsException, IOException, MetastoreException {\n-    hive.getConf().set(HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION.varname, \"skip\");\n+    hive.getConf().set(MetastoreConf.ConfVars.MSCK_PATH_VALIDATION.getVarname(), \"skip\");\n     checker = new HiveMetaStoreChecker(msc, hive.getConf());\n-    Table table = createTestTable();\n+    Table table = createTestTable(false);\n     List<Partition> partitions = hive.getPartitions(table);\n     assertEquals(2, partitions.size());\n     // add a fake partition dir on fs\n     fs = partitions.get(0).getDataLocation().getFileSystem(hive.getConf());\n-    Path fakePart =\n-        new Path(table.getDataLocation().toString(), \"fakedate=2009-01-01/fakecity=sanjose\");\n-    fs.mkdirs(fakePart);\n-    fs.deleteOnExit(fakePart);\n+    addFolderToPath(fs, table.getDataLocation().toString(),\"fakedate=2009-01-01/fakecity=sanjose\");\n     createPartitionsDirectoriesOnFS(table, 2);\n-    CheckResult result = new CheckResult();\n-    checker.checkMetastore(catName, dbName, tableName, null, null, result);\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n     assertEquals(Collections.<String> emptySet(), result.getTablesNotInMs());\n     assertEquals(Collections.<String> emptySet(), result.getTablesNotOnFs());\n     assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n     // only 2 valid partitions should be added\n     assertEquals(2, result.getPartitionsNotInMs().size());\n   }\n \n-  private Table createTestTable() throws HiveException, AlreadyExistsException {\n+  /*\n+   * Tests the case when we have normal delta_dirs in the partition folder\n+   * does not throw HiveException\n+   */\n+  @Test\n+  public void testAddPartitionNormalDeltas() throws Exception {\n+    Table table = createTestTable(true);\n+    List<Partition> partitions = hive.getPartitions(table);\n+    assertEquals(2, partitions.size());\n+    // add a partition dir on fs\n+    fs = partitions.get(0).getDataLocation().getFileSystem(hive.getConf());\n+    Path newPart = addFolderToPath(fs, table.getDataLocation().toString(),\n+        partDateName + \"=2017-01-01/\" + partCityName + \"=paloalto\");\n+\n+    // Add a few deltas\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000001_0000001_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000010_0000010_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000101_0000101_0000\");\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n+    assertEquals(1, result.getPartitionsNotInMs().size());\n+    // Found the highest writeId\n+    assertEquals(101, result.getPartitionsNotInMs().iterator().next().getMaxWriteId());\n+    assertEquals(0, result.getPartitionsNotInMs().iterator().next().getMaxTxnId());\n+  }\n+  /*\n+   * Tests the case when we have normal delta_dirs in the partition folder\n+   * does not throw HiveException\n+   */\n+  @Test\n+  public void testAddPartitionCompactedDeltas() throws Exception {\n+    Table table = createTestTable(true);\n+    List<Partition> partitions = hive.getPartitions(table);\n+    assertEquals(2, partitions.size());\n+    // add a partition dir on fs\n+    fs = partitions.get(0).getDataLocation().getFileSystem(hive.getConf());\n+    Path newPart = addFolderToPath(fs, table.getDataLocation().toString(),\n+        partDateName + \"=2017-01-01/\" + partCityName + \"=paloalto\");\n+\n+    // Add a few deltas\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000001_0000001_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000010_0000015_v0000067\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000101_0000120_v0000087\");\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n+    assertEquals(1, result.getPartitionsNotInMs().size());\n+    // Found the highest writeId\n+    assertEquals(120, result.getPartitionsNotInMs().iterator().next().getMaxWriteId());\n+    assertEquals(87, result.getPartitionsNotInMs().iterator().next().getMaxTxnId());\n+  }\n+  @Test\n+  public void testAddPartitionCompactedBase() throws Exception {\n+    Table table = createTestTable(true);\n+    List<Partition> partitions = hive.getPartitions(table);\n+    assertEquals(2, partitions.size());\n+    // add a partition dir on fs\n+    fs = partitions.get(0).getDataLocation().getFileSystem(hive.getConf());\n+    Path newPart = addFolderToPath(fs, table.getDataLocation().toString(),\n+        partDateName + \"=2017-01-01/\" + partCityName + \"=paloalto\");\n+\n+    // Add a few deltas\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000001_0000001_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000002_0000002_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000003_0000003_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"base_0000003_v0000200\");\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n+    assertEquals(1, result.getPartitionsNotInMs().size());\n+    // Found the highest writeId\n+    assertEquals(3, result.getPartitionsNotInMs().iterator().next().getMaxWriteId());\n+    assertEquals(200, result.getPartitionsNotInMs().iterator().next().getMaxTxnId());\n+  }\n+\n+  @Test\n+  public void testAddPartitionMMBase() throws Exception {\n+    Table table = createTestTable(true);\n+    List<Partition> partitions = hive.getPartitions(table);\n+    assertEquals(2, partitions.size());\n+    // add a partition dir on fs\n+    fs = partitions.get(0).getDataLocation().getFileSystem(hive.getConf());\n+    Path newPart = addFolderToPath(fs, table.getDataLocation().toString(),\n+        partDateName + \"=2017-01-01/\" + partCityName + \"=paloalto\");\n+\n+    // Add a few deltas\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000001_0000001_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000002_0000002_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"delta_0000003_0000003_0000\");\n+    addFolderToPath(fs, newPart.toString(), \"base_0000004\");\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n+    assertEquals(1, result.getPartitionsNotInMs().size());\n+    // Found the highest writeId\n+    assertEquals(4, result.getPartitionsNotInMs().iterator().next().getMaxWriteId());\n+    assertEquals(0, result.getPartitionsNotInMs().iterator().next().getMaxTxnId());\n+  }\n+\n+  @Test\n+  public void testNoNPartitionedTable() throws Exception {\n+    Table table = createNonPartitionedTable();\n+    // add a partition dir on fs\n+    fs = table.getDataLocation().getFileSystem(hive.getConf());\n+\n+    Path tablePath = table.getDataLocation();\n+\n+    // Add a few deltas\n+    addFolderToPath(fs, tablePath.toString(), \"delta_0000001_0000001_0000\");\n+    addFolderToPath(fs, tablePath.toString(), \"delta_0000002_0000002_0000\");\n+    addFolderToPath(fs, tablePath.toString(), \"delta_0000003_0000003_0000\");\n+    addFolderToPath(fs, tablePath.toString(), \"base_0000003_v0000200\");\n+    CheckResult result = checker.checkMetastore(catName, dbName, tableName, null, null);\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotOnFs());\n+    assertEquals(Collections.<CheckResult.PartitionResult> emptySet(), result.getPartitionsNotInMs());\n+    // Found the highest writeId\n+    assertEquals(3, result.getMaxWriteId());\n+    assertEquals(200, result.getMaxTxnId());\n+  }\n+\n+  private Path addFolderToPath(FileSystem fs, String rootPath, String folder) throws IOException{", "originalCommit": "2d5a3b5cc2d5d006b01a2e0d80b37cb4483d3737", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMxOTA4Nw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450319087", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T15:54:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE0MzYwOA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java\nindex 9a04726a6c..d0e020b751 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java\n\n@@ -375,60 +375,6 @@ public void testNoNPartitionedTable() throws Exception {\n     assertEquals(200, result.getMaxTxnId());\n   }\n \n-  private Path addFolderToPath(FileSystem fs, String rootPath, String folder) throws IOException{\n-    Path folderParth = new Path(rootPath, folder);\n-    fs.mkdirs(folderParth);\n-    fs.deleteOnExit(folderParth);\n-    return folderParth;\n-  }\n-\n-  private Table createTestTable(boolean transactional) throws HiveException, AlreadyExistsException {\n-    Database db = new Database();\n-    db.setName(dbName);\n-    hive.createDatabase(db, true);\n-\n-    Table table = new Table(dbName, tableName);\n-    table.setDbName(dbName);\n-    if (transactional) {\n-      table.setInputFormatClass(OrcInputFormat.class);\n-      table.setOutputFormatClass(OrcOutputFormat.class);\n-    } else {\n-      table.setInputFormatClass(TextInputFormat.class);\n-      table.setOutputFormatClass(HiveIgnoreKeyTextOutputFormat.class);\n-    }\n-    table.setPartCols(partCols);\n-    if (transactional) {\n-      table.setProperty(\"transactional\", \"true\");\n-    }\n-\n-    hive.createTable(table);\n-    table = hive.getTable(dbName, tableName);\n-    Assert.assertTrue(table.getTTable().isSetId());\n-    table.getTTable().unsetId();\n-\n-    for (Map<String, String> partSpec : parts) {\n-      hive.createPartition(table, partSpec);\n-    }\n-    return table;\n-  }\n-  private Table createNonPartitionedTable() throws Exception {\n-    Database db = new Database();\n-    db.setName(dbName);\n-    hive.createDatabase(db, true);\n-\n-    Table table = new Table(dbName, tableName);\n-    table.setDbName(dbName);\n-    table.setInputFormatClass(OrcInputFormat.class);\n-    table.setOutputFormatClass(OrcOutputFormat.class);\n-    table.setProperty(\"transactional\", \"true\");\n-\n-    hive.createTable(table);\n-    table = hive.getTable(dbName, tableName);\n-    Assert.assertTrue(table.getTTable().isSetId());\n-    table.getTTable().unsetId();\n-    return table;\n-  }\n-\n   @Test\n   public void testPartitionsCheck() throws HiveException,\n     IOException, TException, MetastoreException {\n"}}, {"oid": "82f8bf293d33b807d33600014018d534bd2b9071", "url": "https://github.com/apache/hive/commit/82f8bf293d33b807d33600014018d534bd2b9071", "message": "Merge remote-tracking branch 'origin/master' into HIVE-23671-msck-repair-2", "committedDate": "2020-06-29T11:34:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAwMTQyMA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447001420", "bodyText": "weird mix of Camel case and underscore", "author": "deniskuzZ", "createdAt": "2020-06-29T14:10:48Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "diffHunk": "@@ -8322,6 +8322,22 @@ public AllocateTableWriteIdsResponse allocate_table_write_ids(\n       return response;\n     }\n \n+    @Override\n+    public MaxAllocatedTableWriteIdResponse get_max_allocated_table_write_id(MaxAllocatedTableWriteIdRequest rqst)", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMyNjg4NQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450326885", "bodyText": "All the functions in HMS looks like this I don't want to break the pattern. On the second glance, I had to change the seedWriteId and seedTxnId to look like this...", "author": "pvargacl", "createdAt": "2020-07-06T16:06:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAwMTQyMA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 2af7f34e20..2cb24156a5 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n\n@@ -8329,12 +8329,12 @@ public MaxAllocatedTableWriteIdResponse get_max_allocated_table_write_id(MaxAllo\n     }\n \n     @Override\n-    public void seedWriteId(SeedTableWriteIdsRequest rqst) throws MetaException {\n+    public void seed_write_id(SeedTableWriteIdsRequest rqst) throws MetaException {\n       getTxnHandler().seedWriteId(rqst);\n     }\n \n     @Override\n-    public void seedTxnId(SeedTxnIdRequest rqst) throws MetaException {\n+    public void seed_txn_id(SeedTxnIdRequest rqst) throws MetaException {\n       getTxnHandler().seedTxnId(rqst);\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAwMzg3MQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447003871", "bodyText": "should we answer this question in a current patch?", "author": "deniskuzZ", "createdAt": "2020-06-29T14:14:22Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java", "diffHunk": "@@ -111,24 +120,24 @@ public IMetaStoreClient getMsc() {\n    * @param partitions\n    *          List of partition name value pairs, if null or empty check all\n    *          partitions\n-   * @param table\n-   * @param result\n-   *          Fill this with the results of the check\n+   * @param table Table we want to run the check for.\n+   * @return Results of the check\n    * @throws MetastoreException\n    *           Failed to get required information from the metastore.\n    * @throws IOException\n    *           Most likely filesystem related\n    */\n-  public void checkMetastore(String catName, String dbName, String tableName,\n-      List<? extends Map<String, String>> partitions, Table table, CheckResult result)\n+  public CheckResult checkMetastore(String catName, String dbName, String tableName,\n+      List<? extends Map<String, String>> partitions, Table table)\n       throws MetastoreException, IOException {\n-\n+    CheckResult result = new CheckResult();\n     if (dbName == null || \"\".equalsIgnoreCase(dbName)) {\n       dbName = Warehouse.DEFAULT_DATABASE_NAME;\n     }\n \n     try {\n       if (tableName == null || \"\".equals(tableName)) {\n+        // TODO: I do not think this is used by anything other than tests", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMzNDM5Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450334396", "bodyText": "I do not know. If I understand correctly there is no way currently to call MSCK repair without a table specified, but it seems like someone made some effort to create that feature and tests for it. But i don't know if we ever want that in production (calling msck repair for every table seems like a quick way to overwhelm the system)  I left this comment here, for anybody who tries to makes sense of this code.", "author": "pvargacl", "createdAt": "2020-07-06T16:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAwMzg3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "7f1629edbacb81ccb58459fcf17d991182472240", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\nindex 2deb30c36e..69c22c3643 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n\n@@ -117,10 +117,10 @@ public IMetaStoreClient getMsc() {\n    * @param tableName\n    *          Table we want to run the check for. If null we'll check all the\n    *          tables in the database.\n-   * @param partitions\n-   *          List of partition name value pairs, if null or empty check all\n-   *          partitions\n-   * @param table Table we want to run the check for.\n+   * @param filterExp\n+   *          Filter expression which is used to prune th partition from the\n+   *          metastore and FileSystem.\n+   * @param table\n    * @return Results of the check\n    * @throws MetastoreException\n    *           Failed to get required information from the metastore.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyODc1Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447028756", "bodyText": "why not use rest.split('_')?", "author": "deniskuzZ", "createdAt": "2020-06-29T14:47:42Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java", "diffHunk": "@@ -429,6 +451,75 @@ void findUnknownPartitions(Table table, Set<Path> partPaths,\n     LOG.debug(\"Number of partitions not in metastore : \" + result.getPartitionsNotInMs().size());\n   }\n \n+  /**\n+   * Calculate the maximum seen writeId from the acid directory structure\n+   * @param partPath Path of the partition directory\n+   * @param res Partition result to write the max ids\n+   * @throws IOException ex\n+   */\n+  private void setMaxTxnAndWriteIdFromPartition(Path partPath, CheckResult.PartitionResult res) throws IOException {\n+    FileSystem fs = partPath.getFileSystem(conf);\n+    FileStatus[] deltaOrBaseFiles = fs.listStatus(partPath, HIDDEN_FILES_PATH_FILTER);\n+\n+    // Read the writeIds from every base and delta directory and find the max\n+    long maxWriteId = 0L;\n+    long maxVisibilityId = 0L;\n+    for(FileStatus fileStatus : deltaOrBaseFiles) {\n+      if (!fileStatus.isDirectory()) {\n+        continue;\n+      }\n+      long writeId = 0L;\n+      long visibilityId = 0L;\n+      String folder = fileStatus.getPath().getName();\n+      if (folder.startsWith(BASE_PREFIX)) {\n+        visibilityId = getVisibilityTxnId(folder);\n+        if (visibilityId > 0) {\n+          folder = removeVisibilityTxnId(folder);\n+        }\n+        writeId = Long.parseLong(folder.substring(BASE_PREFIX.length()));\n+      } else if (folder.startsWith(DELTA_PREFIX) || folder.startsWith(DELETE_DELTA_PREFIX)) {\n+        // See AcidUtils.parseDelta\n+        visibilityId = getVisibilityTxnId(folder);\n+        if (visibilityId > 0) {\n+          folder = removeVisibilityTxnId(folder);\n+        }\n+        boolean isDeleteDelta = folder.startsWith(DELETE_DELTA_PREFIX);\n+        String rest = folder.substring((isDeleteDelta ? DELETE_DELTA_PREFIX : DELTA_PREFIX).length());\n+        int split = rest.indexOf('_');", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMzODM0OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450338348", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T16:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyODc1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\nindex 2deb30c36e..6034fd9f53 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n\n@@ -464,33 +464,27 @@ private void setMaxTxnAndWriteIdFromPartition(Path partPath, CheckResult.Partiti\n     // Read the writeIds from every base and delta directory and find the max\n     long maxWriteId = 0L;\n     long maxVisibilityId = 0L;\n-    for(FileStatus fileStatus : deltaOrBaseFiles) {\n+    for (FileStatus fileStatus : deltaOrBaseFiles) {\n       if (!fileStatus.isDirectory()) {\n         continue;\n       }\n       long writeId = 0L;\n       long visibilityId = 0L;\n       String folder = fileStatus.getPath().getName();\n+      String visParts[] = folder.split(VISIBILITY_PREFIX);\n+      if (visParts.length > 1) {\n+        visibilityId = Long.parseLong(visParts[1]);\n+        folder = visParts[0];\n+      }\n       if (folder.startsWith(BASE_PREFIX)) {\n-        visibilityId = getVisibilityTxnId(folder);\n-        if (visibilityId > 0) {\n-          folder = removeVisibilityTxnId(folder);\n-        }\n         writeId = Long.parseLong(folder.substring(BASE_PREFIX.length()));\n       } else if (folder.startsWith(DELTA_PREFIX) || folder.startsWith(DELETE_DELTA_PREFIX)) {\n         // See AcidUtils.parseDelta\n-        visibilityId = getVisibilityTxnId(folder);\n-        if (visibilityId > 0) {\n-          folder = removeVisibilityTxnId(folder);\n-        }\n         boolean isDeleteDelta = folder.startsWith(DELETE_DELTA_PREFIX);\n         String rest = folder.substring((isDeleteDelta ? DELETE_DELTA_PREFIX : DELTA_PREFIX).length());\n-        int split = rest.indexOf('_');\n-        //split2 may be -1 if no statementId\n-        int split2 = rest.indexOf('_', split + 1);\n+        String[] nameParts = rest.split(\"_\");\n         // We always want the second part (it is either the same or greater if it is a compacted delta)\n-        writeId = split2 == -1 ? Long.parseLong(rest.substring(split + 1)) : Long\n-            .parseLong(rest.substring(split + 1, split2));\n+        writeId = Long.parseLong(nameParts.length > 1 ? nameParts[1] : nameParts[0]);\n       }\n       if (writeId > maxWriteId) {\n         maxWriteId = writeId;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMzc2OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447033768", "bodyText": "why not use regex with pattern matching? removeVisibilityTxnId probably wouldn't even be needed", "author": "deniskuzZ", "createdAt": "2020-06-29T14:54:16Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java", "diffHunk": "@@ -429,6 +451,75 @@ void findUnknownPartitions(Table table, Set<Path> partPaths,\n     LOG.debug(\"Number of partitions not in metastore : \" + result.getPartitionsNotInMs().size());\n   }\n \n+  /**\n+   * Calculate the maximum seen writeId from the acid directory structure\n+   * @param partPath Path of the partition directory\n+   * @param res Partition result to write the max ids\n+   * @throws IOException ex\n+   */\n+  private void setMaxTxnAndWriteIdFromPartition(Path partPath, CheckResult.PartitionResult res) throws IOException {\n+    FileSystem fs = partPath.getFileSystem(conf);\n+    FileStatus[] deltaOrBaseFiles = fs.listStatus(partPath, HIDDEN_FILES_PATH_FILTER);\n+\n+    // Read the writeIds from every base and delta directory and find the max\n+    long maxWriteId = 0L;\n+    long maxVisibilityId = 0L;\n+    for(FileStatus fileStatus : deltaOrBaseFiles) {\n+      if (!fileStatus.isDirectory()) {\n+        continue;\n+      }\n+      long writeId = 0L;\n+      long visibilityId = 0L;\n+      String folder = fileStatus.getPath().getName();\n+      if (folder.startsWith(BASE_PREFIX)) {\n+        visibilityId = getVisibilityTxnId(folder);\n+        if (visibilityId > 0) {\n+          folder = removeVisibilityTxnId(folder);\n+        }\n+        writeId = Long.parseLong(folder.substring(BASE_PREFIX.length()));\n+      } else if (folder.startsWith(DELTA_PREFIX) || folder.startsWith(DELETE_DELTA_PREFIX)) {\n+        // See AcidUtils.parseDelta\n+        visibilityId = getVisibilityTxnId(folder);\n+        if (visibilityId > 0) {\n+          folder = removeVisibilityTxnId(folder);\n+        }\n+        boolean isDeleteDelta = folder.startsWith(DELETE_DELTA_PREFIX);\n+        String rest = folder.substring((isDeleteDelta ? DELETE_DELTA_PREFIX : DELTA_PREFIX).length());\n+        int split = rest.indexOf('_');\n+        //split2 may be -1 if no statementId\n+        int split2 = rest.indexOf('_', split + 1);\n+        // We always want the second part (it is either the same or greater if it is a compacted delta)\n+        writeId = split2 == -1 ? Long.parseLong(rest.substring(split + 1)) : Long\n+            .parseLong(rest.substring(split + 1, split2));\n+      }\n+      if (writeId > maxWriteId) {\n+        maxWriteId = writeId;\n+      }\n+      if (visibilityId > maxVisibilityId) {\n+        maxVisibilityId = visibilityId;\n+      }\n+    }\n+    LOG.debug(\"Max writeId {}, max txnId {} found in partition {}\", maxWriteId, maxVisibilityId,\n+        partPath.toUri().toString());\n+    res.setMaxWriteId(maxWriteId);\n+    res.setMaxTxnId(maxVisibilityId);\n+  }\n+  private long getVisibilityTxnId(String folder) {\n+    int idxOfVis = folder.indexOf(VISIBILITY_PREFIX);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MDAxNg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450360016", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMzc2OA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\nindex 2deb30c36e..6034fd9f53 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n\n@@ -464,33 +464,27 @@ private void setMaxTxnAndWriteIdFromPartition(Path partPath, CheckResult.Partiti\n     // Read the writeIds from every base and delta directory and find the max\n     long maxWriteId = 0L;\n     long maxVisibilityId = 0L;\n-    for(FileStatus fileStatus : deltaOrBaseFiles) {\n+    for (FileStatus fileStatus : deltaOrBaseFiles) {\n       if (!fileStatus.isDirectory()) {\n         continue;\n       }\n       long writeId = 0L;\n       long visibilityId = 0L;\n       String folder = fileStatus.getPath().getName();\n+      String visParts[] = folder.split(VISIBILITY_PREFIX);\n+      if (visParts.length > 1) {\n+        visibilityId = Long.parseLong(visParts[1]);\n+        folder = visParts[0];\n+      }\n       if (folder.startsWith(BASE_PREFIX)) {\n-        visibilityId = getVisibilityTxnId(folder);\n-        if (visibilityId > 0) {\n-          folder = removeVisibilityTxnId(folder);\n-        }\n         writeId = Long.parseLong(folder.substring(BASE_PREFIX.length()));\n       } else if (folder.startsWith(DELTA_PREFIX) || folder.startsWith(DELETE_DELTA_PREFIX)) {\n         // See AcidUtils.parseDelta\n-        visibilityId = getVisibilityTxnId(folder);\n-        if (visibilityId > 0) {\n-          folder = removeVisibilityTxnId(folder);\n-        }\n         boolean isDeleteDelta = folder.startsWith(DELETE_DELTA_PREFIX);\n         String rest = folder.substring((isDeleteDelta ? DELETE_DELTA_PREFIX : DELTA_PREFIX).length());\n-        int split = rest.indexOf('_');\n-        //split2 may be -1 if no statementId\n-        int split2 = rest.indexOf('_', split + 1);\n+        String[] nameParts = rest.split(\"_\");\n         // We always want the second part (it is either the same or greater if it is a compacted delta)\n-        writeId = split2 == -1 ? Long.parseLong(rest.substring(split + 1)) : Long\n-            .parseLong(rest.substring(split + 1, split2));\n+        writeId = Long.parseLong(nameParts.length > 1 ? nameParts[1] : nameParts[0]);\n       }\n       if (writeId > maxWriteId) {\n         maxWriteId = writeId;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzUyMA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447037520", "bodyText": "you can remove 1 nesting level", "author": "deniskuzZ", "createdAt": "2020-06-29T14:59:24Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MDk2OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450360969", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzUyMA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzk2OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447037968", "bodyText": "not formatted", "author": "deniskuzZ", "createdAt": "2020-06-29T14:59:59Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MTI2Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450361266", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:04:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzk2OA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MTU1OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447041559", "bodyText": "you can do success &= writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds)\nfor readability", "author": "deniskuzZ", "createdAt": "2020-06-29T15:04:56Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MTc5Mg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450361792", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:04:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MTU1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MjEyNQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447042125", "bodyText": "same  success &= closeTxn(qualifiedTableName, success, txnId)", "author": "deniskuzZ", "createdAt": "2020-06-29T15:05:38Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MTg4Mw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450361883", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:05:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MjEyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDY2Nw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447044667", "bodyText": "should we use error level here?", "author": "deniskuzZ", "createdAt": "2020-06-29T15:09:11Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MjM4Mg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450362382", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:06:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDY2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDg2Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447044866", "bodyText": "error level?", "author": "deniskuzZ", "createdAt": "2020-06-29T15:09:26Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM2MjQ0OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450362449", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-06T17:06:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDg2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NjAxNg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447046016", "bodyText": "why calling log.info so many times?", "author": "deniskuzZ", "createdAt": "2020-06-29T15:11:06Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc1MzM5Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450753396", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:05:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NjAxNg=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDk3MA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447050970", "bodyText": "should we pass already sorted collection here? it's a bad practice to mutate object parameters in a method.", "author": "deniskuzZ", "createdAt": "2020-06-29T15:17:55Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n+    LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n+    LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n+    LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n+    LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n+  }\n+\n+  private boolean writeResultToFile(MsckInfo msckInfo, CheckResult result, List<String> repairOutput,\n+      long partitionExpirySeconds) {\n+    boolean success = true;\n+    BufferedWriter resultOut = null;\n     try {\n-      loggedInUser = UserGroupInformation.getLoginUser();\n+      Path resFile = new Path(msckInfo.getResFile());\n+      FileSystem fs = resFile.getFileSystem(getConf());\n+      resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n+\n+      boolean firstWritten = false;\n+      firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n+        \"Tables not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n+        \"Tables missing on filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n+        \"Partitions not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n+        \"Partitions missing from filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n+        \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n+      // sorting to stabilize qfile output (msck_repair_drop.q)\n+      Collections.sort(repairOutput);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc1NDA0MA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450754040", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDk3MA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjY2Mw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447052663", "bodyText": "error level?", "author": "deniskuzZ", "createdAt": "2020-06-29T15:20:16Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n+    LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n+    LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n+    LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n+    LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n+  }\n+\n+  private boolean writeResultToFile(MsckInfo msckInfo, CheckResult result, List<String> repairOutput,\n+      long partitionExpirySeconds) {\n+    boolean success = true;\n+    BufferedWriter resultOut = null;\n     try {\n-      loggedInUser = UserGroupInformation.getLoginUser();\n+      Path resFile = new Path(msckInfo.getResFile());\n+      FileSystem fs = resFile.getFileSystem(getConf());\n+      resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n+\n+      boolean firstWritten = false;\n+      firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n+        \"Tables not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n+        \"Tables missing on filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n+        \"Partitions not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n+        \"Partitions missing from filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n+        \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n+      // sorting to stabilize qfile output (msck_repair_drop.q)\n+      Collections.sort(repairOutput);\n+      for (String rout : repairOutput) {\n+        if (firstWritten) {\n+          resultOut.write(terminator);\n+        } else {\n+          firstWritten = true;\n+        }\n+        resultOut.write(rout);\n+      }\n     } catch (IOException e) {\n-      LOG.warn(\"Unable to get logged in user via UGI. err: {}\", e.getMessage());\n+      LOG.warn(\"Failed to save metacheck output: \", e);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc1NDMyOA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450754328", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:07:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjY2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MzYyMQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447053621", "bodyText": "why not use  try-with-resources?", "author": "deniskuzZ", "createdAt": "2020-06-29T15:21:38Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n+    LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n+    LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n+    LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n+    LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n+  }\n+\n+  private boolean writeResultToFile(MsckInfo msckInfo, CheckResult result, List<String> repairOutput,\n+      long partitionExpirySeconds) {\n+    boolean success = true;\n+    BufferedWriter resultOut = null;\n     try {\n-      loggedInUser = UserGroupInformation.getLoginUser();\n+      Path resFile = new Path(msckInfo.getResFile());\n+      FileSystem fs = resFile.getFileSystem(getConf());\n+      resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n+\n+      boolean firstWritten = false;\n+      firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n+        \"Tables not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n+        \"Tables missing on filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n+        \"Partitions not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n+        \"Partitions missing from filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n+        \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n+      // sorting to stabilize qfile output (msck_repair_drop.q)\n+      Collections.sort(repairOutput);\n+      for (String rout : repairOutput) {\n+        if (firstWritten) {\n+          resultOut.write(terminator);\n+        } else {\n+          firstWritten = true;\n+        }\n+        resultOut.write(rout);\n+      }\n     } catch (IOException e) {\n-      LOG.warn(\"Unable to get logged in user via UGI. err: {}\", e.getMessage());\n+      LOG.warn(\"Failed to save metacheck output: \", e);\n+      success = false;\n+    } finally {\n+      if (resultOut != null) {", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc1NjgyMg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450756822", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:12:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MzYyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2MjE3MQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447462171", "bodyText": "could we move error messages to final variables (constants)?", "author": "deniskuzZ", "createdAt": "2020-06-30T07:16:49Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n+    LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n+    LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n+    LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n+    LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n+  }\n+\n+  private boolean writeResultToFile(MsckInfo msckInfo, CheckResult result, List<String> repairOutput,\n+      long partitionExpirySeconds) {\n+    boolean success = true;\n+    BufferedWriter resultOut = null;\n     try {\n-      loggedInUser = UserGroupInformation.getLoginUser();\n+      Path resFile = new Path(msckInfo.getResFile());\n+      FileSystem fs = resFile.getFileSystem(getConf());\n+      resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n+\n+      boolean firstWritten = false;\n+      firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n+        \"Tables not in metastore:\", resultOut, firstWritten);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc2MDE2Mw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450760163", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:18:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2MjE3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2ODU0NQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447468545", "bodyText": "what if we would like to ship FS deltas to the backup HMS due to some replication issue? could you please elaborate what issues do you see in increasing writeId in HMS to match FS?", "author": "deniskuzZ", "createdAt": "2020-06-30T07:28:06Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -229,102 +239,168 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n+          if (result.getMaxWriteId() > 0) {\n+            if (txnId < 0) {\n+              // We need the txnId to check against even if we didn't do the locking\n+              txnId = getMsc().openTxn(getUserName());\n+            }\n+\n+            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n+                table.getDbName(), table.getTableName(), txnId);\n+          }\n+        }\n       }\n       success = true;\n     } catch (Exception e) {\n       LOG.warn(\"Failed to run metacheck: \", e);\n       success = false;\n-      ret = 1;\n     } finally {\n-      if (msckInfo.getResFile() != null) {\n-        BufferedWriter resultOut = null;\n-        try {\n-          Path resFile = new Path(msckInfo.getResFile());\n-          FileSystem fs = resFile.getFileSystem(getConf());\n-          resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n-\n-          boolean firstWritten = false;\n-          firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n-            \"Tables not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n-            \"Tables missing on filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n-            \"Partitions not in metastore:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n-            \"Partitions missing from filesystem:\", resultOut, firstWritten);\n-          firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n-            \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n-          // sorting to stabilize qfile output (msck_repair_drop.q)\n-          Collections.sort(repairOutput);\n-          for (String rout : repairOutput) {\n-            if (firstWritten) {\n-              resultOut.write(terminator);\n-            } else {\n-              firstWritten = true;\n-            }\n-            resultOut.write(rout);\n-          }\n-        } catch (IOException e) {\n-          LOG.warn(\"Failed to save metacheck output: \", e);\n-          ret = 1;\n-        } finally {\n-          if (resultOut != null) {\n-            try {\n-              resultOut.close();\n-            } catch (IOException e) {\n-              LOG.warn(\"Failed to close output file: \", e);\n-              ret = 1;\n-            }\n-          }\n+      if (result!=null) {\n+        logResult(result);\n+        if (msckInfo.getResFile() != null) {\n+          success = writeResultToFile(msckInfo, result, repairOutput, partitionExpirySeconds) && success;\n         }\n       }\n \n-      LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n-      LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n-      LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n-      LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n-      LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n-      if (acquireLock && txnId > 0) {\n-          if (success) {\n-            try {\n-              LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n-              getMsc().commitTxn(txnId);\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          } else {\n-            try {\n-              LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n-              getMsc().abortTxns(Lists.newArrayList(txnId));\n-            } catch (Exception e) {\n-              LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n-              ret = 1;\n-            }\n-          }\n+      if (txnId > 0) {\n+        success = closeTxn(qualifiedTableName, success, txnId) && success;\n       }\n       if (getMsc() != null) {\n         getMsc().close();\n         msc = null;\n       }\n     }\n+    return success ? 0 : 1;\n+  }\n \n+  private boolean closeTxn(String qualifiedTableName, boolean success, long txnId) {\n+    boolean ret = true;\n+    if (success) {\n+      try {\n+        LOG.info(\"txnId: {} succeeded. Committing..\", txnId);\n+        getMsc().commitTxn(txnId);\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while committing txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    } else {\n+      try {\n+        LOG.info(\"txnId: {} failed. Aborting..\", txnId);\n+        getMsc().abortTxns(Lists.newArrayList(txnId));\n+      } catch (Exception e) {\n+        LOG.warn(\"Error while aborting txnId: {} for table: {}\", txnId, qualifiedTableName, e);\n+        ret = false;\n+      }\n+    }\n     return ret;\n   }\n \n-  private LockRequest createLockRequest(final String dbName, final String tableName) throws TException {\n-    UserGroupInformation loggedInUser = null;\n-    String username;\n+  private void logResult(CheckResult result) {\n+    LOG.info(\"Tables not in metastore: {}\", result.getTablesNotInMs());\n+    LOG.info(\"Tables missing on filesystem: {}\", result.getTablesNotOnFs());\n+    LOG.info(\"Partitions not in metastore: {}\", result.getPartitionsNotInMs());\n+    LOG.info(\"Partitions missing from filesystem: {}\", result.getPartitionsNotOnFs());\n+    LOG.info(\"Expired partitions: {}\", result.getExpiredPartitions());\n+  }\n+\n+  private boolean writeResultToFile(MsckInfo msckInfo, CheckResult result, List<String> repairOutput,\n+      long partitionExpirySeconds) {\n+    boolean success = true;\n+    BufferedWriter resultOut = null;\n     try {\n-      loggedInUser = UserGroupInformation.getLoginUser();\n+      Path resFile = new Path(msckInfo.getResFile());\n+      FileSystem fs = resFile.getFileSystem(getConf());\n+      resultOut = new BufferedWriter(new OutputStreamWriter(fs.create(resFile)));\n+\n+      boolean firstWritten = false;\n+      firstWritten |= writeMsckResult(result.getTablesNotInMs(),\n+        \"Tables not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getTablesNotOnFs(),\n+        \"Tables missing on filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotInMs(),\n+        \"Partitions not in metastore:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getPartitionsNotOnFs(),\n+        \"Partitions missing from filesystem:\", resultOut, firstWritten);\n+      firstWritten |= writeMsckResult(result.getExpiredPartitions(),\n+        \"Expired partitions (retention period: \" + partitionExpirySeconds + \"s) :\", resultOut, firstWritten);\n+      // sorting to stabilize qfile output (msck_repair_drop.q)\n+      Collections.sort(repairOutput);\n+      for (String rout : repairOutput) {\n+        if (firstWritten) {\n+          resultOut.write(terminator);\n+        } else {\n+          firstWritten = true;\n+        }\n+        resultOut.write(rout);\n+      }\n     } catch (IOException e) {\n-      LOG.warn(\"Unable to get logged in user via UGI. err: {}\", e.getMessage());\n+      LOG.warn(\"Failed to save metacheck output: \", e);\n+      success = false;\n+    } finally {\n+      if (resultOut != null) {\n+        try {\n+          resultOut.close();\n+        } catch (IOException e) {\n+          LOG.warn(\"Failed to close output file: \", e);\n+          success = false;\n+        }\n+      }\n     }\n-    if (loggedInUser == null) {\n-      username = System.getProperty(\"user.name\");\n-    } else {\n-      username = loggedInUser.getShortUserName();\n+    return success;\n+  }\n+\n+  /**\n+   * When we add new partitions to a transactional table, we have check the writeIds.\n+   * For every newly added partitions, we read the maximum writeId form the directory structure\n+   * and compare it to the maximum allocated writeId in the metastore.\n+   * If the metastore has never allocated any were are good, the use case would be initialize a table with\n+   * existing data. The HMS will be initialized with the maximum writeId. The system will handle every delta directory\n+   * as committed ones.\n+   * If the writeId is higher in the metastore we can still accept the data, the use case would be after some dataloss\n+   * some older data backup was used. The system would able to read the old data.\n+   * If however the writeId in the new partition is greater than the maximum allocated in the HMS\n+   * we must raise an error. The writedId in the HMS should be increased to match the writeIds in the data files,\n+   * but it would most likely cause a lot of problem since the transactional data would become inconsistent\n+   * between the HMS and the filesystem.\n+   * Further more we need to check for the visibilityTransactionIds written by the compaction.\n+   * If we have a higher visibilityId in the directory structure than the current transactionid we need to set\n+   * the transactionId sequence higher in the HMS so the next reads may read the content of the\n+   * compacted base/delta folders.\n+   * @param partsNotInMs partitions only in the FileSystem\n+   * @param dbName database name\n+   * @param tableName table name\n+   * @param txnId actual transactionId\n+   */\n+  private void validateAndAddMaxTxnIdAndWriteId(Set<CheckResult.PartitionResult> partsNotInMs, String dbName,\n+      String tableName, long txnId) throws TException {\n+    long maxWriteIdOnFilesystem =\n+        partsNotInMs.stream().map(CheckResult.PartitionResult::getMaxWriteId).max(Long::compareTo).orElse(0L);\n+    long maxVisibilityTxnId =\n+        partsNotInMs.stream().map(CheckResult.PartitionResult::getMaxTxnId).max(Long::compareTo).orElse(0L);\n+    validateAndAddMaxTxnIdAndWriteId(maxWriteIdOnFilesystem, maxVisibilityTxnId, dbName, tableName, txnId);\n+  }\n+\n+  private void validateAndAddMaxTxnIdAndWriteId(long maxWriteIdOnFilesystem, long maxVisibilityTxnId, String dbName,\n+      String tableName, long txnId) throws TException {\n+    long maxAllocatedWriteId = getMsc().getMaxAllocatedWriteId(dbName, tableName);\n+    if (maxAllocatedWriteId > 0 && maxWriteIdOnFilesystem > maxAllocatedWriteId) {", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgwMTk1OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450801959", "bodyText": "My problem here is, that I don't see how we will end up in a consistent state in that case, when the filesystem will have deltas with writeId-s that are not in the hms. We would start using higher writeIds and the next transactions will start to read those deltas, but how will the compaction work, if there are no transaction information for those writes? who will clean those folders? I think it would become a mess.", "author": "pvargacl", "createdAt": "2020-07-07T11:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ2ODU0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -239,16 +245,14 @@ public int repair(MsckInfo msckInfo) {\n             throw new MetastoreException(e);\n           }\n         }\n-        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table)) {\n-          if (result.getMaxWriteId() > 0) {\n-            if (txnId < 0) {\n-              // We need the txnId to check against even if we didn't do the locking\n-              txnId = getMsc().openTxn(getUserName());\n-            }\n-\n-            validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(),\n-                table.getDbName(), table.getTableName(), txnId);\n+        if (transactionalTable && !MetaStoreServerUtils.isPartitioned(table) && result.getMaxWriteId() > 0) {\n+          if (txnId < 0) {\n+            // We need the txnId to check against even if we didn't do the locking\n+            txnId = getMsc().openTxn(getUserName());\n           }\n+\n+          validateAndAddMaxTxnIdAndWriteId(result.getMaxWriteId(), result.getMaxTxnId(), table.getDbName(),\n+              table.getTableName(), txnId);\n         }\n       }\n       success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3Mjk4OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447472989", "bodyText": "why  not to log content of addMsgs", "author": "deniskuzZ", "createdAt": "2020-06-30T07:36:08Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "diffHunk": "@@ -383,6 +475,7 @@ public Void execute(int size) throws MetastoreException {\n               partsToAdd.add(partition);\n               lastBatch.add(part);\n               addMsgs.add(String.format(addMsgFormat, part.getPartitionName()));\n+              LOG.debug(String.format(addMsgFormat, part.getPartitionName()));", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc2NTkwNQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450765905", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:29:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3Mjk4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\nindex cc757ac261..900806a9b7 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java\n\n@@ -474,8 +472,9 @@ public Void execute(int size) throws MetastoreException {\n               partition.setWriteId(table.getWriteId());\n               partsToAdd.add(partition);\n               lastBatch.add(part);\n-              addMsgs.add(String.format(addMsgFormat, part.getPartitionName()));\n-              LOG.debug(String.format(addMsgFormat, part.getPartitionName()));\n+              String msg = String.format(addMsgFormat, part.getPartitionName());\n+              addMsgs.add(msg);\n+              LOG.debug(msg);\n               currentBatchSize--;\n             }\n             metastoreClient.add_partitions(partsToAdd, true, false);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3NjA4OQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447476089", "bodyText": "minor: i would probably create EnumMap for SEED_FN, and use proper one based on db type.", "author": "deniskuzZ", "createdAt": "2020-06-30T07:41:09Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java", "diffHunk": "@@ -313,6 +313,41 @@ private static void resetTxnSequence(Connection conn, Statement stmt) throws SQL\n     }\n   }\n \n+  /**\n+   * Restarts the txnId sequence with the given seed value.\n+   * It is the responsibility of the caller to not set the sequence backward.\n+   * @param conn database connection\n+   * @param stmt sql statement\n+   * @param seedTxnId the seed value for the sequence\n+   * @throws SQLException ex\n+   */\n+  public static void seedTxnSequence(Connection conn, Statement stmt, long seedTxnId) throws SQLException {\n+    String dbProduct = conn.getMetaData().getDatabaseProductName();\n+    DatabaseProduct databaseProduct = determineDatabaseProduct(dbProduct);\n+    switch (databaseProduct) {\n+\n+    case DERBY:", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc3Nzc1OA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450777758", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T10:53:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3NjA4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java\nindex 75f3a1dc1e..8b5b4e072a 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java\n\n@@ -324,28 +333,7 @@ private static void resetTxnSequence(Connection conn, Statement stmt) throws SQL\n   public static void seedTxnSequence(Connection conn, Statement stmt, long seedTxnId) throws SQLException {\n     String dbProduct = conn.getMetaData().getDatabaseProductName();\n     DatabaseProduct databaseProduct = determineDatabaseProduct(dbProduct);\n-    switch (databaseProduct) {\n-\n-    case DERBY:\n-      stmt.execute(\"ALTER TABLE \\\"TXNS\\\" ALTER \\\"TXN_ID\\\" RESTART WITH \" + seedTxnId);\n-      break;\n-    case MYSQL:\n-      stmt.execute(\"ALTER TABLE \\\"TXNS\\\" AUTO_INCREMENT=\" + seedTxnId);\n-      break;\n-    case POSTGRES:\n-      stmt.execute(\"ALTER SEQUENCE \\\"TXNS_TXN_ID_seq\\\" RESTART WITH \" + seedTxnId);\n-      break;\n-    case ORACLE:\n-      stmt.execute(\n-          \"ALTER TABLE \\\"TXNS\\\" MODIFY \\\"TXN_ID\\\" GENERATED BY DEFAULT AS IDENTITY (START WITH \" + seedTxnId + \")\");\n-      break;\n-    case SQLSERVER:\n-      stmt.execute(\"DBCC CHECKIDENT ('txns', RESEED, \" + seedTxnId + \")\");\n-      break;\n-    case OTHER:\n-    default:\n-      break;\n-    }\n+    stmt.execute(String.format(DB_SEED_FN.get(databaseProduct), seedTxnId));\n   }\n \n   private static boolean truncateTable(Connection conn, Statement stmt, String name) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3NzUyMg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447477522", "bodyText": "should we have a query constant?", "author": "deniskuzZ", "createdAt": "2020-06-30T07:43:40Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2015,8 +2019,49 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds\n       return allocateTableWriteIds(rqst);\n     }\n   }\n+\n+  @Override\n+  public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedTableWriteIdRequest rqst) throws MetaException {\n+    String dbName = rqst.getDbName();\n+    String tableName = rqst.getTableName();\n+    try {\n+      Connection dbConn = null;\n+      PreparedStatement pStmt = null;\n+      ResultSet rs = null;\n+      try {\n+        lockInternal();\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        List<String> params = Arrays.asList(dbName, tableName);\n+        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4NTczOA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450785738", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T11:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3NzUyMg=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2029,13 +2032,11 @@ public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedT\n       PreparedStatement pStmt = null;\n       ResultSet rs = null;\n       try {\n-        lockInternal();\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        List<String> params = Arrays.asList(dbName, tableName);\n-        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n-        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n-        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n-            quoteString(tableName));\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID,\n+            Arrays.asList(dbName, tableName));\n+        LOG.debug(\"Going to execute query <\" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll(\"\\\\?\", \"{}\") + \">\",\n+            quoteString(dbName), quoteString(tableName));\n         rs = pStmt.executeQuery();\n         // If there is no record, we never allocated anything\n         long maxWriteId = 0l;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3ODkxMA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447478910", "bodyText": "what to rollback, you have select here?", "author": "deniskuzZ", "createdAt": "2020-06-30T07:46:00Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2015,8 +2019,49 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds\n       return allocateTableWriteIds(rqst);\n     }\n   }\n+\n+  @Override\n+  public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedTableWriteIdRequest rqst) throws MetaException {\n+    String dbName = rqst.getDbName();\n+    String tableName = rqst.getTableName();\n+    try {\n+      Connection dbConn = null;\n+      PreparedStatement pStmt = null;\n+      ResultSet rs = null;\n+      try {\n+        lockInternal();\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        List<String> params = Arrays.asList(dbName, tableName);\n+        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n+        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n+            quoteString(tableName));\n+        rs = pStmt.executeQuery();\n+        // If there is no record, we never allocated anything\n+        long maxWriteId = 0l;\n+        if (rs.next()) {\n+          // The row contains the nextId not the previously allocated\n+          maxWriteId = rs.getLong(1) - 1;\n+        }\n+        return new MaxAllocatedTableWriteIdResponse(maxWriteId);\n+      } catch (SQLException e) {\n+        LOG.error(\n+            \"Exception during reading the max allocated writeId for dbName={}, tableName={}. Will retry if possible.\",\n+            dbName, tableName, e);\n+        rollbackDBConn(dbConn);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4NTc5Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450785796", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T11:09:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ3ODkxMA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2029,13 +2032,11 @@ public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedT\n       PreparedStatement pStmt = null;\n       ResultSet rs = null;\n       try {\n-        lockInternal();\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        List<String> params = Arrays.asList(dbName, tableName);\n-        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n-        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n-        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n-            quoteString(tableName));\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID,\n+            Arrays.asList(dbName, tableName));\n+        LOG.debug(\"Going to execute query <\" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll(\"\\\\?\", \"{}\") + \">\",\n+            quoteString(dbName), quoteString(tableName));\n         rs = pStmt.executeQuery();\n         // If there is no record, we never allocated anything\n         long maxWriteId = 0l;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MDQxNQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447480415", "bodyText": "lockInternal is required for Derby to simulate S4U, why use here? unlockInternal is not needed as well", "author": "deniskuzZ", "createdAt": "2020-06-30T07:48:30Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2015,8 +2019,49 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds\n       return allocateTableWriteIds(rqst);\n     }\n   }\n+\n+  @Override\n+  public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedTableWriteIdRequest rqst) throws MetaException {\n+    String dbName = rqst.getDbName();\n+    String tableName = rqst.getTableName();\n+    try {\n+      Connection dbConn = null;\n+      PreparedStatement pStmt = null;\n+      ResultSet rs = null;\n+      try {\n+        lockInternal();", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4NTg2Mw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450785863", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T11:10:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MDQxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2029,13 +2032,11 @@ public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedT\n       PreparedStatement pStmt = null;\n       ResultSet rs = null;\n       try {\n-        lockInternal();\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        List<String> params = Arrays.asList(dbName, tableName);\n-        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n-        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n-        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n-            quoteString(tableName));\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID,\n+            Arrays.asList(dbName, tableName));\n+        LOG.debug(\"Going to execute query <\" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll(\"\\\\?\", \"{}\") + \">\",\n+            quoteString(dbName), quoteString(tableName));\n         rs = pStmt.executeQuery();\n         // If there is no record, we never allocated anything\n         long maxWriteId = 0l;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MjA1Mg==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447482052", "bodyText": "minor: i would use try-with-resources instead of doing explicit management", "author": "deniskuzZ", "createdAt": "2020-06-30T07:50:51Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2015,8 +2019,49 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds\n       return allocateTableWriteIds(rqst);\n     }\n   }\n+\n+  @Override\n+  public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedTableWriteIdRequest rqst) throws MetaException {\n+    String dbName = rqst.getDbName();\n+    String tableName = rqst.getTableName();\n+    try {\n+      Connection dbConn = null;\n+      PreparedStatement pStmt = null;\n+      ResultSet rs = null;\n+      try {\n+        lockInternal();\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4NjkxNw==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450786917", "bodyText": "most of Txnhandler uses this pattern, instead of using three nested try-with for dbconn, statement and resultset", "author": "pvargacl", "createdAt": "2020-07-07T11:12:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MjA1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2029,13 +2032,11 @@ public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedT\n       PreparedStatement pStmt = null;\n       ResultSet rs = null;\n       try {\n-        lockInternal();\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        List<String> params = Arrays.asList(dbName, tableName);\n-        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n-        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n-        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n-            quoteString(tableName));\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID,\n+            Arrays.asList(dbName, tableName));\n+        LOG.debug(\"Going to execute query <\" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll(\"\\\\?\", \"{}\") + \">\",\n+            quoteString(dbName), quoteString(tableName));\n         rs = pStmt.executeQuery();\n         // If there is no record, we never allocated anything\n         long maxWriteId = 0l;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MzY5MQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447483691", "bodyText": "minor: you can simply pass params as  Arrays.asList(rqst.getDbName(), rqst.getTableName()) instead of using so many local vars", "author": "deniskuzZ", "createdAt": "2020-06-30T07:53:33Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2015,8 +2019,49 @@ public AllocateTableWriteIdsResponse allocateTableWriteIds(AllocateTableWriteIds\n       return allocateTableWriteIds(rqst);\n     }\n   }\n+\n+  @Override\n+  public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedTableWriteIdRequest rqst) throws MetaException {\n+    String dbName = rqst.getDbName();\n+    String tableName = rqst.getTableName();\n+    try {\n+      Connection dbConn = null;\n+      PreparedStatement pStmt = null;\n+      ResultSet rs = null;\n+      try {\n+        lockInternal();\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        List<String> params = Arrays.asList(dbName, tableName);\n+        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4Nzg0NA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450787844", "bodyText": "fixed", "author": "pvargacl", "createdAt": "2020-07-07T11:14:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4MzY5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2029,13 +2032,11 @@ public MaxAllocatedTableWriteIdResponse getMaxAllocatedTableWrited(MaxAllocatedT\n       PreparedStatement pStmt = null;\n       ResultSet rs = null;\n       try {\n-        lockInternal();\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        List<String> params = Arrays.asList(dbName, tableName);\n-        String query = \"SELECT \\\"NWI_NEXT\\\" FROM \\\"NEXT_WRITE_ID\\\" WHERE \\\"NWI_DATABASE\\\" = ? AND \\\"NWI_TABLE\\\" = ?\";\n-        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, query, params);\n-        LOG.debug(\"Going to execute query <\" + query.replaceAll(\"\\\\?\", \"{}\") + \">\", quoteString(dbName),\n-            quoteString(tableName));\n+        pStmt = sqlGenerator.prepareStmtWithParameters(dbConn, SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID,\n+            Arrays.asList(dbName, tableName));\n+        LOG.debug(\"Going to execute query <\" + SELECT_NWI_NEXT_FROM_NEXT_WRITE_ID.replaceAll(\"\\\\?\", \"{}\") + \">\",\n+            quoteString(dbName), quoteString(tableName));\n         rs = pStmt.executeQuery();\n         // If there is no record, we never allocated anything\n         long maxWriteId = 0l;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4NDIwMA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447484200", "bodyText": "not sure why is it used here", "author": "deniskuzZ", "createdAt": "2020-06-30T07:54:24Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2032,28 +2077,61 @@ public void seedWriteIdOnAcidConversion(InitializeTableWriteIdsRequest rqst)\n         // The initial value for write id should be 1 and hence we add 1 with number of write ids\n         // allocated here\n         String s = \"INSERT INTO \\\"NEXT_WRITE_ID\\\" (\\\"NWI_DATABASE\\\", \\\"NWI_TABLE\\\", \\\"NWI_NEXT\\\") VALUES (?, ?, \"\n-                + Long.toString(rqst.getSeeWriteId() + 1) + \")\";\n-        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, Arrays.asList(rqst.getDbName(), rqst.getTblName()));\n+                + Long.toString(rqst.getSeedWriteId() + 1) + \")\";\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, Arrays.asList(rqst.getDbName(), rqst.getTableName()));\n         LOG.debug(\"Going to execute insert <\" + s.replaceAll(\"\\\\?\", \"{}\") + \">\",\n-                quoteString(rqst.getDbName()), quoteString(rqst.getTblName()));\n+                quoteString(rqst.getDbName()), quoteString(rqst.getTableName()));\n         pst.execute();\n         LOG.debug(\"Going to commit\");\n         dbConn.commit();\n       } catch (SQLException e) {\n-        LOG.debug(\"Going to rollback\");\n         rollbackDBConn(dbConn);\n-        checkRetryable(dbConn, e, \"seedWriteIdOnAcidConversion(\" + rqst + \")\");\n-        throw new MetaException(\"Unable to update transaction database \"\n-            + StringUtils.stringifyException(e));\n+        checkRetryable(dbConn, e, \"seedWriteId(\" + rqst + \")\");\n+        throw new MetaException(\"Unable to update transaction database \" + StringUtils.stringifyException(e));\n       } finally {\n         close(null, pst, dbConn);\n         unlockInternal();", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc4OTA3NQ==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450789075", "bodyText": "fixed, the unique key on NEXT_WRITE_ID is enough", "author": "pvargacl", "createdAt": "2020-07-07T11:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzQ4NDIwMA=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2090,7 +2088,6 @@ public void seedWriteId(SeedTableWriteIdsRequest rqst)\n         throw new MetaException(\"Unable to update transaction database \" + StringUtils.stringifyException(e));\n       } finally {\n         close(null, pst, dbConn);\n-        unlockInternal();\n       }\n     } catch (RetryException e) {\n       seedWriteId(rqst);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUwOTA2Ng==", "url": "https://github.com/apache/hive/pull/1087#discussion_r447509066", "bodyText": "not quite understand this if condition. You have check in validateAndAddMaxTxnIdAndWriteId() if there are already some write ids registered in HMS and we try to do repair - throw exception. Could it be possible due to lack of locking that when we calculate the write ids there is nothing in HMS,  however when we try to seed - some transaction generates a new write id - would it cause some dataloss problems or other issues?", "author": "deniskuzZ", "createdAt": "2020-06-30T08:33:10Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -2032,28 +2077,61 @@ public void seedWriteIdOnAcidConversion(InitializeTableWriteIdsRequest rqst)\n         // The initial value for write id should be 1 and hence we add 1 with number of write ids\n         // allocated here\n         String s = \"INSERT INTO \\\"NEXT_WRITE_ID\\\" (\\\"NWI_DATABASE\\\", \\\"NWI_TABLE\\\", \\\"NWI_NEXT\\\") VALUES (?, ?, \"\n-                + Long.toString(rqst.getSeeWriteId() + 1) + \")\";\n-        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, Arrays.asList(rqst.getDbName(), rqst.getTblName()));\n+                + Long.toString(rqst.getSeedWriteId() + 1) + \")\";\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, Arrays.asList(rqst.getDbName(), rqst.getTableName()));\n         LOG.debug(\"Going to execute insert <\" + s.replaceAll(\"\\\\?\", \"{}\") + \">\",\n-                quoteString(rqst.getDbName()), quoteString(rqst.getTblName()));\n+                quoteString(rqst.getDbName()), quoteString(rqst.getTableName()));\n         pst.execute();\n         LOG.debug(\"Going to commit\");\n         dbConn.commit();\n       } catch (SQLException e) {\n-        LOG.debug(\"Going to rollback\");\n         rollbackDBConn(dbConn);\n-        checkRetryable(dbConn, e, \"seedWriteIdOnAcidConversion(\" + rqst + \")\");\n-        throw new MetaException(\"Unable to update transaction database \"\n-            + StringUtils.stringifyException(e));\n+        checkRetryable(dbConn, e, \"seedWriteId(\" + rqst + \")\");\n+        throw new MetaException(\"Unable to update transaction database \" + StringUtils.stringifyException(e));\n       } finally {\n         close(null, pst, dbConn);\n         unlockInternal();\n       }\n     } catch (RetryException e) {\n-      seedWriteIdOnAcidConversion(rqst);\n+      seedWriteId(rqst);\n     }\n+  }\n+\n+  @Override\n+  public void seedTxnId(SeedTxnIdRequest rqst) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      try {\n+        lockInternal();\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+        /*\n+         * Locking the txnLock an exclusive way, we do not want to set the txnId backward accidentally\n+         * if there are concurrent open transactions\n+         */\n+        acquireTxnLock(stmt, false);\n+        long highWaterMark = getHighWaterMark(stmt);\n+        if (highWaterMark >= rqst.getSeedTxnId()) {", "originalCommit": "82f8bf293d33b807d33600014018d534bd2b9071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDc5MzgzMA==", "url": "https://github.com/apache/hive/pull/1087#discussion_r450793830", "bodyText": "This is not about the writeIds, it is about the txnId. If you have a data from a database where there were high amount of transaction and the compaction run on the table, you will have some high txnId in the visibilityTxnId in the name of the compacted folder.\nIf you then move this data to a cluster with less transaction (ex. a test cluster) and you run the msck repair, you have to skip the txnId forward so the next query will read the compacted folder. Here the race condition is, that somehow the txnId sequence gets ahead of you between the check and the seeding the value, in that case we throw this exception to not to set the sequence backward. Anyway, in this case if you run the msck repair again it will succeed, since the txnid will be high enough.\nThe writeId race condition won't cause a problem I think, since if some other transaction allocated the first writeId the seedWriteId will fail on the unique constraint on the table", "author": "pvargacl", "createdAt": "2020-07-07T11:26:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUwOTA2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "9febcbde8a454d5e971287c970df6f9ec27a087c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex d19d530dc1..e9423d4e06 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -2090,7 +2088,6 @@ public void seedWriteId(SeedTableWriteIdsRequest rqst)\n         throw new MetaException(\"Unable to update transaction database \" + StringUtils.stringifyException(e));\n       } finally {\n         close(null, pst, dbConn);\n-        unlockInternal();\n       }\n     } catch (RetryException e) {\n       seedWriteId(rqst);\n"}}, {"oid": "9febcbde8a454d5e971287c970df6f9ec27a087c", "url": "https://github.com/apache/hive/commit/9febcbde8a454d5e971287c970df6f9ec27a087c", "message": "Fix code review issues", "committedDate": "2020-07-07T11:45:59Z", "type": "commit"}, {"oid": "76622a7c506e863b89a1cffd72cf37a32e746e77", "url": "https://github.com/apache/hive/commit/76622a7c506e863b89a1cffd72cf37a32e746e77", "message": "Test whitespace fix", "committedDate": "2020-07-09T06:55:26Z", "type": "commit"}, {"oid": "7f1629edbacb81ccb58459fcf17d991182472240", "url": "https://github.com/apache/hive/commit/7f1629edbacb81ccb58459fcf17d991182472240", "message": "Merge remote-tracking branch 'origin/master' into HIVE-23671-msck-repair-2\n\n# Conflicts:\n#\tql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckOperation.java\n#\tql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java\n#\tstandalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreChecker.java\n#\tstandalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/Msck.java", "committedDate": "2020-07-14T11:34:19Z", "type": "commit"}]}