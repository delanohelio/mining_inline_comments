{"pr_number": 1548, "pr_title": "HIVE-21052: Make sure transactions get cleaned if they are aborted be\u2026", "pr_createdAt": "2020-10-02T13:44:47Z", "pr_url": "https://github.com/apache/hive/pull/1548", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzQ1OQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137459", "bodyText": "Here we need some correction in expected values, as these tests were written wrt to branch-3.1 in which we don't support minor compaction for MM tables.", "author": "vpnvishv", "createdAt": "2020-10-03T10:47:12Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NTEyNw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499545127", "bodyText": "fixed", "author": "deniskuzZ", "createdAt": "2020-10-05T11:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzQ1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\nindex ff71fdc1b8..2a7149c882 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n\n@@ -2160,8 +2162,8 @@ public void testMmTableAbortWithCompaction() throws Exception {\n     runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n     // The worker should remove the subdir for aborted transaction\n     runWorker(hiveConf);\n-    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n-    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDir(0, Table.MMTBL.toString(), \"\");\n     // 5. Run Cleaner. Shouldn't impact anything.\n     runCleaner(hiveConf);\n     // 6. Run initiator remove aborted entry from TXNS table\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzYyNg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137626", "bodyText": "Here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "author": "vpnvishv", "createdAt": "2020-10-03T10:49:59Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NDU3OQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499544579", "bodyText": "fixed, turned off StatsOptimizer", "author": "deniskuzZ", "createdAt": "2020-10-05T11:58:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzYyNg=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\nindex ff71fdc1b8..2a7149c882 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n\n@@ -2160,8 +2162,8 @@ public void testMmTableAbortWithCompaction() throws Exception {\n     runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n     // The worker should remove the subdir for aborted transaction\n     runWorker(hiveConf);\n-    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n-    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDir(0, Table.MMTBL.toString(), \"\");\n     // 5. Run Cleaner. Shouldn't impact anything.\n     runCleaner(hiveConf);\n     // 6. Run initiator remove aborted entry from TXNS table\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzcxMg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137712", "bodyText": "Again, here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "author": "vpnvishv", "createdAt": "2020-10-03T10:51:18Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData1);\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData1), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 4. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 5. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 6. Perform a MAJOR compaction, expectation is it should remove aborted delta dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 7. Run one more Major compaction this should not have any affect\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    runCleaner(hiveConf);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+\n+  @Test\n+  public void testFullACIDAbortWithMinorMajorCompaction() throws Exception {\n+    // 1. Insert some rows into acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.ACIDTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 2 delta directories.\n+    verifyDeltaDirAndResult(2, Table.ACIDTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+    // 3. insert few more rows in acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.ACIDTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NDYxNw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499544617", "bodyText": "fixed, turned of StatsOptimizer", "author": "deniskuzZ", "createdAt": "2020-10-05T11:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzcxMg=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\nindex ff71fdc1b8..2a7149c882 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java\n\n@@ -2160,8 +2162,8 @@ public void testMmTableAbortWithCompaction() throws Exception {\n     runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n     // The worker should remove the subdir for aborted transaction\n     runWorker(hiveConf);\n-    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n-    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDir(0, Table.MMTBL.toString(), \"\");\n     // 5. Run Cleaner. Shouldn't impact anything.\n     runCleaner(hiveConf);\n     // 6. Run initiator remove aborted entry from TXNS table\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499388356", "bodyText": "This is going issue many filesystem listing on a table with many partitions, that is going to be very slow on S3. I think you should consider changing this logic to be similar to getHdfsDirSnapshots that would do 1 recursive listing, iterate all the files and collect the deltas that needs to be deleted and delete them at the end (possible concurrently)", "author": "pvargacl", "createdAt": "2020-10-05T07:21:30Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyMDUxOA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499620518", "bodyText": "getHdfsDirSnapshots does the same recursive listing, isn't it?\nRemoteIterator<LocatedFileStatus> itr = fs.listFiles(path, true);\nwhile (itr.hasNext()) {", "author": "deniskuzZ", "createdAt": "2020-10-05T14:00:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDE0Mg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754142", "bodyText": "changed to use getHdfsDirSnapshots,\n@pvargacl do you know. if i should access cached data somehow?", "author": "deniskuzZ", "createdAt": "2020-10-05T17:20:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex b294dc8b38..9ca5111721 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2850,83 +2853,26 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n \n   /**\n    * Look for delta directories matching the list of writeIds and deletes them.\n-   * @param rootPartition root partition to look for the delta directories\n+   * @param root path to look for the delta directories\n    * @param conf configuration\n    * @param writeIds list of writeIds to look for in the delta directories\n    * @return list of deleted directories.\n    * @throws IOException\n    */\n-  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+  public static List<Path> deleteDeltaDirectories(Path root, Configuration conf, Set<Long> writeIds)\n       throws IOException {\n-    FileSystem fs = rootPartition.getFileSystem(conf);\n+    FileSystem fs = root.getFileSystem(conf);\n+    Map<Path, HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(fs, root);\n \n-    PathFilter filter = (p) -> {\n-      String name = p.getName();\n-      for (Long wId : writeIds) {\n-        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n-          return true;\n-        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    List<FileStatus> deleted = new ArrayList<>();\n-    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n-    return deleted;\n-  }\n-\n-  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n-      throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n-\n-    while (it.hasNext()) {\n-      FileStatus fStatus = it.next();\n-      if (fStatus.isDirectory()) {\n-        if (filter.accept(fStatus.getPath())) {\n-          fs.delete(fStatus.getPath(), true);\n-          deleted.add(fStatus);\n-        } else {\n-          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n-          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n-            fs.delete(fStatus.getPath(), false);\n-            deleted.add(fStatus);\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n-    return !it.hasNext();\n-  }\n-\n-  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n-      throws IOException {\n-    try {\n-      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n-    } catch (Throwable t) {\n-      return HdfsUtils.listLocatedStatusIterator(fs, path, filter);\n-    }\n-  }\n-\n-  private static final class ToFileStatusIterator implements RemoteIterator<FileStatus> {\n-    private final RemoteIterator<HdfsFileStatusWithId> it;\n-\n-    ToFileStatusIterator(RemoteIterator<HdfsFileStatusWithId> it) {\n-      this.it = it;\n-    }\n-\n-    @Override\n-    public boolean hasNext() throws IOException {\n-      return it.hasNext();\n-    }\n+    List<Path> deleted = hdfsDirSnapshots.values().stream()\n+      .map(HdfsDirSnapshot::getPath)\n+      .filter(p -> writeIds.contains(extractWriteId(p)))\n+      .collect(Collectors.toList());\n \n-    @Override\n-    public FileStatus next() throws IOException {\n-      return it.next().getFileStatus();\n+    for (Path toDelete : deleted) {\n+      fs.delete(toDelete, true);\n     }\n+    return deleted;\n   }\n \n   private static boolean needsLock(Entity entity) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499399709", "bodyText": "Why the contains \"=\", are we checking for a partition where the user named the column exactly like a valid delta dir? I don't think we should support that", "author": "pvargacl", "createdAt": "2020-10-05T07:44:45Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NjM5Nw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499656397", "bodyText": "I was also wondering the same, as this code was there in the earlier patches so I have just kept it. We can remove this.", "author": "vpnvishv", "createdAt": "2020-10-05T14:49:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDQyMw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754423", "bodyText": "changed, included delete_delta as well", "author": "deniskuzZ", "createdAt": "2020-10-05T17:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex b294dc8b38..9ca5111721 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2850,83 +2853,26 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n \n   /**\n    * Look for delta directories matching the list of writeIds and deletes them.\n-   * @param rootPartition root partition to look for the delta directories\n+   * @param root path to look for the delta directories\n    * @param conf configuration\n    * @param writeIds list of writeIds to look for in the delta directories\n    * @return list of deleted directories.\n    * @throws IOException\n    */\n-  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+  public static List<Path> deleteDeltaDirectories(Path root, Configuration conf, Set<Long> writeIds)\n       throws IOException {\n-    FileSystem fs = rootPartition.getFileSystem(conf);\n+    FileSystem fs = root.getFileSystem(conf);\n+    Map<Path, HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(fs, root);\n \n-    PathFilter filter = (p) -> {\n-      String name = p.getName();\n-      for (Long wId : writeIds) {\n-        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n-          return true;\n-        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    List<FileStatus> deleted = new ArrayList<>();\n-    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n-    return deleted;\n-  }\n-\n-  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n-      throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n-\n-    while (it.hasNext()) {\n-      FileStatus fStatus = it.next();\n-      if (fStatus.isDirectory()) {\n-        if (filter.accept(fStatus.getPath())) {\n-          fs.delete(fStatus.getPath(), true);\n-          deleted.add(fStatus);\n-        } else {\n-          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n-          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n-            fs.delete(fStatus.getPath(), false);\n-            deleted.add(fStatus);\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n-    return !it.hasNext();\n-  }\n-\n-  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n-      throws IOException {\n-    try {\n-      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n-    } catch (Throwable t) {\n-      return HdfsUtils.listLocatedStatusIterator(fs, path, filter);\n-    }\n-  }\n-\n-  private static final class ToFileStatusIterator implements RemoteIterator<FileStatus> {\n-    private final RemoteIterator<HdfsFileStatusWithId> it;\n-\n-    ToFileStatusIterator(RemoteIterator<HdfsFileStatusWithId> it) {\n-      this.it = it;\n-    }\n-\n-    @Override\n-    public boolean hasNext() throws IOException {\n-      return it.hasNext();\n-    }\n+    List<Path> deleted = hdfsDirSnapshots.values().stream()\n+      .map(HdfsDirSnapshot::getPath)\n+      .filter(p -> writeIds.contains(extractWriteId(p)))\n+      .collect(Collectors.toList());\n \n-    @Override\n-    public FileStatus next() throws IOException {\n-      return it.next().getFileStatus();\n+    for (Path toDelete : deleted) {\n+      fs.delete(toDelete, true);\n     }\n+    return deleted;\n   }\n \n   private static boolean needsLock(Entity entity) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499402826", "bodyText": "I am wondering are we covering all the use cases here? Is it possible that this dynamic part query was writing to an existing partition with existing older writes and a compaction was running before we managed to delete the aborted delta? I think in this case sadly, we still going to read the aborted data as valid. Could you add a test case to check if it is indeed a problem or not? (I do not have an idea for a solution...)", "author": "pvargacl", "createdAt": "2020-10-05T07:50:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyMzg2NA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499623864", "bodyText": "Why would it read the aborted data as valid if txn is in still in aborted state?", "author": "deniskuzZ", "createdAt": "2020-10-05T14:05:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NTMwNg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499655306", "bodyText": "@pvargacl Sorry I may be missing something here, but with this change, how can compactor read the data of an aborted delta. It should be in the aborted list right, due to this dummy p type entry in TXN_COMPONENTS?", "author": "vpnvishv", "createdAt": "2020-10-05T14:47:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY3MTg0OA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499671848", "bodyText": "You are right, I got confused, the p entry will solve this.", "author": "pvargacl", "createdAt": "2020-10-05T15:10:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex b294dc8b38..9ca5111721 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2850,83 +2853,26 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n \n   /**\n    * Look for delta directories matching the list of writeIds and deletes them.\n-   * @param rootPartition root partition to look for the delta directories\n+   * @param root path to look for the delta directories\n    * @param conf configuration\n    * @param writeIds list of writeIds to look for in the delta directories\n    * @return list of deleted directories.\n    * @throws IOException\n    */\n-  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+  public static List<Path> deleteDeltaDirectories(Path root, Configuration conf, Set<Long> writeIds)\n       throws IOException {\n-    FileSystem fs = rootPartition.getFileSystem(conf);\n+    FileSystem fs = root.getFileSystem(conf);\n+    Map<Path, HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(fs, root);\n \n-    PathFilter filter = (p) -> {\n-      String name = p.getName();\n-      for (Long wId : writeIds) {\n-        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n-          return true;\n-        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    List<FileStatus> deleted = new ArrayList<>();\n-    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n-    return deleted;\n-  }\n-\n-  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n-      throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n-\n-    while (it.hasNext()) {\n-      FileStatus fStatus = it.next();\n-      if (fStatus.isDirectory()) {\n-        if (filter.accept(fStatus.getPath())) {\n-          fs.delete(fStatus.getPath(), true);\n-          deleted.add(fStatus);\n-        } else {\n-          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n-          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n-            fs.delete(fStatus.getPath(), false);\n-            deleted.add(fStatus);\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n-    return !it.hasNext();\n-  }\n-\n-  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n-      throws IOException {\n-    try {\n-      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n-    } catch (Throwable t) {\n-      return HdfsUtils.listLocatedStatusIterator(fs, path, filter);\n-    }\n-  }\n-\n-  private static final class ToFileStatusIterator implements RemoteIterator<FileStatus> {\n-    private final RemoteIterator<HdfsFileStatusWithId> it;\n-\n-    ToFileStatusIterator(RemoteIterator<HdfsFileStatusWithId> it) {\n-      this.it = it;\n-    }\n-\n-    @Override\n-    public boolean hasNext() throws IOException {\n-      return it.hasNext();\n-    }\n+    List<Path> deleted = hdfsDirSnapshots.values().stream()\n+      .map(HdfsDirSnapshot::getPath)\n+      .filter(p -> writeIds.contains(extractWriteId(p)))\n+      .collect(Collectors.toList());\n \n-    @Override\n-    public FileStatus next() throws IOException {\n-      return it.next().getFileStatus();\n+    for (Path toDelete : deleted) {\n+      fs.delete(toDelete, true);\n     }\n+    return deleted;\n   }\n \n   private static boolean needsLock(Entity entity) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499404466", "bodyText": "Are we doing this to delete newly created partitions if there are no other writes? Is this ok, what if we found a valid empty partition that is registered in the HMS? We should not delete that. I think this can be skipped all together, the empty partition dir will not bother anybody", "author": "pvargacl", "createdAt": "2020-10-05T07:52:58Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzOTgzMQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499639831", "bodyText": "agree, that would simplify re-use of getHdfsDirSnapshots", "author": "deniskuzZ", "createdAt": "2020-10-05T14:27:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0OTc0OA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499749748", "bodyText": "partitions are not removed in HMS", "author": "deniskuzZ", "createdAt": "2020-10-05T17:12:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex b294dc8b38..9ca5111721 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2850,83 +2853,26 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n \n   /**\n    * Look for delta directories matching the list of writeIds and deletes them.\n-   * @param rootPartition root partition to look for the delta directories\n+   * @param root path to look for the delta directories\n    * @param conf configuration\n    * @param writeIds list of writeIds to look for in the delta directories\n    * @return list of deleted directories.\n    * @throws IOException\n    */\n-  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+  public static List<Path> deleteDeltaDirectories(Path root, Configuration conf, Set<Long> writeIds)\n       throws IOException {\n-    FileSystem fs = rootPartition.getFileSystem(conf);\n+    FileSystem fs = root.getFileSystem(conf);\n+    Map<Path, HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(fs, root);\n \n-    PathFilter filter = (p) -> {\n-      String name = p.getName();\n-      for (Long wId : writeIds) {\n-        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n-          return true;\n-        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    List<FileStatus> deleted = new ArrayList<>();\n-    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n-    return deleted;\n-  }\n-\n-  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n-      throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n-\n-    while (it.hasNext()) {\n-      FileStatus fStatus = it.next();\n-      if (fStatus.isDirectory()) {\n-        if (filter.accept(fStatus.getPath())) {\n-          fs.delete(fStatus.getPath(), true);\n-          deleted.add(fStatus);\n-        } else {\n-          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n-          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n-            fs.delete(fStatus.getPath(), false);\n-            deleted.add(fStatus);\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n-    return !it.hasNext();\n-  }\n-\n-  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n-      throws IOException {\n-    try {\n-      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n-    } catch (Throwable t) {\n-      return HdfsUtils.listLocatedStatusIterator(fs, path, filter);\n-    }\n-  }\n-\n-  private static final class ToFileStatusIterator implements RemoteIterator<FileStatus> {\n-    private final RemoteIterator<HdfsFileStatusWithId> it;\n-\n-    ToFileStatusIterator(RemoteIterator<HdfsFileStatusWithId> it) {\n-      this.it = it;\n-    }\n-\n-    @Override\n-    public boolean hasNext() throws IOException {\n-      return it.hasNext();\n-    }\n+    List<Path> deleted = hdfsDirSnapshots.values().stream()\n+      .map(HdfsDirSnapshot::getPath)\n+      .filter(p -> writeIds.contains(extractWriteId(p)))\n+      .collect(Collectors.toList());\n \n-    @Override\n-    public FileStatus next() throws IOException {\n-      return it.next().getFileStatus();\n+    for (Path toDelete : deleted) {\n+      fs.delete(toDelete, true);\n     }\n+    return deleted;\n   }\n \n   private static boolean needsLock(Entity entity) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNTcyOA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499405728", "bodyText": "This should be similar to tryListLocatedHdfsStatus don't catch all Throwable. And maybe add all this to the HdfsUtils class", "author": "pvargacl", "createdAt": "2020-10-05T07:55:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n+            fs.delete(fStatus.getPath(), false);\n+            deleted.add(fStatus);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n+    return !it.hasNext();\n+  }\n+\n+  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n+      throws IOException {\n+    try {\n+      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n+    } catch (Throwable t) {", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDk0Ng==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754946", "bodyText": "removed it", "author": "deniskuzZ", "createdAt": "2020-10-05T17:21:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNTcyOA=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex b294dc8b38..9ca5111721 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2850,83 +2853,26 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n \n   /**\n    * Look for delta directories matching the list of writeIds and deletes them.\n-   * @param rootPartition root partition to look for the delta directories\n+   * @param root path to look for the delta directories\n    * @param conf configuration\n    * @param writeIds list of writeIds to look for in the delta directories\n    * @return list of deleted directories.\n    * @throws IOException\n    */\n-  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+  public static List<Path> deleteDeltaDirectories(Path root, Configuration conf, Set<Long> writeIds)\n       throws IOException {\n-    FileSystem fs = rootPartition.getFileSystem(conf);\n+    FileSystem fs = root.getFileSystem(conf);\n+    Map<Path, HdfsDirSnapshot> hdfsDirSnapshots = AcidUtils.getHdfsDirSnapshots(fs, root);\n \n-    PathFilter filter = (p) -> {\n-      String name = p.getName();\n-      for (Long wId : writeIds) {\n-        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n-          return true;\n-        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    };\n-    List<FileStatus> deleted = new ArrayList<>();\n-    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n-    return deleted;\n-  }\n-\n-  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n-      throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n-\n-    while (it.hasNext()) {\n-      FileStatus fStatus = it.next();\n-      if (fStatus.isDirectory()) {\n-        if (filter.accept(fStatus.getPath())) {\n-          fs.delete(fStatus.getPath(), true);\n-          deleted.add(fStatus);\n-        } else {\n-          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n-          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n-            fs.delete(fStatus.getPath(), false);\n-            deleted.add(fStatus);\n-          }\n-        }\n-      }\n-    }\n-  }\n-\n-  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n-    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n-    return !it.hasNext();\n-  }\n-\n-  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n-      throws IOException {\n-    try {\n-      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n-    } catch (Throwable t) {\n-      return HdfsUtils.listLocatedStatusIterator(fs, path, filter);\n-    }\n-  }\n-\n-  private static final class ToFileStatusIterator implements RemoteIterator<FileStatus> {\n-    private final RemoteIterator<HdfsFileStatusWithId> it;\n-\n-    ToFileStatusIterator(RemoteIterator<HdfsFileStatusWithId> it) {\n-      this.it = it;\n-    }\n-\n-    @Override\n-    public boolean hasNext() throws IOException {\n-      return it.hasNext();\n-    }\n+    List<Path> deleted = hdfsDirSnapshots.values().stream()\n+      .map(HdfsDirSnapshot::getPath)\n+      .filter(p -> writeIds.contains(extractWriteId(p)))\n+      .collect(Collectors.toList());\n \n-    @Override\n-    public FileStatus next() throws IOException {\n-      return it.next().getFileStatus();\n+    for (Path toDelete : deleted) {\n+      fs.delete(toDelete, true);\n     }\n+    return deleted;\n   }\n \n   private static boolean needsLock(Entity entity) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499449636", "bodyText": "Two questions here:\n\nIn the original Jira there was discussion about not allowing concurrent cleanings of the same stuff (partition / table). Should we worry about this?\nThe slow cleanAborted will clog the executor service, we should do something about this, either in this patch, or follow up something like https://issues.apache.org/jira/browse/HIVE-21150 immediately after this.", "author": "pvargacl", "createdAt": "2020-10-05T09:07:54Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -97,9 +100,9 @@ public void run() {\n           long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n           LOG.info(\"Cleaning based on min open txn id: \" + minOpenTxnId);\n           List<CompletableFuture> cleanerList = new ArrayList<>();\n-          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+          for (CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n             cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n-                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY0Njg2MQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501646861", "bodyText": "In original patch Map<String, NonReentrantReadWriteLock> tableLock = new ConcurrentHashMap<>() was used to prevent  a concurrent p-clean (where the whole table will be scanned). I think, that is resolved by grouping p-cleans and recording list of writeIds that needs to be removed:\nhttps://github.com/apache/hive/pull/1548/files#diff-9cf3ae764b7a33b568a984d695aff837R328\n@vpnvishv is that correct? Also we do not allow concurrent Cleaners, their execution is mutexed.\n\n\nwas related to the following issue based on Map<String, NonReentrantReadWriteLock> tableLock = new ConcurrentHashMap<>()  design:\n\"Suppose you have p-type clean on table T that is running (i.e. has the Write lock) and you have 30 different partition clean requests (in T).  The 30 per partition cleans will get blocked but they will tie up every thread in the pool while they are blocked, right?  If so, no other clean (on any other table) will actually make progress until the p-type on T is done.\"\n\n\nYes, it's still the case that we'll have to wait for all tasks to complete and if there is one long-running task, we won't be able to submit new ones. However not sure if it's a critical issue. I think, we can address it in a separate jira.", "author": "deniskuzZ", "createdAt": "2020-10-08T11:29:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzA0Mw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501723043", "bodyText": "I agree, it can be addressed in a follow up Jira.", "author": "pvargacl", "createdAt": "2020-10-08T13:32:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499457494", "bodyText": "@vpnvishv Why do we do this here? I understand we can, but why don't we let the Cleaner to delete the files? This just makes the compactor slower. Do we have a functionality reason for this?\nAfter this change it will run in CompactorMR and in MMQueryCompactors, but not in normal QueryCompactors?", "author": "pvargacl", "createdAt": "2020-10-05T09:20:22Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "diffHunk": "@@ -237,6 +237,7 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n+    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0OTkxOQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499649919", "bodyText": "@pvargacl You are right, this is not required, as now compactor run in a transaction and the cleaner has validTxnList with aborted bits set. This we have added wrt to Hive-3, in which cleaner doesn't have aborted bits set, as we create validWriteIdList for cleaner based on the highestWriteId.", "author": "vpnvishv", "createdAt": "2020-10-05T14:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgyNzY0NA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499827644", "bodyText": "removed", "author": "deniskuzZ", "createdAt": "2020-10-05T19:38:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA=="}], "type": "inlineReview", "revised_code": {"commit": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java\nindex fcb62ed84f..fd710af201 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java\n\n@@ -237,7 +237,6 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n-    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);\n \n     List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n     int maxDeltasToHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499475796", "bodyText": "I might be mistaken here, but does this mean, that if we have many \"normal\" aborted txn and 1 aborted dynpart txn, we will not initiate a normal compaction until the dynpart stuff is not cleaned up? Is this ok, shouldn't we doing both?", "author": "pvargacl", "createdAt": "2020-10-05T09:49:37Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "originalCommit": "8fe2b5fb8eda000403f43180e5706e3212b87e13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0NzMzNA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r499747334", "bodyText": "why is that? aborted dynPart is just a special case that would be handled separately (IS_DP=1).", "author": "deniskuzZ", "createdAt": "2020-10-05T17:07:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMjc5NA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501732794", "bodyText": "Previously if you had aborted txn above threshold this would generate a \"normal\" compaction that would clean up everything. However now if you have one dynpart aborted the type will be CLEAN_ABORTED that will only clean the writeids belonging to p-type records and leave everything else. This will delay the normal cleaning. I am not sure that is a problem or not.", "author": "pvargacl", "createdAt": "2020-10-08T13:45:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg4MzQyNw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501883427", "bodyText": "I still don't follow. Aborted txn check is done per db/table/partition, so if you have db1/tbl1/p1/type=NOT_DP and db1/tbl1/null/type=DP - that should generate 2 entries in potential compactions.", "author": "deniskuzZ", "createdAt": "2020-10-08T17:16:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg4NDY2MA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501884660", "bodyText": "oh, sorry, I only considered time based threshold for DYN_PART", "author": "deniskuzZ", "createdAt": "2020-10-08T17:19:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "fb1ec01be132a00b148b20c9615fb0d4815aecf4", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\nindex 15f10b9447..d9e030a39e 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\n\n@@ -107,13 +109,13 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n-            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"\n-            + \"FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" \"\n-            + \"WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" \"\n-            + \"GROUP BY \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\" \"\n-            + (checkAbortedTimeThreshold ? \"\" : \" HAVING COUNT(*) > \" + abortedThreshold);\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \" +\n+          \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" IN (\" +\n+              OperationType.DP_INSERT + \",\" + OperationType.DP_UPDATE + \") THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \" +\n+          \"FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" \" +\n+          \"   WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" \" +\n+          \"GROUP BY \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\" \" +\n+              (checkAbortedTimeThreshold ? \"\" : \" HAVING COUNT(*) > \" + abortedThreshold);\n \n         LOG.debug(\"Going to execute query <\" + sCheckAborted + \">\");\n         rs = stmt.executeQuery(sCheckAborted);\n"}}, {"oid": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "url": "https://github.com/apache/hive/commit/bb7cca22d4dd91c5df48e217749a78e6e334fd17", "message": "added test for dynamic partition update", "committedDate": "2020-10-06T18:09:35Z", "type": "forcePushed"}, {"oid": "eb221eb3cce4deed190438903dc73dee55e5fe30", "url": "https://github.com/apache/hive/commit/eb221eb3cce4deed190438903dc73dee55e5fe30", "message": "added test for dynamic partition update", "committedDate": "2020-10-06T21:41:18Z", "type": "forcePushed"}, {"oid": "ee285982ccc8decdb3ebbbd8bad9ac2604235143", "url": "https://github.com/apache/hive/commit/ee285982ccc8decdb3ebbbd8bad9ac2604235143", "message": "compilation failure due to wildcard import fix", "committedDate": "2020-10-07T07:52:05Z", "type": "forcePushed"}, {"oid": "0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "url": "https://github.com/apache/hive/commit/0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "message": "refactored txn_components cleanup", "committedDate": "2020-10-07T20:40:02Z", "type": "forcePushed"}, {"oid": "7307bd104f19233dee04c3196caf731c43b788f0", "url": "https://github.com/apache/hive/commit/7307bd104f19233dee04c3196caf731c43b788f0", "message": "refactored txn_components cleanup", "committedDate": "2020-10-07T20:48:03Z", "type": "forcePushed"}, {"oid": "72c80a49b896b1fbda719188e9fbaf55b0e2887d", "url": "https://github.com/apache/hive/commit/72c80a49b896b1fbda719188e9fbaf55b0e2887d", "message": "refactored txn_components cleanup", "committedDate": "2020-10-07T21:40:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxMzQ2Mw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501713463", "bodyText": "I think this assert is quit misleading. I might be wrong but the recursive listing skips empty directories, and actually this new version of cleaning will keep the partition directories (it should) and only delete the delta dirs and files.", "author": "pvargacl", "createdAt": "2020-10-08T13:19:13Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));", "originalCommit": "db551f03b7fec385c2291f88a7032a4f64966384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTg3MQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731871", "bodyText": "yes, recursive listing skips empty directories - that's was done intentionally. changed the comment", "author": "deniskuzZ", "createdAt": "2020-10-08T13:44:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxMzQ2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "chunk": "diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\nindex 92da3055e1..58dc7d1df6 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n\n@@ -857,6 +857,54 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfter2ndCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.commitTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfter1stCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.commitTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n   @Test\n   public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n     String dbName = \"default\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxNTMxMQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501715311", "bodyText": "this comment is copied I guess", "author": "pvargacl", "createdAt": "2020-10-08T13:21:40Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions", "originalCommit": "db551f03b7fec385c2291f88a7032a4f64966384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTgwMg==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731802", "bodyText": "fixed", "author": "deniskuzZ", "createdAt": "2020-10-08T13:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxNTMxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "chunk": "diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\nindex 92da3055e1..58dc7d1df6 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n\n@@ -857,6 +857,54 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfter2ndCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.commitTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfter1stCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.commitTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n   @Test\n   public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n     String dbName = \"default\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxODI0OA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501718248", "bodyText": "Could you add two more test with batch size 2. first batch writes to p1 and p2 and commits, second batch writes to p2 and p3 and aborts. And one with the aborted / committed order changed.", "author": "pvargacl", "createdAt": "2020-10-08T13:25:35Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 1, count);\n+\n+    connection.commitTransaction();\n+\n+    // After commit the row should have been deleted\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+  }\n+\n+  @Test\n+  public void testCleanAbortAndMinorCompact() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+    connection.abortTransaction();\n+\n+    executeStatementOnDriver(\"insert into \" + tblName + \" partition (a) values (1, '1')\", driver);\n+    executeStatementOnDriver(\"delete from \" + tblName + \" where b=1\", driver);\n+\n+    conf.setIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD, 0);\n+    runInitiator(conf);\n+    runWorker(conf);\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+    // Cleaning should happen in threads concurrently for the minor compaction and the clean abort one.\n+    runCleaner(conf);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+  }\n+\n+  private HiveStreamingConnection prepareTableAndConnection(String dbName, String tblName, int batchSize) throws Exception {", "originalCommit": "db551f03b7fec385c2291f88a7032a4f64966384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1NzEyMw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501757123", "bodyText": "added", "author": "deniskuzZ", "createdAt": "2020-10-08T14:16:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxODI0OA=="}], "type": "inlineReview", "revised_code": {"commit": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "chunk": "diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\nindex 92da3055e1..58dc7d1df6 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java\n\n@@ -857,6 +857,54 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfter2ndCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.commitTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfter1stCommitAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 2);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 2);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.commitTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName, false);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n   @Test\n   public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n     String dbName = \"default\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501723853", "bodyText": "Shouldn't you mark the compaction failed or cleaned?", "author": "pvargacl", "createdAt": "2020-10-08T13:33:30Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -232,6 +240,51 @@ public Object run() throws Exception {\n   private static String idWatermark(CompactionInfo ci) {\n     return \" id=\" + ci.id;\n   }\n+\n+  private void cleanAborted(CompactionInfo ci) throws MetaException {\n+    if (ci.writeIds == null || ci.writeIds.size() == 0) {\n+      LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");", "originalCommit": "db551f03b7fec385c2291f88a7032a4f64966384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTY5Nw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731697", "bodyText": "yep, good catch!", "author": "deniskuzZ", "createdAt": "2020-10-08T13:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2NjYyMA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501766620", "bodyText": "fixed", "author": "deniskuzZ", "createdAt": "2020-10-08T14:28:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java\nindex c889710d93..a732346b46 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java\n\n@@ -244,6 +244,7 @@ private static String idWatermark(CompactionInfo ci) {\n   private void cleanAborted(CompactionInfo ci) throws MetaException {\n     if (ci.writeIds == null || ci.writeIds.size() == 0) {\n       LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");\n+      txnHandler.markCleaned(ci);\n       return;\n     }\n     LOG.info(\"Starting aborted cleaning for table \" + ci.getFullTableName()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczNzkyMw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r501737923", "bodyText": "cool", "author": "pvargacl", "createdAt": "2020-10-08T13:51:56Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,77 +436,56 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n+        List<String> queries = new ArrayList<>();\n+        Iterator<Long> writeIdsIter = null;\n+        List<Integer> counts = null;\n \n-        pStmt = dbConn.prepareStatement(s);\n-        paramCount = 1;\n-        pStmt.setString(paramCount++, info.dbname);\n-        pStmt.setString(paramCount++, info.tableName);\n-        if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +\n+          \"   SELECT \\\"TXN_ID\\\" FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \") \" +", "originalCommit": "db551f03b7fec385c2291f88a7032a4f64966384", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6e71901f2c713d39afa8a18f392348b4afd4be7c", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\nindex ae12f7ca5d..15f10b9447 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java\n\n@@ -436,56 +437,85 @@ public void markCleaned(CompactionInfo cinfo) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        List<String> queries = new ArrayList<>();\n-        Iterator<Long> writeIdsIter = null;\n-        List<Integer> counts = null;\n+        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n+            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n+        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n+        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n \n-        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +\n-          \"   SELECT \\\"TXN_ID\\\" FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \") \" +\n-          \"AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) {\n-          s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n+        pStmt = dbConn.prepareStatement(s);\n+        paramCount = 1;\n+        pStmt.setString(paramCount++, info.dbname);\n+        pStmt.setString(paramCount++, info.tableName);\n+        if(info.highestWriteId != 0) {\n+          pStmt.setLong(paramCount++, info.highestWriteId);\n         }\n         if (info.partName != null) {\n-          s += \" AND \\\"TC_PARTITION\\\" = ?\";\n+          pStmt.setString(paramCount++, info.partName);\n         }\n         if (info.writeIds != null && info.writeIds.size() > 0) {\n-          StringBuilder prefix = new StringBuilder(s).append(\" AND \");\n-          List<String> questions = Collections.nCopies(info.writeIds.size(), \"?\");\n-          counts = TxnUtils.buildQueryWithINClauseStrings(conf, queries, prefix, new StringBuilder(),\n-              questions, \"\\\"TC_WRITEID\\\"\", false, false);\n-          writeIdsIter = info.writeIds.iterator();\n-        } else {\n-          queries.add(s);\n+          String[] writeIds = info.writeIds.stream().map(String::valueOf).toArray(String[]::new);\n+          s += \" AND \\\"TC_WRITEID\\\" IN (\" + String.join(\",\", writeIds) + \")\";\n+        }\n+        LOG.debug(\"Going to execute update <\" + s + \">\");\n+        rs = pStmt.executeQuery();\n+        List<Long> txnids = new ArrayList<>();\n+        List<String> questions = new ArrayList<>();\n+        while (rs.next()) {\n+          long id = rs.getLong(1);\n+          txnids.add(id);\n+          questions.add(\"?\");\n         }\n+        // Remove entries from txn_components, as there may be aborted txn components\n+        if (txnids.size() > 0) {\n+          List<String> queries = new ArrayList<>();\n \n-        for (int i = 0; i < queries.size(); i++) {\n-          String query = queries.get(i);\n-          int writeIdCount = (counts != null) ? counts.get(i) : 0;\n+          // Prepare prefix and suffix\n+          StringBuilder prefix = new StringBuilder();\n+          StringBuilder suffix = new StringBuilder();\n \n-          LOG.debug(\"Going to execute update <\" + query + \">\");\n-          pStmt = dbConn.prepareStatement(query);\n-          paramCount = 1;\n+          prefix.append(\"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \");\n \n-          pStmt.setString(paramCount++, info.dbname);\n-          pStmt.setString(paramCount++, info.tableName);\n-          if (info.highestWriteId != 0) {\n-            pStmt.setLong(paramCount++, info.highestWriteId);\n-          }\n+          //because 1 txn may include different partitions/tables even in auto commit mode\n+          suffix.append(\" AND \\\"TC_DATABASE\\\" = ?\");\n+          suffix.append(\" AND \\\"TC_TABLE\\\" = ?\");\n           if (info.partName != null) {\n-            pStmt.setString(paramCount++, info.partName);\n+            suffix.append(\" AND \\\"TC_PARTITION\\\" = ?\");\n+          }\n+\n+          if (info.writeIds != null && info.writeIds.size() > 0) {\n+            String[] writeIds = info.writeIds.stream().map(String::valueOf).toArray(String[]::new);\n+            suffix.append(\" AND \\\"TC_WRITEID\\\" IN (\").append(String.join(\",\", writeIds)).append(\")\");\n           }\n-          for (int j = 0; j < writeIdCount; j++) {\n-            if (writeIdsIter.hasNext()) {\n-              pStmt.setLong(paramCount + j, writeIdsIter.next());\n+          // Populate the complete query with provided prefix and suffix\n+          List<Integer> counts = TxnUtils\n+              .buildQueryWithINClauseStrings(conf, queries, prefix, suffix, questions, \"\\\"TC_TXNID\\\"\",\n+                  true, false);\n+          int totalCount = 0;\n+          for (int i = 0; i < queries.size(); i++) {\n+            String query = queries.get(i);\n+            int insertCount = counts.get(i);\n+\n+            LOG.debug(\"Going to execute update <\" + query + \">\");\n+            pStmt = dbConn.prepareStatement(query);\n+            for (int j = 0; j < insertCount; j++) {\n+              pStmt.setLong(j + 1, txnids.get(totalCount + j));\n+            }\n+            totalCount += insertCount;\n+            paramCount = insertCount + 1;\n+            pStmt.setString(paramCount++, info.dbname);\n+            pStmt.setString(paramCount++, info.tableName);\n+            if (info.partName != null) {\n+              pStmt.setString(paramCount++, info.partName);\n             }\n+            int rc = pStmt.executeUpdate();\n+            LOG.debug(\"Removed \" + rc + \" records from txn_components\");\n+\n+            // Don't bother cleaning from the txns table.  A separate call will do that.  We don't\n+            // know here which txns still have components from other tables or partitions in the\n+            // table, so we don't know which ones we can and cannot clean.\n           }\n-          int rc = pStmt.executeUpdate();\n-          LOG.debug(\"Removed \" + rc + \" records from txn_components\");\n-          // Don't bother cleaning from the txns table.  A separate call will do that.  We don't\n-          // know here which txns still have components from other tables or partitions in the\n-          // table, so we don't know which ones we can and cannot clean.\n         }\n+\n         LOG.debug(\"Going to commit\");\n         dbConn.commit();\n       } catch (SQLException e) {\n"}}, {"oid": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "url": "https://github.com/apache/hive/commit/0d96a45094ca0ddfaf8e2334987255483efa92f4", "message": "added tests", "committedDate": "2020-10-08T19:56:31Z", "type": "forcePushed"}, {"oid": "6e71901f2c713d39afa8a18f392348b4afd4be7c", "url": "https://github.com/apache/hive/commit/6e71901f2c713d39afa8a18f392348b4afd4be7c", "message": "HIVE-21052: Make sure transactions get cleaned if they are aborted before addPartitions is called", "committedDate": "2020-10-12T13:23:20Z", "type": "commit"}, {"oid": "e9f9cd714d47eb8e66da712073045d76c04a5efc", "url": "https://github.com/apache/hive/commit/e9f9cd714d47eb8e66da712073045d76c04a5efc", "message": "fixed tests", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "75ce19fc7fe2b38e8ec6946da357fd85f7a2e2fb", "url": "https://github.com/apache/hive/commit/75ce19fc7fe2b38e8ec6946da357fd85f7a2e2fb", "message": "addressing comments", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "cce9f358d5ee56b6ad0fe2dda3bcdac500c65df5", "url": "https://github.com/apache/hive/commit/cce9f358d5ee56b6ad0fe2dda3bcdac500c65df5", "message": "removed cleanup from compactor", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "c821f4acf97fc46f70a50bc25cbe177238cb76b5", "url": "https://github.com/apache/hive/commit/c821f4acf97fc46f70a50bc25cbe177238cb76b5", "message": "restored aborted base dir cleanup in case of IOW, added test", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "3e567a93ceee36f922634fe591b8182a2e65f4e2", "url": "https://github.com/apache/hive/commit/3e567a93ceee36f922634fe591b8182a2e65f4e2", "message": "added test for dynamic partition update", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "3599350ffb042222cf857482b0d3eaffb52f1d92", "url": "https://github.com/apache/hive/commit/3599350ffb042222cf857482b0d3eaffb52f1d92", "message": "compilation failure due to wildcard import fix", "committedDate": "2020-10-12T13:23:27Z", "type": "commit"}, {"oid": "b7406a693e6963c8b3e25b69095ed9b57fa8b375", "url": "https://github.com/apache/hive/commit/b7406a693e6963c8b3e25b69095ed9b57fa8b375", "message": "ported tests from HIVE-3.1 pull request", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "fab03787c7d4b5d32b5984a10ccaf73def06385c", "url": "https://github.com/apache/hive/commit/fab03787c7d4b5d32b5984a10ccaf73def06385c", "message": "findReadyToClean optimization", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "7f662b0048bf073315c25c172b6423467263b59b", "url": "https://github.com/apache/hive/commit/7f662b0048bf073315c25c172b6423467263b59b", "message": "refactored txn_components cleanup", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "4be7152eb58563f230b9646da6c3daf3546f805b", "url": "https://github.com/apache/hive/commit/4be7152eb58563f230b9646da6c3daf3546f805b", "message": "fixed whether should replicate check", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "647dfa179d8e08ebfaecd3dd67ee000b1bc335cd", "url": "https://github.com/apache/hive/commit/647dfa179d8e08ebfaecd3dd67ee000b1bc335cd", "message": "addressing comments", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "ed9f748d77427241ded73b4defe078e70b4fe4e6", "url": "https://github.com/apache/hive/commit/ed9f748d77427241ded73b4defe078e70b4fe4e6", "message": "added tests", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "fb1ec01be132a00b148b20c9615fb0d4815aecf4", "url": "https://github.com/apache/hive/commit/fb1ec01be132a00b148b20c9615fb0d4815aecf4", "message": "master rebase fix", "committedDate": "2020-10-12T13:23:28Z", "type": "commit"}, {"oid": "68e53871d79094984d1092073297120d5a53297b", "url": "https://github.com/apache/hive/commit/68e53871d79094984d1092073297120d5a53297b", "message": "merge with snapshot check changes", "committedDate": "2020-10-12T13:38:33Z", "type": "commit"}, {"oid": "68e53871d79094984d1092073297120d5a53297b", "url": "https://github.com/apache/hive/commit/68e53871d79094984d1092073297120d5a53297b", "message": "merge with snapshot check changes", "committedDate": "2020-10-12T13:38:33Z", "type": "forcePushed"}, {"oid": "e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "url": "https://github.com/apache/hive/commit/e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "message": "fixed test", "committedDate": "2020-10-13T09:11:18Z", "type": "forcePushed"}, {"oid": "3a62c2a2b0141de1eae6cd18d104e205718c35a5", "url": "https://github.com/apache/hive/commit/3a62c2a2b0141de1eae6cd18d104e205718c35a5", "message": "fixed test", "committedDate": "2020-10-13T15:21:32Z", "type": "commit"}, {"oid": "3a62c2a2b0141de1eae6cd18d104e205718c35a5", "url": "https://github.com/apache/hive/commit/3a62c2a2b0141de1eae6cd18d104e205718c35a5", "message": "fixed test", "committedDate": "2020-10-13T15:21:32Z", "type": "forcePushed"}, {"oid": "fac26064dcc8f39748fb0d08fcab899174b8f3b8", "url": "https://github.com/apache/hive/commit/fac26064dcc8f39748fb0d08fcab899174b8f3b8", "message": "address Karen's comments", "committedDate": "2020-10-19T14:42:07Z", "type": "forcePushed"}, {"oid": "29d8a2343b7939d55453c542fd093d6403f10479", "url": "https://github.com/apache/hive/commit/29d8a2343b7939d55453c542fd093d6403f10479", "message": "address Karen's comments", "committedDate": "2020-10-19T14:44:57Z", "type": "forcePushed"}, {"oid": "3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "url": "https://github.com/apache/hive/commit/3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "message": "address Karen's comments", "committedDate": "2020-10-19T14:47:01Z", "type": "forcePushed"}, {"oid": "cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "url": "https://github.com/apache/hive/commit/cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "message": "address Karen's comments, recheck", "committedDate": "2020-10-19T17:55:54Z", "type": "commit"}, {"oid": "cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "url": "https://github.com/apache/hive/commit/cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "message": "address Karen's comments, recheck", "committedDate": "2020-10-19T17:55:54Z", "type": "forcePushed"}, {"oid": "c36860e8142bc117fafbb127797bf7de90697711", "url": "https://github.com/apache/hive/commit/c36860e8142bc117fafbb127797bf7de90697711", "message": "add partition filter", "committedDate": "2020-10-20T07:57:37Z", "type": "commit"}, {"oid": "f3006aa6464e83336fec309965b86f7904fd1db3", "url": "https://github.com/apache/hive/commit/f3006aa6464e83336fec309965b86f7904fd1db3", "message": "fixed txn_components cleanup query", "committedDate": "2020-10-20T08:05:25Z", "type": "commit"}, {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "url": "https://github.com/apache/hive/commit/d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "message": "partition condition fix", "committedDate": "2020-10-20T08:59:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MTQzMw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508641433", "bodyText": "This can be consolidated with most of isDynPartIngest in CompactionUtils", "author": "klcopp", "createdAt": "2020-10-20T15:54:35Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "diffHunk": "@@ -589,4 +593,9 @@ private void checkInterrupt() throws InterruptedException {\n       throw new InterruptedException(\"Compaction execution is interrupted\");\n     }\n   }\n-}\n+\n+  private static boolean isDynPartAbort(Table t, CompactionInfo ci) {", "originalCommit": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwOTk0NA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508809944", "bodyText": "those are actually 2 diff methods the only common part is the check for isDynPart. Also there is no CompactionUtils only CompactorUtil, that contains thread factory stuff.", "author": "deniskuzZ", "createdAt": "2020-10-20T20:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MTQzMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MjE5OQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508642199", "bodyText": "FYI MM tests are usually in TestTxnCommandsForMmTable.java but I don't really care about this", "author": "klcopp", "createdAt": "2020-10-20T15:55:13Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,24 +2129,601 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n-  private void verifyDirAndResult(int expectedDeltas) throws Exception {\n-    FileSystem fs = FileSystem.get(hiveConf);\n-    // Verify the content of subdirs\n-    FileStatus[] status = fs.listStatus(new Path(TEST_WAREHOUSE_DIR + \"/\" +\n-        (Table.MMTBL).toString().toLowerCase()), FileUtils.HIDDEN_FILES_PATH_FILTER);\n+  @Test\n+  public void testMmTableAbortWithCompaction() throws Exception {", "originalCommit": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NDU3MA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508644570", "bodyText": "Why was this changed?", "author": "klcopp", "createdAt": "2020-10-20T15:57:16Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -400,11 +389,11 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n           pStmt.setString(paramCount++, info.partName);\n         }\n         if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+          pStmt.setLong(paramCount, info.highestWriteId);", "originalCommit": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNDAzOQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508804039", "bodyText": "redundant post increment", "author": "deniskuzZ", "createdAt": "2020-10-20T20:03:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NDU3MA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NzU4Mw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508647583", "bodyText": "Any ideas about why this was here? Just curious", "author": "klcopp", "createdAt": "2020-10-20T16:00:26Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -134,9 +132,6 @@ public CompactionTxnHandler() {\n             response.add(info);\n           }\n         }\n-\n-        LOG.debug(\"Going to rollback\");\n-        dbConn.rollback();", "originalCommit": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNDA3MA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508804070", "bodyText": "no idea :)", "author": "deniskuzZ", "createdAt": "2020-10-20T20:03:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NzU4Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508708398", "bodyText": "This is just refactoring right? LGTM but can you make sure @pvary sees this as well?", "author": "klcopp", "createdAt": "2020-10-20T17:25:04Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "originalCommit": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNTE2Mw==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508805163", "bodyText": "this is an optimization that makes everything in 1 db request instead of 2 (select + delete)", "author": "deniskuzZ", "createdAt": "2020-10-20T20:05:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNzQ5OQ==", "url": "https://github.com/apache/hive/pull/1548#discussion_r508807499", "bodyText": "@pvary, could you please take a quick look? thanks!", "author": "deniskuzZ", "createdAt": "2020-10-20T20:10:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA1NDQzMA==", "url": "https://github.com/apache/hive/pull/1548#discussion_r509054430", "bodyText": "never mind, LGTM", "author": "klcopp", "createdAt": "2020-10-21T07:37:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}], "type": "inlineReview", "revised_code": null}]}