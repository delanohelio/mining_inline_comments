{"pr_number": 1251, "pr_title": "HIVE-23840: Use LLAP to get orc metadata", "pr_createdAt": "2020-07-14T09:48:45Z", "pr_url": "https://github.com/apache/hive/pull/1251", "timeline": [{"oid": "67a71b4a9953a3e897342bf7c1de39ba93bdfbfd", "url": "https://github.com/apache/hive/commit/67a71b4a9953a3e897342bf7c1de39ba93bdfbfd", "message": "HIVE-23840: Use LLAP to get orc metadata", "committedDate": "2020-07-14T09:07:10Z", "type": "commit"}, {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "url": "https://github.com/apache/hive/commit/c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "message": "Addendum", "committedDate": "2020-07-14T10:06:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454390429", "bodyText": "We could spare the deserialization of MapWork from JobConf here, if we pass the MapWork instance already present in LlapRecordReader to VectorizedOrcAcidRowBatchReader ctor. (Downside is that in turn we would need to adjust the other ctor's of VectorizedOrcAcidRowBatchReader too)", "author": "szlta", "createdAt": "2020-07-14T14:18:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -232,6 +250,17 @@ private VectorizedOrcAcidRowBatchReader(JobConf conf, OrcSplit orcSplit, Reporte\n \n     this.syntheticProps = orcSplit.getSyntheticAcidProps();\n \n+    if (LlapHiveUtils.isLlapMode(conf) && LlapProxy.isDaemon()\n+            && HiveConf.getBoolVar(conf, ConfVars.LLAP_TRACK_CACHE_USAGE))\n+    {\n+      MapWork mapWork = LlapHiveUtils.findMapWork(conf);", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjcyNw==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602727", "bodyText": "Good idea, done!", "author": "pvary", "createdAt": "2020-07-14T19:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "57c575ffe2cec83e44008f65574cce4b6711b0e2", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 1a145135ec..b26213e90e 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -253,7 +252,9 @@ private VectorizedOrcAcidRowBatchReader(JobConf conf, OrcSplit orcSplit, Reporte\n     if (LlapHiveUtils.isLlapMode(conf) && LlapProxy.isDaemon()\n             && HiveConf.getBoolVar(conf, ConfVars.LLAP_TRACK_CACHE_USAGE))\n     {\n-      MapWork mapWork = LlapHiveUtils.findMapWork(conf);\n+      if (mapWork == null) {\n+        mapWork = LlapHiveUtils.findMapWork(conf);\n+      }\n       PartitionDesc partitionDesc =\n           LlapHiveUtils.partitionDescForPath(orcSplit.getPath(), mapWork.getPathToPartitionInfo());\n       cacheTag = LlapHiveUtils.getDbAndTableNameForMetrics(orcSplit.getPath(), true, partitionDesc);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454393621", "bodyText": "Initialized to true on purpose for now? If not, I don't see it getting set to false.", "author": "szlta", "createdAt": "2020-07-14T14:22:30Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -129,6 +137,16 @@\n    */\n   private SearchArgument deleteEventSarg = null;\n \n+  /**\n+   * Cachetag associated with the Split\n+   */\n+  private final CacheTag cacheTag;\n+\n+  /**\n+   * Skip using Llap IO cache for checking delete_delta files if the configuration is not correct\n+   */\n+  private static boolean skipLlapCache = true;", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjkwNA==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602904", "bodyText": "That was a mistake. Corrected, and initialized as false", "author": "pvary", "createdAt": "2020-07-14T19:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "57c575ffe2cec83e44008f65574cce4b6711b0e2", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 1a145135ec..b26213e90e 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -145,7 +144,7 @@\n   /**\n    * Skip using Llap IO cache for checking delete_delta files if the configuration is not correct\n    */\n-  private static boolean skipLlapCache = true;\n+  private static boolean skipLlapCache = false;\n \n   //OrcInputFormat c'tor\n   VectorizedOrcAcidRowBatchReader(OrcSplit inputSplit, JobConf conf,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454403919", "bodyText": "nit: comment could be more verbose, like: Reader can be reused if it was created before: only for non-LLAP cache cases, otherwise we need to create it here", "author": "szlta", "createdAt": "2020-07-14T14:35:46Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1562,20 +1580,31 @@ public int compareTo(CompressedOwid other) {\n       try {\n         final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n         if (deleteDeltaDirs.length > 0) {\n+          FileSystem fs = orcSplit.getPath().getFileSystem(conf);\n+          AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n+              AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            FileSystem fs = deleteDeltaDir.getFileSystem(conf);\n+            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+              continue;\n+            }\n             Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n                 new OrcRawRecordMerger.Options().isCompacting(false), null);\n             for (Path deleteDeltaFile : deleteDeltaFiles) {\n               try {\n-                /**\n-                 * todo: we have OrcSplit.orcTail so we should be able to get stats from there\n-                 */\n-                Reader deleteDeltaReader = OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                if (deleteDeltaReader.getNumberOfRows() <= 0) {\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n                   continue; // just a safe check to ensure that we are not reading empty delete files.\n                 }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Create the reader if we got the OrcTail from cache", "originalCommit": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMzA0Mg==", "url": "https://github.com/apache/hive/pull/1251#discussion_r454603042", "bodyText": "Added more comment", "author": "pvary", "createdAt": "2020-07-14T19:48:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "57c575ffe2cec83e44008f65574cce4b6711b0e2", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 1a145135ec..b26213e90e 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -1592,7 +1588,7 @@ public int compareTo(CompressedOwid other) {\n                 new OrcRawRecordMerger.Options().isCompacting(false), null);\n             for (Path deleteDeltaFile : deleteDeltaFiles) {\n               try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n                 OrcTail orcTail = readerData.orcTail;\n                 if (orcTail.getFooter().getNumberOfRows() <= 0) {\n                   continue; // just a safe check to ensure that we are not reading empty delete files.\n"}}, {"oid": "57c575ffe2cec83e44008f65574cce4b6711b0e2", "url": "https://github.com/apache/hive/commit/57c575ffe2cec83e44008f65574cce4b6711b0e2", "message": "Addressing Adam's comments, and test failures", "committedDate": "2020-07-14T19:33:35Z", "type": "commit"}, {"oid": "328b2477910ec0a3e45355e6716338d2eace6143", "url": "https://github.com/apache/hive/commit/328b2477910ec0a3e45355e6716338d2eace6143", "message": "Fixed test failures - prepared the stuff for multi statement transactions.\nAlso fix some minor formatting/niceity issues", "committedDate": "2020-07-15T09:45:15Z", "type": "commit"}]}