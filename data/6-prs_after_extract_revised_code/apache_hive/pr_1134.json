{"pr_number": 1134, "pr_title": "HIVE-23703: Major QB compaction with multiple FileSinkOperators results in data loss and one original file", "pr_createdAt": "2020-06-17T07:59:15Z", "pr_url": "https://github.com/apache/hive/pull/1134", "timeline": [{"oid": "06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "url": "https://github.com/apache/hive/commit/06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "message": "HIVE-23703: Major QB compaction with multiple FileSinkOperators results in data loss and one original file", "committedDate": "2020-06-17T07:57:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MjIzNQ==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441382235", "bodyText": "Can we do this outside the process loop? Maybe saving some time, since this part of the code runs for every item, and this can cause performance degradation (I guess)", "author": "pvary", "createdAt": "2020-06-17T08:43:19Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java", "diffHunk": "@@ -113,7 +116,11 @@ private void processKeyValuePairs(Object key, Object value)\n \n       // store the orc configuration from the first file. All other files should\n       // match this configuration before merging else will not be merged\n-      if (outWriter == null) {\n+      int bucketId = 0;\n+      if (conf.getIsCompactionTable()) {", "originalCommit": "06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxNTk0NA==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441415944", "bodyText": "Ok.. as discussed this was an overreaction \ud83d\ude04", "author": "pvary", "createdAt": "2020-06-17T09:36:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MjIzNQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MzY0NA==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441383644", "bodyText": "Could you please add some clarification to the tables used in compaction, like technical/helper tables used in compaction.", "author": "deniskuzZ", "createdAt": "2020-06-17T08:45:20Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -334,6 +334,18 @@ public void initializeBucketPaths(int filesIdx, String taskId, boolean isNativeT\n         if (!isMmTable && !isDirectInsert) {\n           if (!bDynParts && !isSkewedStoredAsSubDirectories) {\n             finalPaths[filesIdx] = new Path(parent, taskWithExt);\n+            if (conf.isCompactionTable()) {\n+              // tables used in compaction are external and non-acid. We need to keep track of", "originalCommit": "06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxMDEwMg==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441510102", "bodyText": "Done", "author": "klcopp", "createdAt": "2020-06-17T12:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MzY0NA=="}], "type": "inlineReview", "revised_code": {"commit": "eff3cb725ee8bfdc272c63416eb023083ca13a8d", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\nindex ff064d0a60..023f7df3a9 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\n\n@@ -335,12 +335,12 @@ public void initializeBucketPaths(int filesIdx, String taskId, boolean isNativeT\n           if (!bDynParts && !isSkewedStoredAsSubDirectories) {\n             finalPaths[filesIdx] = new Path(parent, taskWithExt);\n             if (conf.isCompactionTable()) {\n-              // tables used in compaction are external and non-acid. We need to keep track of\n-              // the taskId to avoid overwrites in the case of multiple FileSinkOperators, and the\n-              // file names need to reflect the correct bucketId because the files will\n-              // eventually be placed in an acid table, and the OrcFileMergeOperator should not\n-              // merge data belonging to different buckets. Therefore during compaction, data\n-              // will be stored in the final directory like:\n+              // Helper tables used for compaction are external and non-acid. We need to keep\n+              // track of the taskId to avoid overwrites in the case of multiple\n+              // FileSinkOperators, and the file names need to reflect the correct bucketId\n+              // because the files will eventually be placed in an acid table, and the\n+              // OrcFileMergeOperator should not merge data belonging to different buckets.\n+              // Therefore during compaction, data will be stored in the final directory like:\n               // ${hive_staging_dir}/final_dir/taskid/bucketId\n               // For example, ${hive_staging dir}/-ext-10002/000000_0/bucket_00000\n               finalPaths[filesIdx] = new Path(finalPaths[filesIdx],\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MzcxMA==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441383710", "bodyText": "nit: I see this kind of formatting very rarely in Hive code", "author": "pvary", "createdAt": "2020-06-17T08:45:27Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "diffHunk": "@@ -4123,9 +4125,28 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,\n           }\n           throw new HiveException(e);\n         }\n-      } else {\n+      else {", "originalCommit": "06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxMDE3Mw==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441510173", "bodyText": "Typo, done.", "author": "klcopp", "createdAt": "2020-06-17T12:35:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MzcxMA=="}], "type": "inlineReview", "revised_code": {"commit": "eff3cb725ee8bfdc272c63416eb023083ca13a8d", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java\nindex 158dbc85fc..044ce5c91a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java\n\n@@ -4125,16 +4125,18 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,\n           }\n           throw new HiveException(e);\n         }\n-      else {\n+      } else {\n         files = new FileStatus[] {src};\n       }\n-      \n+\n       if (isCompactionTable) {\n-        // Compaction tables have a special layout after filesink: tmpdir/attemptid/bucketid.\n-        // We don't care about the attemptId anymore so just move the bucket files.\n+        // Helper tables used for query-based compaction have a special file structure after\n+        // filesink: tmpdir/attemptid/bucketid.\n+        // We don't care about the attemptId anymore and don't want it in the table's final\n+        // structure so just move the bucket files.\n         try {\n           List<FileStatus> fileStatuses = new ArrayList<>();\n-          for (FileStatus file: files) {\n+          for (FileStatus file : files) {\n             if (file.isDirectory() && AcidUtils.originalBucketFilter.accept(file.getPath())) {\n               FileStatus[] taskDir = srcFs.listStatus(file.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);\n               fileStatuses.addAll(Arrays.asList(taskDir));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4NTAzMA==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441385030", "bodyText": "nit: Maybe add a missing space too :)", "author": "pvary", "createdAt": "2020-06-17T08:47:36Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveCopyFiles.java", "diffHunk": "@@ -83,7 +83,8 @@ public void testRenameNewFilesOnSameFileSystem() throws IOException {\n     FileSystem targetFs = targetPath.getFileSystem(hiveConf);\n \n     try {\n-      Hive.copyFiles(hiveConf, sourcePath, targetPath, targetFs, isSourceLocal, NO_ACID, false,null, false, false, false);\n+      Hive.copyFiles(hiveConf, sourcePath, targetPath, targetFs, isSourceLocal, NO_ACID, false,null,", "originalCommit": "06e1ce9e18ad39431fa7d17a211047b2e6be9e1e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxMDE4NQ==", "url": "https://github.com/apache/hive/pull/1134#discussion_r441510185", "bodyText": "Done", "author": "klcopp", "createdAt": "2020-06-17T12:35:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4NTAzMA=="}], "type": "inlineReview", "revised_code": {"commit": "eff3cb725ee8bfdc272c63416eb023083ca13a8d", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveCopyFiles.java b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveCopyFiles.java\nindex 7837cdb068..f9c10f5ffb 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveCopyFiles.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveCopyFiles.java\n\n@@ -83,8 +83,8 @@ public void testRenameNewFilesOnSameFileSystem() throws IOException {\n     FileSystem targetFs = targetPath.getFileSystem(hiveConf);\n \n     try {\n-      Hive.copyFiles(hiveConf, sourcePath, targetPath, targetFs, isSourceLocal, NO_ACID, false,null,\n-          false, false, false, false);\n+      Hive.copyFiles(hiveConf, sourcePath, targetPath, targetFs, isSourceLocal, NO_ACID, false,\n+          null, false, false, false, false);\n     } catch (HiveException e) {\n       e.printStackTrace();\n       assertTrue(\"Hive.copyFiles() threw an unexpected exception.\", false);\n"}}, {"oid": "eff3cb725ee8bfdc272c63416eb023083ca13a8d", "url": "https://github.com/apache/hive/commit/eff3cb725ee8bfdc272c63416eb023083ca13a8d", "message": "Addressed review comments, added a check for writeid/rowid ascending order in compaction result files", "committedDate": "2020-06-17T13:44:57Z", "type": "commit"}]}