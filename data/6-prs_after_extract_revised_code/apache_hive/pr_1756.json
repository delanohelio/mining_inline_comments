{"pr_number": 1756, "pr_title": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "pr_createdAt": "2020-12-08T23:23:36Z", "pr_url": "https://github.com/apache/hive/pull/1756", "timeline": [{"oid": "34d4f16342262c2f1141a701735a002538f00992", "url": "https://github.com/apache/hive/commit/34d4f16342262c2f1141a701735a002538f00992", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T02:54:50Z", "type": "forcePushed"}, {"oid": "b704f8292717d18d0b6d95a2345ae41e552cbfb6", "url": "https://github.com/apache/hive/commit/b704f8292717d18d0b6d95a2345ae41e552cbfb6", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T09:58:51Z", "type": "forcePushed"}, {"oid": "45375271e127db5186799ed4798ac8fc4225e785", "url": "https://github.com/apache/hive/commit/45375271e127db5186799ed4798ac8fc4225e785", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2020-12-09T17:41:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTY5Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791696", "bodyText": "No need to define this here.  Just use JDK StandardCharsets.", "author": "belugabehr", "createdAt": "2021-01-01T18:09:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDA1NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804054", "bodyText": "Fixed, didn't know about that.", "author": "miklosgergely", "createdAt": "2021-01-01T20:31:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTY5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTgxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791817", "bodyText": "I personally hate NULL values.  Can you get rid of this check (exclude == null) and simply call this method with Collections.emptySet() ?", "author": "belugabehr", "createdAt": "2021-01-01T18:11:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTUxMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805513", "bodyText": "Totally agree, fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTgxNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTg5Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791896", "bodyText": "Please add JavaDoc here and also use JDK7+ ability of not needing to explicitly define the Type on the right hand side in several places, for example:\nList<String> realProps = new ArrayList<>();", "author": "belugabehr", "createdAt": "2021-01-01T18:12:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTQ1OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805459", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:50:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTg5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTk0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550791945", "bodyText": "Can now just use Java String#join method instead of something third party.", "author": "belugabehr", "createdAt": "2021-01-01T18:12:56Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDY1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804652", "bodyText": "Done", "author": "miklosgergely", "createdAt": "2021-01-01T20:39:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MTk0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjAxNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792016", "bodyText": "Use StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:13:33Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDAyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804026", "bodyText": "Fixed, thanks.", "author": "miklosgergely", "createdAt": "2021-01-01T20:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjAxNg=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjQ3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792472", "bodyText": "new String[0]\nhttps://docs.oracle.com/javase/8/docs/api/java/util/Collection.html#toArray-T:A-", "author": "belugabehr", "createdAt": "2021-01-01T18:18:17Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzkwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923905", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-02T21:38:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjQ3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjU4MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792581", "bodyText": "Math#min", "author": "belugabehr", "createdAt": "2021-01-01T18:19:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDcyOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924729", "bodyText": "Math#max - fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjU4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792617", "bodyText": "Arrays.asList", "author": "belugabehr", "createdAt": "2021-01-01T18:20:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDg0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924846", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:49:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYxNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792633", "bodyText": "new String[0]", "author": "belugabehr", "createdAt": "2021-01-01T18:20:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));\n+    }\n+\n+    public String renderTable(boolean isOutputPadded) {\n+      StringBuilder stringBuilder = new StringBuilder();\n+      for (List<String> row : table) {\n+        formatOutput(row.toArray(new String[] {}), stringBuilder, isOutputPadded, isOutputPadded);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDU3Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924576", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:46:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MjYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjg5MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792891", "bodyText": "There's got to be a better way of doing this...\nList#addAll or something other than 1-by-1 iteration.", "author": "belugabehr", "createdAt": "2021-01-01T18:24:07Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java", "diffHunk": "@@ -0,0 +1,397 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.common.type.Timestamp;\n+import org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.DateColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.LongColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.StringColumnStatsData;\n+import org.apache.hadoop.hive.metastore.api.TimestampColumnStatsData;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.serde2.io.DateWritableV2;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritableV2;\n+import org.apache.hive.common.util.HiveStringUtils;\n+import org.codehaus.jackson.map.ObjectMapper;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.io.OutputStreamWriter;\n+import java.math.BigInteger;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+/**\n+ * Utilities for SHOW ... commands.\n+ */\n+public final class ShowUtils {\n+  private ShowUtils() {\n+    throw new UnsupportedOperationException(\"ShowUtils should not be instantiated\");\n+  }\n+\n+  public static final Charset UTF_8 = Charset.forName(\"UTF-8\");\n+\n+  public static DataOutputStream getOutputStream(Path outputFile, DDLOperationContext context) throws HiveException {\n+    try {\n+      FileSystem fs = outputFile.getFileSystem(context.getConf());\n+      return fs.create(outputFile);\n+    } catch (Exception e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  public static String propertiesToString(Map<String, String> props, Set<String> exclude) {\n+    if (props.isEmpty()) {\n+      return \"\";\n+    }\n+  \n+    SortedMap<String, String> sortedProperties = new TreeMap<String, String>(props);\n+    List<String> realProps = new ArrayList<String>();\n+    for (Map.Entry<String, String> e : sortedProperties.entrySet()) {\n+      if (e.getValue() != null && (exclude == null || !exclude.contains(e.getKey()))) {\n+        realProps.add(\"  '\" + e.getKey() + \"'='\" + HiveStringUtils.escapeHiveCommand(e.getValue()) + \"'\");\n+      }\n+    }\n+    return StringUtils.join(realProps, \", \\n\");\n+  }\n+\n+  public static void writeToFile(String data, String file, DDLOperationContext context) throws IOException {\n+    if (StringUtils.isEmpty(data)) {\n+      return;\n+    }\n+  \n+    Path resFile = new Path(file);\n+    FileSystem fs = resFile.getFileSystem(context.getConf());\n+    try (FSDataOutputStream out = fs.create(resFile);\n+         OutputStreamWriter writer = new OutputStreamWriter(out, \"UTF-8\")) {\n+      writer.write(data);\n+      writer.write((char) Utilities.newLineCode);\n+      writer.flush();\n+    }\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value) {\n+    appendNonNull(builder, value, false);\n+  }\n+\n+  public static void appendNonNull(StringBuilder builder, Object value, boolean firstColumn) {\n+    if (!firstColumn) {\n+      builder.append((char)Utilities.tabCode);\n+    } else if (builder.length() > 0) {\n+      builder.append((char)Utilities.newLineCode);\n+    }\n+    if (value != null) {\n+      builder.append(value);\n+    }\n+  }\n+\n+\n+  public static String[] extractColumnValues(FieldSchema column, boolean isColumnStatsAvailable,\n+      ColumnStatisticsObj columnStatisticsObj) {\n+    List<String> values = new ArrayList<>();\n+    values.add(column.getName());\n+    values.add(column.getType());\n+\n+    if (isColumnStatsAvailable) {\n+      if (columnStatisticsObj != null) {\n+        ColumnStatisticsData statsData = columnStatisticsObj.getStatsData();\n+        if (statsData.isSetBinaryStats()) {\n+          BinaryColumnStatsData binaryStats = statsData.getBinaryStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + binaryStats.getNumNulls(), \"\",\n+              \"\" + binaryStats.getAvgColLen(), \"\" + binaryStats.getMaxColLen(), \"\", \"\",\n+              convertToString(binaryStats.getBitVectors())));\n+        } else if (statsData.isSetStringStats()) {\n+          StringColumnStatsData stringStats = statsData.getStringStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + stringStats.getNumNulls(), \"\" + stringStats.getNumDVs(),\n+              \"\" + stringStats.getAvgColLen(), \"\" + stringStats.getMaxColLen(), \"\", \"\",\n+              convertToString(stringStats.getBitVectors())));\n+        } else if (statsData.isSetBooleanStats()) {\n+          BooleanColumnStatsData booleanStats = statsData.getBooleanStats();\n+          values.addAll(Lists.newArrayList(\"\", \"\", \"\" + booleanStats.getNumNulls(), \"\", \"\", \"\",\n+              \"\" + booleanStats.getNumTrues(), \"\" + booleanStats.getNumFalses(),\n+              convertToString(booleanStats.getBitVectors())));\n+        } else if (statsData.isSetDecimalStats()) {\n+          DecimalColumnStatsData decimalStats = statsData.getDecimalStats();\n+          values.addAll(Lists.newArrayList(convertToString(decimalStats.getLowValue()),\n+              convertToString(decimalStats.getHighValue()), \"\" + decimalStats.getNumNulls(),\n+              \"\" + decimalStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(decimalStats.getBitVectors())));\n+        } else if (statsData.isSetDoubleStats()) {\n+          DoubleColumnStatsData doubleStats = statsData.getDoubleStats();\n+          values.addAll(Lists.newArrayList(\"\" + doubleStats.getLowValue(), \"\" + doubleStats.getHighValue(),\n+              \"\" + doubleStats.getNumNulls(), \"\" + doubleStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(doubleStats.getBitVectors())));\n+        } else if (statsData.isSetLongStats()) {\n+          LongColumnStatsData longStats = statsData.getLongStats();\n+          values.addAll(Lists.newArrayList(\"\" + longStats.getLowValue(), \"\" + longStats.getHighValue(),\n+              \"\" + longStats.getNumNulls(), \"\" + longStats.getNumDVs(), \"\", \"\", \"\", \"\",\n+              convertToString(longStats.getBitVectors())));\n+        } else if (statsData.isSetDateStats()) {\n+          DateColumnStatsData dateStats = statsData.getDateStats();\n+          values.addAll(Lists.newArrayList(convertToString(dateStats.getLowValue()),\n+              convertToString(dateStats.getHighValue()), \"\" + dateStats.getNumNulls(), \"\" + dateStats.getNumDVs(),\n+              \"\", \"\", \"\", \"\", convertToString(dateStats.getBitVectors())));\n+        } else if (statsData.isSetTimestampStats()) {\n+          TimestampColumnStatsData timestampStats = statsData.getTimestampStats();\n+          values.addAll(Lists.newArrayList(convertToString(timestampStats.getLowValue()),\n+              convertToString(timestampStats.getHighValue()), \"\" + timestampStats.getNumNulls(),\n+              \"\" + timestampStats.getNumDVs(), \"\", \"\", \"\", \"\", convertToString(timestampStats.getBitVectors())));\n+        }\n+      } else {\n+        values.addAll(Lists.newArrayList(\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"));\n+      }\n+    }\n+\n+    values.add(column.getComment() != null ? column.getComment() : \"\");\n+    return values.toArray(new String[] {});\n+  }\n+\n+  public static String convertToString(Decimal val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    return (result != null) ? result.toString() : \"\";\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    DateWritableV2 writableValue = new DateWritableV2((int) val.getDaysSinceEpoch());\n+    return writableValue.toString();\n+  }\n+\n+  private static String convertToString(byte[] buffer) {\n+    if (buffer == null || buffer.length == 0) {\n+      return \"\";\n+    }\n+    return new String(Arrays.copyOfRange(buffer, 0, 2));\n+  }\n+\n+  public static String convertToString(org.apache.hadoop.hive.metastore.api.Timestamp val) {\n+    if (val == null) {\n+      return \"\";\n+    }\n+\n+    TimestampWritableV2 writableValue = new TimestampWritableV2(Timestamp.ofEpochSecond(val.getSecondsSinceEpoch()));\n+    return writableValue.toString();\n+  }\n+\n+  /**\n+   * Convert the map to a JSON string.\n+   */\n+  public static void asJson(OutputStream out, Map<String, Object> data) throws HiveException {\n+    try {\n+      new ObjectMapper().writeValue(out, data);\n+    } catch (IOException e) {\n+      throw new HiveException(\"Unable to convert to json\", e);\n+    }\n+  }\n+\n+  public static final String FIELD_DELIM = \"\\t\";\n+  public static final String LINE_DELIM = \"\\n\";\n+\n+  public static final int DEFAULT_STRINGBUILDER_SIZE = 2048;\n+  public static final int ALIGNMENT = 20;\n+\n+  /**\n+   * Prints a row with the given fields into the builder.\n+   * The last field could be a multiline field, and the extra lines should be padded.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   * @param isLastLinePadded Is the last field could be printed in multiple lines, if contains newlines?\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo, boolean isLastLinePadded,\n+      boolean isFormatted) {\n+    if (!isFormatted) {\n+      for (int i = 0; i < fields.length; i++) {\n+        Object value = HiveStringUtils.escapeJava(fields[i]);\n+        if (value != null) {\n+          tableInfo.append(value);\n+        }\n+        tableInfo.append((i == fields.length - 1) ? LINE_DELIM : FIELD_DELIM);\n+      }\n+    } else {\n+      int[] paddings = new int[fields.length - 1];\n+      if (fields.length > 1) {\n+        for (int i = 0; i < fields.length - 1; i++) {\n+          if (fields[i] == null) {\n+            tableInfo.append(FIELD_DELIM);\n+            continue;\n+          }\n+          tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", fields[i])).append(FIELD_DELIM);\n+          paddings[i] = ALIGNMENT > fields[i].length() ? ALIGNMENT : fields[i].length();\n+        }\n+      }\n+      if (fields.length > 0) {\n+        String value = fields[fields.length - 1];\n+        String unescapedValue = (isLastLinePadded && value != null) ?\n+            value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+        indentMultilineValue(unescapedValue, tableInfo, paddings, false);\n+      } else {\n+        tableInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Prints a row the given fields to a formatted line.\n+   * \n+   * @param fields The fields to print\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String[] fields, StringBuilder tableInfo) {\n+    formatOutput(fields, tableInfo, false, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair, and if the value contains newlines, it adds one more empty field\n+   * before the two values (Assumes, the name value pair is already indented with it).\n+   * \n+   * @param name The field name to print\n+   * @param value The value to print - might contain newlines\n+   * @param tableInfo The target builder\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo) {\n+    tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", name)).append(FIELD_DELIM);\n+    int colNameLength = ALIGNMENT > name.length() ? ALIGNMENT : name.length();\n+    indentMultilineValue(value, tableInfo, new int[] {0, colNameLength}, true);\n+  }\n+\n+  /**\n+   * Prints the name value pair\n+   * If the output is padded then unescape the value, so it could be printed in multiple lines.\n+   * In this case it assumes the pair is already indented with a field delimiter\n+   * \n+   * @param name The field name to print\n+   * @param value The value t print\n+   * @param tableInfo The target builder\n+   * @param isOutputPadded Should the value printed as a padded string?\n+   */\n+  public static void formatOutput(String name, String value, StringBuilder tableInfo, boolean isOutputPadded) {\n+    String unescapedValue = (isOutputPadded && value != null) ?\n+        value.replaceAll(\"\\\\\\\\n|\\\\\\\\r|\\\\\\\\r\\\\\\\\n\", \"\\n\") : value;\n+    formatOutput(name, unescapedValue, tableInfo);\n+  }\n+\n+  /**\n+   * Indent processing for multi-line values.\n+   * Values should be indented the same amount on each line.\n+   * If the first line comment starts indented by k, the following line comments should also be indented by k.\n+   * \n+   * @param value the value to write\n+   * @param tableInfo the buffer to write to\n+   * @param columnWidths the widths of the previous columns\n+   * @param printNull print null as a string, or do not print anything\n+   */\n+  private static void indentMultilineValue(String value, StringBuilder tableInfo, int[] columnWidths,\n+      boolean printNull) {\n+    if (value == null) {\n+      if (printNull) {\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", value));\n+      }\n+      tableInfo.append(LINE_DELIM);\n+    } else {\n+      String[] valueSegments = value.split(\"\\n|\\r|\\r\\n\");\n+      tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[0])).append(LINE_DELIM);\n+      for (int i = 1; i < valueSegments.length; i++) {\n+        printPadding(tableInfo, columnWidths);\n+        tableInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", valueSegments[i])).append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Print the rigth padding, with the given column widths.\n+   * \n+   * @param tableInfo The buffer to write to\n+   * @param columnWidths The column widths\n+   */\n+  private static void printPadding(StringBuilder tableInfo, int[] columnWidths) {\n+    for (int columnWidth : columnWidths) {\n+      if (columnWidth == 0) {\n+        tableInfo.append(FIELD_DELIM);\n+      } else {\n+        tableInfo.append(String.format(\"%\" + columnWidth + \"s\" + FIELD_DELIM, \"\"));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Helps to format tables in SHOW ... command outputs.\n+   */\n+  public static class TextMetaDataTable {\n+    private List<List<String>> table = new ArrayList<>();\n+\n+    public void addRow(String... values) {\n+      table.add(Lists.<String> newArrayList(values));\n+    }\n+\n+    public String renderTable(boolean isOutputPadded) {\n+      StringBuilder stringBuilder = new StringBuilder();\n+      for (List<String> row : table) {\n+        formatOutput(row.toArray(new String[] {}), stringBuilder, isOutputPadded, isOutputPadded);\n+      }\n+      return stringBuilder.toString();\n+    }\n+\n+    public void transpose() {\n+      if (table.size() == 0) {\n+        return;\n+      }\n+      List<List<String>> newTable = new ArrayList<List<String>>();\n+      for (int i = 0; i < table.get(0).size(); i++) {\n+        newTable.add(new ArrayList<>());\n+      }\n+      for (List<String> sourceRow : table) {\n+        if (newTable.size() != sourceRow.size()) {\n+          throw new RuntimeException(\"invalid table size\");\n+        }\n+        for (int i = 0; i < sourceRow.size(); i++) {\n+          newTable.get(i).add(sourceRow.get(i));\n+        }\n+      }\n+      table = newTable;\n+    }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyODczNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550928737", "bodyText": "I don't think it is possible in any other way. Transposing a table must be done like this in essence.", "author": "miklosgergely", "createdAt": "2021-01-02T22:37:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjg5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\nindex e760787957..effdd956bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/ShowUtils.java\n\n@@ -49,7 +49,7 @@\n import java.io.OutputStream;\n import java.io.OutputStreamWriter;\n import java.math.BigInteger;\n-import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792946", "bodyText": "StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:24:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.database.desc;\n+\n+import org.apache.commons.collections.MapUtils;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.PrincipalType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Formats DESC DATABASES results.\n+ */\n+abstract class DescDatabaseFormatter {\n+  static DescDatabaseFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonDescDatabaseFormatter();\n+    } else {\n+      return new TextDescDatabaseFormatter();\n+    }\n+  }\n+\n+  abstract void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+      String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+      throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonDescDatabaseFormatter extends DescDatabaseFormatter {\n+    @Override\n+    void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+        String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+        throws HiveException {\n+      MapBuilder builder = MapBuilder.create()\n+          .put(\"database\", database)\n+          .put(\"comment\", comment)\n+          .put(\"location\", location);\n+      if (managedLocation != null) {\n+        builder.put(\"managedLocation\", managedLocation);\n+      }\n+      if (ownerName != null) {\n+        builder.put(\"owner\", ownerName);\n+      }\n+      if (ownerType != null) {\n+        builder.put(\"ownerType\", ownerType.name());\n+      }\n+      if (MapUtils.isNotEmpty(params)) {\n+        builder.put(\"params\", params);\n+      }\n+      ShowUtils.asJson(out, builder.build());\n+    }\n+  }\n+\n+  static class TextDescDatabaseFormatter extends DescDatabaseFormatter {\n+    @Override\n+    void showDatabaseDescription(DataOutputStream out, String database, String comment, String location,\n+        String managedLocation, String ownerName, PrincipalType ownerType, Map<String, String> params)\n+        throws HiveException {\n+      try {\n+        out.write(database.getBytes(\"UTF-8\"));\n+        out.write(Utilities.tabCode);\n+        if (comment != null) {\n+          out.write(HiveStringUtils.escapeJava(comment).getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (location != null) {\n+          out.write(location.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (managedLocation != null) {\n+          out.write(managedLocation.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (ownerName != null) {\n+          out.write(ownerName.getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (ownerType != null) {\n+          out.write(ownerType.name().getBytes(\"UTF-8\"));\n+        }\n+        out.write(Utilities.tabCode);\n+        if (MapUtils.isNotEmpty(params)) {\n+          out.write(params.toString().getBytes(\"UTF-8\"));\n+        }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzczOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923738", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java\nindex be7d2b3680..f6cd633c8a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/desc/DescDatabaseFormatter.java\n\n@@ -30,6 +30,7 @@\n \n import java.io.DataOutputStream;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.Map;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk2NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550792965", "bodyText": "StandardCharsets.UTF_8", "author": "belugabehr", "createdAt": "2021-01-01T18:25:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.database.show;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW DATABASES results.\n+ */\n+abstract class ShowDatabasesFormatter {\n+  static ShowDatabasesFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowDatabasesFormatter();\n+    } else {\n+      return new TextShowDatabasesFormatter();\n+    }\n+  }\n+\n+  abstract void showDatabases(DataOutputStream out, List<String> databases) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowDatabasesFormatter extends ShowDatabasesFormatter {\n+    @Override\n+    void showDatabases(DataOutputStream out, List<String> databases) throws HiveException {\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"databases\", databases).build());\n+    }\n+  }\n+\n+  static class TextShowDatabasesFormatter extends ShowDatabasesFormatter {\n+    @Override\n+    void showDatabases(DataOutputStream out, List<String> databases) throws HiveException {\n+      try {\n+        for (String database : databases) {\n+          out.write(database.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzcyMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923722", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:36:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mjk2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java\nindex 458f73d4e2..26931db24c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/database/show/ShowDatabasesFormatter.java\n\n@@ -27,6 +27,7 @@\n \n import java.io.DataOutputStream;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793072", "bodyText": "Remove this formatting change.  Little value and adds to this already large review.", "author": "belugabehr", "createdAt": "2021-01-01T18:26:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java", "diffHunk": "@@ -32,7 +32,8 @@\n   private static final long serialVersionUID = 1L;\n \n   public static final String SCHEMA =\n-      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n+      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,\" +\n+      \"errormessage#\" +", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTU1OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925559", "bodyText": "There is a 120 limit in the Hive checkstyle, and I'm trying to make all DDL codes checkstyle violation free. This patch is about making Show kind commands cleaner, that is why it is here.", "author": "miklosgergely", "createdAt": "2021-01-02T21:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzMDYyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550930626", "bodyText": "Added @Formatter:off - @Formatter:on instead", "author": "miklosgergely", "createdAt": "2021-01-02T23:01:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzA3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\nindex 3896b383b8..f2939baee1 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\n\n@@ -31,10 +31,11 @@\n public class ShowCompactionsDesc implements DDLDesc, Serializable {\n   private static final long serialVersionUID = 1L;\n \n+  // @formatter:off\n   public static final String SCHEMA =\n-      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,\" +\n-      \"errormessage#\" +\n+      \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n       \"string:string:string:string:string:string:string:string:string:string:string:string:string\";\n+  // @formatter:on\n \n   private String resFile;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzIxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793217", "bodyText": "... =new ArrayList<>(columns.size());", "author": "belugabehr", "createdAt": "2021-01-01T18:27:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats DESC TABLE results to json format.\n+ */\n+public class JsonDescTableFormatter extends DescTableFormatter {\n+  private static final String COLUMN_NAME = \"name\";\n+  private static final String COLUMN_TYPE = \"type\";\n+  private static final String COLUMN_COMMENT = \"comment\";\n+  private static final String COLUMN_MIN = \"min\";\n+  private static final String COLUMN_MAX = \"max\";\n+  private static final String COLUMN_NUM_NULLS = \"numNulls\";\n+  private static final String COLUMN_NUM_TRUES = \"numTrues\";\n+  private static final String COLUMN_NUM_FALSES = \"numFalses\";\n+  private static final String COLUMN_DISTINCT_COUNT = \"distinctCount\";\n+  private static final String COLUMN_AVG_LENGTH = \"avgColLen\";\n+  private static final String COLUMN_MAX_LENGTH = \"maxColLen\";\n+\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"columns\", createColumnsInfo(columns, columnStats));\n+\n+    if (isExtended) {\n+      addExtendedInfo(table, partition, builder);\n+    }\n+\n+    ShowUtils.asJson(out, builder.build());\n+  }\n+\n+  public static List<Map<String, Object>> createColumnsInfo(List<FieldSchema> columns,\n+      List<ColumnStatisticsObj> columnStatisticsList) {\n+    List<Map<String, Object>> columnsInfo = new ArrayList<>();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDUyMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924522", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzIxNw=="}], "type": "inlineReview", "revised_code": {"commit": "636c0af4548c0da4b23644b16dabfef97a04f1ef", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java\nindex 3a629e2d6d..8fcd28bc46 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/JsonDescTableFormatter.java\n\n@@ -71,7 +71,7 @@ public void describeTable(HiveConf conf, DataOutputStream out, String columnPath\n \n   public static List<Map<String, Object>> createColumnsInfo(List<FieldSchema> columns,\n       List<ColumnStatisticsObj> columnStatisticsList) {\n-    List<Map<String, Object>> columnsInfo = new ArrayList<>();\n+    List<Map<String, Object>> columnsInfo = new ArrayList<>(columns.size());\n     for (FieldSchema column : columns) {\n       ColumnStatisticsData statistics = getStatistics(column, columnStatisticsList);\n       columnsInfo.add(createColumnInfo(column, statistics));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793374", "bodyText": "new String[0]", "author": "belugabehr", "createdAt": "2021-01-01T18:29:14Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzk3Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923972", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3NA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793378", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:29:34Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzY2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923661", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzM3OA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzQ2OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793468", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:30:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzY0MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923640", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzQ2OA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzUyOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793528", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:31:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyMzYyNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550923625", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:35:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzUyOA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzYxNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793617", "bodyText": "tableInfo.append(LINE_DELIM).apppend('#').......append(LINE_DELIM);", "author": "belugabehr", "createdAt": "2021-01-01T18:33:19Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTQwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929405", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:46:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzYxNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793707", "bodyText": ".append().append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:33:46Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTUxNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929515", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcwNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcyMQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793721", "bodyText": ".append().append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:33:52Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTUyNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929524", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzcyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzc1MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793751", "bodyText": "(CollectionUtils.isNotEmpty(skewedColValues) (just like immediately below)", "author": "belugabehr", "createdAt": "2021-01-01T18:34:23Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDAyOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924029", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzc1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzg2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793861", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:35:37Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTU0Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929543", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:47:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5Mzg2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkxOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793918", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:36:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTU4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929585", "bodyText": "Fixed..", "author": "miklosgergely", "createdAt": "2021-01-02T22:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkxOA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550793933", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:36:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTYxMg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929612", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:48:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5MzkzMw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDM1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794352", "bodyText": "Should be able to replace these methods easily with Lambdas...\nlist.stream().sorted(...).collect(Collectors.toList());", "author": "belugabehr", "createdAt": "2021-01-01T18:40:02Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTMzOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925339", "bodyText": "Fixed, nice catch!", "author": "miklosgergely", "createdAt": "2021-01-02T21:56:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDM1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDQ5Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794497", "bodyText": "intern of a constant values is probably useless.  Please remove.", "author": "belugabehr", "createdAt": "2021-01-01T18:41:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNTQyMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550925420", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:57:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDQ5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDU0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794545", "bodyText": ".append().append()", "author": "belugabehr", "createdAt": "2021-01-01T18:42:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyOTcyOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550929728", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T22:50:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDU0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYxMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794610", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:21Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA1Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924057", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYxMA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYzMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794633", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:34Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    }\n+  }\n+\n+  private void addExtendedConstraintData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      out.write((\"Constraints\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+        out.write(table.getPrimaryKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+        out.write(table.getForeignKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+        out.write(table.getUniqueKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+        out.write(table.getNotNullConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+        out.write(table.getDefaultConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+        out.write(table.getCheckConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+    }", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA2MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924060", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDY2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794661", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:43:49Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter;\n+\n+import org.apache.commons.collections4.CollectionUtils;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.commons.text.StringEscapeUtils;\n+import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableDesc;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint;\n+import org.apache.hadoop.hive.ql.metadata.CheckConstraint.CheckConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\n+import org.apache.hadoop.hive.ql.metadata.DefaultConstraint.DefaultConstraintCol;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\n+import org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo.ForeignKeyCol;\n+import org.apache.hadoop.hive.ql.metadata.UniqueConstraint.UniqueConstraintCol;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hive.common.util.HiveStringUtils;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.Map.Entry;\n+\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.ALIGNMENT;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.DEFAULT_STRINGBUILDER_SIZE;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.FIELD_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.LINE_DELIM;\n+import static org.apache.hadoop.hive.ql.ddl.ShowUtils.formatOutput;\n+\n+/**\n+ * Formats DESC TABLE results to text format.\n+ */\n+class TextDescTableFormatter extends DescTableFormatter {\n+  @Override\n+  public void describeTable(HiveConf conf, DataOutputStream out, String columnPath, String tableName, Table table,\n+      Partition partition, List<FieldSchema> columns, boolean isFormatted, boolean isExtended, boolean isOutputPadded,\n+      List<ColumnStatisticsObj> columnStats) throws HiveException {\n+    try {\n+      addStatsData(out, columnPath, columns, isFormatted, columnStats, isOutputPadded);\n+      addPartitionData(out, conf, columnPath, table, isFormatted, isOutputPadded);\n+\n+      if (columnPath == null) {\n+        if (isFormatted) {\n+          addFormattedTableData(out, table, partition, isOutputPadded);\n+        }\n+\n+        if (isExtended) {\n+          out.write(Utilities.newLineCode);\n+          addExtendedTableData(out, table, partition);\n+          addExtendedConstraintData(out, table);\n+          addExtendedStorageData(out, table);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void addStatsData(DataOutputStream out, String columnPath, List<FieldSchema> columns, boolean isFormatted,\n+      List<ColumnStatisticsObj> columnStats, boolean isOutputPadded) throws IOException {\n+    String statsData = \"\";\n+    \n+    TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+    boolean needColStats = isFormatted && columnPath != null;\n+    if (needColStats) {\n+      metaDataTable.addRow(DescTableDesc.COLUMN_STATISTICS_HEADERS.toArray(new String[]{}));\n+    } else if (isFormatted && !SessionState.get().isHiveServerQuery()) {\n+      statsData += \"# \";\n+      metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+    }\n+    for (FieldSchema column : columns) {\n+      metaDataTable.addRow(ShowUtils.extractColumnValues(column, needColStats,\n+          getColumnStatisticsObject(column.getName(), column.getType(), columnStats)));\n+    }\n+    if (needColStats) {\n+      metaDataTable.transpose();\n+    }\n+    statsData += metaDataTable.renderTable(isOutputPadded);\n+    out.write(statsData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private ColumnStatisticsObj getColumnStatisticsObject(String columnName, String columnType,\n+      List<ColumnStatisticsObj> columnStats) {\n+    if (CollectionUtils.isNotEmpty(columnStats)) {\n+      for (ColumnStatisticsObj columnStat : columnStats) {\n+        if (columnStat.getColName().equalsIgnoreCase(columnName) &&\n+            columnStat.getColType().equalsIgnoreCase(columnType)) {\n+          return columnStat;\n+        }\n+      }\n+    }\n+    return null;\n+  }\n+\n+  private void addPartitionData(DataOutputStream out, HiveConf conf, String columnPath, Table table,\n+      boolean isFormatted, boolean isOutputPadded) throws IOException {\n+    String partitionData = \"\";\n+    if (columnPath == null) {\n+      List<FieldSchema> partitionColumns = table.isPartitioned() ? table.getPartCols() : null;\n+      if (CollectionUtils.isNotEmpty(partitionColumns) &&\n+          conf.getBoolVar(ConfVars.HIVE_DISPLAY_PARTITION_COLUMNS_SEPARATELY)) {\n+        TextMetaDataTable metaDataTable = new TextMetaDataTable();\n+        partitionData += LINE_DELIM + \"# Partition Information\" + LINE_DELIM + \"# \";\n+        metaDataTable.addRow(DescTableDesc.SCHEMA.split(\"#\")[0].split(\",\"));\n+        for (FieldSchema partitionColumn : partitionColumns) {\n+          metaDataTable.addRow(ShowUtils.extractColumnValues(partitionColumn, false, null));\n+        }\n+        partitionData += metaDataTable.renderTable(isOutputPadded);\n+      }\n+    } else {\n+      String statsState = table.getParameters().get(StatsSetupConst.COLUMN_STATS_ACCURATE);\n+      if (table.getParameters() != null && statsState != null) {\n+        StringBuilder stringBuilder = new StringBuilder();\n+        formatOutput(StatsSetupConst.COLUMN_STATS_ACCURATE,\n+            isFormatted ? StringEscapeUtils.escapeJava(statsState) : HiveStringUtils.escapeJava(statsState),\n+            stringBuilder, isOutputPadded);\n+        partitionData += stringBuilder.toString();\n+      }\n+    }\n+    out.write(partitionData.getBytes(\"UTF-8\"));\n+  }\n+\n+  private void addFormattedTableData(DataOutputStream out, Table table, Partition partition, boolean isOutputPadded)\n+      throws IOException, UnsupportedEncodingException {\n+    String formattedTableInfo = null;\n+    if (partition != null) {\n+      formattedTableInfo = getPartitionInformation(partition);\n+    } else {\n+      formattedTableInfo = getTableInformation(table, isOutputPadded);\n+    }\n+\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      formattedTableInfo += getConstraintsInformation(table);\n+    }\n+    out.write(formattedTableInfo.getBytes(\"UTF-8\"));\n+  }\n+\n+  private String getTableInformation(Table table, boolean isOutputPadded) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM).append(\"# Detailed Table Information\").append(LINE_DELIM);\n+    getTableMetaDataInformation(tableInfo, table, isOutputPadded);\n+\n+    tableInfo.append(LINE_DELIM).append(\"# Storage Information\").append(LINE_DELIM);\n+    getStorageDescriptorInfo(tableInfo, table.getTTable().getSd());\n+\n+    if (table.isView() || table.isMaterializedView()) {\n+      tableInfo.append(LINE_DELIM + \"# \" + (table.isView() ? \"\" : \"Materialized \") + \"View Information\" + LINE_DELIM);\n+      getViewInfo(tableInfo, table);\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private String getPartitionInformation(Partition partition) {\n+    StringBuilder tableInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+    tableInfo.append(LINE_DELIM + \"# Detailed Partition Information\" + LINE_DELIM);\n+    getPartitionMetaDataInformation(tableInfo, partition);\n+\n+    if (partition.getTable().getTableType() != TableType.VIRTUAL_VIEW) {\n+      tableInfo.append(LINE_DELIM + \"# Storage Information\" + LINE_DELIM);\n+      getStorageDescriptorInfo(tableInfo, partition.getTPartition().getSd());\n+    }\n+\n+    return tableInfo.toString();\n+  }\n+\n+  private void getViewInfo(StringBuilder tableInfo, Table table) {\n+    formatOutput(\"Original Query:\", table.getViewOriginalText(), tableInfo);\n+    formatOutput(\"Expanded Query:\", table.getViewExpandedText(), tableInfo);\n+    if (table.isMaterializedView()) {\n+      formatOutput(\"Rewrite Enabled:\", table.isRewriteEnabled() ? \"Yes\" : \"No\", tableInfo);\n+      formatOutput(\"Outdated for Rewriting:\", table.isOutdatedForRewriting() == null ? \"Unknown\"\n+          : table.isOutdatedForRewriting() ? \"Yes\" : \"No\", tableInfo);\n+    }\n+  }\n+\n+  private void getStorageDescriptorInfo(StringBuilder tableInfo, StorageDescriptor storageDesc) {\n+    formatOutput(\"SerDe Library:\", storageDesc.getSerdeInfo().getSerializationLib(), tableInfo);\n+    formatOutput(\"InputFormat:\", storageDesc.getInputFormat(), tableInfo);\n+    formatOutput(\"OutputFormat:\", storageDesc.getOutputFormat(), tableInfo);\n+    formatOutput(\"Compressed:\", storageDesc.isCompressed() ? \"Yes\" : \"No\", tableInfo);\n+    formatOutput(\"Num Buckets:\", String.valueOf(storageDesc.getNumBuckets()), tableInfo);\n+    formatOutput(\"Bucket Columns:\", storageDesc.getBucketCols().toString(), tableInfo);\n+    formatOutput(\"Sort Columns:\", storageDesc.getSortCols().toString(), tableInfo);\n+\n+    if (storageDesc.isStoredAsSubDirectories()) {\n+      formatOutput(\"Stored As SubDirectories:\", \"Yes\", tableInfo);\n+    }\n+\n+    if (storageDesc.getSkewedInfo() != null) {\n+      List<String> skewedColNames = sortList(storageDesc.getSkewedInfo().getSkewedColNames());\n+      if ((skewedColNames != null) && (skewedColNames.size() > 0)) {\n+        formatOutput(\"Skewed Columns:\", skewedColNames.toString(), tableInfo);\n+      }\n+\n+      List<List<String>> skewedColValues = sortList(\n+          storageDesc.getSkewedInfo().getSkewedColValues(), new VectorComparator<String>());\n+      if (CollectionUtils.isNotEmpty(skewedColValues)) {\n+        formatOutput(\"Skewed Values:\", skewedColValues.toString(), tableInfo);\n+      }\n+\n+      Map<List<String>, String> skewedColMap = new TreeMap<>(new VectorComparator<String>());\n+      skewedColMap.putAll(storageDesc.getSkewedInfo().getSkewedColValueLocationMaps());\n+      if (MapUtils.isNotEmpty(skewedColMap)) {\n+        formatOutput(\"Skewed Value to Path:\", skewedColMap.toString(), tableInfo);\n+        Map<List<String>, String> truncatedSkewedColMap =\n+            new TreeMap<List<String>, String>(new VectorComparator<String>());\n+        // walk through existing map to truncate path so that test won't mask it then we can verify location is right\n+        Set<Entry<List<String>, String>> entries = skewedColMap.entrySet();\n+        for (Entry<List<String>, String> entry : entries) {\n+          truncatedSkewedColMap.put(entry.getKey(), PlanUtils.removePrefixFromWarehouseConfig(entry.getValue()));\n+        }\n+        formatOutput(\"Skewed Value to Truncated Path:\", truncatedSkewedColMap.toString(), tableInfo);\n+      }\n+    }\n+\n+    if (storageDesc.getSerdeInfo().getParametersSize() > 0) {\n+      tableInfo.append(\"Storage Desc Params:\" + LINE_DELIM);\n+      displayAllParameters(storageDesc.getSerdeInfo().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private void getTableMetaDataInformation(StringBuilder tableInfo, Table table, boolean isOutputPadded) {\n+    formatOutput(\"Database:\", table.getDbName(), tableInfo);\n+    formatOutput(\"OwnerType:\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\", tableInfo);\n+    formatOutput(\"Owner:\", table.getOwner(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(table.getTTable().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(table.getTTable().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Retention:\", Integer.toString(table.getRetention()), tableInfo);\n+    \n+    if (!table.isView()) {\n+      formatOutput(\"Location:\", table.getDataLocation().toString(), tableInfo);\n+    }\n+    formatOutput(\"Table Type:\", table.getTableType().name(), tableInfo);\n+\n+    if (table.getParameters().size() > 0) {\n+      tableInfo.append(\"Table Parameters:\" + LINE_DELIM);\n+      displayAllParameters(table.getParameters(), tableInfo, false, isOutputPadded);\n+    }\n+  }\n+\n+  private void getPartitionMetaDataInformation(StringBuilder tableInfo, Partition partition) {\n+    formatOutput(\"Partition Value:\", partition.getValues().toString(), tableInfo);\n+    formatOutput(\"Database:\", partition.getTPartition().getDbName(), tableInfo);\n+    formatOutput(\"Table:\", partition.getTable().getTableName(), tableInfo);\n+    formatOutput(\"CreateTime:\", formatDate(partition.getTPartition().getCreateTime()), tableInfo);\n+    formatOutput(\"LastAccessTime:\", formatDate(partition.getTPartition().getLastAccessTime()), tableInfo);\n+    formatOutput(\"Location:\", partition.getLocation(), tableInfo);\n+\n+    if (partition.getTPartition().getParameters().size() > 0) {\n+      tableInfo.append(\"Partition Parameters:\" + LINE_DELIM);\n+      displayAllParameters(partition.getTPartition().getParameters(), tableInfo);\n+    }\n+  }\n+\n+  private class VectorComparator<T extends Comparable<T>>  implements Comparator<List<T>>{\n+    @Override\n+    public int compare(List<T> listA, List<T> listB) {\n+      for (int i = 0; i < listA.size() && i < listB.size(); i++) {\n+        T valA = listA.get(i);\n+        T valB = listB.get(i);\n+        if (valA != null) {\n+          int ret = valA.compareTo(valB);\n+          if (ret != 0) {\n+            return ret;\n+          }\n+        } else {\n+          if (valB != null) {\n+            return -1;\n+          }\n+        }\n+      }\n+      return Integer.compare(listA.size(), listB.size());\n+    }\n+  }\n+\n+  private <T extends Comparable<T>> List<T> sortList(List<T> list){\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret);\n+    return ret;\n+  }\n+\n+  private <T> List<T> sortList(List<T> list, Comparator<T> comparator) {\n+    if (list == null || list.size() <= 1) {\n+      return list;\n+    }\n+    List<T> ret = new ArrayList<>(list);\n+    Collections.sort(ret, comparator);\n+    return ret;\n+  }\n+\n+  private String formatDate(long timeInSeconds) {\n+    if (timeInSeconds != 0) {\n+      Date date = new Date(timeInSeconds * 1000);\n+      return date.toString();\n+    }\n+    return \"UNKNOWN\";\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo) {\n+    displayAllParameters(params, tableInfo, true, false);\n+  }\n+\n+  private void displayAllParameters(Map<String, String> params, StringBuilder tableInfo, boolean escapeUnicode,\n+      boolean isOutputPadded) {\n+    List<String> keys = new ArrayList<String>(params.keySet());\n+    Collections.sort(keys);\n+    for (String key : keys) {\n+      String value = params.get(key);\n+      if (key.equals(StatsSetupConst.NUM_ERASURE_CODED_FILES)) {\n+        if (\"0\".equals(value)) {\n+          continue;\n+        }\n+      }\n+      tableInfo.append(FIELD_DELIM); // Ensures all params are indented.\n+      formatOutput(key, escapeUnicode ? StringEscapeUtils.escapeJava(value) : HiveStringUtils.escapeJava(value),\n+          tableInfo, isOutputPadded);\n+    }\n+  }\n+\n+  private String getConstraintsInformation(Table table) {\n+    StringBuilder constraintsInfo = new StringBuilder(DEFAULT_STRINGBUILDER_SIZE);\n+\n+    constraintsInfo.append(LINE_DELIM + \"# Constraints\" + LINE_DELIM);\n+    if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Primary Key\" + LINE_DELIM);\n+      getPrimaryKeyInformation(constraintsInfo, table.getPrimaryKeyInfo());\n+    }\n+    if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Foreign Keys\" + LINE_DELIM);\n+      getForeignKeysInformation(constraintsInfo, table.getForeignKeyInfo());\n+    }\n+    if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Unique Constraints\" + LINE_DELIM);\n+      getUniqueConstraintsInformation(constraintsInfo, table.getUniqueKeyInfo());\n+    }\n+    if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Not Null Constraints\" + LINE_DELIM);\n+      getNotNullConstraintsInformation(constraintsInfo, table.getNotNullConstraint());\n+    }\n+    if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Default Constraints\" + LINE_DELIM);\n+      getDefaultConstraintsInformation(constraintsInfo, table.getDefaultConstraint());\n+    }\n+    if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+      constraintsInfo.append(LINE_DELIM + \"# Check Constraints\" + LINE_DELIM);\n+      getCheckConstraintsInformation(constraintsInfo, table.getCheckConstraint());\n+    }\n+    return constraintsInfo.toString();\n+  }\n+\n+  private void getPrimaryKeyInformation(StringBuilder constraintsInfo, PrimaryKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    formatOutput(\"Constraint Name:\", constraint.getConstraintName(), constraintsInfo);\n+    Map<Integer, String> columnNames = constraint.getColNames();\n+    String title = \"Column Name:\".intern();\n+    for (String columnName : columnNames.values()) {\n+      constraintsInfo.append(String.format(\"%-\" + ALIGNMENT + \"s\", title) + FIELD_DELIM);\n+      formatOutput(new String[] {columnName}, constraintsInfo);\n+    }\n+  }\n+\n+  private void getForeignKeysInformation(StringBuilder constraintsInfo, ForeignKeyInfo constraint) {\n+    formatOutput(\"Table:\", constraint.getChildDatabaseName() + \".\" + constraint.getChildTableName(), constraintsInfo);\n+    Map<String, List<ForeignKeyCol>> foreignKeys = constraint.getForeignKeys();\n+    if (MapUtils.isNotEmpty(foreignKeys)) {\n+      for (Map.Entry<String, List<ForeignKeyCol>> entry : foreignKeys.entrySet()) {\n+        getForeignKeyRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getForeignKeyRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<ForeignKeyCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (ForeignKeyCol column : columns) {\n+        String[] fields = new String[3];\n+        fields[0] = \"Parent Column Name:\" +\n+            column.parentDatabaseName + \".\"+ column.parentTableName + \".\" + column.parentColName;\n+        fields[1] = \"Column Name:\" + column.childColName;\n+        fields[2] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getUniqueConstraintsInformation(StringBuilder constraintsInfo, UniqueConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<UniqueConstraintCol>> uniqueConstraints = constraint.getUniqueConstraints();\n+    if (MapUtils.isNotEmpty(uniqueConstraints)) {\n+      for (Map.Entry<String, List<UniqueConstraintCol>> entry : uniqueConstraints.entrySet()) {\n+        getUniqueConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getUniqueConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<UniqueConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (UniqueConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Key Sequence:\" + column.position;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getNotNullConstraintsInformation(StringBuilder constraintsInfo, NotNullConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, String> notNullConstraints = constraint.getNotNullConstraints();\n+    if (MapUtils.isNotEmpty(notNullConstraints)) {\n+      for (Map.Entry<String, String> entry : notNullConstraints.entrySet()) {\n+        formatOutput(\"Constraint Name:\", entry.getKey(), constraintsInfo);\n+        formatOutput(\"Column Name:\", entry.getValue(), constraintsInfo);\n+        constraintsInfo.append(LINE_DELIM);\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintsInformation(StringBuilder constraintsInfo, DefaultConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<DefaultConstraintCol>> defaultConstraints = constraint.getDefaultConstraints();\n+    if (MapUtils.isNotEmpty(defaultConstraints)) {\n+      for (Map.Entry<String, List<DefaultConstraintCol>> entry : defaultConstraints.entrySet()) {\n+        getDefaultConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getDefaultConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<DefaultConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (DefaultConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Default Value:\" + column.defaultVal;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void getCheckConstraintsInformation(StringBuilder constraintsInfo, CheckConstraint constraint) {\n+    formatOutput(\"Table:\", constraint.getDatabaseName() + \".\" + constraint.getTableName(), constraintsInfo);\n+    Map<String, List<CheckConstraintCol>> checkConstraints = constraint.getCheckConstraints();\n+    if (MapUtils.isNotEmpty(checkConstraints)) {\n+      for (Map.Entry<String, List<CheckConstraintCol>> entry : checkConstraints.entrySet()) {\n+        getCheckConstraintRelInformation(constraintsInfo, entry.getKey(), entry.getValue());\n+      }\n+    }\n+  }\n+\n+  private void getCheckConstraintRelInformation(StringBuilder constraintsInfo, String constraintName,\n+      List<CheckConstraintCol> columns) {\n+    formatOutput(\"Constraint Name:\", constraintName, constraintsInfo);\n+    if (CollectionUtils.isNotEmpty(columns)) {\n+      for (CheckConstraintCol column : columns) {\n+        String[] fields = new String[2];\n+        fields[0] = \"Column Name:\" + column.colName;\n+        fields[1] = \"Check Value:\" + column.checkExpression;\n+        formatOutput(fields, constraintsInfo);\n+      }\n+    }\n+    constraintsInfo.append(LINE_DELIM);\n+  }\n+\n+  private void addExtendedTableData(DataOutputStream out, Table table, Partition partition) throws IOException {\n+    if (partition != null) {\n+      out.write((\"Detailed Partition Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(partition.getTPartition().toString().getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    } else {\n+      out.write((\"Detailed Table Information\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      String tableDesc = HiveStringUtils.escapeJava(table.getTTable().toString());\n+      out.write(tableDesc.getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      out.write(Utilities.newLineCode); // comment column is empty\n+    }\n+  }\n+\n+  private void addExtendedConstraintData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getTableConstraintsInfo().isTableConstraintsInfoNotEmpty()) {\n+      out.write((\"Constraints\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.tabCode);\n+      if (PrimaryKeyInfo.isPrimaryKeyInfoNotEmpty(table.getPrimaryKeyInfo())) {\n+        out.write(table.getPrimaryKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (ForeignKeyInfo.isForeignKeyInfoNotEmpty(table.getForeignKeyInfo())) {\n+        out.write(table.getForeignKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (UniqueConstraint.isUniqueConstraintNotEmpty(table.getUniqueKeyInfo())) {\n+        out.write(table.getUniqueKeyInfo().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (NotNullConstraint.isNotNullConstraintNotEmpty(table.getNotNullConstraint())) {\n+        out.write(table.getNotNullConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (DefaultConstraint.isCheckConstraintNotEmpty(table.getDefaultConstraint())) {\n+        out.write(table.getDefaultConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+      if (CheckConstraint.isCheckConstraintNotEmpty(table.getCheckConstraint())) {\n+        out.write(table.getCheckConstraint().toString().getBytes(\"UTF-8\"));\n+        out.write(Utilities.newLineCode);\n+      }\n+    }\n+  }\n+\n+  private void addExtendedStorageData(DataOutputStream out, Table table)\n+      throws IOException, UnsupportedEncodingException {\n+    if (table.getStorageHandlerInfo() != null) {\n+      out.write((\"StorageHandlerInfo\").getBytes(\"UTF-8\"));\n+      out.write(Utilities.newLineCode);\n+      out.write(table.getStorageHandlerInfo().formatAsText().getBytes(\"UTF-8\"));\n+      out.write(Utilities.newLineCode);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDA2OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924068", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDY2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\nindex c183f042a4..512814606c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/desc/formatter/TextDescTableFormatter.java\n\n@@ -52,6 +52,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Comparator;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDczOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794739", "bodyText": "Collections.emptyList()", "author": "belugabehr", "createdAt": "2021-01-01T18:44:56Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter.JsonDescTableFormatter;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to json format.\n+ */\n+public class JsonShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    List<Map<String, Object>> tableData = new ArrayList<>();\n+    try {\n+      for (Table table : tables) {\n+        tableData.add(makeOneTableStatus(table, db, conf, partition));\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+    ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tableData).build());\n+  }\n+\n+  private Map<String, Object> makeOneTableStatus(Table table, Hive db, HiveConf conf, Partition partition)\n+      throws HiveException, IOException {\n+    StorageInfo storageInfo = getStorageInfo(table, partition);\n+\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"tableName\", table.getTableName());\n+    builder.put(\"ownerType\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\");\n+    builder.put(\"owner\", table.getOwner());\n+    builder.put(\"location\", storageInfo.location);\n+    builder.put(\"inputFormat\", storageInfo.inputFormatClass);\n+    builder.put(\"outputFormat\", storageInfo.outputFormatClass);\n+    builder.put(\"columns\", JsonDescTableFormatter.createColumnsInfo(table.getCols(), new ArrayList<>()));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDQzNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924436", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:45:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDczOQ=="}], "type": "inlineReview", "revised_code": {"commit": "636c0af4548c0da4b23644b16dabfef97a04f1ef", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\nindex 63f615b055..073db26e75 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\n\n@@ -32,6 +32,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDc0NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794745", "bodyText": "Collections.emptyList()", "author": "belugabehr", "createdAt": "2021-01-01T18:45:01Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.info.desc.formatter.JsonDescTableFormatter;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to json format.\n+ */\n+public class JsonShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    List<Map<String, Object>> tableData = new ArrayList<>();\n+    try {\n+      for (Table table : tables) {\n+        tableData.add(makeOneTableStatus(table, db, conf, partition));\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+    ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tableData).build());\n+  }\n+\n+  private Map<String, Object> makeOneTableStatus(Table table, Hive db, HiveConf conf, Partition partition)\n+      throws HiveException, IOException {\n+    StorageInfo storageInfo = getStorageInfo(table, partition);\n+\n+    MapBuilder builder = MapBuilder.create();\n+    builder.put(\"tableName\", table.getTableName());\n+    builder.put(\"ownerType\", (table.getOwnerType() != null) ? table.getOwnerType().name() : \"null\");\n+    builder.put(\"owner\", table.getOwner());\n+    builder.put(\"location\", storageInfo.location);\n+    builder.put(\"inputFormat\", storageInfo.inputFormatClass);\n+    builder.put(\"outputFormat\", storageInfo.outputFormatClass);\n+    builder.put(\"columns\", JsonDescTableFormatter.createColumnsInfo(table.getCols(), new ArrayList<>()));\n+\n+    builder.put(\"partitioned\", table.isPartitioned());\n+    if (table.isPartitioned()) {\n+      builder.put(\"partitionColumns\", JsonDescTableFormatter.createColumnsInfo(table.getPartCols(), new ArrayList<>()));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDMzMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924330", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDc0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "636c0af4548c0da4b23644b16dabfef97a04f1ef", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\nindex 63f615b055..073db26e75 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/JsonShowTableStatusFormatter.java\n\n@@ -32,6 +32,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDg1OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794858", "bodyText": "\"Cannot access File System. File System status will be unknown.", "author": "belugabehr", "createdAt": "2021-01-01T18:46:00Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDM3OQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924379", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:45:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDg1OA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDkzNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794937", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:46:43Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDIwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924209", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:43:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDkzNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk1MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794951", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:46:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924109", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk5OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550794998", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:11Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEyMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924123", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NDk5OA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795009", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDEzOA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924138", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAyNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795027", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:22Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {\n+        fileData.minFileSize = fileLength;\n+      }\n+\n+      if (entryStatus.getAccessTime() > fileData.lastAccessTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkyNDE0Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550924142", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-02T21:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAyNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAzNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795035", "bodyText": "Math.max", "author": "belugabehr", "createdAt": "2021-01-01T18:47:27Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS results.\n+ */\n+public abstract class ShowTableStatusFormatter {\n+  private static final Logger LOG = LoggerFactory.getLogger(ShowTableStatusFormatter.class);\n+\n+  public static ShowTableStatusFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTableStatusFormatter();\n+    } else {\n+      return new TextShowTableStatusFormatter();\n+    }\n+  }\n+\n+  public abstract void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition par)\n+      throws HiveException;\n+\n+  StorageInfo getStorageInfo(Table table, Partition partition) throws HiveException {\n+    String location = null;\n+    String inputFormatClass = null;\n+    String outputFormatClass = null;\n+    if (partition != null) {\n+      if (partition.getLocation() != null) {\n+        location = partition.getDataLocation().toString();\n+      }\n+      inputFormatClass = partition.getInputFormatClass() == null ? null : partition.getInputFormatClass().getName();\n+      outputFormatClass = partition.getOutputFormatClass() == null ? null : partition.getOutputFormatClass().getName();\n+    } else {\n+      if (table.getPath() != null) {\n+        location = table.getDataLocation().toString();\n+      }\n+      inputFormatClass = table.getInputFormatClass() == null ? null : table.getInputFormatClass().getName();\n+      outputFormatClass = table.getOutputFormatClass() == null ? null : table.getOutputFormatClass().getName();\n+    }\n+\n+    return new StorageInfo(location, inputFormatClass, outputFormatClass);\n+  }\n+\n+  final static class StorageInfo {\n+    final String location;\n+    final String inputFormatClass;\n+    final String outputFormatClass;\n+\n+    private StorageInfo(String location, String inputFormatClass, String outputFormatClass) {\n+      this.location = location;\n+      this.inputFormatClass = inputFormatClass;\n+      this.outputFormatClass = outputFormatClass;\n+    }\n+  }\n+\n+  List<Path> getLocations(Hive db, Partition partition, Table table) throws HiveException {\n+    List<Path> locations = new ArrayList<Path>();\n+    if (table.isPartitioned()) {\n+      if (partition == null) {\n+        for (Partition currPartition : db.getPartitions(table)) {\n+          if (currPartition.getLocation() != null) {\n+            locations.add(new Path(currPartition.getLocation()));\n+          }\n+        }\n+      } else {\n+        if (partition.getLocation() != null) {\n+          locations.add(new Path(partition.getLocation()));\n+        }\n+      }\n+    } else {\n+      if (table.getPath() != null) {\n+        locations.add(table.getPath());\n+      }\n+    }\n+    return locations;\n+  }\n+\n+  FileData getFileData(HiveConf conf, List<Path> locations, Path tablePath) throws IOException {\n+    FileData fileData = new FileData();\n+    FileSystem fileSystem = tablePath.getFileSystem(conf);\n+    // in case all files in locations do not exist\n+    try {\n+      FileStatus tmpStatus = fileSystem.getFileStatus(tablePath);\n+      fileData.lastAccessTime = tmpStatus.getAccessTime();\n+      fileData.lastUpdateTime = tmpStatus.getModificationTime();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot access File System. File System status will be unknown: \", e);\n+      fileData.unknown = true;\n+    }\n+\n+    if (!fileData.unknown) {\n+      for (Path location : locations) {\n+        try {\n+          FileStatus status = fileSystem.getFileStatus(location);\n+          // no matter loc is the table location or part location, it must be a\n+          // directory.\n+          if (!status.isDirectory()) {\n+            continue;\n+          }\n+          processDir(status, fileSystem, fileData);\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+    return fileData;\n+  }\n+\n+  private void processDir(FileStatus status, FileSystem fileSystem, FileData fileData) throws IOException {\n+    if (status.getAccessTime() > fileData.lastAccessTime) {\n+      fileData.lastAccessTime = status.getAccessTime();\n+    }\n+    if (status.getModificationTime() > fileData.lastUpdateTime) {\n+      fileData.lastUpdateTime = status.getModificationTime();\n+    }\n+\n+    FileStatus[] entryStatuses = fileSystem.listStatus(status.getPath());\n+    for (FileStatus entryStatus : entryStatuses) {\n+      if (entryStatus.isDirectory()) {\n+        processDir(entryStatus, fileSystem, fileData);\n+        continue;\n+      }\n+\n+      fileData.numOfFiles++;\n+      if (entryStatus.isErasureCoded()) {\n+        fileData.numOfErasureCodedFiles++;\n+      }\n+\n+      long fileLength = entryStatus.getLen();\n+      fileData.totalFileSize += fileLength;\n+      if (fileLength > fileData.maxFileSize) {\n+        fileData.maxFileSize = fileLength;\n+      }\n+      if (fileLength < fileData.minFileSize) {\n+        fileData.minFileSize = fileLength;\n+      }\n+\n+      if (entryStatus.getAccessTime() > fileData.lastAccessTime) {\n+        fileData.lastAccessTime = entryStatus.getAccessTime();\n+      }\n+      if (entryStatus.getModificationTime() > fileData.lastUpdateTime) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNTY5NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550805694", "bodyText": "Nice catch, fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:54:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTAzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\nindex 8845c000f8..d64fd6c849 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/ShowTableStatusFormatter.java\n\n@@ -73,7 +73,7 @@ StorageInfo getStorageInfo(Table table, Partition partition) throws HiveExceptio\n     return new StorageInfo(location, inputFormatClass, outputFormatClass);\n   }\n \n-  final static class StorageInfo {\n+  static final class StorageInfo {\n     final String location;\n     final String inputFormatClass;\n     final String outputFormatClass;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTA3OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795078", "bodyText": "StandardCharsets\nAlso,...\nout.write(\"owner:\");\nout.write(tabler.getOwner());\n```\n\nNo need to concat strings here.", "author": "belugabehr", "createdAt": "2021-01-01T18:48:12Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.status.formatter;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW TABLE STATUS commands to text format.\n+ */\n+public class TextShowTableStatusFormatter extends ShowTableStatusFormatter {\n+  @Override\n+  public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf, List<Table> tables, Partition partition)\n+      throws HiveException {\n+    try {\n+      for (Table table : tables) {\n+        writeBasicInfo(out, table);\n+        writeStorageInfo(out, partition, table);\n+        writeColumnsInfo(out, table);\n+        writeFileSystemInfo(out, db, conf, partition, table);\n+      }\n+    } catch (IOException e) {\n+      throw new HiveException(e);\n+    }\n+  }\n+\n+  private void writeBasicInfo(DataOutputStream out, Table table) throws IOException, UnsupportedEncodingException {\n+    out.write((\"tableName:\" + table.getTableName()).getBytes(\"UTF-8\"));\n+    out.write(Utilities.newLineCode);\n+    out.write((\"owner:\" + table.getOwner()).getBytes(\"UTF-8\"));\n+    out.write(Utilities.newLineCode);", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDI0Ng==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804246", "bodyText": "Fixed.", "author": "miklosgergely", "createdAt": "2021-01-01T20:34:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTA3OA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java\nindex b6c73e9624..552dc31046 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/status/formatter/TextShowTableStatusFormatter.java\n\n@@ -30,6 +30,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTIwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795207", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:49:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.info.show.tables;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils.TextMetaDataTable;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW TABLES results.\n+ */\n+public abstract class ShowTablesFormatter {\n+  public static ShowTablesFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowTablesFormatter();\n+    } else {\n+      return new TextShowTablesFormatter();\n+    }\n+  }\n+\n+  public abstract void showTables(DataOutputStream out, List<String> tables) throws HiveException;\n+\n+  abstract void showTablesExtended(DataOutputStream out, List<Table> tables) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowTablesFormatter extends ShowTablesFormatter {\n+    @Override\n+    public void showTables(DataOutputStream out, List<String> tables) throws HiveException {\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"tables\", tables).build());\n+    }\n+\n+    @Override\n+    void showTablesExtended(DataOutputStream out, List<Table> tables) throws HiveException {\n+      if (tables.isEmpty()) {\n+        return;\n+      }\n+\n+      List<Map<String, Object>> tableDataList = new ArrayList<>();\n+      for (Table table : tables) {\n+        Map<String, Object> tableData = ImmutableMap.of(\n+            \"Table Name\", table.getTableName(),\n+            \"Table Type\", table.getTableType().toString());\n+        tableDataList.add(tableData);\n+      }\n+\n+      ShowUtils.asJson(out, ImmutableMap.of(\"tables\", tableDataList));\n+    }\n+  }\n+\n+  static class TextShowTablesFormatter extends ShowTablesFormatter {\n+    @Override\n+    public void showTables(DataOutputStream out, List<String> tables) throws HiveException {\n+      Iterator<String> iterTbls = tables.iterator();\n+\n+      try {\n+        while (iterTbls.hasNext()) {\n+          // create a row per table name\n+          out.write(iterTbls.next().getBytes(\"UTF-8\"));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDQ0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804441", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-01T20:36:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTIwNw=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java\nindex 5b42abf1e9..df5420cd6b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/info/show/tables/ShowTablesFormatter.java\n\n@@ -31,6 +31,7 @@\n \n import java.io.DataOutputStream;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTI0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795241", "bodyText": "JDK String.split", "author": "belugabehr", "createdAt": "2021-01-01T18:50:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.partition.show;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.common.FileUtils;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MapBuilder;\n+import org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Formats SHOW PARTITIONS results.\n+ */\n+abstract class ShowPartitionsFormatter {\n+  static ShowPartitionsFormatter getFormatter(HiveConf conf) {\n+    if (MetaDataFormatUtils.isJson(conf)) {\n+      return new JsonShowPartitionsFormatter();\n+    } else {\n+      return new TextShowPartitionsFormatter();\n+    }\n+  }\n+\n+  abstract void showTablePartitions(DataOutputStream out, List<String> partitions) throws HiveException;\n+\n+  // ------ Implementations ------\n+\n+  static class JsonShowPartitionsFormatter extends ShowPartitionsFormatter {\n+    @Override\n+    void showTablePartitions(DataOutputStream out, List<String> partitions) throws HiveException {\n+      List<Map<String, Object>> partitionData = new ArrayList<>(partitions.size());\n+      for (String partition : partitions) {\n+        partitionData.add(makeOneTablePartition(partition));\n+      }\n+      ShowUtils.asJson(out, MapBuilder.create().put(\"partitions\", partitionData).build());\n+    }\n+\n+    // TODO: This seems like a very wrong implementation.\n+    private Map<String, Object> makeOneTablePartition(String partition) {\n+      List<Map<String, Object>> result = new ArrayList<>();\n+\n+      List<String> names = new ArrayList<String>();\n+      for (String part : StringUtils.split(partition, \"/\")) {", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDM1Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804352", "bodyText": "Done", "author": "miklosgergely", "createdAt": "2021-01-01T20:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTI0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java\nindex 85d6dac4c2..f2cd405f09 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/show/ShowPartitionsFormatter.java\n\n@@ -68,7 +68,7 @@ void showTablePartitions(DataOutputStream out, List<String> partitions) throws H\n       List<Map<String, Object>> result = new ArrayList<>();\n \n       List<String> names = new ArrayList<String>();\n-      for (String part : StringUtils.split(partition, \"/\")) {\n+      for (String part : partition.split(\"/\")) {\n         String name = part;\n         String value = null;\n         String[] keyValue = StringUtils.split(part, \"=\", 2);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTM2MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550795360", "bodyText": "StandardCharsets", "author": "belugabehr", "createdAt": "2021-01-01T18:52:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.workloadmanagement.resourceplan.show.formatter;\n+\n+import org.apache.hadoop.hive.metastore.api.WMFullResourcePlan;\n+import org.apache.hadoop.hive.metastore.api.WMResourcePlan;\n+import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Formats SHOW RESOURCE PLAN(S) results to text format.\n+ */\n+class TextShowResourcePlanFormatter extends ShowResourcePlanFormatter {\n+  @Override\n+  public void showResourcePlans(DataOutputStream out, List<WMResourcePlan> resourcePlans) throws HiveException {\n+    try {\n+      for (WMResourcePlan plan : resourcePlans) {\n+        out.write(plan.getName().getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        out.write(plan.getStatus().name().getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        String queryParallelism = plan.isSetQueryParallelism() ? Integer.toString(plan.getQueryParallelism()) : \"null\";\n+        out.write(queryParallelism.getBytes(ShowUtils.UTF_8));\n+        out.write(Utilities.tabCode);\n+        String defaultPoolPath = plan.isSetDefaultPoolPath() ? plan.getDefaultPoolPath() : \"null\";\n+        out.write(defaultPoolPath.getBytes(ShowUtils.UTF_8));", "originalCommit": "45375271e127db5186799ed4798ac8fc4225e785", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDgwNDUyMA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r550804520", "bodyText": "Fixed", "author": "miklosgergely", "createdAt": "2021-01-01T20:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDc5NTM2MA=="}], "type": "inlineReview", "revised_code": {"commit": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java\nindex 037846259e..2c00b7a4d7 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/workloadmanagement/resourceplan/show/formatter/TextShowResourcePlanFormatter.java\n\n@@ -20,12 +20,12 @@\n \n import org.apache.hadoop.hive.metastore.api.WMFullResourcePlan;\n import org.apache.hadoop.hive.metastore.api.WMResourcePlan;\n-import org.apache.hadoop.hive.ql.ddl.ShowUtils;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n \n import java.io.DataOutputStream;\n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n \n /**\n"}}, {"oid": "68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "url": "https://github.com/apache/hive/commit/68bc6de90ed3447ed7a4650183b1ff7c1e47cecb", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-01T21:07:15Z", "type": "forcePushed"}, {"oid": "4e3d3432aa90f33c2c2f7600ded88b992c866cb8", "url": "https://github.com/apache/hive/commit/4e3d3432aa90f33c2c2f7600ded88b992c866cb8", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-01T23:39:59Z", "type": "forcePushed"}, {"oid": "4162a391704b6519a4bfed0074fe14b19c5a9099", "url": "https://github.com/apache/hive/commit/4162a391704b6519a4bfed0074fe14b19c5a9099", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T14:00:09Z", "type": "forcePushed"}, {"oid": "dd51df4b8e215b16c573ffc0896004b1737a4f8f", "url": "https://github.com/apache/hive/commit/dd51df4b8e215b16c573ffc0896004b1737a4f8f", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T16:41:46Z", "type": "forcePushed"}, {"oid": "636c0af4548c0da4b23644b16dabfef97a04f1ef", "url": "https://github.com/apache/hive/commit/636c0af4548c0da4b23644b16dabfef97a04f1ef", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T22:02:18Z", "type": "forcePushed"}, {"oid": "9a75f17ed23d20705d6702fb7e6cf81f91c09349", "url": "https://github.com/apache/hive/commit/9a75f17ed23d20705d6702fb7e6cf81f91c09349", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T22:50:32Z", "type": "forcePushed"}, {"oid": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "url": "https://github.com/apache/hive/commit/6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-02T23:01:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTEwNw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552889107", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:28:45Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java", "diffHunk": "@@ -20,25 +20,17 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.HashMap;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTA3NA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905074", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTEwNw=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java\nindex 5e5e1ce0d7..364f110113 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/drop/AbstractDropPartitionAnalyzer.java\n\n@@ -20,17 +20,25 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n \n+import org.antlr.runtime.tree.CommonTree;\n+import org.antlr.runtime.tree.Tree;\n import org.apache.hadoop.hive.common.TableName;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableAnalyzer;\n import org.apache.hadoop.hive.ql.ddl.table.AlterTableType;\n+import org.apache.hadoop.hive.ql.ddl.table.partition.PartitionUtils;\n+import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.hooks.ReadEntity;\n import org.apache.hadoop.hive.ql.hooks.WriteEntity;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTc1OA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552889758", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:29:54Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java", "diffHunk": "@@ -68,7 +68,8 @@ public void analyzeInternal(ASTNode root) throws SemanticException {\n     }\n \n     Table table = getTable(tableName);\n-    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf, false);\n+    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf,\n+        false);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTIxNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905214", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg4OTc1OA=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java\nindex 507a3a911a..14b6c486dd 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/misc/msck/MsckAnalyzer.java\n\n@@ -68,8 +68,7 @@ public void analyzeInternal(ASTNode root) throws SemanticException {\n     }\n \n     Table table = getTable(tableName);\n-    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf,\n-        false);\n+    Map<Integer, List<ExprNodeGenericFuncDesc>> partitionSpecs = ParseUtils.getFullPartitionSpecs(root, table, conf, false);\n     byte[] filterExp = null;\n     if (partitionSpecs != null & !partitionSpecs.isEmpty()) {\n       // expression proxy class needs to be PartitionExpressionForMetastore since we intend to use the\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDE0MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890140", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.\nI think this is probably an artifact of your IDE.", "author": "belugabehr", "createdAt": "2021-01-06T18:30:41Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java", "diffHunk": "@@ -31,9 +31,11 @@\n public class ShowCompactionsDesc implements DDLDesc, Serializable {\n   private static final long serialVersionUID = 1L;\n \n+  // @formatter:off\n   public static final String SCHEMA =\n       \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n       \"string:string:string:string:string:string:string:string:string:string:string:string:string\";\n+  // @formatter:on", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTEzNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905134", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDE0MA=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\nindex f2939baee1..ae1618d6cc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/process/show/compactions/ShowCompactionsDesc.java\n\n@@ -31,11 +31,9 @@\n public class ShowCompactionsDesc implements DDLDesc, Serializable {\n   private static final long serialVersionUID = 1L;\n \n-  // @formatter:off\n   public static final String SCHEMA =\n       \"compactionid,dbname,tabname,partname,type,state,hostname,workerid,enqueuetime,starttime,duration,hadoopjobid,errormessage#\" +\n       \"string:string:string:string:string:string:string:string:string:string:string:string:string\";\n-  // @formatter:on\n \n   private String resFile;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDI1Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890253", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:30:55Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java", "diffHunk": "@@ -35,7 +35,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.constraint.add.AlterTableAddConstraintOperation;\n-import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTMyNg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905326", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDI1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java\nindex 55eda7ae12..0d2cee5a3b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AbstractAlterTableOperation.java\n\n@@ -35,6 +35,7 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.constraint.add.AlterTableAddConstraintOperation;\n+import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n import org.apache.hadoop.hive.ql.hooks.ReadEntity;\n import org.apache.hadoop.hive.ql.hooks.WriteEntity;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDM5MA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890390", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:31:10Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java", "diffHunk": "@@ -79,7 +79,8 @@ protected void analyzeCommand(TableName tableName, Map<String, String> partition\n       throw new SemanticException(ErrorMsg.NOT_RECOGNIZED_CONSTRAINT.getMsg(constraintNode.getToken().getText()));\n     }\n \n-    Constraints constraints = new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);\n+    Constraints constraints =\n+        new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTI5Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905297", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDM5MA=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java\nindex 420ef53198..ec326a2a01 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java\n\n@@ -79,8 +79,7 @@ protected void analyzeCommand(TableName tableName, Map<String, String> partition\n       throw new SemanticException(ErrorMsg.NOT_RECOGNIZED_CONSTRAINT.getMsg(constraintNode.getToken().getText()));\n     }\n \n-    Constraints constraints =\n-        new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);\n+    Constraints constraints = new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null, checkConstraints);\n     AlterTableAddConstraintDesc desc = new AlterTableAddConstraintDesc(tableName, null, constraints);\n     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(), desc)));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MDgwNQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552890805", "bodyText": "I believe these changes are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:31:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java", "diffHunk": "@@ -83,5 +82,4 @@ public Long getWriteId() {\n   public boolean mayNeedWriteId() {\n     return true;\n   }\n-", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java\nindex 4b175dc598..b86abbd04f 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/drop/AlterTableDropConstraintDesc.java\n\n@@ -82,4 +83,5 @@ public Long getWriteId() {\n   public boolean mayNeedWriteId() {\n     return true;\n   }\n+\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTUwOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891509", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java", "diffHunk": "@@ -23,9 +23,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n-import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n-import org.apache.hadoop.hive.ql.lockmgr.LockException;\n-import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTM4Nw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905387", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTUwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java\nindex d4be520dd4..9dbca9ca13 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowDbLocksAnalyzer.java\n\n@@ -23,6 +23,9 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n+import org.apache.hadoop.hive.ql.lockmgr.LockException;\n+import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;\n import org.apache.hadoop.hive.ql.parse.ASTNode;\n import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n import org.apache.hadoop.hive.ql.parse.HiveParser;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTYxMQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891611", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:36Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java", "diffHunk": "@@ -26,9 +26,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n-import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n-import org.apache.hadoop.hive.ql.lockmgr.LockException;\n-import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTcxMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905713", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTYxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java\nindex 96f52bff06..4bfbc70da8 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/lock/show/ShowLocksAnalyzer.java\n\n@@ -26,6 +26,9 @@\n import org.apache.hadoop.hive.ql.ddl.DDLWork;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n+import org.apache.hadoop.hive.ql.lockmgr.LockException;\n+import org.apache.hadoop.hive.ql.lockmgr.TxnManagerFactory;\n import org.apache.hadoop.hive.ql.parse.ASTNode;\n import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n import org.apache.hadoop.hive.ql.parse.HiveParser;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTg0MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891841", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:33:59Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java", "diffHunk": "@@ -234,7 +233,7 @@ public void setWriteId(long writeId) {\n \n   @Override\n   public String getFullTableName() {\n-    return AcidUtils.getFullTableName(dbName,tableName);\n+    return AcidUtils.getFullTableName(dbName, tableName);", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTQ0Mg==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905442", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTg0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java\nindex 231e3f14fc..d61c575173 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/add/AlterTableAddPartitionDesc.java\n\n@@ -233,7 +234,7 @@ public void setWriteId(long writeId) {\n \n   @Override\n   public String getFullTableName() {\n-    return AcidUtils.getFullTableName(dbName, tableName);\n+    return AcidUtils.getFullTableName(dbName,tableName);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MTkxOQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552891919", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:11Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java", "diffHunk": "@@ -26,7 +26,6 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.AlterTableUtils;\n-import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java\nindex fab95484ad..59bd797c9c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/partition/rename/AlterTableRenamePartitionOperation.java\n\n@@ -26,6 +26,7 @@\n import org.apache.hadoop.hive.ql.ddl.DDLOperationContext;\n import org.apache.hadoop.hive.ql.ddl.DDLUtils;\n import org.apache.hadoop.hive.ql.ddl.table.AlterTableUtils;\n+import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n import org.apache.hadoop.hive.ql.hooks.ReadEntity;\n import org.apache.hadoop.hive.ql.hooks.WriteEntity;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjAwMw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892003", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:22Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java", "diffHunk": "@@ -20,7 +20,6 @@\n \n import java.net.URI;\n import java.net.URISyntaxException;\n-import java.util.ArrayList;", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTQ4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905485", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:00:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjAwMw=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java\nindex 72a6572eba..d9fe3520cd 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/storage/skewed/AlterTableSetSkewedLocationAnalyzer.java\n\n@@ -20,6 +20,7 @@\n \n import java.net.URI;\n import java.net.URISyntaxException;\n+import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjA4NQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892085", "bodyText": "All changes in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:34:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java", "diffHunk": "@@ -84,7 +84,7 @@ private void validateCreateView(AlterViewAsDesc desc, SemanticAnalyzer analyzer)\n \n     if (oldView == null) {\n       String viewNotExistErrorMsg = \"The following view does not exist: \" + desc.getViewName();\n-      throw new SemanticException( ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));\n+      throw new SemanticException(ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTUyNA==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905524", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjA4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java\nindex 5884a29e2e..f0ece2940c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/view/create/AlterViewAsAnalyzer.java\n\n@@ -84,7 +84,7 @@ private void validateCreateView(AlterViewAsDesc desc, SemanticAnalyzer analyzer)\n \n     if (oldView == null) {\n       String viewNotExistErrorMsg = \"The following view does not exist: \" + desc.getViewName();\n-      throw new SemanticException(ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));\n+      throw new SemanticException( ErrorMsg.ALTER_VIEW_AS_SELECT_NOT_EXIST.getMsg(viewNotExistErrorMsg));\n     }\n \n     validateReplaceWithPartitions(desc.getViewName(), oldView, null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjY2Mw==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552892663", "bodyText": "All changes (there are several) in this file are out of scope of the PR.  Please revert.", "author": "belugabehr", "createdAt": "2021-01-06T18:35:42Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java", "diffHunk": "@@ -17,51 +17,53 @@\n  */\n package org.apache.hadoop.hive.ql.metadata.formatting;\n \n-import java.util.HashMap;\n import java.util.LinkedHashMap;\n import java.util.Map;\n \n /**\n  * Helper class to build Maps consumed by the JSON formatter.  Only\n  * add non-null entries to the Map.\n  */\n-public class MapBuilder {\n-    private Map<String, Object> map = new LinkedHashMap<String, Object>();\n+public final class MapBuilder {\n+  private Map<String, Object> map = new LinkedHashMap<String, Object>();\n \n-    private MapBuilder() {}\n+  private MapBuilder() {\n+  }\n \n-    public static MapBuilder create() {\n-        return new MapBuilder();\n-    }\n+  public static MapBuilder create() {\n+    return new MapBuilder();\n+  }\n \n-    public MapBuilder put(String name, Object val) {\n-        if (val != null)\n-            map.put(name, val);\n-        return this;\n+  public MapBuilder put(String name, Object val) {\n+    if (val != null) {\n+      map.put(name, val);\n     }\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, boolean val) {\n-        map.put(name, Boolean.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, boolean val) {\n+    map.put(name, Boolean.valueOf(val));\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, int val) {\n-        map.put(name, Integer.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, int val) {\n+    map.put(name, Integer.valueOf(val));\n+    return this;\n+  }\n \n-    public MapBuilder put(String name, long val) {\n-        map.put(name, Long.valueOf(val));\n-        return this;\n-    }\n+  public MapBuilder put(String name, long val) {\n+    map.put(name, Long.valueOf(val));\n+    return this;\n+  }\n \n-    public <T> MapBuilder put(String name, T val, boolean use) {\n-        if (use)\n-            put(name, val);\n-        return this;\n+  public <T> MapBuilder put(String name, T val, boolean use) {\n+    if (use) {\n+      put(name, val);\n     }\n+    return this;\n+  }\n \n-    public Map<String, Object> build() {\n-        return map;\n-    }\n+  public Map<String, Object> build() {\n+    return map;\n+  }", "originalCommit": "6b933d0b35c795e73569314dd8ac0ec0db4fe8fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkwNTU2MQ==", "url": "https://github.com/apache/hive/pull/1756#discussion_r552905561", "bodyText": "Removed.", "author": "miklosgergely", "createdAt": "2021-01-06T19:01:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjg5MjY2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java\nindex d76f906a09..6b011c62c8 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MapBuilder.java\n\n@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.metadata.formatting;\n \n+import java.util.HashMap;\n import java.util.LinkedHashMap;\n import java.util.Map;\n \n"}}, {"oid": "0dbd8020ff5779ede7ab743e2df2b1da89fee455", "url": "https://github.com/apache/hive/commit/0dbd8020ff5779ede7ab743e2df2b1da89fee455", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-06T18:59:34Z", "type": "forcePushed"}, {"oid": "c75da22d29c78c81dbb0aed961261ec2affda89e", "url": "https://github.com/apache/hive/commit/c75da22d29c78c81dbb0aed961261ec2affda89e", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-08T17:15:37Z", "type": "commit"}, {"oid": "c75da22d29c78c81dbb0aed961261ec2affda89e", "url": "https://github.com/apache/hive/commit/c75da22d29c78c81dbb0aed961261ec2affda89e", "message": "HIVE-24509 Move show specific codes under DDL and cut MetaDataFormatter classes to pieces", "committedDate": "2021-01-08T17:15:37Z", "type": "forcePushed"}]}