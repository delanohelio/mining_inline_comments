{"pr_number": 1085, "pr_title": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "pr_createdAt": "2020-06-09T23:53:23Z", "pr_url": "https://github.com/apache/hive/pull/1085", "timeline": [{"oid": "9356e8806f0c2bd5a59258408d4a91c6c4ab7663", "url": "https://github.com/apache/hive/commit/9356e8806f0c2bd5a59258408d4a91c6c4ab7663", "message": "Merge pull request #1 from apache/master\n\nsyncing forked repo using upstream - 05-29-2018", "committedDate": "2018-05-29T23:29:51Z", "type": "commit"}, {"oid": "ff83095bba9aceaf83dcc3dcea011f3e9f15d2fc", "url": "https://github.com/apache/hive/commit/ff83095bba9aceaf83dcc3dcea011f3e9f15d2fc", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2018-11-10T04:16:58Z", "type": "commit"}, {"oid": "e7ab4541e40a4755211131562ea9ac94ec013845", "url": "https://github.com/apache/hive/commit/e7ab4541e40a4755211131562ea9ac94ec013845", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2018-11-12T21:03:19Z", "type": "commit"}, {"oid": "b87f4af7d714f907e4dcd503c09228048b578ef2", "url": "https://github.com/apache/hive/commit/b87f4af7d714f907e4dcd503c09228048b578ef2", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-02-28T05:53:28Z", "type": "commit"}, {"oid": "d0bf01999555fd45e427c96e8f32978470385a53", "url": "https://github.com/apache/hive/commit/d0bf01999555fd45e427c96e8f32978470385a53", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-03-04T19:15:20Z", "type": "commit"}, {"oid": "2c3c26a201be3ebaff9c5eaf257b255902552f84", "url": "https://github.com/apache/hive/commit/2c3c26a201be3ebaff9c5eaf257b255902552f84", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-03-24T21:58:36Z", "type": "commit"}, {"oid": "c26b2f15d9f95c5c22f3dd2bd57a68fcc193321d", "url": "https://github.com/apache/hive/commit/c26b2f15d9f95c5c22f3dd2bd57a68fcc193321d", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-03-28T22:33:16Z", "type": "commit"}, {"oid": "a6131dedbb788556dc761285c94e29abc6ee49d3", "url": "https://github.com/apache/hive/commit/a6131dedbb788556dc761285c94e29abc6ee49d3", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-03-29T15:24:42Z", "type": "commit"}, {"oid": "3f83d0d080c7fff9b2367822b2a4f947a528ac44", "url": "https://github.com/apache/hive/commit/3f83d0d080c7fff9b2367822b2a4f947a528ac44", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-04-01T18:15:27Z", "type": "commit"}, {"oid": "2930b607b7a6389e527c2a42beaa277eea712551", "url": "https://github.com/apache/hive/commit/2930b607b7a6389e527c2a42beaa277eea712551", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-04-02T17:33:41Z", "type": "commit"}, {"oid": "28de80dc6595b23820b975006eef90f6f99271cf", "url": "https://github.com/apache/hive/commit/28de80dc6595b23820b975006eef90f6f99271cf", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-04-04T00:59:19Z", "type": "commit"}, {"oid": "d6c469894076445c06621847890db0df0788a4b5", "url": "https://github.com/apache/hive/commit/d6c469894076445c06621847890db0df0788a4b5", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-04-17T23:00:55Z", "type": "commit"}, {"oid": "10df87f2269ebf0b311cd4f5a54144a1d1f36e62", "url": "https://github.com/apache/hive/commit/10df87f2269ebf0b311cd4f5a54144a1d1f36e62", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-05-14T22:56:01Z", "type": "commit"}, {"oid": "d696e87a89ad9b8b1bdc8dca7c2477c45b1135f3", "url": "https://github.com/apache/hive/commit/d696e87a89ad9b8b1bdc8dca7c2477c45b1135f3", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-05-30T20:51:46Z", "type": "commit"}, {"oid": "e1d161d08d735e83c536c98047890c3a9ac0627f", "url": "https://github.com/apache/hive/commit/e1d161d08d735e83c536c98047890c3a9ac0627f", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-06-07T20:40:46Z", "type": "commit"}, {"oid": "dd00d0130ebb0cb17e4ef4191345929c8a6aa5e6", "url": "https://github.com/apache/hive/commit/dd00d0130ebb0cb17e4ef4191345929c8a6aa5e6", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-06-08T23:09:32Z", "type": "commit"}, {"oid": "0c487a830fa5e8f2dda36c4b35fbf6b959e1dae5", "url": "https://github.com/apache/hive/commit/0c487a830fa5e8f2dda36c4b35fbf6b959e1dae5", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-06-27T05:54:25Z", "type": "commit"}, {"oid": "cf73160ddb17e14f00270f16f59bd58726fea7f9", "url": "https://github.com/apache/hive/commit/cf73160ddb17e14f00270f16f59bd58726fea7f9", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-01T17:34:58Z", "type": "commit"}, {"oid": "2966320f1460598d96ad072d8b4ecd18826d64c6", "url": "https://github.com/apache/hive/commit/2966320f1460598d96ad072d8b4ecd18826d64c6", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-03T00:08:49Z", "type": "commit"}, {"oid": "c8b229e0638c1327ae000edaa6708dbe3ff3c60e", "url": "https://github.com/apache/hive/commit/c8b229e0638c1327ae000edaa6708dbe3ff3c60e", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-07T21:42:35Z", "type": "commit"}, {"oid": "5dc24803128fe0d7416810241b900e77f249aaa4", "url": "https://github.com/apache/hive/commit/5dc24803128fe0d7416810241b900e77f249aaa4", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-08T17:59:23Z", "type": "commit"}, {"oid": "452a15ea3bcf0cc3b13caa3dd5bf290f5efa70b3", "url": "https://github.com/apache/hive/commit/452a15ea3bcf0cc3b13caa3dd5bf290f5efa70b3", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-09T04:23:15Z", "type": "commit"}, {"oid": "03021f28579f46273c6bb9e14cb0d8e2edf94b2b", "url": "https://github.com/apache/hive/commit/03021f28579f46273c6bb9e14cb0d8e2edf94b2b", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-10T22:23:33Z", "type": "commit"}, {"oid": "4a0ac53b9fdf5a4928959e5885ab3407d924c36b", "url": "https://github.com/apache/hive/commit/4a0ac53b9fdf5a4928959e5885ab3407d924c36b", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-26T04:40:47Z", "type": "commit"}, {"oid": "b8a5d230377c412db74f51b06f069d886ba02c0a", "url": "https://github.com/apache/hive/commit/b8a5d230377c412db74f51b06f069d886ba02c0a", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-07-31T23:08:43Z", "type": "commit"}, {"oid": "eaa61164c1242e6f91101ac9e3ba2cfe137de46e", "url": "https://github.com/apache/hive/commit/eaa61164c1242e6f91101ac9e3ba2cfe137de46e", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-08-02T22:51:26Z", "type": "commit"}, {"oid": "f90630fd43e31962e6c0a1433ee6f1bf75ef98a4", "url": "https://github.com/apache/hive/commit/f90630fd43e31962e6c0a1433ee6f1bf75ef98a4", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-08-15T22:50:26Z", "type": "commit"}, {"oid": "0cec5671a408c78144ef1e716147273655b1ea9b", "url": "https://github.com/apache/hive/commit/0cec5671a408c78144ef1e716147273655b1ea9b", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-08-22T19:46:47Z", "type": "commit"}, {"oid": "ac29032cdc8069f1f47a9369aa91f5b6ff82279f", "url": "https://github.com/apache/hive/commit/ac29032cdc8069f1f47a9369aa91f5b6ff82279f", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-08-26T18:39:15Z", "type": "commit"}, {"oid": "0b958f50ba094a7fee696b88b85c89886efa6a8d", "url": "https://github.com/apache/hive/commit/0b958f50ba094a7fee696b88b85c89886efa6a8d", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-09-16T19:15:34Z", "type": "commit"}, {"oid": "094ea53742319da3c6a3fe976e2e6aa330c9e879", "url": "https://github.com/apache/hive/commit/094ea53742319da3c6a3fe976e2e6aa330c9e879", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-10-07T19:10:50Z", "type": "commit"}, {"oid": "e231fbe8766fa4bceb60ad355bb7bba747f8e6c0", "url": "https://github.com/apache/hive/commit/e231fbe8766fa4bceb60ad355bb7bba747f8e6c0", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-10-16T22:04:06Z", "type": "commit"}, {"oid": "fe3870423ec0fbe0659862edc826425492410f65", "url": "https://github.com/apache/hive/commit/fe3870423ec0fbe0659862edc826425492410f65", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-10-23T22:13:53Z", "type": "commit"}, {"oid": "519eb174c502740f468f45d8a985d500da2e4808", "url": "https://github.com/apache/hive/commit/519eb174c502740f468f45d8a985d500da2e4808", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-11-06T21:18:52Z", "type": "commit"}, {"oid": "f073eb005fd49e657a4fea7afc10c52836021ba5", "url": "https://github.com/apache/hive/commit/f073eb005fd49e657a4fea7afc10c52836021ba5", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-11-21T00:41:56Z", "type": "commit"}, {"oid": "fff0fc6f73e27a5fc860cc377aeae17e4d32cef2", "url": "https://github.com/apache/hive/commit/fff0fc6f73e27a5fc860cc377aeae17e4d32cef2", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2019-12-11T23:16:50Z", "type": "commit"}, {"oid": "00981a4be904eed888d342fd831f8c228fc0f1f4", "url": "https://github.com/apache/hive/commit/00981a4be904eed888d342fd831f8c228fc0f1f4", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-01-09T19:47:53Z", "type": "commit"}, {"oid": "81bf0923138150b32a1ddbea7152e9a986c6dc21", "url": "https://github.com/apache/hive/commit/81bf0923138150b32a1ddbea7152e9a986c6dc21", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-01-22T21:50:33Z", "type": "commit"}, {"oid": "1a94c8ce7392268eacb8e05367a9ed5f9b0e5a81", "url": "https://github.com/apache/hive/commit/1a94c8ce7392268eacb8e05367a9ed5f9b0e5a81", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-01-23T22:22:17Z", "type": "commit"}, {"oid": "d433ba6184ca3e21c7a7e3be57ec337f4cb64ebf", "url": "https://github.com/apache/hive/commit/d433ba6184ca3e21c7a7e3be57ec337f4cb64ebf", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-02-08T02:14:27Z", "type": "commit"}, {"oid": "f63c809f63b01bc9048df72a928015cf757859b3", "url": "https://github.com/apache/hive/commit/f63c809f63b01bc9048df72a928015cf757859b3", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-02-14T17:06:01Z", "type": "commit"}, {"oid": "b55f884342f2679f0dad73be5777067aa7a4c20e", "url": "https://github.com/apache/hive/commit/b55f884342f2679f0dad73be5777067aa7a4c20e", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-05-18T23:55:34Z", "type": "commit"}, {"oid": "0dca013df443e7fca07c6240d1bb667f16f5eb36", "url": "https://github.com/apache/hive/commit/0dca013df443e7fca07c6240d1bb667f16f5eb36", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-05-31T05:20:09Z", "type": "commit"}, {"oid": "34eb94e9fd1d8230afdbde5d1624a155270a320a", "url": "https://github.com/apache/hive/commit/34eb94e9fd1d8230afdbde5d1624a155270a320a", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-06-02T03:22:34Z", "type": "commit"}, {"oid": "f11427ad7d9d7ac501b31d343c188ce80d852971", "url": "https://github.com/apache/hive/commit/f11427ad7d9d7ac501b31d343c188ce80d852971", "message": "Merge remote-tracking branch 'upstream/master'", "committedDate": "2020-06-08T21:29:14Z", "type": "commit"}, {"oid": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "url": "https://github.com/apache/hive/commit/91ad99ab38ec5149350a997c0b0b3beb66b1f922", "message": "HIVE-22255: Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-09T23:42:56Z", "type": "commit"}, {"oid": "15431ae0cf6959adb166048be29989b39e82a4ba", "url": "https://github.com/apache/hive/commit/15431ae0cf6959adb166048be29989b39e82a4ba", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-11T21:14:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzMzIwNg==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439333206", "bodyText": "Why do we need this?", "author": "pvary", "createdAt": "2020-06-12T10:16:45Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -282,6 +282,7 @@ private CompactionType checkForCompaction(final CompactionInfo ci,\n     }\n \n     if (runJobAsSelf(runAs)) {\n+      ci.runAs = runAs;", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTY2Ng==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443905666", "bodyText": "without setting explicitly here, insert query find it null and skip the here", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzMzIwNg=="}], "type": "inlineReview", "revised_code": {"commit": "523433cc6e14d0a37f01fda3ac548bff114de927", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\nindex 1dfc91f2fc..ed5ed8d379 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n\n@@ -282,7 +290,6 @@ private CompactionType checkForCompaction(final CompactionInfo ci,\n     }\n \n     if (runJobAsSelf(runAs)) {\n-      ci.runAs = runAs;\n       return determineCompactionType(ci, writeIds, sd, tblproperties);\n     } else {\n       LOG.info(\"Going to initiate as user \" + runAs + \" for \" + ci.getFullPartitionName());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzNjQxNg==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439336416", "bodyText": "Maybe use findAny() instead of count()?", "author": "pvary", "createdAt": "2020-06-12T10:24:19Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -353,6 +354,16 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n           HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_PCT_THRESHOLD) :\n           Float.parseFloat(deltaPctProp);\n       boolean bigEnough =   (float)deltaSize/(float)baseSize > deltaPctThreshold;\n+      boolean multiBase = dir.getObsolete().stream()\n+              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).count() >= 1;", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTc5Nw==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443905797", "bodyText": "incorporated the suggested change.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:08:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzNjQxNg=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\nindex 1dfc91f2fc..805c3221c6 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n\n@@ -355,8 +363,8 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n           Float.parseFloat(deltaPctProp);\n       boolean bigEnough =   (float)deltaSize/(float)baseSize > deltaPctThreshold;\n       boolean multiBase = dir.getObsolete().stream()\n-              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).count() >= 1;\n-      if ((deltaSize == 0  && dir.getObsolete().size() > 0) && multiBase) {\n+              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).findAny().isPresent();\n+      if (deltaSize == 0  && multiBase) {\n         try {\n           txnHandler.requestCleanup(ci);\n           return null;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzNzE2Mg==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439337162", "bodyText": "Unnecessary parenthesis and also I think dir.getObsolete().size() check is not needed, since multiBase is true", "author": "pvary", "createdAt": "2020-06-12T10:25:57Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -353,6 +354,16 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n           HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_PCT_THRESHOLD) :\n           Float.parseFloat(deltaPctProp);\n       boolean bigEnough =   (float)deltaSize/(float)baseSize > deltaPctThreshold;\n+      boolean multiBase = dir.getObsolete().stream()\n+              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).count() >= 1;\n+      if ((deltaSize == 0  && dir.getObsolete().size() > 0) && multiBase) {", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTg0NQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443905845", "bodyText": "incorporated the suggested change.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:09:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzNzE2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\nindex 1dfc91f2fc..805c3221c6 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n\n@@ -355,8 +363,8 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n           Float.parseFloat(deltaPctProp);\n       boolean bigEnough =   (float)deltaSize/(float)baseSize > deltaPctThreshold;\n       boolean multiBase = dir.getObsolete().stream()\n-              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).count() >= 1;\n-      if ((deltaSize == 0  && dir.getObsolete().size() > 0) && multiBase) {\n+              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).findAny().isPresent();\n+      if (deltaSize == 0  && multiBase) {\n         try {\n           txnHandler.requestCleanup(ci);\n           return null;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzOTk4NQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439339985", "bodyText": "Meybe we should move this select out to a different method, since this is the copy of the one used in compact().\nAlso I would add state cleaning as well. We do not want 2 cleaners running parallel", "author": "pvary", "createdAt": "2020-06-12T10:32:55Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTczMQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439341731", "bodyText": "Can we use preparedstatement here as well?", "author": "pvary", "createdAt": "2020-06-12T10:36:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzOTk4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNDEzNw==", "url": "https://github.com/apache/hive/pull/1085#discussion_r442034137", "bodyText": "Further clarified thoughts:\n\nIt would be good to have a single method inserting to the COMPACTION_QUEUE table\nWe should use a preparedstatement for this so JDBC execution could be faster\nTxnHandler.compact() should check for only INITIATED/WORKING status - it might still worth tho start a new compaction, even if the cleanup not finished yet\nTxnHandler.requestCleanup() should check for INITIATED/WORKING/READY_TO_CLEAN status (the first 2 should not be there anyway), so we do not queue multiple compactions for the same table/partition\n\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-06-18T07:49:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzOTk4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTk1MA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443905950", "bodyText": "incorporated the suggested change.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:09:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMzOTk4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MDUyMA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439340520", "bodyText": "I think, we really want to ignore this, so we would like to return here.", "author": "pvary", "createdAt": "2020-06-12T10:34:10Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if(ci.partName == null) {\n+          sb.append(\"cq_partition is null\");\n+        } else {\n+          sb.append(\"cq_partition=?\");\n+          params.add(ci.partName);\n+        }\n+\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, sb.toString(), params);\n+        LOG.debug(\"Going to execute query <\" + sb.toString() + \">\");\n+        ResultSet rs = pst.executeQuery();\n+        if(rs.next()) {\n+          long enqueuedId = rs.getLong(1);\n+          String state = compactorStateToResponse(rs.getString(2).charAt(0));\n+          LOG.info(\"Ignoring request to clean up for \" + ci.dbname + \"/\" + ci.tableName +\n+                  \"/\" + ci.partName + \" since it is already \" + quoteString(state) +\n+                  \" with id=\" + enqueuedId);", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNjA3MQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443906071", "bodyText": "moved this logline to debug and return from here.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MDUyMA=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MDY3Ng==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439340676", "bodyText": "Could we do this in a try with resource construct?", "author": "pvary", "createdAt": "2020-06-12T10:34:32Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if(ci.partName == null) {\n+          sb.append(\"cq_partition is null\");\n+        } else {\n+          sb.append(\"cq_partition=?\");\n+          params.add(ci.partName);\n+        }\n+\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, sb.toString(), params);\n+        LOG.debug(\"Going to execute query <\" + sb.toString() + \">\");\n+        ResultSet rs = pst.executeQuery();\n+        if(rs.next()) {\n+          long enqueuedId = rs.getLong(1);\n+          String state = compactorStateToResponse(rs.getString(2).charAt(0));\n+          LOG.info(\"Ignoring request to clean up for \" + ci.dbname + \"/\" + ci.tableName +\n+                  \"/\" + ci.partName + \" since it is already \" + quoteString(state) +\n+                  \" with id=\" + enqueuedId);\n+        }\n+        close(rs);\n+        closeStmt(pst);", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNjEyOA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443906128", "bodyText": "incorporated the suggested change.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:10:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MDY3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTAyMg==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439341022", "bodyText": "Again this is very similar that we have in compact(), we might to create a new method for it and reuse.", "author": "pvary", "createdAt": "2020-06-12T10:35:21Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if(ci.partName == null) {\n+          sb.append(\"cq_partition is null\");\n+        } else {\n+          sb.append(\"cq_partition=?\");\n+          params.add(ci.partName);\n+        }\n+\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, sb.toString(), params);\n+        LOG.debug(\"Going to execute query <\" + sb.toString() + \">\");\n+        ResultSet rs = pst.executeQuery();\n+        if(rs.next()) {\n+          long enqueuedId = rs.getLong(1);\n+          String state = compactorStateToResponse(rs.getString(2).charAt(0));\n+          LOG.info(\"Ignoring request to clean up for \" + ci.dbname + \"/\" + ci.tableName +\n+                  \"/\" + ci.partName + \" since it is already \" + quoteString(state) +\n+                  \" with id=\" + enqueuedId);\n+        }\n+        close(rs);\n+        closeStmt(pst);\n+        params.clear();\n+        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n+                \"cq_table, \");\n+        String partName = ci.partName;", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTUxOA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439341518", "bodyText": "Also using a prepared statement would be nice, I think", "author": "pvary", "createdAt": "2020-06-12T10:36:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNjEzNQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443906135", "bodyText": "incorporated the suggested change.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:10:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTAyMg=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTk1MQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439341951", "bodyText": "try with resource would be nice here too", "author": "pvary", "createdAt": "2020-06-12T10:37:31Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if(ci.partName == null) {\n+          sb.append(\"cq_partition is null\");\n+        } else {\n+          sb.append(\"cq_partition=?\");\n+          params.add(ci.partName);\n+        }\n+\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, sb.toString(), params);\n+        LOG.debug(\"Going to execute query <\" + sb.toString() + \">\");\n+        ResultSet rs = pst.executeQuery();\n+        if(rs.next()) {\n+          long enqueuedId = rs.getLong(1);\n+          String state = compactorStateToResponse(rs.getString(2).charAt(0));\n+          LOG.info(\"Ignoring request to clean up for \" + ci.dbname + \"/\" + ci.tableName +\n+                  \"/\" + ci.partName + \" since it is already \" + quoteString(state) +\n+                  \" with id=\" + enqueuedId);\n+        }\n+        close(rs);\n+        closeStmt(pst);\n+        params.clear();\n+        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n+                \"cq_table, \");\n+        String partName = ci.partName;\n+        if (partName != null) {\n+          buf.append(\"cq_partition, \");\n+        }\n+        buf.append(\"cq_state, cq_type\");\n+        if (ci.properties != null) {\n+          buf.append(\", cq_tblproperties\");\n+        }\n+        if (ci.runAs != null) {\n+          buf.append(\", cq_run_as\");\n+        }\n+        buf.append(\") values (\");\n+        buf.append(id);\n+        buf.append(\", ?\");\n+        buf.append(\", ?\");\n+        buf.append(\", \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if (partName != null) {\n+          buf.append(\"?, '\");\n+          params.add(partName);\n+        } else {\n+          buf.append(\"'\");\n+        }\n+        buf.append(READY_FOR_CLEANING);\n+        buf.append(\"', '\");\n+        buf.append(MAJOR_TYPE);\n+        buf.append(\"'\");\n+        if (ci.properties != null) {\n+          buf.append(\", ?\");\n+          params.add(ci.properties);\n+        }\n+        if (ci.runAs != null) {\n+          buf.append(\", ?\");\n+          params.add(ci.runAs);\n+        }\n+        buf.append(\")\");\n+        String s = buf.toString();\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, params);\n+        LOG.debug(\"Going to execute update <\" + s + \">\");\n+        pst.executeUpdate();\n+        LOG.debug(\"Going to commit\");\n+        dbConn.commit();\n+      } catch (SQLException e) {\n+        LOG.debug(\"Going to rollback\");\n+        rollbackDBConn(dbConn);\n+        checkRetryable(dbConn, e, \"requestCleanup(\" + ci + \")\");\n+        throw new MetaException(\"Unable to select from transaction database \" +\n+                StringUtils.stringifyException(e));\n+      } finally {\n+        closeStmt(pst);\n+        closeStmt(stmt);\n+        closeDbConn(dbConn);", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNjQ4MQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443906481", "bodyText": "incorporated the suggested change for stmt and pst. try-resource with dbConn make code clumsy with so many nested try-catch so I skipped it.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:11:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MTk1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MjMyNw==", "url": "https://github.com/apache/hive/pull/1085#discussion_r439342327", "bodyText": "If there is some exception before in the finally, this unlockInternal will not be called. Isn't this a problem?", "author": "pvary", "createdAt": "2020-06-12T10:38:24Z", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -5363,6 +5363,121 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n     LOG.debug(\"TXN lock locked by {} in mode {}\", quoteString(TxnHandler.hostname), shared);\n   }\n \n+\n+  @Override\n+  @RetrySemantics.Idempotent\n+  public void requestCleanup(CompactionInfo ci) throws MetaException {\n+    try {\n+      Connection dbConn = null;\n+      Statement stmt = null;\n+      PreparedStatement pst = null;\n+      TxnStore.MutexAPI.LockHandle handle = null;\n+      try {\n+        lockInternal();\n+        /**\n+         * MUTEX_KEY.CompactionScheduler lock ensures that there is only 1 entry in\n+         * Initiated/Working state for any resource.  This ensures that we don't run concurrent\n+         * compactions for any resource.\n+         */\n+        handle = getMutexAPI().acquireLock(MUTEX_KEY.CompactionScheduler.name());\n+        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+        stmt = dbConn.createStatement();\n+\n+        long id = generateCompactionQueueId(stmt);\n+\n+        List<String> params = new ArrayList<>();\n+        StringBuilder sb = new StringBuilder(\"select cq_id, cq_state from COMPACTION_QUEUE where\").\n+                append(\" cq_state IN(\").append(quoteChar(INITIATED_STATE)).\n+                append(\",\").append(quoteChar(WORKING_STATE)).\n+                append(\") AND cq_database=?\").\n+                append(\" AND cq_table=?\").append(\" AND \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if(ci.partName == null) {\n+          sb.append(\"cq_partition is null\");\n+        } else {\n+          sb.append(\"cq_partition=?\");\n+          params.add(ci.partName);\n+        }\n+\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, sb.toString(), params);\n+        LOG.debug(\"Going to execute query <\" + sb.toString() + \">\");\n+        ResultSet rs = pst.executeQuery();\n+        if(rs.next()) {\n+          long enqueuedId = rs.getLong(1);\n+          String state = compactorStateToResponse(rs.getString(2).charAt(0));\n+          LOG.info(\"Ignoring request to clean up for \" + ci.dbname + \"/\" + ci.tableName +\n+                  \"/\" + ci.partName + \" since it is already \" + quoteString(state) +\n+                  \" with id=\" + enqueuedId);\n+        }\n+        close(rs);\n+        closeStmt(pst);\n+        params.clear();\n+        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n+                \"cq_table, \");\n+        String partName = ci.partName;\n+        if (partName != null) {\n+          buf.append(\"cq_partition, \");\n+        }\n+        buf.append(\"cq_state, cq_type\");\n+        if (ci.properties != null) {\n+          buf.append(\", cq_tblproperties\");\n+        }\n+        if (ci.runAs != null) {\n+          buf.append(\", cq_run_as\");\n+        }\n+        buf.append(\") values (\");\n+        buf.append(id);\n+        buf.append(\", ?\");\n+        buf.append(\", ?\");\n+        buf.append(\", \");\n+        params.add(ci.dbname);\n+        params.add(ci.tableName);\n+        if (partName != null) {\n+          buf.append(\"?, '\");\n+          params.add(partName);\n+        } else {\n+          buf.append(\"'\");\n+        }\n+        buf.append(READY_FOR_CLEANING);\n+        buf.append(\"', '\");\n+        buf.append(MAJOR_TYPE);\n+        buf.append(\"'\");\n+        if (ci.properties != null) {\n+          buf.append(\", ?\");\n+          params.add(ci.properties);\n+        }\n+        if (ci.runAs != null) {\n+          buf.append(\", ?\");\n+          params.add(ci.runAs);\n+        }\n+        buf.append(\")\");\n+        String s = buf.toString();\n+        pst = sqlGenerator.prepareStmtWithParameters(dbConn, s, params);\n+        LOG.debug(\"Going to execute update <\" + s + \">\");\n+        pst.executeUpdate();\n+        LOG.debug(\"Going to commit\");\n+        dbConn.commit();\n+      } catch (SQLException e) {\n+        LOG.debug(\"Going to rollback\");\n+        rollbackDBConn(dbConn);\n+        checkRetryable(dbConn, e, \"requestCleanup(\" + ci + \")\");\n+        throw new MetaException(\"Unable to select from transaction database \" +\n+                StringUtils.stringifyException(e));\n+      } finally {\n+        closeStmt(pst);\n+        closeStmt(stmt);\n+        closeDbConn(dbConn);\n+        if(handle != null) {\n+          handle.releaseLocks();\n+        }\n+        unlockInternal();", "originalCommit": "91ad99ab38ec5149350a997c0b0b3beb66b1f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNjkwNg==", "url": "https://github.com/apache/hive/pull/1085#discussion_r443906906", "bodyText": "unlockInternal is mostly applicable for derby so I think it should not create the problem.", "author": "rajkrrsingh", "createdAt": "2020-06-23T01:13:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM0MjMyNw=="}], "type": "inlineReview", "revised_code": {"commit": "3d12959339b22becee0aa986852049b46867f016", "chunk": "diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\nindex fe8f09acd2..ede793d866 100644\n--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java\n\n@@ -5369,8 +5234,6 @@ private void acquireTxnLock(Statement stmt, boolean shared) throws SQLException,\n   public void requestCleanup(CompactionInfo ci) throws MetaException {\n     try {\n       Connection dbConn = null;\n-      Statement stmt = null;\n-      PreparedStatement pst = null;\n       TxnStore.MutexAPI.LockHandle handle = null;\n       try {\n         lockInternal();\n"}}, {"oid": "c9812ad1ba04b3e37656fdd0c738646dea24cf81", "url": "https://github.com/apache/hive/commit/c9812ad1ba04b3e37656fdd0c738646dea24cf81", "message": "merge from upstream master", "committedDate": "2020-06-22T03:56:06Z", "type": "commit"}, {"oid": "3d12959339b22becee0aa986852049b46867f016", "url": "https://github.com/apache/hive/commit/3d12959339b22becee0aa986852049b46867f016", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-23T01:03:34Z", "type": "commit"}, {"oid": "523433cc6e14d0a37f01fda3ac548bff114de927", "url": "https://github.com/apache/hive/commit/523433cc6e14d0a37f01fda3ac548bff114de927", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-23T21:25:11Z", "type": "commit"}, {"oid": "41b6a05a8df17e62b5c4f102771cd22af127e73a", "url": "https://github.com/apache/hive/commit/41b6a05a8df17e62b5c4f102771cd22af127e73a", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-23T21:28:54Z", "type": "commit"}, {"oid": "12a95f18239db835745c20445ed1763cc6549e07", "url": "https://github.com/apache/hive/commit/12a95f18239db835745c20445ed1763cc6549e07", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-23T21:35:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcwOTM5MA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r444709390", "bodyText": "Nit: I think this is misleading, and unnecessary since we have already logged the values of deltaSize and multiBase.", "author": "klcopp", "createdAt": "2020-06-24T07:51:15Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -361,18 +361,25 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n           HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_PCT_THRESHOLD) :\n           Float.parseFloat(deltaPctProp);\n       boolean bigEnough =   (float)deltaSize/(float)baseSize > deltaPctThreshold;\n+      boolean multiBase = dir.getObsolete().stream()\n+              .filter(path -> path.getName().startsWith(AcidUtils.BASE_PREFIX)).findAny().isPresent();\n+\n       if (LOG.isDebugEnabled()) {\n         StringBuilder msg = new StringBuilder(\"delta size: \");\n         msg.append(deltaSize);\n         msg.append(\" base size: \");\n         msg.append(baseSize);\n+        msg.append(\" multiBase \");\n+        msg.append(multiBase);\n+        msg.append(\" deltaSize \");\n+        msg.append(deltaSize);\n         msg.append(\" threshold: \");\n         msg.append(deltaPctThreshold);\n         msg.append(\" will major compact: \");\n-        msg.append(bigEnough);\n+        msg.append(bigEnough || (deltaSize == 0  && multiBase));", "originalCommit": "12a95f18239db835745c20445ed1763cc6549e07", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6e289cb20d793faf56c47c8fee1f55577545c78e", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\nindex ed5ed8d379..d35e229d18 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java\n\n@@ -376,7 +376,7 @@ private CompactionType determineCompactionType(CompactionInfo ci, ValidWriteIdLi\n         msg.append(\" threshold: \");\n         msg.append(deltaPctThreshold);\n         msg.append(\" will major compact: \");\n-        msg.append(bigEnough || (deltaSize == 0  && multiBase));\n+        msg.append(bigEnough);\n         LOG.debug(msg.toString());\n       }\n       if (bigEnough || (deltaSize == 0  && multiBase)) return CompactionType.MAJOR;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcxMDUzMQ==", "url": "https://github.com/apache/hive/pull/1085#discussion_r444710531", "bodyText": "opportunity: add:\nstartWorker();\nAssert.assertEquals(\"ready for cleaning\",rsp.getCompacts().get(0).getState());", "author": "klcopp", "createdAt": "2020-06-24T07:53:30Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java", "diffHunk": "@@ -1031,6 +1031,34 @@ private ShowCompactResponseElement generateElement(long id, String db, String ta\n     return element;\n   }\n \n+  @Test\n+  public void compactTableWithMultipleBase() throws Exception {\n+    Table t = newTable(\"default\", \"nctdpnhe\", false);\n+\n+    addBaseFile(t, null, 50L, 50);\n+    addBaseFile(t, null, 100L, 50);\n+\n+    burnThroughTransactions(\"default\", \"nctdpnhe\", 102);\n+\n+    long txnid = openTxn();\n+    LockComponent comp = new LockComponent(LockType.SHARED_WRITE, LockLevel.TABLE, \"default\");\n+    comp.setTablename(\"nctdpnhe\");\n+    comp.setOperationType(DataOperationType.UPDATE);\n+    List<LockComponent> components = new ArrayList<LockComponent>(1);\n+    components.add(comp);\n+    LockRequest req = new LockRequest(components, \"me\", \"localhost\");\n+    req.setTxnid(txnid);\n+    LockResponse res = txnHandler.lock(req);\n+    long writeid = allocateWriteId(\"default\", \"nctdpnhe\", txnid);\n+    txnHandler.commitTxn(new CommitTxnRequest(txnid));\n+\n+    startInitiator();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompactsSize());\n+    Assert.assertEquals(\"initiated\",rsp.getCompacts().get(0).getState());\n+  }", "originalCommit": "12a95f18239db835745c20445ed1763cc6549e07", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6e289cb20d793faf56c47c8fee1f55577545c78e", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\nindex 6b0c7fea3e..8125a48444 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\n\n@@ -1057,6 +1057,11 @@ public void compactTableWithMultipleBase() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     Assert.assertEquals(1, rsp.getCompactsSize());\n     Assert.assertEquals(\"initiated\",rsp.getCompacts().get(0).getState());\n+\n+    startWorker();\n+    Thread.sleep(1L);\n+    ShowCompactResponse response = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(\"ready for cleaning\",response.getCompacts().get(0).getState());\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcxMDg5MA==", "url": "https://github.com/apache/hive/pull/1085#discussion_r444710890", "bodyText": "nit: no newline at end of file", "author": "klcopp", "createdAt": "2020-06-24T07:54:08Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java", "diffHunk": "@@ -1040,4 +1068,4 @@ boolean useHive130DeltaDirName() {\n   public void tearDown() throws Exception {\n     compactorTestCleanup();\n   }\n-}\n+}", "originalCommit": "12a95f18239db835745c20445ed1763cc6549e07", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f3fc598fa7d3f95d8c2ed2e65b8f52cf40a7e615", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\nindex 6b0c7fea3e..e7b56a25a9 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java\n\n@@ -1068,4 +1073,4 @@ boolean useHive130DeltaDirName() {\n   public void tearDown() throws Exception {\n     compactorTestCleanup();\n   }\n-}\n\\ No newline at end of file\n+}\n"}}, {"oid": "6e289cb20d793faf56c47c8fee1f55577545c78e", "url": "https://github.com/apache/hive/commit/6e289cb20d793faf56c47c8fee1f55577545c78e", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-24T18:36:51Z", "type": "commit"}, {"oid": "f3fc598fa7d3f95d8c2ed2e65b8f52cf40a7e615", "url": "https://github.com/apache/hive/commit/f3fc598fa7d3f95d8c2ed2e65b8f52cf40a7e615", "message": "Hive 22255 : Hive don't trigger Major Compaction automatically if table contains only base files", "committedDate": "2020-06-24T18:46:27Z", "type": "commit"}]}