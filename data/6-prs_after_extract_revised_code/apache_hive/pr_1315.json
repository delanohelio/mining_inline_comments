{"pr_number": 1315, "pr_title": "[HIVE-23951] Support parameterized queries in WHERE/HAVING clause", "pr_createdAt": "2020-07-25T00:16:25Z", "pr_url": "https://github.com/apache/hive/pull/1315", "timeline": [{"oid": "42cfe01b3862d3440494970a51f37a70b13351da", "url": "https://github.com/apache/hive/commit/42cfe01b3862d3440494970a51f37a70b13351da", "message": "Support for prepare/execute statements", "committedDate": "2020-07-27T20:06:43Z", "type": "forcePushed"}, {"oid": "23795aae907faff6c1523a3ba98f610571420de2", "url": "https://github.com/apache/hive/commit/23795aae907faff6c1523a3ba98f610571420de2", "message": "Support for prepare/execute statements", "committedDate": "2020-07-29T17:29:03Z", "type": "commit"}, {"oid": "23795aae907faff6c1523a3ba98f610571420de2", "url": "https://github.com/apache/hive/commit/23795aae907faff6c1523a3ba98f610571420de2", "message": "Support for prepare/execute statements", "committedDate": "2020-07-29T17:29:03Z", "type": "forcePushed"}, {"oid": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "url": "https://github.com/apache/hive/commit/8d580d27dcbe34622721ec4963400656a8cfe1f8", "message": "support for decimal type info", "committedDate": "2020-08-02T23:01:44Z", "type": "commit"}, {"oid": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "url": "https://github.com/apache/hive/commit/b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "message": "Merge remote-tracking branch 'upstream/master' into PREPARE_EXECUTE_SUPPORT", "committedDate": "2020-08-03T16:15:15Z", "type": "commit"}, {"oid": "88dab3fd22da65fa8e89f32019552383bc3fc2e0", "url": "https://github.com/apache/hive/commit/88dab3fd22da65fa8e89f32019552383bc3fc2e0", "message": "Updating tests", "committedDate": "2020-08-03T16:46:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE0MDA3MQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464140071", "bodyText": "nit. required", "author": "jcamachor", "createdAt": "2020-08-03T00:02:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Compiler.java", "diffHunk": "@@ -338,12 +339,22 @@ private QueryPlan createPlan(BaseSemanticAnalyzer sem) {\n     plan.setOptimizedCBOPlan(context.getCalcitePlan());\n     plan.setOptimizedQueryString(context.getOptimizedSql());\n \n+    // this is require so that later driver can skip executing prepare queries", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\nindex 2077323954..d166b9d1bb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\n\n@@ -339,16 +339,16 @@ private QueryPlan createPlan(BaseSemanticAnalyzer sem) {\n     plan.setOptimizedCBOPlan(context.getCalcitePlan());\n     plan.setOptimizedQueryString(context.getOptimizedSql());\n \n-    // this is require so that later driver can skip executing prepare queries\n+    // this is required so that later driver can skip executing prepare queries\n     if (sem.getIsPrepareQuery()) {\n-      plan.setIsPrepareQuery(true);\n+      plan.setPrepareQuery(true);\n     }\n     return plan;\n   }\n \n   protected void initializeFetchTask(QueryPlan plan) {\n     // for PREPARE statement we should avoid initializing operators\n-    if (plan.getIsPrepareQuery()) {\n+    if (plan.isPrepareQuery()) {\n       return;\n     }\n     // initialize FetchTask right here\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE4NjA5MQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464186091", "bodyText": "TODO ?", "author": "jcamachor", "createdAt": "2020-08-03T04:17:46Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -433,6 +433,12 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n     driverState.lock();\n     try {\n+      if (driverContext != null && driverContext.getPlan() != null\n+          && driverContext.getPlan().getIsPrepareQuery()\n+          && !driverContext.getPlan().isExplain()) {\n+        //TODO: LOG", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex 03ad779ef7..5a0cb0d017 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n\n@@ -434,9 +434,9 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n     driverState.lock();\n     try {\n       if (driverContext != null && driverContext.getPlan() != null\n-          && driverContext.getPlan().getIsPrepareQuery()\n+          && driverContext.getPlan().isPrepareQuery()\n           && !driverContext.getPlan().isExplain()) {\n-        //TODO: LOG\n+        LOG.info(\"Skip running tasks for prepare plan\");\n         return;\n       }\n       if (alreadyCompiled) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE4NjQ5OQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464186499", "bodyText": "nit. isPrepareQuery -> prepareQuery. Please change setters and getters accordingly below, e.g., getIsPrepareQuery -> isPrepareQuery.", "author": "jcamachor", "createdAt": "2020-08-03T04:19:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -121,6 +121,8 @@\n   private final DDLDescWithWriteId acidDdlDesc;\n   private Boolean autoCommitValue;\n \n+  private Boolean isPrepareQuery;", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java\nindex 48fc241976..796ff3b77f 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java\n\n@@ -121,7 +121,7 @@\n   private final DDLDescWithWriteId acidDdlDesc;\n   private Boolean autoCommitValue;\n \n-  private Boolean isPrepareQuery;\n+  private Boolean prepareQuery;\n \n   public QueryPlan() {\n     this(null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE4ODcyMw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464188723", "bodyText": "nit. typo: kyro", "author": "jcamachor", "createdAt": "2020-08-03T04:30:42Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5MDAyOQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464190029", "bodyText": "You can use type.getPrimitiveCategory so you do not have to care about this. Also you will be able to use a switch statement instead of if...else", "author": "jcamachor", "createdAt": "2020-08-03T04:37:21Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5MjQ0Mg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464192442", "bodyText": "This method should probably rely on ExprNodeTypeCheck.genExprNode to avoid having two different parsing / interpretation logic.\nA possible idea is to do something like it is done for default constraint values. You could rely on same logic to generate the expr node:\n    TypeCheckCtx typeCheckCtx = new TypeCheckCtx(null);\n    ExprNodeDesc defaultValExpr = ExprNodeTypeCheck.genExprNode(node, typeCheckCtx).get(node);\n\nThen verify type is matching. I think that will provide more reliable logic. What do you think?\nMy take is that it's better to be too strict wrt type rather than generating wrong / different results.", "author": "jcamachor", "createdAt": "2020-08-03T04:49:12Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5ODM1OQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464698359", "bodyText": "You are right, I will update the code.", "author": "vineetgarg02", "createdAt": "2020-08-03T22:38:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5MjQ0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5MjcwOA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464192708", "bodyText": "typo. retreives", "author": "jcamachor", "createdAt": "2020-08-03T04:50:33Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NDQ4NA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464194484", "bodyText": "I think it would be better to create a CachedPlan class rather than caching the complete SemanticAnalyzer.\nIn addition, doesn't QueryPlan have all the information you need? If it has most of it, couldn't you extend it?", "author": "jcamachor", "createdAt": "2020-08-03T04:58:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM2MjM3Mg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r465362372", "bodyText": "Are you suggesting to cache QueryPlan? I went that route before but it created issues with EXPLAIN PLAN. QueryPlan keeps set of tasks so it was hard to create an explain task and then hook it with existing cached tasks for explain plan to work. Caching semantic analyzer helped make the whole implementation simple.\nI guess creating CachedPlan and caching only the information required (tasks, config, inputs, outputs etc) will help reduce memory footprints. Are there other concerns/issues wrt caching the whole semantic analyzer object?\nIf not do you mind if I make this change in a follow-up?", "author": "vineetgarg02", "createdAt": "2020-08-04T22:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NDQ4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0MzA3MA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r465443070", "bodyText": "Yes, I thought about QueryPlan since I assumed it would have all the information that is needed. It seems more natural to cache the plan rather than the analyzer.\nOne concern that came to mind is precisely what you mentioned, memory footprint of the analyzer and the implications of caching it, with all the additional information that we extract during analysis and will be mostly useless for our purpose.\nIt is fine defer to follow-up.", "author": "jcamachor", "createdAt": "2020-08-05T02:59:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NDQ4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2MzQzMA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466563430", "bodyText": "Follow-up: https://issues.apache.org/jira/browse/HIVE-24005", "author": "vineetgarg02", "createdAt": "2020-08-06T17:16:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NDQ4NA=="}], "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NTQwMg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464195402", "bodyText": "I think we talked about this offline. Do we support specifying binary constants in the query? If we do not, should we just throw some exception?", "author": "jcamachor", "createdAt": "2020-08-03T05:02:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra\n+        // check is used (based on instanceof) to determine if it is char/varchar types\n+        || typeInfo instanceof CharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.varcharTypeInfo)\n+        || typeInfo instanceof VarcharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return factory.createFloatConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return factory.createDoubleConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return factory.createTinyintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return factory.createSmallintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {\n+      return factory.createDateConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      return factory.createTimestampConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {\n+      return factory.createIntervalYearMonthConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {\n+      return factory.createIntervalDayTimeConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5NzU1Ng==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464197556", "bodyText": "I think this should not be part of this patch (only FILTER)?", "author": "jcamachor", "createdAt": "2020-08-03T05:12:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra\n+        // check is used (based on instanceof) to determine if it is char/varchar types\n+        || typeInfo instanceof CharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.varcharTypeInfo)\n+        || typeInfo instanceof VarcharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return factory.createFloatConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return factory.createDoubleConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return factory.createTinyintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return factory.createSmallintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {\n+      return factory.createDateConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      return factory.createTimestampConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {\n+      return factory.createIntervalYearMonthConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {\n+      return factory.createIntervalDayTimeConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    }\n+    // we will let constant expression itself infer the type\n+    return new ExprNodeConstantDesc(parameterMap.get(dynamicExpr.getIndex()));\n+  }\n+\n+  /**\n+   * Given a list of expressions this method traverse the expression tree and replaces\n+   * all {@link ExprDynamicParamDesc} nodes with constant expression.\n+   * @param exprList\n+   * @param paramMap\n+   */\n+  private List<ExprNodeDesc> replaceDynamicParamsInExprList(List<ExprNodeDesc> exprList,\n+      Map<Integer, ASTNode> paramMap) {\n+    List<ExprNodeDesc> updatedExprList = new ArrayList<>();\n+    for (ExprNodeDesc expr:exprList) {\n+      expr = replaceDynamicParamsWithConstant(expr, expr.getTypeInfo(), paramMap);\n+      updatedExprList.add(expr);\n+    }\n+    return updatedExprList;\n+  }\n+\n+  /**\n+   * Given an expression tree root at expr and type info of the expression this method traverse\n+   * the expression tree and replaces all dynamic expression with the constant expression.\n+   * This method also does type inference for the new constant expression.\n+   * Note about type inference\n+   * Since dynamic parameter lacks type we need to figure out appropriate type to create constant\n+   * out of string value. To do this, we choose the type of first child of the parent expression\n+   * which isn't dynamic parameter\n+   */\n+  private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> paramMap) {\n+    if (expr.getChildren() == null || expr.getChildren().isEmpty()) {\n+      if (expr instanceof ExprDynamicParamDesc) {\n+        return getConstant((ExprDynamicParamDesc)expr, typeInfo, paramMap);\n+      }\n+      return expr;\n+    }\n+\n+    for(ExprNodeDesc child:expr.getChildren()) {\n+      // we need typeinfo\n+      if(child instanceof ExprDynamicParamDesc) {\n+        continue;\n+      } else if( child.getTypeInfo() != TypeInfoFactory.voidTypeInfo\n+          && !child.getTypeInfo().getTypeName().equals(\n+          TypeInfoFactory.voidTypeInfo.getTypeName())){\n+        typeInfo = child.getTypeInfo();\n+        break;\n+      }\n+    }\n+    assert(typeInfo != null);\n+\n+    List<ExprNodeDesc> exprList = new ArrayList<>();\n+    for(ExprNodeDesc child: expr.getChildren()) {\n+      if(child instanceof ExprDynamicParamDesc) {\n+        child = getConstant((ExprDynamicParamDesc)child, typeInfo, paramMap);\n+      } else {\n+        child = replaceDynamicParamsWithConstant(child, typeInfo, paramMap);\n+      }\n+      exprList.add(child);\n+    }\n+    expr.getChildren().clear();\n+    expr.getChildren().addAll(exprList);\n+    return expr;\n+  }\n+\n+  /**\n+   * Given map of index and ASTNode this traverse all operators within all tasks\n+   * including Fetch Task and all root tasks to find and replace all dynamic expressions\n+   */\n+  private void bindDynamicParams(Map<Integer, ASTNode> parameterMap) throws SemanticException{\n+    assert(!parameterMap.isEmpty());\n+\n+    Set<Operator<?>> operators = new HashSet<>();\n+    if (this.getFetchTask() != null) {\n+      operators.addAll(OperatorUtils.getAllFetchOperators(this.getFetchTask()));\n+    }\n+    List<Task<?>> allTasks = this.getRootTasks();\n+    List<TezTask> rootTasks = Utilities.getTezTasks(this.getRootTasks());\n+    for(Task task:allTasks) {\n+      List<BaseWork> baseWorks = new ArrayList<>();\n+      if (task instanceof ExplainTask) {\n+        ExplainTask explainTask = (ExplainTask) task;\n+        for (Task explainRootTask : explainTask.getWork().getRootTasks()) {\n+          if (explainRootTask instanceof TezTask) {\n+            TezTask explainTezTask = (TezTask) explainRootTask;\n+            baseWorks.addAll(explainTezTask.getWork().getAllWork());\n+          }\n+        }\n+      } else if (task instanceof TezTask) {\n+        baseWorks = ((TezTask) task).getWork().getAllWork();\n+      }\n+      for (BaseWork baseWork : baseWorks) {\n+        operators.addAll(baseWork.getAllOperators());\n+      }\n+    }\n+\n+    for (Operator<?> op : operators) {\n+      switch(op.getType()) {\n+      case FILTER:\n+        FilterOperator filterOp = (FilterOperator)op;\n+        ExprNodeDesc predicate = filterOp.getConf().getPredicate();\n+        filterOp.getConf().setPredicate(\n+            replaceDynamicParamsWithConstant(predicate, TypeInfoFactory.booleanTypeInfo, parameterMap));\n+        break;\n+      case SELECT:", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5ODMwOQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464198309", "bodyText": "Preconditions instead of assert?", "author": "jcamachor", "createdAt": "2020-08-03T05:16:04Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra\n+        // check is used (based on instanceof) to determine if it is char/varchar types\n+        || typeInfo instanceof CharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.varcharTypeInfo)\n+        || typeInfo instanceof VarcharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return factory.createFloatConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return factory.createDoubleConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return factory.createTinyintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return factory.createSmallintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {\n+      return factory.createDateConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      return factory.createTimestampConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {\n+      return factory.createIntervalYearMonthConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {\n+      return factory.createIntervalDayTimeConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    }\n+    // we will let constant expression itself infer the type\n+    return new ExprNodeConstantDesc(parameterMap.get(dynamicExpr.getIndex()));\n+  }\n+\n+  /**\n+   * Given a list of expressions this method traverse the expression tree and replaces\n+   * all {@link ExprDynamicParamDesc} nodes with constant expression.\n+   * @param exprList\n+   * @param paramMap\n+   */\n+  private List<ExprNodeDesc> replaceDynamicParamsInExprList(List<ExprNodeDesc> exprList,\n+      Map<Integer, ASTNode> paramMap) {\n+    List<ExprNodeDesc> updatedExprList = new ArrayList<>();\n+    for (ExprNodeDesc expr:exprList) {\n+      expr = replaceDynamicParamsWithConstant(expr, expr.getTypeInfo(), paramMap);\n+      updatedExprList.add(expr);\n+    }\n+    return updatedExprList;\n+  }\n+\n+  /**\n+   * Given an expression tree root at expr and type info of the expression this method traverse\n+   * the expression tree and replaces all dynamic expression with the constant expression.\n+   * This method also does type inference for the new constant expression.\n+   * Note about type inference\n+   * Since dynamic parameter lacks type we need to figure out appropriate type to create constant\n+   * out of string value. To do this, we choose the type of first child of the parent expression\n+   * which isn't dynamic parameter\n+   */\n+  private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> paramMap) {\n+    if (expr.getChildren() == null || expr.getChildren().isEmpty()) {\n+      if (expr instanceof ExprDynamicParamDesc) {\n+        return getConstant((ExprDynamicParamDesc)expr, typeInfo, paramMap);\n+      }\n+      return expr;\n+    }\n+\n+    for(ExprNodeDesc child:expr.getChildren()) {\n+      // we need typeinfo\n+      if(child instanceof ExprDynamicParamDesc) {\n+        continue;\n+      } else if( child.getTypeInfo() != TypeInfoFactory.voidTypeInfo\n+          && !child.getTypeInfo().getTypeName().equals(\n+          TypeInfoFactory.voidTypeInfo.getTypeName())){\n+        typeInfo = child.getTypeInfo();\n+        break;\n+      }\n+    }\n+    assert(typeInfo != null);", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5OTY2OA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464199668", "bodyText": "Isn't child.getTypeInfo() != TypeInfoFactory.voidTypeInfo sufficient?", "author": "jcamachor", "createdAt": "2020-08-03T05:22:28Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra\n+        // check is used (based on instanceof) to determine if it is char/varchar types\n+        || typeInfo instanceof CharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.varcharTypeInfo)\n+        || typeInfo instanceof VarcharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return factory.createFloatConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return factory.createDoubleConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return factory.createTinyintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return factory.createSmallintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {\n+      return factory.createDateConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      return factory.createTimestampConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {\n+      return factory.createIntervalYearMonthConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {\n+      return factory.createIntervalDayTimeConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    }\n+    // we will let constant expression itself infer the type\n+    return new ExprNodeConstantDesc(parameterMap.get(dynamicExpr.getIndex()));\n+  }\n+\n+  /**\n+   * Given a list of expressions this method traverse the expression tree and replaces\n+   * all {@link ExprDynamicParamDesc} nodes with constant expression.\n+   * @param exprList\n+   * @param paramMap\n+   */\n+  private List<ExprNodeDesc> replaceDynamicParamsInExprList(List<ExprNodeDesc> exprList,\n+      Map<Integer, ASTNode> paramMap) {\n+    List<ExprNodeDesc> updatedExprList = new ArrayList<>();\n+    for (ExprNodeDesc expr:exprList) {\n+      expr = replaceDynamicParamsWithConstant(expr, expr.getTypeInfo(), paramMap);\n+      updatedExprList.add(expr);\n+    }\n+    return updatedExprList;\n+  }\n+\n+  /**\n+   * Given an expression tree root at expr and type info of the expression this method traverse\n+   * the expression tree and replaces all dynamic expression with the constant expression.\n+   * This method also does type inference for the new constant expression.\n+   * Note about type inference\n+   * Since dynamic parameter lacks type we need to figure out appropriate type to create constant\n+   * out of string value. To do this, we choose the type of first child of the parent expression\n+   * which isn't dynamic parameter\n+   */\n+  private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> paramMap) {\n+    if (expr.getChildren() == null || expr.getChildren().isEmpty()) {\n+      if (expr instanceof ExprDynamicParamDesc) {\n+        return getConstant((ExprDynamicParamDesc)expr, typeInfo, paramMap);\n+      }\n+      return expr;\n+    }\n+\n+    for(ExprNodeDesc child:expr.getChildren()) {\n+      // we need typeinfo\n+      if(child instanceof ExprDynamicParamDesc) {\n+        continue;\n+      } else if( child.getTypeInfo() != TypeInfoFactory.voidTypeInfo", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMTE4Nw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464701187", "bodyText": "Yes I think so, I don't recall why I added comparison with the name as well. I will update the code to remove it.", "author": "vineetgarg02", "createdAt": "2020-08-03T22:47:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE5OTY2OA=="}], "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIwMDQwMw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464200403", "bodyText": "Could this possibly be simplified to child = replaceDynamicParamsWithConstant(child, typeInfo, paramMap); instead of having two code paths in the loop?", "author": "jcamachor", "createdAt": "2020-08-03T05:25:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeDescExprFactory;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retreives cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kyro serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    assert(paramMap.containsKey(paramIndex));\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) {\n+    assert(parameterMap.containsKey(dynamicExpr.getIndex()));\n+\n+    String value = getParamLiteralValue(parameterMap, dynamicExpr.getIndex());\n+\n+    ExprNodeDescExprFactory factory = new ExprNodeDescExprFactory();\n+\n+    if (typeInfo.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return factory.createBooleanConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intTypeInfo)) {\n+      return factory.createIntConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.longTypeInfo)) {\n+      return factory.createBigintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.charTypeInfo)\n+        // CHAR and VARCHAR typeinfo could differ due to different length, therefore an extra\n+        // check is used (based on instanceof) to determine if it is char/varchar types\n+        || typeInfo instanceof CharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.varcharTypeInfo)\n+        || typeInfo instanceof VarcharTypeInfo) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return factory.createFloatConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return factory.createDoubleConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return factory.createTinyintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return factory.createSmallintConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.dateTypeInfo)) {\n+      return factory.createDateConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      return factory.createTimestampConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalYearMonthTypeInfo)) {\n+      return factory.createIntervalYearMonthConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.intervalDayTimeTypeInfo)) {\n+      return factory.createIntervalDayTimeConstantExpr(value);\n+    } else if (typeInfo.equals(TypeInfoFactory.binaryTypeInfo)) {\n+      //TODO: is it okay to create string\n+      return factory.createStringConstantExpr(value);\n+    }\n+    // we will let constant expression itself infer the type\n+    return new ExprNodeConstantDesc(parameterMap.get(dynamicExpr.getIndex()));\n+  }\n+\n+  /**\n+   * Given a list of expressions this method traverse the expression tree and replaces\n+   * all {@link ExprDynamicParamDesc} nodes with constant expression.\n+   * @param exprList\n+   * @param paramMap\n+   */\n+  private List<ExprNodeDesc> replaceDynamicParamsInExprList(List<ExprNodeDesc> exprList,\n+      Map<Integer, ASTNode> paramMap) {\n+    List<ExprNodeDesc> updatedExprList = new ArrayList<>();\n+    for (ExprNodeDesc expr:exprList) {\n+      expr = replaceDynamicParamsWithConstant(expr, expr.getTypeInfo(), paramMap);\n+      updatedExprList.add(expr);\n+    }\n+    return updatedExprList;\n+  }\n+\n+  /**\n+   * Given an expression tree root at expr and type info of the expression this method traverse\n+   * the expression tree and replaces all dynamic expression with the constant expression.\n+   * This method also does type inference for the new constant expression.\n+   * Note about type inference\n+   * Since dynamic parameter lacks type we need to figure out appropriate type to create constant\n+   * out of string value. To do this, we choose the type of first child of the parent expression\n+   * which isn't dynamic parameter\n+   */\n+  private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> paramMap) {\n+    if (expr.getChildren() == null || expr.getChildren().isEmpty()) {\n+      if (expr instanceof ExprDynamicParamDesc) {\n+        return getConstant((ExprDynamicParamDesc)expr, typeInfo, paramMap);\n+      }\n+      return expr;\n+    }\n+\n+    for(ExprNodeDesc child:expr.getChildren()) {\n+      // we need typeinfo\n+      if(child instanceof ExprDynamicParamDesc) {\n+        continue;\n+      } else if( child.getTypeInfo() != TypeInfoFactory.voidTypeInfo\n+          && !child.getTypeInfo().getTypeName().equals(\n+          TypeInfoFactory.voidTypeInfo.getTypeName())){\n+        typeInfo = child.getTypeInfo();\n+        break;\n+      }\n+    }\n+    assert(typeInfo != null);\n+\n+    List<ExprNodeDesc> exprList = new ArrayList<>();\n+    for(ExprNodeDesc child: expr.getChildren()) {\n+      if(child instanceof ExprDynamicParamDesc) {", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nindex 83c3ec3364..b11216ee57 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n\n@@ -42,9 +42,11 @@\n import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;\n+import org.apache.parquet.format.DecimalType;\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIwOTE3MA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464209170", "bodyText": "If we rely on ExprNodeTypeCheck.genExprNode, this visibility does not need to change (it makes sense to limit the scope of these methods).", "author": "jcamachor", "createdAt": "2020-08-03T06:02:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprFactory.java", "diffHunk": "@@ -112,40 +112,45 @@ protected boolean isAllConstants(List<T> exprs) {\n    */\n   protected abstract T createNullConstantExpr();\n \n+  /**\n+   * Creates a dynamic parameter expression with void type.\n+   */\n+  protected abstract T createDynamicParamExpr(int index);\n+\n   /**\n    * Creates a boolean constant expression from input value.\n    */\n-  protected abstract T createBooleanConstantExpr(String value);\n+  public abstract T createBooleanConstantExpr(String value);", "originalCommit": "23795aae907faff6c1523a3ba98f610571420de2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b22e56df0e30cc2655a7e30a6d7d6fe8d892ae99", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprFactory.java\nindex cd149cc715..8b0dfa8524 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprFactory.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/ExprFactory.java\n\n@@ -158,7 +158,7 @@ protected boolean isAllConstants(List<T> exprs) {\n    * 1) a constant expression containing null value if allowNullValueConstantExpr is true, or\n    * 2) null if allowNullValueConstantExpr is false.\n    */\n-  protected abstract T createDecimalConstantExpr(String value, boolean allowNullValueConstantExpr);\n+  public abstract T createDecimalConstantExpr(String value, boolean allowNullValueConstantExpr);\n \n   /**\n    * Creates a string constant expression from input value.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUzNTg0NQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464535845", "bodyText": "This analyzer should probably not be in table/drop folder?", "author": "jcamachor", "createdAt": "2020-08-03T16:50:52Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,381 @@\n+/*", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\nsimilarity index 68%\nrename from ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\nrename to ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\nindex b11216ee57..e5047d7ce8 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hadoop.hive.ql.ddl.table.drop;\n+package org.apache.hadoop.hive.ql.parse;\n \n import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUzNTk4Nw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464535987", "bodyText": "This analyzer should probably not be in table/drop folder?", "author": "jcamachor", "createdAt": "2020-08-03T16:51:09Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\nsimilarity index 88%\nrename from ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java\nrename to ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\nindex bfe0733eeb..7a27022e72 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\n\n@@ -16,14 +16,10 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hadoop.hive.ql.ddl.table.drop;\n+package org.apache.hadoop.hive.ql.parse;\n \n import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n-import org.apache.hadoop.hive.ql.parse.ASTNode;\n-import org.apache.hadoop.hive.ql.parse.CalcitePlanner;\n-import org.apache.hadoop.hive.ql.parse.HiveParser;\n-import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUzNjkxMQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464536911", "bodyText": "Could we have a single map that stores all necessary information in a single object value, rather than a map for the prepare plans and a map for the config?", "author": "jcamachor", "createdAt": "2020-08-03T16:52:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.drop;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.CalcitePlanner;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+/**\n+ * Analyzer for Prepare queries. This analyzer generates plan for the parameterized query\n+ * and save it in cache\n+ */\n+@DDLType(types = HiveParser.TOK_PREPARE)\n+public class PrepareStatementAnalyzer extends CalcitePlanner {\n+\n+  public PrepareStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * This method saves the current {@link PrepareStatementAnalyzer} object as well as\n+   * the config used to compile the plan.\n+   * @param root\n+   * @throws SemanticException\n+   */\n+  private void savePlan(String queryName) throws SemanticException{\n+    SessionState ss = SessionState.get();\n+    assert(ss != null);\n+\n+    if (ss.getPreparePlans().containsKey(queryName)) {\n+      throw new SemanticException(\"Prepare query: \" + queryName + \" already exists.\");\n+    }\n+    ss.getPreparePlans().put(queryName, this);", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyMTgzMQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464721831", "bodyText": "Yes, will update the code.", "author": "vineetgarg02", "createdAt": "2020-08-03T23:55:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUzNjkxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\nsimilarity index 88%\nrename from ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java\nrename to ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\nindex bfe0733eeb..7a27022e72 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/drop/PrepareStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PrepareStatementAnalyzer.java\n\n@@ -16,14 +16,10 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hadoop.hive.ql.ddl.table.drop;\n+package org.apache.hadoop.hive.ql.parse;\n \n import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n-import org.apache.hadoop.hive.ql.parse.ASTNode;\n-import org.apache.hadoop.hive.ql.parse.CalcitePlanner;\n-import org.apache.hadoop.hive.ql.parse.HiveParser;\n-import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1NTYxNQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464555615", "bodyText": "We should probably move this method to TaskUtils or Utilities rather than being used directly from AnnotateRunTimeStatsOptimizer.", "author": "jcamachor", "createdAt": "2020-08-03T17:28:06Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "diffHunk": "@@ -49,6 +50,8 @@\n import com.google.common.collect.Lists;\n import com.google.common.collect.Multimap;\n \n+import static org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer.getAllOperatorsForSimpleFetch;", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyMjAxMQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464722011", "bodyText": "You mean move getAllOperatorsForSimpleFetch from AnnotateRunTimeStatsOptimizer?", "author": "vineetgarg02", "createdAt": "2020-08-03T23:55:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1NTYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTg1NQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464729855", "bodyText": "Yes, I meant getAllOperatorsForSimpleFetch, since it seems it used beyond the scope of AnnotateRunTimeStatsOptimizer?", "author": "jcamachor", "createdAt": "2020-08-04T00:24:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1NTYxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\nindex 05d492bfa4..435cd9203c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\n\n@@ -50,8 +49,6 @@\n import com.google.common.collect.Lists;\n import com.google.common.collect.Multimap;\n \n-import static org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer.getAllOperatorsForSimpleFetch;\n-\n public class OperatorUtils {\n \n   private static final Logger LOG = LoggerFactory.getLogger(OperatorUtils.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDQxMQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464570411", "bodyText": "Is this correct? Could we leave a comment stating why the call to processGByExpr is needed?", "author": "jcamachor", "createdAt": "2020-08-03T17:51:33Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java", "diffHunk": "@@ -283,6 +283,33 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n \n   }\n \n+  /**\n+   * Processor for processing Dynamic expression.\n+   */\n+  public class DynamicParameterProcessor implements SemanticNodeProcessor {\n+\n+    @Override\n+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n+        Object... nodeOutputs) throws SemanticException {\n+      TypeCheckCtx ctx = (TypeCheckCtx) procCtx;\n+      if (ctx.getError() != null) {\n+        return null;\n+      }\n+\n+      T desc = processGByExpr(nd, procCtx);", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyMjA5Ng==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464722096", "bodyText": "No I believe this is not required. I will remove it.", "author": "vineetgarg02", "createdAt": "2020-08-03T23:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDQxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java\nindex 59da3af326..6eefc5dece 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/TypeCheckProcFactory.java\n\n@@ -296,11 +296,6 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         return null;\n       }\n \n-      T desc = processGByExpr(nd, procCtx);\n-      if (desc != null) {\n-        return desc;\n-      }\n-\n       ASTNode node = (ASTNode)nd;\n       String indexStr = ((ASTNode)(node)).getText();\n       int index = Integer.parseInt(indexStr);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjA3Nw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464572077", "bodyText": "I am wondering whether we need to add SELECT privilege here or it will be taken care of since we are relying on query execution logic? Could we investigate this in follow-up (probably it is important to avoid anyone bypassing authorization)?", "author": "jcamachor", "createdAt": "2020-08-03T17:54:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java", "diffHunk": "@@ -205,7 +205,9 @@\n   DROP_MAPPING(\"DROP MAPPING\", HiveParser.TOK_DROP_MAPPING, null, null, false, false),\n   CREATE_SCHEDULED_QUERY(\"CREATE SCHEDULED QUERY\", HiveParser.TOK_CREATE_SCHEDULED_QUERY, null, null),\n   ALTER_SCHEDULED_QUERY(\"ALTER SCHEDULED QUERY\", HiveParser.TOK_ALTER_SCHEDULED_QUERY, null, null),\n-  DROP_SCHEDULED_QUERY(\"DROP SCHEDULED QUERY\", HiveParser.TOK_DROP_SCHEDULED_QUERY, null, null)\n+  DROP_SCHEDULED_QUERY(\"DROP SCHEDULED QUERY\", HiveParser.TOK_DROP_SCHEDULED_QUERY, null, null),\n+  PREPARE(\"PREPARE QUERY\", HiveParser.TOK_PREPARE, null, null),\n+  EXECUTE(\"EXECUTE QUERY\", HiveParser.TOK_EXECUTE, null, null)", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyMjMwOA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464722308", "bodyText": "IIRC I had to make this change for explain plan to work. Let me re-investigate, I will get back.", "author": "vineetgarg02", "createdAt": "2020-08-03T23:56:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjA3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2Nzg4Mg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466567882", "bodyText": "Follow-up: https://issues.apache.org/jira/browse/HIVE-24007", "author": "vineetgarg02", "createdAt": "2020-08-06T17:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjA3Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjU5OQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464572599", "bodyText": "typo. colecting\nWhat will happen in this case? Could we add a few lines explaining the behavior?", "author": "jcamachor", "createdAt": "2020-08-03T17:55:50Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "diffHunk": "@@ -1619,6 +1620,9 @@ public static ColStatistics getColStatisticsFromExpression(HiveConf conf, Statis\n       colName = enfd.getFieldName();\n       colType = enfd.getTypeString();\n       countDistincts = numRows;\n+    } else if (end instanceof ExprDynamicParamDesc) {\n+      //skip colecting stats for parameters", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDYzMg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464714632", "bodyText": "This method tries to figure out column statistics involved in the given expression. I guess the stats are used by parent callers to do various estimation like map join, aggregate min/max. For dynamic expression stats are returned as null. I think it makes more sense to do what buildColStatForConstant is doing and return an estimation instead of null. I will update the code.", "author": "vineetgarg02", "createdAt": "2020-08-03T23:30:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjU5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1Njc1Mg==", "url": "https://github.com/apache/hive/pull/1315#discussion_r465356752", "bodyText": "Nevermind, colstats require column name and type name, since both are missing for dynamic param expression it is not possible to create col stats object, i will update the comment as you suggested.", "author": "vineetgarg02", "createdAt": "2020-08-04T22:06:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MjU5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java\nindex 7b9bec2576..815c2415bf 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java\n\n@@ -1621,7 +1621,10 @@ public static ColStatistics getColStatisticsFromExpression(HiveConf conf, Statis\n       colType = enfd.getTypeString();\n       countDistincts = numRows;\n     } else if (end instanceof ExprDynamicParamDesc) {\n-      //skip colecting stats for parameters\n+      //skip collecting stats for parameters\n+      // ideally we should estimate and create colstats object, because otherwise it could lead to\n+      // planning as if stats are missing. But since colstats require column name and type it is not\n+      // possible to create colstats object\n       return null;\n     } else {\n       throw new IllegalArgumentException(\"not supported expr type \" + end.getClass());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYxNTA1MQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464615051", "bodyText": "nit. outdated comment?", "author": "jcamachor", "createdAt": "2020-08-03T19:23:13Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.plan;\n+\n+import java.io.Serializable;\n+import java.util.List;\n+\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.hadoop.hive.common.StringInternUtils;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+\n+/**\n+ * A constant expression.", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\nindex 052c2ddd6d..bb462015d8 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\n\n@@ -34,7 +34,8 @@\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n \n /**\n- * A constant expression.\n+ * An expression representing dynamic parameter.\n+ * This is required for Prepare/Execute statements\n  */\n public class ExprDynamicParamDesc extends ExprNodeDesc implements Serializable {\n   private static final long serialVersionUID = 1L;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYxOTIxNA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464619214", "bodyText": "I was thinking we could use something more compact for the string representation in the plan -> \":\" + index. What do you think?", "author": "jcamachor", "createdAt": "2020-08-03T19:32:24Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.plan;\n+\n+import java.io.Serializable;\n+import java.util.List;\n+\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.hadoop.hive.common.StringInternUtils;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.BaseCharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+\n+/**\n+ * A constant expression.\n+ */\n+public class ExprDynamicParamDesc extends ExprNodeDesc implements Serializable {\n+  private static final long serialVersionUID = 1L;\n+  final protected transient static char[] hexArray = \"0123456789ABCDEF\".toCharArray();\n+\n+  private int index;\n+  private Object value;\n+\n+  public ExprDynamicParamDesc() {\n+  }\n+\n+  public ExprDynamicParamDesc(TypeInfo typeInfo, int index, Object value) {\n+    super(typeInfo);\n+    this.index =  index;\n+    this.value = value;\n+  }\n+\n+  public Object getValue() {\n+    return value;\n+  }\n+\n+  public int getIndex() {\n+    return index;\n+  }\n+\n+\n+  @Override\n+  public String toString() {\n+    return \"Dynamic Parameter \" + \" index: \" + index;", "originalCommit": "8d580d27dcbe34622721ec4963400656a8cfe1f8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwOTE3Nw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464709177", "bodyText": "\"Dynamic Parameter\" makes it clear that the expression in an explain plan is dynamic expression. Just showing index will make it hard to read.\nWhat is the benefit of making it more compact?", "author": "vineetgarg02", "createdAt": "2020-08-03T23:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYxOTIxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMjc1NA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r464712754", "bodyText": "I just think that using \":\" + index, \"$\" + index, or any other simpler variants, will make the plans more readable rather than having the dynamic parameter index: idx interleaved in the middle of complex expressions. What do other RDBMSs show, e.g., Postgres or MySQL?\nMaybe we could have different variants for user vs normal explain plans.", "author": "jcamachor", "createdAt": "2020-08-03T23:24:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYxOTIxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM0OTY3OQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r465349679", "bodyText": "Ok let me update it to $<idx for now. I am not sure what other databases use. I will try to figure out.", "author": "vineetgarg02", "createdAt": "2020-08-04T21:48:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYxOTIxNA=="}], "type": "inlineReview", "revised_code": {"commit": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\nindex 052c2ddd6d..bb462015d8 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprDynamicParamDesc.java\n\n@@ -34,7 +34,8 @@\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n \n /**\n- * A constant expression.\n+ * An expression representing dynamic parameter.\n+ * This is required for Prepare/Execute statements\n  */\n public class ExprDynamicParamDesc extends ExprNodeDesc implements Serializable {\n   private static final long serialVersionUID = 1L;\n"}}, {"oid": "e312d161533d83c8cb226ff7e6586b78102b7c6c", "url": "https://github.com/apache/hive/commit/e312d161533d83c8cb226ff7e6586b78102b7c6c", "message": "addressing review comments", "committedDate": "2020-08-04T22:19:58Z", "type": "commit"}, {"oid": "dd42f72215137c84ed5ffb45791d61492e9a7824", "url": "https://github.com/apache/hive/commit/dd42f72215137c84ed5ffb45791d61492e9a7824", "message": "Merge remote-tracking branch 'upstream/master' into PREPARE_EXECUTE_SUPPORT", "committedDate": "2020-08-04T22:20:29Z", "type": "commit"}, {"oid": "41d17e05c6dbacc283379a3b893661678bf1eb4e", "url": "https://github.com/apache/hive/commit/41d17e05c6dbacc283379a3b893661678bf1eb4e", "message": "fixing test issue", "committedDate": "2020-08-05T16:21:24Z", "type": "commit"}, {"oid": "a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "url": "https://github.com/apache/hive/commit/a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "message": "Test fixes", "committedDate": "2020-08-05T16:52:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NTE4Nw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466085187", "bodyText": "getIsPrepareQuery -> isPrepareQuery", "author": "jcamachor", "createdAt": "2020-08-06T01:00:51Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/Compiler.java", "diffHunk": "@@ -338,12 +339,22 @@ private QueryPlan createPlan(BaseSemanticAnalyzer sem) {\n     plan.setOptimizedCBOPlan(context.getCalcitePlan());\n     plan.setOptimizedQueryString(context.getOptimizedSql());\n \n+    // this is required so that later driver can skip executing prepare queries\n+    if (sem.getIsPrepareQuery()) {", "originalCommit": "a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2a4327dad0a9f9221684c319d8e0dec448897377", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\nindex d166b9d1bb..84100e17b4 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Compiler.java\n\n@@ -340,7 +340,7 @@ private QueryPlan createPlan(BaseSemanticAnalyzer sem) {\n     plan.setOptimizedQueryString(context.getOptimizedSql());\n \n     // this is required so that later driver can skip executing prepare queries\n-    if (sem.getIsPrepareQuery()) {\n+    if (sem.isPrepareQuery()) {\n       plan.setPrepareQuery(true);\n     }\n     return plan;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NTUzNw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466085537", "bodyText": "It seems this should be a boolean given the return type of the methods getter/setter.", "author": "jcamachor", "createdAt": "2020-08-06T01:02:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -121,6 +121,8 @@\n   private final DDLDescWithWriteId acidDdlDesc;\n   private Boolean autoCommitValue;\n \n+  private Boolean prepareQuery;", "originalCommit": "a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NjAxOA==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466086018", "bodyText": "Code commented out.", "author": "jcamachor", "createdAt": "2020-08-06T01:04:24Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.parse;\n+\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.exec.ExplainTask;\n+import org.apache.hadoop.hive.ql.exec.FetchTask;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.tez.TezTask;\n+import org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck;\n+import org.apache.hadoop.hive.ql.parse.type.TypeCheckCtx;\n+import org.apache.hadoop.hive.ql.plan.BaseWork;\n+import org.apache.hadoop.hive.ql.plan.ExprDynamicParamDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.parquet.Preconditions;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Analyzer for Execute statement.\n+ * This analyzer\n+ *  retrieves cached {@link BaseSemanticAnalyzer},\n+ *  makes copy of all tasks by serializing/deserializing it,\n+ *  bind dynamic parameters inside cached {@link BaseSemanticAnalyzer} using values provided\n+ */\n+@DDLType(types = HiveParser.TOK_EXECUTE)\n+public class ExecuteStatementAnalyzer extends BaseSemanticAnalyzer {\n+\n+  public ExecuteStatementAnalyzer(QueryState queryState) throws SemanticException {\n+    super(queryState);\n+  }\n+\n+  /**\n+   * This class encapsulate all {@link Task} required to be copied.\n+   * This is required because {@link FetchTask} list of {@link Task} may hold reference to same\n+   * objects (e.g. list of result files) and are required to be serialized/de-serialized together.\n+   */\n+  private class PlanCopy {\n+    FetchTask fetchTask;\n+    List<Task<?>> tasks;\n+\n+    PlanCopy(FetchTask fetchTask, List<Task<?>> tasks) {\n+      this.fetchTask = fetchTask;\n+      this.tasks = tasks;\n+    }\n+\n+    FetchTask getFetchTask() {\n+      return fetchTask;\n+    }\n+\n+    List<Task<?>> getTasks()  {\n+      return tasks;\n+    }\n+  }\n+\n+  private String getQueryName(ASTNode root) {\n+    ASTNode queryNameAST = (ASTNode)(root.getChild(1));\n+    return queryNameAST.getText();\n+  }\n+\n+  /**\n+   * Utility method to create copy of provided object using kryo serialization/de-serialization.\n+   */\n+  private <T> T makeCopy(final Object task, Class<T> objClass) {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    SerializationUtilities.serializePlan(task, baos);\n+\n+    return SerializationUtilities.deserializePlan(\n+        new ByteArrayInputStream(baos.toByteArray()), objClass);\n+  }\n+\n+  /**\n+   * Given a {@link BaseSemanticAnalyzer} (cached) this method make copies of all tasks\n+   * (including {@link FetchTask}) and update the existing {@link ExecuteStatementAnalyzer}\n+   */\n+  private void createTaskCopy(final BaseSemanticAnalyzer cachedPlan) {\n+    PlanCopy planCopy = new PlanCopy(cachedPlan.getFetchTask(), cachedPlan.getAllRootTasks());\n+    planCopy = makeCopy(planCopy, planCopy.getClass());\n+    this.setFetchTask(planCopy.getFetchTask());\n+    this.rootTasks = planCopy.getTasks();\n+  }\n+\n+  private String getParamLiteralValue(Map<Integer, ASTNode> paramMap, int paramIndex) {\n+    Preconditions.checkArgument(paramMap.containsKey(paramIndex), \"Index not found.\");\n+    ASTNode node = paramMap.get(paramIndex);\n+\n+    if (node.getType() == HiveParser.StringLiteral) {\n+      // remove quotes\n+      return BaseSemanticAnalyzer.unescapeSQLString(node.getText());\n+\n+    } else {\n+      return node.getText();\n+    }\n+  }\n+\n+\n+  /**\n+   * This method creates a constant expression to replace the given dynamic expression.\n+   * @param dynamicExpr Expression node representing Dynamic expression\n+   * @param typeInfo Type info used to create constant expression from ASTNode\n+   * @param parameterMap Integer to AST node map\n+   */\n+  private ExprNodeConstantDesc getConstant(ExprDynamicParamDesc dynamicExpr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> parameterMap) throws SemanticException {\n+    Preconditions.checkArgument(parameterMap.containsKey(dynamicExpr.getIndex()),\n+        \"Paramter index not found\");\n+\n+    ASTNode paramNode = parameterMap.get(dynamicExpr.getIndex());\n+\n+    TypeCheckCtx typeCheckCtx = new TypeCheckCtx(null);\n+    ExprNodeDesc node = ExprNodeTypeCheck.genExprNode(paramNode, typeCheckCtx).get(paramNode);\n+    Preconditions.checkArgument(node instanceof ExprNodeConstantDesc,\n+        \"Invalid expression created\");\n+    return (ExprNodeConstantDesc)node;\n+  }\n+\n+  /**\n+   * Given a list of expressions this method traverse the expression tree and replaces\n+   * all {@link ExprDynamicParamDesc} nodes with constant expression.\n+   * @param exprList\n+   * @param paramMap\n+   */\n+  private List<ExprNodeDesc> replaceDynamicParamsInExprList(List<ExprNodeDesc> exprList,\n+      Map<Integer, ASTNode> paramMap) throws SemanticException{\n+    List<ExprNodeDesc> updatedExprList = new ArrayList<>();\n+    for (ExprNodeDesc expr:exprList) {\n+      expr = replaceDynamicParamsWithConstant(expr, expr.getTypeInfo(), paramMap);\n+      updatedExprList.add(expr);\n+    }\n+    return updatedExprList;\n+  }\n+\n+  /**\n+   * Given an expression tree root at expr and type info of the expression this method traverse\n+   * the expression tree and replaces all dynamic expression with the constant expression.\n+   * This method also does type inference for the new constant expression.\n+   * Note about type inference\n+   * Since dynamic parameter lacks type we need to figure out appropriate type to create constant\n+   * out of string value. To do this, we choose the type of first child of the parent expression\n+   * which isn't dynamic parameter\n+   */\n+  private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInfo typeInfo,\n+      Map<Integer, ASTNode> paramMap) throws SemanticException{\n+    if (expr.getChildren() == null || expr.getChildren().isEmpty()) {\n+      if (expr instanceof ExprDynamicParamDesc) {\n+        return getConstant((ExprDynamicParamDesc)expr, typeInfo, paramMap);\n+      }\n+      return expr;\n+    }\n+\n+    for(ExprNodeDesc child:expr.getChildren()) {\n+      // we need typeinfo\n+      if(child instanceof ExprDynamicParamDesc) {\n+        continue;\n+      } else if( child.getTypeInfo() != TypeInfoFactory.voidTypeInfo) {\n+        typeInfo = child.getTypeInfo();\n+        break;\n+      }\n+    }\n+    Preconditions.checkArgument(typeInfo != null, \"TypeInfo is null\");\n+\n+    List<ExprNodeDesc> exprList = new ArrayList<>();\n+    for(ExprNodeDesc child: expr.getChildren()) {\n+      //if(child instanceof ExprDynamicParamDesc) {", "originalCommit": "a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2a4327dad0a9f9221684c319d8e0dec448897377", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\nindex e5047d7ce8..9ff62025d2 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ExecuteStatementAnalyzer.java\n\n@@ -194,11 +194,7 @@ private ExprNodeDesc replaceDynamicParamsWithConstant(ExprNodeDesc expr, TypeInf\n \n     List<ExprNodeDesc> exprList = new ArrayList<>();\n     for(ExprNodeDesc child: expr.getChildren()) {\n-      //if(child instanceof ExprDynamicParamDesc) {\n-      //  child = getConstant((ExprDynamicParamDesc)child, typeInfo, paramMap);\n-      //} else {\n-        child = replaceDynamicParamsWithConstant(child, typeInfo, paramMap);\n-      //}\n+      child = replaceDynamicParamsWithConstant(child, typeInfo, paramMap);\n       exprList.add(child);\n     }\n     expr.getChildren().clear();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4Njc1MQ==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466086751", "bodyText": "JIRA?", "author": "jcamachor", "createdAt": "2020-08-06T01:07:15Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "diffHunk": "@@ -1619,6 +1620,12 @@ public static ColStatistics getColStatisticsFromExpression(HiveConf conf, Statis\n       colName = enfd.getFieldName();\n       colType = enfd.getTypeString();\n       countDistincts = numRows;\n+    } else if (end instanceof ExprDynamicParamDesc) {\n+      //skip collecting stats for parameters", "originalCommit": "a2b2eb0a70a794228b866d19a8b4d781d6e8f370", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzMzU4Mw==", "url": "https://github.com/apache/hive/pull/1315#discussion_r466533583", "bodyText": "https://issues.apache.org/jira/browse/HIVE-24003", "author": "vineetgarg02", "createdAt": "2020-08-06T16:24:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4Njc1MQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "2a4327dad0a9f9221684c319d8e0dec448897377", "url": "https://github.com/apache/hive/commit/2a4327dad0a9f9221684c319d8e0dec448897377", "message": "review comments + test update", "committedDate": "2020-08-06T17:12:58Z", "type": "commit"}]}