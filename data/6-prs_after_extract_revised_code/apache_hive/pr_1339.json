{"pr_number": 1339, "pr_title": "HIVE-23956: Delete delta fileIds should be pushed execution", "pr_createdAt": "2020-07-30T14:46:36Z", "pr_url": "https://github.com/apache/hive/pull/1339", "timeline": [{"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "url": "https://github.com/apache/hive/commit/65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "message": "HIVE-23956: Delete delta fileIds should be pushed execution", "committedDate": "2020-07-30T14:45:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1MDYwMg==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463150602", "bodyText": "Nit: maybe a space after the for keyword?", "author": "pvary", "createdAt": "2020-07-30T17:17:39Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 65a09e423c..b887d4f0d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -128,15 +131,16 @@\n     public DeltaMetaData() {\n       this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n      * @param minWriteId min writeId of the delta directory\n      * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n-     * @param deltaFileStatuses bucketFiles in the directory\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n-        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1MTIzMw==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463151233", "bodyText": "Nit: newline", "author": "pvary", "createdAt": "2020-07-30T17:18:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 65a09e423c..b887d4f0d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -128,15 +131,16 @@\n     public DeltaMetaData() {\n       this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n      * @param minWriteId min writeId of the delta directory\n      * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n-     * @param deltaFileStatuses bucketFiles in the directory\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n-        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1NjMyNw==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463156327", "bodyText": "Nit: newline", "author": "pvary", "createdAt": "2020-07-30T17:27:31Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 65a09e423c..b887d4f0d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -128,15 +131,16 @@\n     public DeltaMetaData() {\n       this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n      * @param minWriteId min writeId of the delta directory\n      * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n-     * @param deltaFileStatuses bucketFiles in the directory\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n-        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1NjczNQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463156735", "bodyText": "Nit: newline", "author": "pvary", "createdAt": "2020-07-30T17:28:16Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+  final class DeltaFileMetaData implements Writable {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 65a09e423c..b887d4f0d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -128,15 +131,16 @@\n     public DeltaMetaData() {\n       this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n      * @param minWriteId min writeId of the delta directory\n      * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n-     * @param deltaFileStatuses bucketFiles in the directory\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n-        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1OTkxMg==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463159912", "bodyText": "Nit: newline", "author": "pvary", "createdAt": "2020-07-30T17:34:04Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+  final class DeltaFileMetaData implements Writable {\n+    private static final int HAS_LONG_FILEID_FLAG = 1;\n+    private static final int HAS_ATTEMPTID_FLAG = 2;\n+\n+    private long modTime;\n+    private long length;\n+    // Optional\n+    private Integer attemptId;\n+    // Optional\n+    private Long fileId;\n+\n+    public DeltaFileMetaData() {\n+    }\n+\n+    public DeltaFileMetaData(HadoopShims.HdfsFileStatusWithId fileStatus) {\n+      modTime = fileStatus.getFileStatus().getModificationTime();\n+      length = fileStatus.getFileStatus().getLen();\n+      String attempt = AcidUtils.parseAttemptId(fileStatus.getFileStatus().getPath());\n+      attemptId = StringUtils.isEmpty(attempt) ? null : Integer.parseInt(attempt);\n+      fileId = fileStatus.getFileId();\n+    }\n+\n+    public DeltaFileMetaData(long modTime, long length, @Nullable Integer attemptId, @Nullable Long fileId) {\n+      this.modTime = modTime;\n+      this.length = length;\n+      this.attemptId = attemptId;\n+      this.fileId = fileId;\n+    }\n+\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      int flags = (fileId != null ? HAS_LONG_FILEID_FLAG : 0) |\n+          (attemptId != null ? HAS_ATTEMPTID_FLAG : 0);\n+      out.writeByte(flags);\n+      out.writeLong(modTime);\n+      out.writeLong(length);\n+      if (attemptId != null) {\n+        out.writeInt(attemptId);\n+      }\n+      if (fileId != null) {\n+        out.writeLong(fileId);\n+      }\n+    }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      byte flags = in.readByte();\n+      boolean hasLongFileId = (HAS_LONG_FILEID_FLAG & flags) != 0,\n+          hasAttemptId = (HAS_ATTEMPTID_FLAG & flags) != 0;\n+      modTime = in.readLong();\n+      length = in.readLong();\n+      if (hasAttemptId) {\n+        attemptId = in.readInt();\n+      }\n+      if (hasLongFileId) {\n+        fileId = in.readLong();\n+      }\n+    }\n+\n+    public Object getFileId(Path deltaDirectory, int bucketId) {\n+      if (fileId != null) {\n+        return fileId;\n+      }\n+      // Calculate the synthetic fileid\n+      Path realPath = getPath(deltaDirectory, bucketId);\n+      return new SyntheticFileId(realPath, length, modTime);\n+    }\n+    public Path getPath(Path deltaDirectory, int bucketId) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 65a09e423c..b887d4f0d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -128,15 +131,16 @@\n     public DeltaMetaData() {\n       this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n      * @param minWriteId min writeId of the delta directory\n      * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n-     * @param deltaFileStatuses bucketFiles in the directory\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n-        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2MzE2MA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463163160", "bodyText": "Nit: space before for", "author": "pvary", "createdAt": "2020-07-30T17:40:03Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1113,10 +1119,13 @@ else if(statementId != parsedDelta.statementId) {\n               && (last.getMinWriteId() == parsedDelta.getMinWriteId())\n               && (last.getMaxWriteId() == parsedDelta.getMaxWriteId())) {\n         last.getStmtIds().add(parsedDelta.getStatementId());\n+        for(HadoopShims.HdfsFileStatusWithId fileStatus : parsedDelta.getFiles()) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex a77053a57c..252272e8b3 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -1104,33 +1105,38 @@ else if(statementId != parsedDelta.statementId) {\n   }\n \n   /**\n-   * todo: rename serializeDeleteDelta()?\n    * Convert the list of deltas into an equivalent list of begin/end\n    * write id pairs.  Assumes {@code deltas} is sorted.\n    * @param deltas\n    * @return the list of write ids to serialize\n    */\n-  public static List<AcidInputFormat.DeltaMetaData> serializeDeltas(List<ParsedDelta> deltas) {\n+  public static List<AcidInputFormat.DeltaMetaData> serializeDeleteDeltas(List<ParsedDelta> deltas) {\n     List<AcidInputFormat.DeltaMetaData> result = new ArrayList<>(deltas.size());\n     AcidInputFormat.DeltaMetaData last = null;\n     for (ParsedDelta parsedDelta : deltas) {\n       assert parsedDelta.isDeleteDelta() : \"expected delete_delta, got \" + parsedDelta.getPath();\n+      final Integer stmtId = parsedDelta.statementId >= 0 ? parsedDelta.statementId : null;\n       if ((last != null)\n               && (last.getMinWriteId() == parsedDelta.getMinWriteId())\n               && (last.getMaxWriteId() == parsedDelta.getMaxWriteId())) {\n-        last.getStmtIds().add(parsedDelta.getStatementId());\n-        for(HadoopShims.HdfsFileStatusWithId fileStatus : parsedDelta.getFiles()) {\n-          last.getDeltaFiles().add(new AcidInputFormat.DeltaFileMetaData(fileStatus));\n+        if (stmtId != null) {\n+          last.getStmtIds().add(stmtId);\n         }\n-        continue;\n-      }\n-      last = new AcidInputFormat.DeltaMetaData(parsedDelta.getMinWriteId(),\n-              parsedDelta.getMaxWriteId(), new ArrayList<>(), parsedDelta.getVisibilityTxnId(), parsedDelta.getFiles());\n-      result.add(last);\n-      if (parsedDelta.statementId >= 0) {\n         last.getStmtIds().add(parsedDelta.getStatementId());\n+        for (HadoopShims.HdfsFileStatusWithId fileStatus : parsedDelta.getFiles()) {\n+          last.getDeltaFiles().add(new AcidInputFormat.DeltaFileMetaData(fileStatus, stmtId));\n+        }\n+      } else {\n+        List<Integer> stmtIds = stmtId == null ? Collections.emptyList() : Collections.singletonList(stmtId);\n+        last = new AcidInputFormat.DeltaMetaData(parsedDelta.getMinWriteId(), parsedDelta.getMaxWriteId(),\n+            stmtIds, parsedDelta.getVisibilityTxnId(), parsedDelta.getFiles().stream().map(fs -> new AcidInputFormat.DeltaFileMetaData(fs, stmtId)).collect(\n+            Collectors.toList()));\n+        result.add(last);\n       }\n     }\n+    // For a small optimization clear the stmtIds for deltas that don't have more\n+    result.stream().filter(delta -> delta.getStmtIds().size() <= 1).forEach(\n+        deltaMetaData -> deltaMetaData.getDeltaFiles().forEach(AcidInputFormat.DeltaFileMetaData::clearStmtId));\n     return result;\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2MzkzNg==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463163936", "bodyText": "Nit: maybe do it in java8 way?", "author": "pvary", "createdAt": "2020-07-30T17:41:29Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1165,8 +1168,14 @@ private static ParsedDelta parseDelta(Path path, String deltaPrefix, FileSystem\n     throws IOException {\n     ParsedDelta p = parsedDelta(path, deltaPrefix, fs, dirSnapshot);\n     boolean isDeleteDelta = deltaPrefix.equals(DELETE_DELTA_PREFIX);\n+    List<HdfsFileStatusWithId> files = new ArrayList<>();\n+    for (FileStatus fileStatus : dirSnapshot.getFiles()) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex a77053a57c..252272e8b3 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -1164,31 +1170,34 @@ public static ParsedDelta parsedDelta(Path deltaDir, FileSystem fs) throws IOExc\n     return parsedDelta(deltaDir, DELTA_PREFIX, fs, null); // default prefix is delta_prefix\n   }\n \n-  private static ParsedDelta parseDelta(Path path, String deltaPrefix, FileSystem fs, HdfsDirSnapshot dirSnapshot)\n-    throws IOException {\n-    ParsedDelta p = parsedDelta(path, deltaPrefix, fs, dirSnapshot);\n-    boolean isDeleteDelta = deltaPrefix.equals(DELETE_DELTA_PREFIX);\n-    List<HdfsFileStatusWithId> files = new ArrayList<>();\n-    for (FileStatus fileStatus : dirSnapshot.getFiles()) {\n-      if (bucketFileFilter.accept(fileStatus.getPath())) {\n-        files.add(new HdfsFileStatusWithoutId(fileStatus));\n-      }\n-    }\n-    return new ParsedDelta(p.getMinWriteId(),\n-        p.getMaxWriteId(), path, p.statementId, isDeleteDelta, p.isRawFormat(), p.visibilityTxnId, files);\n-  }\n-\n   public static ParsedDelta parsedDelta(Path deltaDir, String deltaPrefix, FileSystem fs, HdfsDirSnapshot dirSnapshot)\n-    throws IOException {\n+      throws IOException {\n     String filename = deltaDir.getName();\n     boolean isDeleteDelta = deltaPrefix.equals(DELETE_DELTA_PREFIX);\n     if (filename.startsWith(deltaPrefix)) {\n       //small optimization - delete delta can't be in raw format\n       boolean isRawFormat = !isDeleteDelta && MetaDataFile.isRawFormat(deltaDir, fs, dirSnapshot);\n-      return parsedDelta(deltaDir, isRawFormat);\n+      ParsedDelta p = parsedDelta(deltaDir, isRawFormat);\n+      List<HdfsFileStatusWithId> files = null;\n+      if (dirSnapshot != null) {\n+        files = dirSnapshot.getFiles().stream()\n+            .filter(fileStatus -> bucketFileFilter.accept(fileStatus.getPath()))\n+            .map(HdfsFileStatusWithoutId::new)\n+            .collect(Collectors.toList());\n+      } else if (isDeleteDelta) {\n+        // For delete deltas we need the files for AcidState\n+        try {\n+          files = SHIMS.listLocatedHdfsStatus(fs, deltaDir, bucketFileFilter);\n+        } catch (UnsupportedOperationException uoe) {\n+          files = Arrays.stream(fs.listStatus(deltaDir, bucketFileFilter))\n+              .map(HdfsFileStatusWithoutId::new)\n+              .collect(Collectors.toList());\n+        }\n+      }\n+      return new ParsedDelta(p.getMinWriteId(), p.getMaxWriteId(), deltaDir, p.statementId, isDeleteDelta, p.isRawFormat(),\n+          p.visibilityTxnId, files);\n     }\n-    throw new IllegalArgumentException(deltaDir + \" does not start with \" +\n-                                       deltaPrefix);\n+    throw new IllegalArgumentException(deltaDir + \" does not start with \" + deltaPrefix);\n   }\n \n   public static ParsedDelta parsedDelta(Path deltaDir, boolean isRawFormat) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463166514", "bodyText": "Maybe 2 different getOrcTail method on the LLAP IO interface? @szlta?", "author": "pvary", "createdAt": "2020-07-30T17:46:15Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -680,14 +681,15 @@ public void setBaseAndInnerReader(\n    * @param path The Orc file path we want to get the OrcTail for\n    * @param conf The Configuration to access LLAP\n    * @param cacheTag The cacheTag needed to get OrcTail from LLAP IO cache\n+   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId)\n    * @return ReaderData object where the orcTail is not null. Reader can be null, but if we had to create\n    * one we return that as well for further reuse.\n    */\n-  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag) throws IOException {\n+  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag, Object fileKey) throws IOException {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIzNzIxMQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463237211", "bodyText": "I am not sure, we will ever want to use it. without fileId.", "author": "pvargacl", "createdAt": "2020-07-30T19:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwMTc3NA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464301774", "bodyText": "I'm fine with leaving just this one endpoint, and fileKey being optionally null.\nPlease do emphasize the optionality of fileKey arg in the javadoc part: aka it will be generated if not given, etc..", "author": "szlta", "createdAt": "2020-08-03T09:33:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA=="}], "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 47c0ede6a5..46ea8e285f 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -681,7 +682,9 @@ public void setBaseAndInnerReader(\n    * @param path The Orc file path we want to get the OrcTail for\n    * @param conf The Configuration to access LLAP\n    * @param cacheTag The cacheTag needed to get OrcTail from LLAP IO cache\n-   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId)\n+   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId).\n+   *                Optional, if it is not provided, it will be generated, see:\n+   *                {@link org.apache.hadoop.hive.ql.io.HdfsUtils.getFileId()}\n    * @return ReaderData object where the orcTail is not null. Reader can be null, but if we had to create\n    * one we return that as well for further reuse.\n    */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2ODI0Ng==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463168246", "bodyText": "isQualifiedDeleteDelta basically reparses the delta dir. Can we prevent this?", "author": "pvary", "createdAt": "2020-07-30T17:49:04Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 47c0ede6a5..818e7d3abc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -1581,40 +1582,40 @@ public int compareTo(CompressedOwid other) {\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n-            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n-              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+            // We got one path for each statement in a multiStmt transaction\n+            for (Pair<Path,Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              Integer stmtId = deleteDeltaDir.getRight();\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deltaMetaData, stmtId)) {\n                 LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n                 continue;\n               }\n-              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n-                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n-                try {\n-                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n-                  OrcTail orcTail = readerData.orcTail;\n-                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                    continue; // just a safe check to ensure that we are not reading empty delete files.\n-                  }\n-                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                    // If there is no intersection between data and delete delta, do not read delete file\n-                    continue;\n-                  }\n-                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                  // For LLAP cases we need to create it here.\n-                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n-                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                  DeleteReaderValue deleteReaderValue =\n-                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n-                          isBucketedTable, conf, keyInterval, orcSplit);\n-                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                  if (deleteReaderValue.next(deleteRecordKey)) {\n-                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                  } else {\n-                    deleteReaderValue.close();\n-                  }\n-                } catch (FileNotFoundException fnf) {\n-                  // We may not have deletes for the bucket being taken into consideration for this split processing.\n+              Path deleteDeltaPath = deleteDeltaDir.getLeft();\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFilesForStmtId(stmtId)) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaPath, bucket);\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag,\n+                    fileMetaData.getFileId(deleteDeltaPath, bucket));\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                  continue; // just a safe check to ensure that we are not reading empty delete files.\n+                }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                // For LLAP cases we need to create it here.\n+                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                    .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                DeleteReaderValue deleteReaderValue =\n+                    new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                        isBucketedTable, conf, keyInterval, orcSplit);\n+                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                if (deleteReaderValue.next(deleteRecordKey)) {\n+                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                } else {\n+                  deleteReaderValue.close();\n                 }\n               }\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463169065", "bodyText": "Do we still need this?", "author": "pvary", "createdAt": "2020-07-30T17:50:25Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+                LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n+                continue;\n+              }\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n+                try {\n+                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n+                  OrcTail orcTail = readerData.orcTail;\n+                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                    continue; // just a safe check to ensure that we are not reading empty delete files.\n+                  }\n+                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                    // If there is no intersection between data and delete delta, do not read delete file\n+                    continue;\n+                  }\n+                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                  // For LLAP cases we need to create it here.\n+                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                  DeleteReaderValue deleteReaderValue =\n+                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                          isBucketedTable, conf, keyInterval, orcSplit);\n+                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                  if (deleteReaderValue.next(deleteRecordKey)) {\n+                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                  } else {\n+                    deleteReaderValue.close();\n+                  }\n+                } catch (FileNotFoundException fnf) {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI0OTkzMg==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463249932", "bodyText": "Technically we need this, because of the multistatement case is not handled too well. There is one DeltaMetaData for one writeId and the statementIds are collected there. I did not want to disturb this structure, but this way I have one merged fileList for the different folders and it will try each file for each folder. This is far from ideal, but I don't think it is worth the effort to change this, before the multistatement feature is developed. But I will change the comment to reflect that.", "author": "pvargacl", "createdAt": "2020-07-30T20:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQwNjU4MA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463406580", "bodyText": "Multitable insersts also uses stmtId when inserting data. Not sure if we can insert twice in the same table with a single query....", "author": "pvary", "createdAt": "2020-07-31T05:04:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 47c0ede6a5..818e7d3abc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -1581,40 +1582,40 @@ public int compareTo(CompressedOwid other) {\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n-            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n-              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+            // We got one path for each statement in a multiStmt transaction\n+            for (Pair<Path,Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              Integer stmtId = deleteDeltaDir.getRight();\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deltaMetaData, stmtId)) {\n                 LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n                 continue;\n               }\n-              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n-                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n-                try {\n-                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n-                  OrcTail orcTail = readerData.orcTail;\n-                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                    continue; // just a safe check to ensure that we are not reading empty delete files.\n-                  }\n-                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                    // If there is no intersection between data and delete delta, do not read delete file\n-                    continue;\n-                  }\n-                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                  // For LLAP cases we need to create it here.\n-                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n-                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                  DeleteReaderValue deleteReaderValue =\n-                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n-                          isBucketedTable, conf, keyInterval, orcSplit);\n-                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                  if (deleteReaderValue.next(deleteRecordKey)) {\n-                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                  } else {\n-                    deleteReaderValue.close();\n-                  }\n-                } catch (FileNotFoundException fnf) {\n-                  // We may not have deletes for the bucket being taken into consideration for this split processing.\n+              Path deleteDeltaPath = deleteDeltaDir.getLeft();\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFilesForStmtId(stmtId)) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaPath, bucket);\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag,\n+                    fileMetaData.getFileId(deleteDeltaPath, bucket));\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                  continue; // just a safe check to ensure that we are not reading empty delete files.\n+                }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                // For LLAP cases we need to create it here.\n+                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                    .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                DeleteReaderValue deleteReaderValue =\n+                    new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                        isBucketedTable, conf, keyInterval, orcSplit);\n+                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                if (deleteReaderValue.next(deleteRecordKey)) {\n+                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                } else {\n+                  deleteReaderValue.close();\n                 }\n               }\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE3MTE1OQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r463171159", "bodyText": "Test for all of the different serialization options", "author": "pvary", "createdAt": "2020-07-30T17:53:50Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "diffHunk": "@@ -83,6 +94,34 @@ public void testDeltaMetaConstructWithState() throws Exception {\n     assertThat(deltaMetaData.getStmtIds().get(2), is(99));\n   }\n \n+  @Test\n+  public void testDeltaMetaWithFile() throws Exception {", "originalCommit": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java\nindex 561c074c31..486675270b 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java\n\n@@ -96,9 +96,102 @@ public void testDeltaMetaConstructWithState() throws Exception {\n \n   @Test\n   public void testDeltaMetaWithFile() throws Exception {\n+    FileStatus fs = new FileStatus(200, false, 100, 100, 100, new Path(\"mypath\"));\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, new ArrayList<>(), 0,\n+        Collections.singletonList(new AcidInputFormat.DeltaFileMetaData(new AcidUtils.HdfsFileStatusWithoutId(fs), null)));\n+\n+    assertEquals(2000L, deltaMetaData.getMinWriteId());\n+    assertEquals(2001L, deltaMetaData.getMaxWriteId());\n+    assertEquals(0, deltaMetaData.getStmtIds().size());\n+\n+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\n+    deltaMetaData.write(new DataOutputStream(byteArrayOutputStream));\n+\n+    byte[] bytes = byteArrayOutputStream.toByteArray();\n+    DeltaMetaData copy = new DeltaMetaData();\n+    copy.readFields(new DataInputStream(new ByteArrayInputStream(bytes)));\n+\n+    assertEquals(2000L, copy.getMinWriteId());\n+    assertEquals(2001L, copy.getMaxWriteId());\n+    assertEquals(0, copy.getStmtIds().size());\n+    AcidInputFormat.DeltaFileMetaData fileMetaData = copy.getDeltaFiles().get(0);\n+    Object fileId = fileMetaData.getFileId(new Path(\"deleteDelta\"), 1);\n+\n+    Assert.assertTrue(fileId instanceof SyntheticFileId);\n+    assertEquals(100, ((SyntheticFileId)fileId).getModTime());\n+    assertEquals(200, ((SyntheticFileId)fileId).getLength());\n+\n+    String fileName = fileMetaData.getPath(new Path(\"deleteDelta\"), 1).getName();\n+    Assert.assertEquals(\"bucket_00001\", fileName);\n+  }\n+\n+  @Test\n+  public void testDeltaMetaWithHdfsFileId() throws Exception {\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, new ArrayList<>(), 0,\n+        Collections.singletonList(new AcidInputFormat.DeltaFileMetaData(100, 200, null, 123L, null)));\n+\n+    assertEquals(2000L, deltaMetaData.getMinWriteId());\n+    assertEquals(2001L, deltaMetaData.getMaxWriteId());\n+    assertEquals(0, deltaMetaData.getStmtIds().size());\n+\n+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\n+    deltaMetaData.write(new DataOutputStream(byteArrayOutputStream));\n+\n+    byte[] bytes = byteArrayOutputStream.toByteArray();\n+    DeltaMetaData copy = new DeltaMetaData();\n+    copy.readFields(new DataInputStream(new ByteArrayInputStream(bytes)));\n+\n+    assertEquals(2000L, copy.getMinWriteId());\n+    assertEquals(2001L, copy.getMaxWriteId());\n+    assertEquals(0, copy.getStmtIds().size());\n+    AcidInputFormat.DeltaFileMetaData fileMetaData = copy.getDeltaFiles().get(0);\n+\n+    Object fileId = fileMetaData.getFileId(new Path(\"deleteDelta\"), 1);\n+    Assert.assertTrue(fileId instanceof Long);\n+    long fId = (Long)fileId;\n+    assertEquals(123L, fId);\n+\n+    String fileName = fileMetaData.getPath(new Path(\"deleteDelta\"), 1).getName();\n+    Assert.assertEquals(\"bucket_00001\", fileName);\n+  }\n+  @Test\n+  public void testDeltaMetaWithAttemptId() throws Exception {\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, new ArrayList<>(), 0,\n+        Collections.singletonList(new AcidInputFormat.DeltaFileMetaData(100, 200, 123, null, null)));\n+\n+    assertEquals(2000L, deltaMetaData.getMinWriteId());\n+    assertEquals(2001L, deltaMetaData.getMaxWriteId());\n+    assertEquals(0, deltaMetaData.getStmtIds().size());\n+\n+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+\n+    deltaMetaData.write(new DataOutputStream(byteArrayOutputStream));\n+\n+    byte[] bytes = byteArrayOutputStream.toByteArray();\n+    DeltaMetaData copy = new DeltaMetaData();\n+    copy.readFields(new DataInputStream(new ByteArrayInputStream(bytes)));\n+\n+    assertEquals(2000L, copy.getMinWriteId());\n+    assertEquals(2001L, copy.getMaxWriteId());\n+    assertEquals(0, copy.getStmtIds().size());\n+    AcidInputFormat.DeltaFileMetaData fileMetaData = copy.getDeltaFiles().get(0);\n+    Object fileId = fileMetaData.getFileId(new Path(\"deleteDelta\"), 1);\n+\n+    Assert.assertTrue(fileId instanceof SyntheticFileId);\n+    assertEquals(100, ((SyntheticFileId)fileId).getModTime());\n+    assertEquals(200, ((SyntheticFileId)fileId).getLength());\n+\n+    String fileName = fileMetaData.getPath(new Path(\"deleteDelta\"), 1).getName();\n+    Assert.assertEquals(\"bucket_00001_123\", fileName);\n+  }\n+\n+  @Test\n+  public void testDeltaMetaWithFileMultiStatement() throws Exception {\n     FileStatus fs = new FileStatus(200, false, 100, 100, 100, new Path(\"mypath\"));\n     DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, Arrays.asList(97, 98, 99), 0,\n-        Collections.singletonList(new AcidUtils.HdfsFileStatusWithoutId(fs)));\n+        Collections.singletonList(new AcidInputFormat.DeltaFileMetaData(new AcidUtils.HdfsFileStatusWithoutId(fs), 97)));\n \n     assertEquals(2000L, deltaMetaData.getMinWriteId());\n     assertEquals(2001L, deltaMetaData.getMaxWriteId());\n"}}, {"oid": "761cf4ae777e0f8a63db2f93252031ad49cdbf36", "url": "https://github.com/apache/hive/commit/761cf4ae777e0f8a63db2f93252031ad49cdbf36", "message": "Review comments and multistatementhandling improvement", "committedDate": "2020-07-31T16:00:56Z", "type": "commit"}, {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "url": "https://github.com/apache/hive/commit/f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "message": "Small fix", "committedDate": "2020-08-03T07:24:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNDY0Nw==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464304647", "bodyText": "nit: too many newline. If we need any fix, please remove them", "author": "pvary", "createdAt": "2020-08-03T09:38:37Z", "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "diffHunk": "@@ -250,18 +255,71 @@ public void testGetOrcTailForPath() throws Exception {\n     Configuration jobConf = new Configuration();\n     Configuration daemonConf = new Configuration();\n     CacheTag tag = CacheTag.build(\"test-table\");\n-    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n-    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n     assertEquals(uncached.getFileTail(), cached.getFileTail());\n   }\n \n+  @Test\n+  public void testGetOrcTailForPathWithFileId() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    FileSystem fs = FileSystem.get(daemonConf);\n+    FileStatus fileStatus = fs.getFileStatus(path);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, new SyntheticFileId(fileStatus));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    // this should work from the cache, by recalculating the same fileId\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, null);\n+    assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n+    assertEquals(uncached.getFileTail(), cached.getFileTail());\n+  }\n+\n+  @Test\n+  public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 100));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    Exception ex = null;\n+    try {\n+      // this should miss the cache, since the fileKey changed\n+      OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));\n+    } catch (IOException e) {\n+      ex = e;\n+    }\n+    Assert.assertNotNull(ex);\n+    Assert.assertTrue(ex.getMessage().contains(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname));\n+  }\n+\n+", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\nindex 531003a9d7..29aab30def 100644\n--- a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\n+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\n\n@@ -306,15 +306,13 @@ public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n     try {\n       // this should miss the cache, since the fileKey changed\n       OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));\n+      fail();\n     } catch (IOException e) {\n       ex = e;\n     }\n-    Assert.assertNotNull(ex);\n     Assert.assertTrue(ex.getMessage().contains(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname));\n   }\n \n-\n-\n   @Test(expected = IllegalCacheConfigurationException.class)\n   public void testGetOrcTailForPathCacheNotReady() throws Exception {\n     Path path = new Path(\"../data/files/alltypesorc\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwMzM2Ng==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464303366", "bodyText": "you can add a fail call here, as it should always jump from line 308 to catch clause.", "author": "szlta", "createdAt": "2020-08-03T09:36:12Z", "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "diffHunk": "@@ -250,18 +255,71 @@ public void testGetOrcTailForPath() throws Exception {\n     Configuration jobConf = new Configuration();\n     Configuration daemonConf = new Configuration();\n     CacheTag tag = CacheTag.build(\"test-table\");\n-    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n-    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n     assertEquals(uncached.getFileTail(), cached.getFileTail());\n   }\n \n+  @Test\n+  public void testGetOrcTailForPathWithFileId() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    FileSystem fs = FileSystem.get(daemonConf);\n+    FileStatus fileStatus = fs.getFileStatus(path);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, new SyntheticFileId(fileStatus));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    // this should work from the cache, by recalculating the same fileId\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, null);\n+    assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n+    assertEquals(uncached.getFileTail(), cached.getFileTail());\n+  }\n+\n+  @Test\n+  public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 100));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    Exception ex = null;\n+    try {\n+      // this should miss the cache, since the fileKey changed\n+      OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\nindex 531003a9d7..29aab30def 100644\n--- a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\n+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java\n\n@@ -306,15 +306,13 @@ public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n     try {\n       // this should miss the cache, since the fileKey changed\n       OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));\n+      fail();\n     } catch (IOException e) {\n       ex = e;\n     }\n-    Assert.assertNotNull(ex);\n     Assert.assertTrue(ex.getMessage().contains(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname));\n   }\n \n-\n-\n   @Test(expected = IllegalCacheConfigurationException.class)\n   public void testGetOrcTailForPathCacheNotReady() throws Exception {\n     Path path = new Path(\"../data/files/alltypesorc\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNTE4OQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464305189", "bodyText": "The generation of fileId may be more complicated based on what configuration is given. Please refer to https://github.com/apache/hive/blob/master/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java#L496-L517", "author": "szlta", "createdAt": "2020-08-03T09:39:38Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new ImmutablePair<>(new Path(root, getName()), null));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream()\n+            .map(stmtId -> new ImmutablePair<>(new Path(root, getName(stmtId)), stmtId)).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+\n+  final class DeltaFileMetaData implements Writable {\n+    private static final int HAS_LONG_FILEID_FLAG = 1;\n+    private static final int HAS_ATTEMPTID_FLAG = 2;\n+    private static final int HAS_STMTID_FLAG = 4;\n+\n+    private long modTime;\n+    private long length;\n+    // Optional\n+    private Integer attemptId;\n+    // Optional\n+    private Long fileId;\n+    // Optional, if the deltaMeta contains multiple stmtIds, it will contain this files parent's stmtId\n+    private Integer stmtId;\n+\n+    public DeltaFileMetaData() {\n+    }\n+\n+    public DeltaFileMetaData(HadoopShims.HdfsFileStatusWithId fileStatus, Integer stmtId) {\n+      modTime = fileStatus.getFileStatus().getModificationTime();\n+      length = fileStatus.getFileStatus().getLen();\n+      String attempt = AcidUtils.parseAttemptId(fileStatus.getFileStatus().getPath());\n+      attemptId = StringUtils.isEmpty(attempt) ? null : Integer.parseInt(attempt);\n+      fileId = fileStatus.getFileId();\n+      this.stmtId = stmtId;\n+    }\n+\n+    public DeltaFileMetaData(long modTime, long length, @Nullable Integer attemptId, @Nullable Long fileId,\n+        @Nullable Integer stmtId) {\n+      this.modTime = modTime;\n+      this.length = length;\n+      this.attemptId = attemptId;\n+      this.fileId = fileId;\n+      this.stmtId = stmtId;\n+    }\n+\n+    public void clearStmtId() {\n+      stmtId = null;\n+    }\n+\n+    public Integer getStmtId() {\n+      return stmtId;\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      int flags = (fileId != null ? HAS_LONG_FILEID_FLAG : 0) |\n+          (attemptId != null ? HAS_ATTEMPTID_FLAG : 0) |\n+          (stmtId != null ? HAS_STMTID_FLAG : 0);\n+      out.writeByte(flags);\n+      out.writeLong(modTime);\n+      out.writeLong(length);\n+      if (attemptId != null) {\n+        out.writeInt(attemptId);\n+      }\n+      if (fileId != null) {\n+        out.writeLong(fileId);\n+      }\n+      if (stmtId != null) {\n+        out.writeInt(stmtId);\n+      }\n+    }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      byte flags = in.readByte();\n+      boolean hasLongFileId = (HAS_LONG_FILEID_FLAG & flags) != 0,\n+          hasAttemptId = (HAS_ATTEMPTID_FLAG & flags) != 0,\n+          hasStmtId = (HAS_STMTID_FLAG & flags) != 0;\n+      modTime = in.readLong();\n+      length = in.readLong();\n+      if (hasAttemptId) {\n+        attemptId = in.readInt();\n+      }\n+      if (hasLongFileId) {\n+        fileId = in.readLong();\n+      }\n+      if (hasStmtId) {\n+        stmtId = in.readInt();\n+      }\n+    }\n+\n+    public Object getFileId(Path deltaDirectory, int bucketId) {\n+      if (fileId != null) {\n+        return fileId;\n+      }\n+      // Calculate the synthetic fileid\n+      Path realPath = getPath(deltaDirectory, bucketId);\n+      return new SyntheticFileId(realPath, length, modTime);\n+    }", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3NjMzOA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464476338", "bodyText": "I will add the forceSynthetic parameter, it has valid use-casee (https://issues.apache.org/jira/browse/HIVE-20338) but I have a problem with this: boolean allowSynthetic = HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_CACHE_ALLOW_SYNTHETIC_FILEID);\nIf someone disables this, it will render the llap cache useless, even more, your orctailcache will just throw an IllegalCacheConfigurationException", "author": "pvargacl", "createdAt": "2020-08-03T15:09:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNTE4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 3245cb16e2..c41c4f2af0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -323,8 +324,9 @@ public void readFields(DataInput in) throws IOException {\n       }\n     }\n \n-    public Object getFileId(Path deltaDirectory, int bucketId) {\n-      if (fileId != null) {\n+    public Object getFileId(Path deltaDirectory, int bucketId, Configuration conf) {\n+      boolean forceSynthetic = !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_IO_USE_FILEID_PATH);\n+      if (fileId != null && !forceSynthetic) {\n         return fileId;\n       }\n       // Calculate the synthetic fileid\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNjM0MQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464306341", "bodyText": "Question: How often do we call this? Is it ok to calculate this every time, or it would be better to store in a way that is already filtered, like a map?", "author": "pvary", "createdAt": "2020-08-03T09:41:43Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4MzQ2NQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464483465", "bodyText": "I will be a very small list, I don't think it matters.", "author": "pvargacl", "createdAt": "2020-08-03T15:21:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNjM0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 3245cb16e2..c41c4f2af0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -323,8 +324,9 @@ public void readFields(DataInput in) throws IOException {\n       }\n     }\n \n-    public Object getFileId(Path deltaDirectory, int bucketId) {\n-      if (fileId != null) {\n+    public Object getFileId(Path deltaDirectory, int bucketId, Configuration conf) {\n+      boolean forceSynthetic = !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_IO_USE_FILEID_PATH);\n+      if (fileId != null && !forceSynthetic) {\n         return fileId;\n       }\n       // Calculate the synthetic fileid\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwODUzMA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464308530", "bodyText": "nit: extra space", "author": "pvary", "createdAt": "2020-08-03T09:45:48Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2493,7 +2514,7 @@ private static Path chooseFile(Path baseOrDeltaDir, FileSystem fs) throws IOExce\n       }\n       FileStatus[] dataFiles;\n       try {\n-        dataFiles = fs.listStatus(new Path[]{baseOrDeltaDir}, originalBucketFilter);\n+        dataFiles = fs.listStatus(baseOrDeltaDir , originalBucketFilter);", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\nindex 252272e8b3..1eb08ebd5a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java\n\n@@ -2514,7 +2516,7 @@ private static Path chooseFile(Path baseOrDeltaDir, FileSystem fs) throws IOExce\n       }\n       FileStatus[] dataFiles;\n       try {\n-        dataFiles = fs.listStatus(baseOrDeltaDir , originalBucketFilter);\n+        dataFiles = fs.listStatus(baseOrDeltaDir, originalBucketFilter);\n       } catch (FileNotFoundException e) {\n         // HIVE-22001: If the file was not found, this means that baseOrDeltaDir (which was listed\n         // earlier during AcidUtils.getAcidState()) was removed sometime between the FS list call\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwOTI2NQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464309265", "bodyText": "nit: space <Path, Integer>", "author": "pvary", "createdAt": "2020-08-03T09:47:18Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,20 +1577,23 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            // We got one path for each statement in a multiStmt transaction\n+            for (Pair<Path,Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\nindex 818e7d3abc..46ea8e285f 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java\n\n@@ -1583,7 +1585,7 @@ public int compareTo(CompressedOwid other) {\n           int totalDeleteEventCount = 0;\n           for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n             // We got one path for each statement in a multiStmt transaction\n-            for (Pair<Path,Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+            for (Pair<Path, Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n               Integer stmtId = deleteDeltaDir.getRight();\n               if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deltaMetaData, stmtId)) {\n                 LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMDIyOA==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464310228", "bodyText": "Do we need the order? Why not map?", "author": "pvary", "createdAt": "2020-08-03T09:49:13Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3MjAwNQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464472005", "bodyText": "I think the List is much more straightforward, it will keep the stmid order.", "author": "pvargacl", "createdAt": "2020-08-03T15:02:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMDIyOA=="}], "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\nindex 3245cb16e2..c41c4f2af0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java\n\n@@ -323,8 +324,9 @@ public void readFields(DataInput in) throws IOException {\n       }\n     }\n \n-    public Object getFileId(Path deltaDirectory, int bucketId) {\n-      if (fileId != null) {\n+    public Object getFileId(Path deltaDirectory, int bucketId, Configuration conf) {\n+      boolean forceSynthetic = !HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_IO_USE_FILEID_PATH);\n+      if (fileId != null && !forceSynthetic) {\n         return fileId;\n       }\n       // Calculate the synthetic fileid\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464311382", "bodyText": "nit: extra spaces?", "author": "pvary", "createdAt": "2020-08-03T09:51:21Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1641,28 +1645,26 @@ public int compareTo(CompressedOwid other) {\n      * Check if the delete delta folder needs to be scanned for a given split's min/max write ids.\n      *\n      * @param orcSplitMinMaxWriteIds\n-     * @param deleteDeltaDir\n+     * @param deleteDelta\n+     * @param stmtId statementId of the deleteDelta if present\n      * @return true when  delete delta dir has to be scanned.\n      */\n     @VisibleForTesting\n     protected static boolean isQualifiedDeleteDeltaForSplit(AcidOutputFormat.Options orcSplitMinMaxWriteIds,\n-        Path deleteDeltaDir)\n-    {\n-      AcidUtils.ParsedDelta deleteDelta = AcidUtils.parsedDelta(deleteDeltaDir, false);\n+        AcidInputFormat.DeltaMetaData deleteDelta, Integer stmtId) {", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3MzEyNQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464473125", "bodyText": "it is the second line of parameters, no extra space here", "author": "pvargacl", "createdAt": "2020-08-03T15:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTg5OQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464859899", "bodyText": ":)", "author": "pvary", "createdAt": "2020-08-04T07:40:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMjYxNQ==", "url": "https://github.com/apache/hive/pull/1339#discussion_r464312615", "bodyText": "This is a valid test, but I think the testMultipleInserts test for inserts, and this is test for deletes. Maybe create its' own test method named testDeleteOfInserts like testUpdateOfInserts?", "author": "pvary", "createdAt": "2020-08-03T09:54:00Z", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java", "diffHunk": "@@ -618,7 +618,13 @@ public void testMultipleInserts() throws Exception {\n     dumpTableData(Table.ACIDTBL, 1, 1);\n     List<String> rs1 = runStatementOnDriver(\"select a,b from \" + Table.ACIDTBL + \" order by a,b\");\n     Assert.assertEquals(\"Content didn't match after commit rs1\", allData, rs1);\n+    runStatementOnDriver(\"delete from \" + Table.ACIDTBL + \" where b = 2\");", "originalCommit": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "461b45479e29b68c4afccfd19153a7f404459a86", "chunk": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java\nindex 2fbb3ac9e7..e1185de25e 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java\n\n@@ -618,6 +618,16 @@ public void testMultipleInserts() throws Exception {\n     dumpTableData(Table.ACIDTBL, 1, 1);\n     List<String> rs1 = runStatementOnDriver(\"select a,b from \" + Table.ACIDTBL + \" order by a,b\");\n     Assert.assertEquals(\"Content didn't match after commit rs1\", allData, rs1);\n+  }\n+\n+  @Test\n+  public void testDeleteOfMultipleInserts() throws Exception {\n+    runStatementOnDriver(\"START TRANSACTION\");\n+    int[][] rows1 = {{1,2},{3,4}};\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) \" + makeValuesClause(rows1));\n+    int[][] rows2 = {{5,6},{7,8}};\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) \" + makeValuesClause(rows2));\n+    runStatementOnDriver(\"commit\");\n     runStatementOnDriver(\"delete from \" + Table.ACIDTBL + \" where b = 2\");\n     runStatementOnDriver(\"delete from \" + Table.ACIDTBL + \" where b = 8\");\n     List<String> rs2 = runStatementOnDriver(\"select a,b from \" + Table.ACIDTBL + \" order by a,b\");\n"}}, {"oid": "461b45479e29b68c4afccfd19153a7f404459a86", "url": "https://github.com/apache/hive/commit/461b45479e29b68c4afccfd19153a7f404459a86", "message": "Fix review comments and tests", "committedDate": "2020-08-03T15:28:07Z", "type": "commit"}, {"oid": "8ce08622ca78cc7ada02d1fbe20e0f082620777e", "url": "https://github.com/apache/hive/commit/8ce08622ca78cc7ada02d1fbe20e0f082620777e", "message": "Forget to add the bucketFilter for the deltas", "committedDate": "2020-08-03T22:53:17Z", "type": "commit"}, {"oid": "133ba44627899060ec10549cbf9f14815b60ef53", "url": "https://github.com/apache/hive/commit/133ba44627899060ec10549cbf9f14815b60ef53", "message": "Test fix", "committedDate": "2020-08-04T06:37:29Z", "type": "commit"}]}