{"pr_number": 1327, "pr_title": "HIVE-23763: Query based minor compaction produces wrong files when ro\u2026", "pr_createdAt": "2020-07-28T08:21:53Z", "pr_url": "https://github.com/apache/hive/pull/1327", "timeline": [{"oid": "7e13485bcd8417a289f6fac81bda197df1f46d1b", "url": "https://github.com/apache/hive/commit/7e13485bcd8417a289f6fac81bda197df1f46d1b", "message": "HIVE-23763: Query based minor compaction produces wrong files when rows with different buckets Ids are processed by the same FileSinkOperator", "committedDate": "2020-07-28T08:11:02Z", "type": "commit"}, {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "url": "https://github.com/apache/hive/commit/f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "message": "HIVE-23763: Fix the MM compaction tests", "committedDate": "2020-07-30T15:40:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4ODgwNw==", "url": "https://github.com/apache/hive/pull/1327#discussion_r463488807", "bodyText": "Can we leave a comment why are these settings needed?", "author": "pvary", "createdAt": "2020-07-31T08:53:54Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -95,6 +98,10 @@ private void setupTez(HiveConf conf) {\n     conf.set(\"hive.tez.container.size\", \"128\");\n     conf.setBoolean(\"hive.merge.tezfiles\", false);\n     conf.setBoolean(\"hive.in.tez.test\", true);\n+    if (!mmCompaction) {\n+      conf.set(\"tez.grouping.max-size\", \"1024\");", "originalCommit": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMzkzMQ==", "url": "https://github.com/apache/hive/pull/1327#discussion_r464313931", "bodyText": "Sure, added a comment.", "author": "kuczoram", "createdAt": "2020-08-03T09:56:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4ODgwNw=="}], "type": "inlineReview", "revised_code": {"commit": "707ae82cef2c6568cda535d0080847ef7de69e07", "chunk": "diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\nindex 5c63f16741..08b9039804 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\n\n@@ -99,6 +99,13 @@ private void setupTez(HiveConf conf) {\n     conf.setBoolean(\"hive.merge.tezfiles\", false);\n     conf.setBoolean(\"hive.in.tez.test\", true);\n     if (!mmCompaction) {\n+      // We need these settings to create a table which is not bucketed, but contains multiple files.\n+      // If these parameters are set when inserting 100 rows into the table, the rows will\n+      // be distributed into multiple files. This setup is used in the testMinorCompactionWithoutBuckets,\n+      // testMinorCompactionWithoutBucketsInsertOverwrite and testMajorCompactionWithoutBucketsInsertAndDeleteInsertOverwrite\n+      // tests in the TestCrudCompactorOnTez class.\n+      // This use case has to be tested because of HIVE-23763. The MM compaction is not affected by this issue,\n+      // therefore no need to set these configs for MM compaction.\n       conf.set(\"tez.grouping.max-size\", \"1024\");\n       conf.set(\"tez.grouping.min-size\", \"1\");\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4OTQxNA==", "url": "https://github.com/apache/hive/pull/1327#discussion_r463489414", "bodyText": "Could we leave a comment with the expected file structure?", "author": "pvary", "createdAt": "2020-07-31T08:55:02Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -217,6 +224,64 @@ void insertTestData(String dbName, String tblName) throws Exception {\n       executeStatementOnDriver(\"delete from \" + tblName + \" where a = '1'\", driver);\n     }\n \n+    void createTableWithoutBucketWithMultipleSplits(String dbName, String tblName, String tempTblName,", "originalCommit": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2ODUxOA==", "url": "https://github.com/apache/hive/pull/1327#discussion_r464368518", "bodyText": "Sure, added a comment.", "author": "kuczoram", "createdAt": "2020-08-03T12:00:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4OTQxNA=="}], "type": "inlineReview", "revised_code": {"commit": "707ae82cef2c6568cda535d0080847ef7de69e07", "chunk": "diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\nindex 5c63f16741..08b9039804 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java\n\n@@ -224,6 +231,19 @@ void insertTestData(String dbName, String tblName) throws Exception {\n       executeStatementOnDriver(\"delete from \" + tblName + \" where a = '1'\", driver);\n     }\n \n+    /**\n+     * This method is for creating a non-bucketed table in which the data is distributed\n+     * into multiple splits. The initial data is 100 rows and it should be split into\n+     * multiple files, like bucket_000001, bucket_000002, ...\n+     * This is needed because the MINOR compactions had issues with tables like this. (HIVE-23763)\n+     * @param dbName\n+     * @param tblName\n+     * @param tempTblName\n+     * @param createDeletes\n+     * @param createInserts\n+     * @param insertOverwrite\n+     * @throws Exception\n+     */\n     void createTableWithoutBucketWithMultipleSplits(String dbName, String tblName, String tempTblName,\n         boolean createDeletes, boolean createInserts, boolean insertOverwrite) throws Exception {\n       if (dbName != null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MDQzNA==", "url": "https://github.com/apache/hive/pull/1327#discussion_r463490434", "bodyText": "I have seen issues with queries using virtual columns (INPUT__FILE__NAME, ROW__ID). The row number in the results were different with and without the virtual columns. - This is just a note, maybe no action is needed here", "author": "pvary", "createdAt": "2020-07-31T08:57:06Z", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -261,22 +326,77 @@ protected void insertMmTestData(String tblName, int iterations) throws Exception\n     }\n \n     List<String> getAllData(String tblName) throws Exception {\n-      return getAllData(null, tblName);\n+      return getAllData(null, tblName, false);\n     }\n \n-    List<String> getAllData(String dbName, String tblName) throws Exception {\n+    List<String> getAllData(String tblName, boolean withRowId) throws Exception {\n+      return getAllData(null, tblName, withRowId);\n+    }\n+\n+    List<String> getAllData(String dbName, String tblName, boolean withRowId) throws Exception {\n       if (dbName != null) {\n         tblName = dbName + \".\" + tblName;\n       }\n-      List<String> result = executeStatementOnDriverAndReturnResults(\"select * from \" + tblName, driver);\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      if (withRowId) {\n+        query.append(\"ROW__ID, \");\n+      }\n+      query.append(\"* from \");\n+      query.append(tblName);\n+      List<String> result = executeStatementOnDriverAndReturnResults(query.toString(), driver);\n       Collections.sort(result);\n       return result;\n     }\n \n+    List<String> getDataWithInputFileNames(String dbName, String tblName) throws Exception {\n+      if (dbName != null) {\n+        tblName = dbName + \".\" + tblName;\n+      }\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      query.append(\"INPUT__FILE__NAME, a from \");", "originalCommit": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2OTI5NA==", "url": "https://github.com/apache/hive/pull/1327#discussion_r464369294", "bodyText": "Thanks a lot for the info. I will check, but I haven't seen any issues with this. Maybe because I don't check the row number here, just the file names?", "author": "kuczoram", "createdAt": "2020-08-03T12:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MDQzNA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MTU5OA==", "url": "https://github.com/apache/hive/pull/1327#discussion_r463491598", "bodyText": "Are the results in order? Can we close writers for old buckets? That could save memory", "author": "pvary", "createdAt": "2020-07-31T08:59:26Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -1063,7 +1076,11 @@ public void process(Object row, int tag) throws HiveException {\n       // RecordUpdater expects to get the actual row, not a serialized version of it.  Thus we\n       // pass the row rather than recordValue.\n       if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID || conf.isMmTable() || conf.isCompactionTable()) {\n-        rowOutWriters[findWriterOffset(row)].write(recordValue);\n+        writerOffset = bucketId;\n+        if (!conf.isCompactionTable()) {\n+          writerOffset = findWriterOffset(row);\n+        }\n+        rowOutWriters[writerOffset].write(recordValue);", "originalCommit": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM3MzIwMg==", "url": "https://github.com/apache/hive/pull/1327#discussion_r464373202", "bodyText": "They should be in order, because the result temp table for the compaction is created like \"clustered by (bucket) sorted by (bucket, originalTransaction, rowId) into 10 buckets\". I would assume that because of this, the rows in the table will be in order by bucket, originalTransaction and rowId. I haven't seen otherwise during my testing.\nI don't think we can close the writers here, because they will be used in the closeOp method as well and they are closed there.", "author": "kuczoram", "createdAt": "2020-08-03T12:11:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MTU5OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MjEwMw==", "url": "https://github.com/apache/hive/pull/1327#discussion_r463492103", "bodyText": "Can we leave a comment why this settings are needed?", "author": "pvary", "createdAt": "2020-07-31T09:00:20Z", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java", "diffHunk": "@@ -115,6 +115,10 @@ void runCompactionQueries(HiveConf conf, String tmpTableName, StorageDescriptor\n       }\n       for (String query : compactionQueries) {\n         LOG.info(\"Running {} compaction via query: {}\", compactionInfo.isMajorCompaction() ? \"major\" : \"minor\", query);\n+        if (!compactionInfo.isMajorCompaction()) {", "originalCommit": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwMTY3MQ==", "url": "https://github.com/apache/hive/pull/1327#discussion_r464501671", "bodyText": "Sure, added a comment.", "author": "kuczoram", "createdAt": "2020-08-03T15:50:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MjEwMw=="}], "type": "inlineReview", "revised_code": {"commit": "707ae82cef2c6568cda535d0080847ef7de69e07", "chunk": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java\nindex f4c3a2a8f8..8f6a977b26 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java\n\n@@ -116,6 +116,14 @@ void runCompactionQueries(HiveConf conf, String tmpTableName, StorageDescriptor\n       for (String query : compactionQueries) {\n         LOG.info(\"Running {} compaction via query: {}\", compactionInfo.isMajorCompaction() ? \"major\" : \"minor\", query);\n         if (!compactionInfo.isMajorCompaction()) {\n+          // There was an issue with the query-based MINOR compaction (HIVE-23763), that the row distribution between the FileSinkOperators\n+          // was not correlated correctly with the bucket numbers. So we could end up with files containing rows from\n+          // multiple buckets or rows from the same bucket could end up in different FileSinkOperator. This behaviour resulted\n+          // corrupted files. To fix this, the FileSinkOperator has been extended to be able to handle rows from different buckets.\n+          // But we also had to be sure that all rows from the same bucket would end up in the same FileSinkOperator. Therefore\n+          // the ReduceSinkOperator has also been extended to distribute the rows by bucket numbers. To use this logic,\n+          // these two optimisations have to be turned off for the MINOR compaction. The MAJOR compaction works differently\n+          // and its query doesn't use reducers, so these optimisations should not be turned off for MAJOR compaction.\n           conf.set(\"hive.optimize.bucketingsorting\", \"false\");\n           conf.set(\"hive.vectorized.execution.enabled\", \"false\");\n         }\n"}}, {"oid": "707ae82cef2c6568cda535d0080847ef7de69e07", "url": "https://github.com/apache/hive/commit/707ae82cef2c6568cda535d0080847ef7de69e07", "message": "HIVE-23763: Address the review comments", "committedDate": "2020-08-03T16:12:29Z", "type": "commit"}]}