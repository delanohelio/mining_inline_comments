{"pr_number": 770, "pr_title": "Issue #765 #768 Bulkdata import metrics and error handling enhancements", "pr_createdAt": "2020-03-10T22:31:46Z", "pr_url": "https://github.com/IBM/FHIR/pull/770", "timeline": [{"oid": "90364ce0d281be0ea0299c589aef428fdc7af169", "url": "https://github.com/IBM/FHIR/commit/90364ce0d281be0ea0299c589aef428fdc7af169", "message": "issue #765 #768 Bulkdata batchJob metrics and error handling enhancement\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-10T22:23:42Z", "type": "commit"}, {"oid": "b75adb84d3d4dabc985a23286536b022aaae40df", "url": "https://github.com/IBM/FHIR/commit/b75adb84d3d4dabc985a23286536b022aaae40df", "message": "issue #765 adding the changed Constants.java\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-10T22:25:03Z", "type": "commit"}, {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "url": "https://github.com/IBM/FHIR/commit/b377ad83612ea30e7361c65d3e07f9b83bb398d3", "message": "issue #765 use logger instead of system.out for metric\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T02:42:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzOTk5Nw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390939997", "bodyText": "maybe convert the values to constants explaining REQUEST_TIMEOUT_IN_SECONDS or something similar -- 1000 = second", "author": "prb112", "createdAt": "2020-03-11T12:37:09Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -84,8 +85,10 @@ public static AmazonS3 getCosClient(String cosCredentialIbm, String cosApiKeyPro\n             credentials = new BasicAWSCredentials(cosApiKeyProperty, cosSrvinstId);\n         }\n \n-        ClientConfiguration clientConfig = new ClientConfiguration().withRequestTimeout(8000);\n-        clientConfig.setUseTcpKeepAlive(true);\n+        ClientConfiguration clientConfig = new ClientConfiguration()\n+                .withRequestTimeout(10*1000)\n+                .withTcpKeepAlive(true)\n+                .withSocketTimeout(120*1000);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MTkyNw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390951927", "bodyText": "good idea!", "author": "albertwang-ibm", "createdAt": "2020-03-11T12:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzOTk5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2MjE1OQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390962159", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:18:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzOTk5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -86,9 +86,9 @@ public class BulkDataUtils {\n         }\n \n         ClientConfiguration clientConfig = new ClientConfiguration()\n-                .withRequestTimeout(10*1000)\n+                .withRequestTimeout(Constants.COS_REQUEST_TIMEOUT)\n                 .withTcpKeepAlive(true)\n-                .withSocketTimeout(120*1000);\n+                .withSocketTimeout(Constants.COS_SOCKET_TIMEOUT);\n \n         return AmazonS3ClientBuilder.standard().withCredentials(new AWSStaticCredentialsProvider(credentials))\n                 .withEndpointConfiguration(new EndpointConfiguration(cosEndpintUrl, cosLocation))\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDM2NA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390940364", "bodyText": "interesting... I hadn't thought so much about this case... make sense", "author": "prb112", "createdAt": "2020-03-11T12:37:50Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,9 +137,18 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n     private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "900e9ea2cfee8c7b3b577b47b210a317d1536e68", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..28f77abfca 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -139,12 +139,14 @@ public class BulkDataUtils {\n \n     /**\n      * @param resReader - the buffer reader to read FHIR resource from.\n-     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param numOfProcessedLines - number of the already processed lines.\n      * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n      * @return - the number of parsing failures.\n      * @throws Exception\n      */\n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n         int parseFailures = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDUzNw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390940537", "bodyText": "you can switch back to a while loop", "author": "prb112", "createdAt": "2020-03-11T12:38:09Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,9 +137,18 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n     private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1Mzk2MQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391053961", "bodyText": "en, I would prefer to keep it unchanged ...", "author": "albertwang-ibm", "createdAt": "2020-03-11T15:25:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDUzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1OTAzOQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391059039", "bodyText": "no worries - it wasn't a requirement - just an observation", "author": "prb112", "createdAt": "2020-03-11T15:32:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDUzNw=="}], "type": "inlineReview", "revised_code": {"commit": "900e9ea2cfee8c7b3b577b47b210a317d1536e68", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..28f77abfca 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -139,12 +139,14 @@ public class BulkDataUtils {\n \n     /**\n      * @param resReader - the buffer reader to read FHIR resource from.\n-     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param numOfProcessedLines - number of the already processed lines.\n      * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n      * @return - the number of parsing failures.\n      * @throws Exception\n      */\n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n         int parseFailures = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390943260", "bodyText": "it may be worth logging out the line number as well", "author": "prb112", "createdAt": "2020-03-11T12:43:19Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MjkyMA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390952920", "bodyText": "thought about this also, need a little bit calculation ...", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:01:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MzAxMQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390953011", "bodyText": "let me try to add", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:01:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1NDQyOQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391054429", "bodyText": "added, a little bit more complicated ... :)", "author": "albertwang-ibm", "createdAt": "2020-03-11T15:26:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1OTQzMg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391059432", "bodyText": "I think it'll be helpful.  thanks for adding", "author": "prb112", "createdAt": "2020-03-11T15:32:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NDc5MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390944790", "bodyText": "why would export result in parse failures? wouldnt it already be valid and stored in the db\nalso why is it reporting back the number of parseFailures? maybe update the method signature to explain the contract and purpose", "author": "prb112", "createdAt": "2020-03-11T12:46:17Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1NDQyMA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390954420", "bodyText": "this is the read step, the success ones are in the fhirresource List, so can easily get the number from the list using size(), failure number is needed to update the data source for the metrics in the write step later.", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:04:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NDc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA1OTc4Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391059782", "bodyText": "got it", "author": "prb112", "createdAt": "2020-03-11T15:33:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NDc5MA=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NTA2OQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390945069", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n          \n          \n            \n                public static void cleanupForTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n          \n      \n    \n    \n  \n\nI don't think we should use the number 4 here", "author": "prb112", "createdAt": "2020-03-11T12:46:49Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1NDcwMQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390954701", "bodyText": "haha, good point! will change", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:05:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NTA2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk3NTg5NQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390975895", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NTA2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NjM5MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390946390", "bodyText": "it's cool to see this works in the context of the webapp (I'm sure this simplifies things)", "author": "prb112", "createdAt": "2020-03-11T12:49:22Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -8,29 +8,46 @@\n \n import java.util.HashMap;\n import java.util.List;\n+import java.util.logging.Logger;\n \n import javax.batch.api.listener.JobListener;\n+import javax.batch.operations.JobOperator;\n+import javax.batch.runtime.BatchRuntime;\n+import javax.batch.runtime.JobExecution;\n import javax.batch.runtime.context.JobContext;\n import javax.inject.Inject;\n \n public class ImportJobListener implements JobListener {\n+    private static final Logger logger = Logger.getLogger(ImportJobListener.class.getName());\n     @Inject\n     JobContext jobContext;\n \n-    private long jobStartTimeInMS, jobEndTimeInMS;\n-\n     public ImportJobListener() {\n \n     }\n \n-    @SuppressWarnings(\"unchecked\")\n+\n+    @SuppressWarnings({\"unchecked\" })\n     @Override\n     public void afterJob() {\n+        // jobExecution.getEndTime() for current execution always returns null, so we use system current time as the end time for current execution.\n+        long currentExecutionEndTimeInMS = System.currentTimeMillis();;\n+\n         // Used for generating response for all the import data resources.\n         List<ImportCheckPointData> partitionSummaries = (List<ImportCheckPointData>)jobContext.getTransientUserData();\n         // Used for generating performance measurement per each resource type.\n         HashMap<String, ImportCheckPointData> importedResourceTypeSummaries = new HashMap<>();\n \n+        JobOperator jobOperator = BatchRuntime.getJobOperator();", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzNzI3Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391037272", "bodyText": "yes, totally agree", "author": "albertwang-ibm", "createdAt": "2020-03-11T15:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NjM5MA=="}], "type": "inlineReview", "revised_code": {"commit": "05203d7b61438d06d745079822ea59b4d262d3ca", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\nindex e58f9df0e4..d8759846c7 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\n\n@@ -27,13 +27,13 @@ public class ImportJobListener implements JobListener {\n     }\n \n \n-    @SuppressWarnings({\"unchecked\" })\n     @Override\n     public void afterJob() {\n         // jobExecution.getEndTime() for current execution always returns null, so we use system current time as the end time for current execution.\n         long currentExecutionEndTimeInMS = System.currentTimeMillis();;\n \n         // Used for generating response for all the import data resources.\n+        @SuppressWarnings(\"unchecked\")\n         List<ImportCheckPointData> partitionSummaries = (List<ImportCheckPointData>)jobContext.getTransientUserData();\n         // Used for generating performance measurement per each resource type.\n         HashMap<String, ImportCheckPointData> importedResourceTypeSummaries = new HashMap<>();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Njg2NQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390946865", "bodyText": "double?  does this need to be a double or is a long just fine?", "author": "prb112", "createdAt": "2020-03-11T12:50:12Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -49,26 +66,25 @@ public void afterJob() {\n             }\n         }\n \n-        jobEndTimeInMS = System.currentTimeMillis();\n-        double jobProcessingSeconds = (jobEndTimeInMS - jobStartTimeInMS)/1000.0;\n+\n+        double jobProcessingSeconds = (totalJobExecutionMilliSeconds)/1000.0;", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MTM2NQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391041365", "bodyText": "made it double just because the rate is calculated as totalImportedFhirResources/jobProcessingSeconds", "author": "albertwang-ibm", "createdAt": "2020-03-11T15:08:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Njg2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0NjU3Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391046572", "bodyText": "got it... in case, it falls below a second", "author": "prb112", "createdAt": "2020-03-11T15:15:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Njg2NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzI2Nw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947267", "bodyText": "is this outputting a report?", "author": "prb112", "createdAt": "2020-03-11T12:50:56Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -49,26 +66,25 @@ public void afterJob() {\n             }\n         }\n \n-        jobEndTimeInMS = System.currentTimeMillis();\n-        double jobProcessingSeconds = (jobEndTimeInMS - jobStartTimeInMS)/1000.0;\n+\n+        double jobProcessingSeconds = (totalJobExecutionMilliSeconds)/1000.0;\n         jobProcessingSeconds = jobProcessingSeconds < 1 ? 1.0 : jobProcessingSeconds;\n \n-        // Print out the simple metrics to console.\n-        System.out.println(\" ---- Fhir resources imported in \" + jobProcessingSeconds + \"seconds ----\");\n-        System.out.println(\"ResourceType \\t Imported \\t Failed\");\n+        // log the simple metrics.\n+        logger.info(\" ---- Fhir resources imported in \" + jobProcessingSeconds + \"seconds ----\");\n+        logger.info(\"ResourceType \\t Imported \\t Failed\");", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1NTU5Mw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390955593", "bodyText": "yes, this is the report that I pasted into the issue for tracking progress...", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:06:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzI2Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Nzc0Ng==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947746", "bodyText": "I suggested a signature change in a prior comment.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, true);\n          \n          \n            \n                        BulkDataUtils.cleanupForTransientUserData(partitionSummaryData, true);", "author": "prb112", "createdAt": "2020-03-11T12:51:51Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java", "diffHunk": "@@ -79,16 +78,7 @@ public Serializable collectPartitionData() throws Exception{\n \n         // If the job is being stopped or in other status except for \"started\", then do cleanup for the partition.\n         if (!batchStatus.equals(BatchStatus.STARTED)) {\n-            if (partitionSummaryData.getInputStream() != null) {\n-                if (partitionSummaryData.getInputStream() instanceof S3ObjectInputStream) {\n-                    ((S3ObjectInputStream)partitionSummaryData.getInputStream()).abort();\n-                }\n-                partitionSummaryData.getInputStream().close();\n-            }\n-\n-            if (partitionSummaryData.getBufferReader() != null) {\n-                partitionSummaryData.getBufferReader().close();\n-            }\n+            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, true);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1NTcxNA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390955714", "bodyText": "make sense", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Nzc0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk3NjEyNA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390976124", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Nzc0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\nindex 0555701e8c..f2e32d013a 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\n\n@@ -78,7 +78,7 @@ public class ImportPartitionCollector implements PartitionCollector {\n \n         // If the job is being stopped or in other status except for \"started\", then do cleanup for the partition.\n         if (!batchStatus.equals(BatchStatus.STARTED)) {\n-            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, true);\n+            BulkDataUtils.cleanupTransientUserData(partitionSummaryData, true);\n             return null;\n         }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzgyNw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947827", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, false);\n          \n          \n            \n                        BulkDataUtils.cleanupForTransientUserData(partitionSummaryData, false);", "author": "prb112", "createdAt": "2020-03-11T12:52:01Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java", "diffHunk": "@@ -152,13 +142,8 @@ public Serializable collectPartitionData() throws Exception{\n                 }\n             }\n \n-            if (partitionSummaryData.getBufferReader() != null) {\n-                partitionSummaryData.getBufferReader().close();\n-            }\n-\n-            if (partitionSummaryData.getInputStream() != null) {\n-                partitionSummaryData.getInputStream().close();\n-            }\n+            // Clean up.\n+            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, false);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1NTc5OQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390955799", "bodyText": "will do", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:07:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzgyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk3NjQxMg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390976412", "bodyText": "done in my eclipse with refactor", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:40:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzgyNw=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\nindex 0555701e8c..f2e32d013a 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java\n\n@@ -143,7 +143,7 @@ public class ImportPartitionCollector implements PartitionCollector {\n             }\n \n             // Clean up.\n-            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, false);\n+            BulkDataUtils.cleanupTransientUserData(partitionSummaryData, false);\n \n             return ImportCheckPointData.fromImportTransientUserData(partitionSummaryData);\n         } else {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODE3Nw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948177", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"Failed to import \" + fhirResource.getId() + \" due to error: \" + e.getMessage());\n          \n          \n            \n                                logger.warning(\"Failed to import '\" + fhirResource.getId() + \"' due to error: \" + e.getMessage());", "author": "prb112", "createdAt": "2020-03-11T12:52:39Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java", "diffHunk": "@@ -151,7 +151,7 @@ public void writeItems(List<java.lang.Object> arg0) throws Exception {\n                         }\n                     }\n                 } catch (FHIRPersistenceException e) {\n-                    logger.warning(\"Failed to import due to error: \" + e.getMessage());\n+                    logger.warning(\"Failed to import \" + fhirResource.getId() + \" due to error: \" + e.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk5OTQ1Ng==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390999456", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T14:13:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODE3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java\nindex 70552fa837..b3865bcc8f 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java\n\n@@ -151,7 +151,7 @@ public class ChunkWriter extends AbstractItemWriter {\n                         }\n                     }\n                 } catch (FHIRPersistenceException e) {\n-                    logger.warning(\"Failed to import \" + fhirResource.getId() + \" due to error: \" + e.getMessage());\n+                    logger.warning(\"Failed to import '\" + fhirResource.getId() + \"' due to error: \" + e.getMessage());\n                     failedNum++;\n                     if (Constants.IMPORT_IS_COLLECT_OPERATIONOUTCOMES) {\n                         FHIRGenerator.generator(Format.JSON).generate(FHIRUtil.buildOperationOutcome(e, false), chunkData.getBufferStream4ImportError());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODc3Mw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948773", "bodyText": "Thinking about this... it's probably worth adding a logger.finer() just in case the code changes and hits default", "author": "prb112", "createdAt": "2020-03-11T12:53:46Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java", "diffHunk": "@@ -142,17 +140,20 @@ public Object readItem() throws Exception {\n             } else {\n                 logger.finer(\"readItem: Got CosClient successfully!\");\n             }\n-            imported = BulkDataUtils.readFhirResourceFromObjectStore(cosClient, cosBucketName, importPartitionWorkitem,\n-                    numOfLinesToSkip, loadedFhirResources, Constants.IMPORT_IS_REUSE_INPUTSTREAM, chunkData);\n+            numOfParseFailures = BulkDataUtils.readFhirResourceFromObjectStore(cosClient, cosBucketName, importPartitionWorkitem,\n+                    numOfLinesToSkip, loadedFhirResources, chunkData);\n             break;\n         default:\n             break;", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4MDQxMQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390980411", "bodyText": "make sense. adding  logger.warning(\"readItem: Data source storage type not found!\");", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODc3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java\nindex 53ac17d816..a48a8c816d 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java\n\n@@ -144,6 +144,7 @@ public class ChunkReader extends AbstractItemReader {\n                     numOfLinesToSkip, loadedFhirResources, chunkData);\n             break;\n         default:\n+            logger.warning(\"readItem: Data source storage type not found!\");\n             break;\n         }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODk1Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948952", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                            cleanupForTransientUserData(transientUserData, true);", "author": "prb112", "createdAt": "2020-03-11T12:54:08Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTE1Nw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949157", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromLocalFile: Retry ...\");\n          \n      \n    \n    \n  \n\nno need to split string", "author": "prb112", "createdAt": "2020-03-11T12:54:31Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4MTQ3Mw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390981473", "bodyText": "good catch!", "author": "albertwang-ibm", "createdAt": "2020-03-11T13:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTE1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTI3OQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949279", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n          \n          \n            \n                            logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file [\" + dataUrl + \"] - \" + ex.getMessage());", "author": "prb112", "createdAt": "2020-03-11T12:54:47Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTQzNA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949434", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file [\" + dataUrl + \"] - \" + ex.getMessage());", "author": "prb112", "createdAt": "2020-03-11T12:55:04Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDQ0NA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950444", "bodyText": "what does fhirResources.clear accomplish and why?  I see it's passed into this method.", "author": "prb112", "createdAt": "2020-03-11T12:56:56Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk5MTg2Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390991862", "bodyText": "wanted to clear before the retry, the retry will reopen the input stream, skip all the processed lines in the last batch, and then redo the whole read for this batch again. let me improve the codes a little bit to skip the success reads in this batch also ... then I will not need to clear it it ...", "author": "albertwang-ibm", "createdAt": "2020-03-11T14:02:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAwNDM5OQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391004399", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T14:19:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDQ0NA=="}], "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDYwOQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950609", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                        cleanupForTransientUserData(transientUserData, true);", "author": "prb112", "createdAt": "2020-03-11T12:57:14Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDc4Mw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950783", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                            cleanupForTransientUserData(transientUserData, true);", "author": "prb112", "createdAt": "2020-03-11T12:57:35Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDkxNg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950916", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n          \n          \n            \n                            logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file [\" + itemName + \"] - \" + ex.getMessage());", "author": "prb112", "createdAt": "2020-03-11T12:57:51Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MTA1Mg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390951052", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromObjectStore: Retry ...\");", "author": "prb112", "createdAt": "2020-03-11T12:58:08Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MTE3OA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390951178", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n          \n          \n            \n                        logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file [\" + filePath + \"] - \" + ex.getMessage());", "author": "prb112", "createdAt": "2020-03-11T12:58:24Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());", "originalCommit": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 7eceb1920f..90171c268c 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -175,7 +175,7 @@ public class BulkDataUtils {\n         return parseFailures;\n     }\n \n-    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+    public static void cleanupTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n         if (transientUserData.getInputStream() != null) {\n             if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n                 // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n"}}, {"oid": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "url": "https://github.com/IBM/FHIR/commit/8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "message": "issue #765 #768 updates per review comments\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T14:11:18Z", "type": "commit"}, {"oid": "48eeec8a4bb62a15813109a7c485ebf36350132f", "url": "https://github.com/IBM/FHIR/commit/48eeec8a4bb62a15813109a7c485ebf36350132f", "message": "issue #765 update logging\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T14:19:36Z", "type": "commit"}, {"oid": "900e9ea2cfee8c7b3b577b47b210a317d1536e68", "url": "https://github.com/IBM/FHIR/commit/900e9ea2cfee8c7b3b577b47b210a317d1536e68", "message": "issue #765 add line number and file info for parsing error.\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T15:23:02Z", "type": "commit"}, {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293", "url": "https://github.com/IBM/FHIR/commit/9c74a610037b7bec59fc7fba4cd612a831c51293", "message": "issue #765 reuse thread local COS/S3 client\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T16:16:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391123370", "bodyText": "why not log the stack trace?  might it have some useful info for figuring out what is invalid about the resource that failed to parse?", "author": "lmsurpre", "createdAt": "2020-03-11T17:02:17Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,112 +137,181 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfProcessedLines - number of the already processed lines.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;\n         do {\n             resLine = resReader.readLine();\n             if (resLine != null) {\n                 lineRed++;\n-                if (lineRed <= numOfLinesToSkip) {\n+                if (isSkipProcessed && lineRed <= numOfProcessedLines) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());", "originalCommit": "9c74a610037b7bec59fc7fba4cd612a831c51293", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTAyNw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391139027", "bodyText": "I thought stack trace is a little bit too heavy in the log, with the exception info and the line number, we should already have enough info to follow up if need ...", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:26:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE0MjcxMg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391142712", "bodyText": "OK.  If not taking my full suggestion, I'd still suggest to update the log statement to level INFO.   This will be more consistent with the way we handle invalid data sent by the client on the REST side.", "author": "lmsurpre", "createdAt": "2020-03-11T17:31:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE0MzQyNA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391143424", "bodyText": "ok, sounds good", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:32:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE0ODI2MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391148260", "bodyText": "accepted you change and fixed the error.", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:39:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA=="}], "type": "inlineReview", "revised_code": {"commit": "39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 28f77abfca..726a5268c2 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -167,10 +167,9 @@ public class BulkDataUtils {\n                     }\n                 } catch (FHIRParserException e) {\n                     // Log and skip the invalid FHIR resource.\n-                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n                     parseFailures++;\n-                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n-                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");\n+                    logger.log(Level.INFO, \"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n+                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\", e);\n                     continue;\n                 }\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyNDc4Nw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391124787", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                // Log and skip the invalid FHIR resource.\n          \n          \n            \n                                logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n          \n          \n            \n                                parseFailures++;\n          \n          \n            \n                                logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n          \n          \n            \n                                        + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");\n          \n          \n            \n                                // Log and skip the invalid FHIR resource.\n          \n          \n            \n                                parseFailures++;\n          \n          \n            \n                                logger.log(Level.INFO, \"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n          \n          \n            \n                                        + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\", e);", "author": "lmsurpre", "createdAt": "2020-03-11T17:04:32Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,112 +137,181 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfProcessedLines - number of the already processed lines.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;\n         do {\n             resLine = resReader.readLine();\n             if (resLine != null) {\n                 lineRed++;\n-                if (lineRed <= numOfLinesToSkip) {\n+                if (isSkipProcessed && lineRed <= numOfProcessedLines) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    parseFailures++;\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n+                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");", "originalCommit": "9c74a610037b7bec59fc7fba4cd612a831c51293", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\nindex 28f77abfca..726a5268c2 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\n@@ -167,10 +167,9 @@ public class BulkDataUtils {\n                     }\n                 } catch (FHIRParserException e) {\n                     // Log and skip the invalid FHIR resource.\n-                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n                     parseFailures++;\n-                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n-                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");\n+                    logger.log(Level.INFO, \"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n+                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\", e);\n                     continue;\n                 }\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzMjc3NQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391132775", "bodyText": "maybe turn these comments into javadoc?", "author": "lmsurpre", "createdAt": "2020-03-11T17:16:43Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/Constants.java", "diffHunk": "@@ -48,6 +48,11 @@\n \n     // Control if push OperationOutcomes to COS/S3.\n     public static final boolean IMPORT_IS_COLLECT_OPERATIONOUTCOMES = false;\n-    // Control if reuse the input stream of the data source across the chunks.\n-    public static final boolean IMPORT_IS_REUSE_INPUTSTREAM = true;\n+    // Retry times when https or amazon s3 client timeout or other error happens, e.g, timeout can happen if the batch write to DB takes\n+    // longer than the socket timeout, set to retry once for now.\n+    public static final int IMPORT_RETRY_TIMES = 1;\n+    public static final int COS_REQUEST_TIMEOUT = 10000;\n+    // Batch writing to DB can take long time which can make the idle COS/S3 client connection timeout, so set the client socket timeout\n+    // to 120 seconds which is the default DB2 timeout.\n+    public static final int COS_SOCKET_TIMEOUT = 120000;", "originalCommit": "9c74a610037b7bec59fc7fba4cd612a831c51293", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzNTMxMw==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391135313", "bodyText": "Just confirming:  ChunkWriter get re-instantiated for each job, right?", "author": "lmsurpre", "createdAt": "2020-03-11T17:20:28Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java", "diffHunk": "@@ -38,6 +38,7 @@\n  */\n public class ChunkWriter extends AbstractItemWriter {\n     private static final Logger logger = Logger.getLogger(ChunkWriter.class.getName());\n+    AmazonS3 cosClient = null;", "originalCommit": "9c74a610037b7bec59fc7fba4cd612a831c51293", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTg0NQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391139845", "bodyText": "yes", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:27:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzNTMxMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391138476", "bodyText": "which warning is being suppressed?  can we avoid it?", "author": "lmsurpre", "createdAt": "2020-03-11T17:25:19Z", "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -8,29 +8,46 @@\n \n import java.util.HashMap;\n import java.util.List;\n+import java.util.logging.Logger;\n \n import javax.batch.api.listener.JobListener;\n+import javax.batch.operations.JobOperator;\n+import javax.batch.runtime.BatchRuntime;\n+import javax.batch.runtime.JobExecution;\n import javax.batch.runtime.context.JobContext;\n import javax.inject.Inject;\n \n public class ImportJobListener implements JobListener {\n+    private static final Logger logger = Logger.getLogger(ImportJobListener.class.getName());\n     @Inject\n     JobContext jobContext;\n \n-    private long jobStartTimeInMS, jobEndTimeInMS;\n-\n     public ImportJobListener() {\n \n     }\n \n-    @SuppressWarnings(\"unchecked\")\n+\n+    @SuppressWarnings({\"unchecked\" })", "originalCommit": "9c74a610037b7bec59fc7fba4cd612a831c51293", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE0MjQyMQ==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391142421", "bodyText": "List<ImportCheckPointData> partitionSummaries = (List<ImportCheckPointData>)jobContext.getTransientUserData();", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:30:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE0NDg3MA==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391144870", "bodyText": "can we apply the annotation to this variable declaration instead of to the whole method then?", "author": "lmsurpre", "createdAt": "2020-03-11T17:34:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1MDMyMg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391150322", "bodyText": "yes, we can. :)", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:42:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1MTgwNg==", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391151806", "bodyText": "done", "author": "albertwang-ibm", "createdAt": "2020-03-11T17:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "05203d7b61438d06d745079822ea59b4d262d3ca", "chunk": "diff --git a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\nindex e58f9df0e4..d8759846c7 100644\n--- a/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\n+++ b/fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java\n\n@@ -27,13 +27,13 @@ public class ImportJobListener implements JobListener {\n     }\n \n \n-    @SuppressWarnings({\"unchecked\" })\n     @Override\n     public void afterJob() {\n         // jobExecution.getEndTime() for current execution always returns null, so we use system current time as the end time for current execution.\n         long currentExecutionEndTimeInMS = System.currentTimeMillis();;\n \n         // Used for generating response for all the import data resources.\n+        @SuppressWarnings(\"unchecked\")\n         List<ImportCheckPointData> partitionSummaries = (List<ImportCheckPointData>)jobContext.getTransientUserData();\n         // Used for generating performance measurement per each resource type.\n         HashMap<String, ImportCheckPointData> importedResourceTypeSummaries = new HashMap<>();\n"}}, {"oid": "39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "url": "https://github.com/IBM/FHIR/commit/39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "message": "Update fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\nCo-Authored-By: Lee Surprenant <lmsurpre@us.ibm.com>", "committedDate": "2020-03-11T17:26:38Z", "type": "commit"}, {"oid": "16f960cf476556ccb0998fcb24c7632616d28d9a", "url": "https://github.com/IBM/FHIR/commit/16f960cf476556ccb0998fcb24c7632616d28d9a", "message": "issue #765 update to logging per review comments\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T17:38:39Z", "type": "commit"}, {"oid": "05203d7b61438d06d745079822ea59b4d262d3ca", "url": "https://github.com/IBM/FHIR/commit/05203d7b61438d06d745079822ea59b4d262d3ca", "message": "issue #765 reduce supresswarning scope per review\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>", "committedDate": "2020-03-11T17:44:10Z", "type": "commit"}]}