{"pr_number": 1020, "pr_title": "HBASE-23653 Expose content of meta table in web ui", "pr_createdAt": "2020-01-10T22:50:09Z", "pr_url": "https://github.com/apache/hbase/pull/1020", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3OTY2Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365479666", "bodyText": "this should be HConstants.STATE_QUALIFIER", "author": "ndimiduk", "createdAt": "2020-01-11T00:24:16Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ2ODQ1MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365468451", "bodyText": "Any reason not to use Iterators.limit() from guava?", "author": "bharathv", "createdAt": "2020-01-10T23:21:39Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAxOTQwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366019408", "bodyText": "Please read the rest of the comment :)", "author": "ndimiduk", "createdAt": "2020-01-13T20:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ2ODQ1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java\ndeleted file mode 100644\nindex 4a0f8e2cb3..0000000000\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java\n+++ /dev/null\n\n@@ -1,65 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.master.webapp;\n-\n-import java.util.Iterator;\n-import java.util.NoSuchElementException;\n-import org.apache.yetus.audience.InterfaceAudience;\n-import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n-\n-/**\n- * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}\n- * entries.\n- * <p>Could just use {@link Iterators#limit(Iterator, int)} except that our consumer needs an API\n- * to check if the underlying iterator is not yet exhausted.\n- */\n-@InterfaceAudience.Private\n-public class LimitIterator<T> implements Iterator<T> {\n-\n-  private final Iterator<T> delegate;\n-  private final int limit;\n-  private int count;\n-\n-  LimitIterator(final Iterator<T> delegate, final int limit) {\n-    this.delegate = delegate;\n-    this.limit = limit;\n-    this.count = 0;\n-  }\n-\n-  /**\n-   * @return {@code true} when {@code delegate} has more entries, {@code false} otherwise.\n-   */\n-  public boolean delegateHasMore() {\n-    return delegate.hasNext();\n-  }\n-\n-  @Override\n-  public boolean hasNext() {\n-    if (count < limit) {\n-      return delegate.hasNext();\n-    }\n-    return false;\n-  }\n-\n-  @Override\n-  public T next() {\n-    if (!hasNext()) { throw new NoSuchElementException(); }\n-    count++;\n-    return delegate.next();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365475813", "bodyText": "Doesn't ResultScanner support an iterator()? Wrap it with a limit iterator instead of scan all? This way, I guess 10 such requests with max limit can be heavy for the master.", "author": "bharathv", "createdAt": "2020-01-11T00:00:30Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0MDAxMw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366040013", "bodyText": "So this gets a bit messy. A ResultScanner needs to be closed when the underlying iterator is exhausted. Right now there's no interface for closing that loop. I haven't found a great way to both (1) keep the interface in table.jsp limited to an Iterator<RegionReplicaInfo> and (2) ensure the scanner resources are cleaned up. The alternative I've thought of is to have MetaBrowser return a ResultScanner and leave table.jsp to do the work of transforming the results and limiting iteration. It seems like too much work left up to the caller.\nSuggestions?", "author": "ndimiduk", "createdAt": "2020-01-13T21:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5MzA5NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366093095", "bodyText": "Okay, I believe I have a solution to this. One sec.", "author": "ndimiduk", "createdAt": "2020-01-14T00:03:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NjE5OQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365476199", "bodyText": "Curious why it runs in its own thread. Add a clarifying comment?", "author": "bharathv", "createdAt": "2020-01-11T00:02:28Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAyMjUwOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366022509", "bodyText": "The docs say it's dangerous to not provide an executor service for callback handlers that do work. Without it, they'll execute on an RPC thread, which leads to starvation. On the other hand, it looks like AsyncTableImpl doesn't use provided ExecutorService when executing getScanner. Bleh, confusing. Will remove.", "author": "ndimiduk", "createdAt": "2020-01-13T20:54:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NjE5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzA1MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477051", "bodyText": "haha , functional style is not as readable :D (my personal opinion)", "author": "bharathv", "createdAt": "2020-01-11T00:07:10Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAyNzE1NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366027154", "bodyText": "Yeah, it's probably a bit overboard here. I started a back port to branch-1 and found there's only a couple places where Optional makes things easier.", "author": "ndimiduk", "createdAt": "2020-01-13T21:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzA1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzE3MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477170", "bodyText": "nit: I think this could use a javadoc.", "author": "bharathv", "createdAt": "2020-01-11T00:07:55Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)\n+      .map(MetaBrowser::buildScanRegionStateFilter)\n+      .ifPresent(filters::add);\n+\n+    if (filters.size() == 1) {\n+      return Optional.of(filters.get(0));\n+    }\n+\n+    return Optional.of(new FilterList(FilterList.Operator.MUST_PASS_ALL, filters));\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit(Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT) + 1);\n+    Optional.ofNullable(scanStart)\n+      .ifPresent(startRow -> metaScan.withStartRow(startRow, false));\n+    buildScanFilter().ifPresent(metaScan::setFilter);\n+    return metaScan;\n+  }\n+\n+  private <T> void maybeAddParam(final QueryStringEncoder encoder, final String paramName,", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3ODcwMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365478702", "bodyText": "nit: the accessors methods could work directly on regioninfo object ? (less code).", "author": "bharathv", "createdAt": "2020-01-11T00:17:59Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0NTc5NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366045795", "bodyText": "Pushes the null-checking out to access instead of at object creation. I generally prefer checking to happen at object creation, since the values don't change. You think the one approach will be easier to read/maintain than the other?", "author": "ndimiduk", "createdAt": "2020-01-13T21:46:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3ODcwMg=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\nindex b00820bd68..554d49bfc3 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n\n@@ -19,7 +19,6 @@ package org.apache.hadoop.hbase.master.webapp;\n \n import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n import java.util.stream.Collectors;\n import java.util.stream.StreamSupport;\n import org.apache.commons.lang3.builder.EqualsBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDMwNg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480306", "bodyText": "nice.. for some reason, I thought we already had this somewhere, looked around but didn't find one.", "author": "bharathv", "createdAt": "2020-01-11T00:28:23Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\nindex 26ba171c08..6400eb8553 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n\n@@ -25,13 +25,39 @@ import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import org.apache.hadoop.hbase.client.AsyncAdmin;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n+import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ * A {@link TestRule} that clears all user namespaces and tables\n+ * {@link ExternalResource#before() before} the test executes. Can be used in either the\n+ * {@link Rule} or {@link ClassRule} positions. Lazily realizes the provided\n+ * {@link AsyncConnection} so as to avoid initialization races with other {@link Rule Rules}.\n+ * <b>Does not</b> {@link AsyncConnection#close() close()} provided connection instance when\n+ * finished.\n+ * </p>\n+ * Use in combination with {@link MiniClusterRule} and {@link ConnectionRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     @ClassRule\n+ *     public static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *\n+ *     private final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *     private final ClearUserNamespacesAndTablesRule clearUserNamespacesAndTablesRule =\n+ *       new ClearUserNamespacesAndTablesRule(connectionRule::getConnection);\n+ *\n+ *     @Rule\n+ *     public TestRule rule = RuleChain\n+ *       .outerRule(connectionRule)\n+ *       .around(clearUserNamespacesAndTablesRule);\n+ *   }\n+ * }</pre>\n  */\n public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n   private static final Logger logger =\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDk1Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480952", "bodyText": "May be add a comment that this class doesn't close the connection by the end of it and needs to chained with TestConnectionRule which does it.", "author": "bharathv", "createdAt": "2020-01-11T00:31:57Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\nindex 26ba171c08..6400eb8553 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n\n@@ -25,13 +25,39 @@ import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import org.apache.hadoop.hbase.client.AsyncAdmin;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n+import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ * A {@link TestRule} that clears all user namespaces and tables\n+ * {@link ExternalResource#before() before} the test executes. Can be used in either the\n+ * {@link Rule} or {@link ClassRule} positions. Lazily realizes the provided\n+ * {@link AsyncConnection} so as to avoid initialization races with other {@link Rule Rules}.\n+ * <b>Does not</b> {@link AsyncConnection#close() close()} provided connection instance when\n+ * finished.\n+ * </p>\n+ * Use in combination with {@link MiniClusterRule} and {@link ConnectionRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     @ClassRule\n+ *     public static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *\n+ *     private final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *     private final ClearUserNamespacesAndTablesRule clearUserNamespacesAndTablesRule =\n+ *       new ClearUserNamespacesAndTablesRule(connectionRule::getConnection);\n+ *\n+ *     @Rule\n+ *     public TestRule rule = RuleChain\n+ *       .outerRule(connectionRule)\n+ *       .around(clearUserNamespacesAndTablesRule);\n+ *   }\n+ * }</pre>\n  */\n public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n   private static final Logger logger =\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MTA1OQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365481059", "bodyText": "nit: You'll probably run into checkstyle issues with inline if blocks.", "author": "bharathv", "createdAt": "2020-01-11T00:32:42Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of the {@link MiniHBaseCluster}.\n+ */\n+public class TestClusterRule extends ExternalResource {\n+  private final HBaseTestingUtility testingUtility;\n+  private final StartMiniClusterOption miniClusterOptions;\n+\n+  private MiniHBaseCluster miniCluster;\n+\n+  public TestClusterRule() {\n+    this(StartMiniClusterOption.builder().build());\n+  }\n+\n+  public TestClusterRule(final StartMiniClusterOption miniClusterOptions) {\n+    this.testingUtility = new HBaseTestingUtility();\n+    this.miniClusterOptions = miniClusterOptions;\n+  }\n+\n+  public CompletableFuture<AsyncConnection> createConnection() {\n+    if (miniCluster == null) { throw new IllegalStateException(\"test cluster not initialized\"); }", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0NzQ3MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366047470", "bodyText": "Yeah but I like them better... reminds me I want to see if check style supports this style and propose the change on dev@.", "author": "ndimiduk", "createdAt": "2020-01-13T21:50:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MTA1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniClusterRule.java\nsimilarity index 58%\nrename from hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java\nrename to hbase-server/src/test/java/org/apache/hadoop/hbase/MiniClusterRule.java\nindex 393c354a17..6ac4838275 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniClusterRule.java\n\n@@ -21,29 +21,58 @@ import java.io.IOException;\n import java.util.concurrent.CompletableFuture;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n+import org.junit.rules.TestRule;\n \n /**\n- * A {@link Rule} that manages an instance of the {@link MiniHBaseCluster}.\n+ * A {@link TestRule} that manages an instance of the {@link MiniHBaseCluster}. Can be used in\n+ * either the {@link Rule} or {@link ClassRule} positions. Built on top of an instance of\n+ * {@link HBaseTestingUtility}, so be weary of intermixing direct use of that class with this Rule.\n+ * </p>\n+ * Use in combination with {@link ConnectionRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     @ClassRule\n+ *     public static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *\n+ *     @Rule\n+ *     public final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *   }\n+ * }</pre>\n  */\n-public class TestClusterRule extends ExternalResource {\n+public class MiniClusterRule extends ExternalResource {\n   private final HBaseTestingUtility testingUtility;\n   private final StartMiniClusterOption miniClusterOptions;\n \n   private MiniHBaseCluster miniCluster;\n \n-  public TestClusterRule() {\n+  /**\n+   * Create an instance over the default options provided by {@link StartMiniClusterOption}.\n+   */\n+  public MiniClusterRule() {\n     this(StartMiniClusterOption.builder().build());\n   }\n \n-  public TestClusterRule(final StartMiniClusterOption miniClusterOptions) {\n+  /**\n+   * Create an instance using the provided {@link StartMiniClusterOption}.\n+   */\n+  public MiniClusterRule(final StartMiniClusterOption miniClusterOptions) {\n     this.testingUtility = new HBaseTestingUtility();\n     this.miniClusterOptions = miniClusterOptions;\n   }\n \n+  /**\n+   * Create a {@link AsyncConnection} to the managed {@link MiniHBaseCluster}. It's up to the caller\n+   * to {@link AsyncConnection#close() close()} the connection when finished.\n+   */\n   public CompletableFuture<AsyncConnection> createConnection() {\n-    if (miniCluster == null) { throw new IllegalStateException(\"test cluster not initialized\"); }\n+    if (miniCluster == null) {\n+      throw new IllegalStateException(\"test cluster not initialized\");\n+    }\n     return ConnectionFactory.createAsyncConnection(miniCluster.getConf());\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA2NDg2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366064862", "bodyText": "Oops. All this logging should be at TRACE level.", "author": "ndimiduk", "createdAt": "2020-01-13T22:33:17Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\nindex 26ba171c08..6400eb8553 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n\n@@ -25,13 +25,39 @@ import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import org.apache.hadoop.hbase.client.AsyncAdmin;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n+import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ * A {@link TestRule} that clears all user namespaces and tables\n+ * {@link ExternalResource#before() before} the test executes. Can be used in either the\n+ * {@link Rule} or {@link ClassRule} positions. Lazily realizes the provided\n+ * {@link AsyncConnection} so as to avoid initialization races with other {@link Rule Rules}.\n+ * <b>Does not</b> {@link AsyncConnection#close() close()} provided connection instance when\n+ * finished.\n+ * </p>\n+ * Use in combination with {@link MiniClusterRule} and {@link ConnectionRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     @ClassRule\n+ *     public static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *\n+ *     private final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *     private final ClearUserNamespacesAndTablesRule clearUserNamespacesAndTablesRule =\n+ *       new ClearUserNamespacesAndTablesRule(connectionRule::getConnection);\n+ *\n+ *     @Rule\n+ *     public TestRule rule = RuleChain\n+ *       .outerRule(connectionRule)\n+ *       .around(clearUserNamespacesAndTablesRule);\n+ *   }\n+ * }</pre>\n  */\n public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n   private static final Logger logger =\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDQ0Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074442", "bodyText": "Good", "author": "saintstack", "createdAt": "2020-01-13T22:59:42Z", "path": "hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java", "diffHunk": "@@ -31,7 +34,7 @@\n  * (assuming small number of locations)\n  */\n @InterfaceAudience.Private\n-public class RegionLocations {\n+public class RegionLocations implements Iterable<HRegionLocation> {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDk1MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074950", "bodyText": "hbase-common? Especially given this is generic.", "author": "saintstack", "createdAt": "2020-01-13T23:01:12Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}\n+ * entries.\n+ * <p>Could just use {@link Iterators#limit(Iterator, int)} except that our consumer needs an API\n+ * to check if the underlying iterator is not yet exhausted.\n+ */\n+@InterfaceAudience.Private\n+public class LimitIterator<T> implements Iterator<T> {\n+\n+  private final Iterator<T> delegate;\n+  private final int limit;\n+  private int count;\n+\n+  LimitIterator(final Iterator<T> delegate, final int limit) {\n+    this.delegate = delegate;\n+    this.limit = limit;\n+    this.count = 0;\n+  }\n+\n+  /**\n+   * @return {@code true} when {@code delegate} has more entries, {@code false} otherwise.\n+   */\n+  public boolean delegateHasMore() {\n+    return delegate.hasNext();\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    if (count < limit) {\n+      return delegate.hasNext();\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public T next() {\n+    if (!hasNext()) { throw new NoSuchElementException(); }\n+    count++;\n+    return delegate.next();\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5MTczNw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366091737", "bodyText": "Actually, now that I've refactored off the delegateHasMore method, I can go back to using the implementation provided by Guava.", "author": "ndimiduk", "createdAt": "2020-01-13T23:57:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDk1MA=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java\ndeleted file mode 100644\nindex 4a0f8e2cb3..0000000000\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java\n+++ /dev/null\n\n@@ -1,65 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.master.webapp;\n-\n-import java.util.Iterator;\n-import java.util.NoSuchElementException;\n-import org.apache.yetus.audience.InterfaceAudience;\n-import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n-\n-/**\n- * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}\n- * entries.\n- * <p>Could just use {@link Iterators#limit(Iterator, int)} except that our consumer needs an API\n- * to check if the underlying iterator is not yet exhausted.\n- */\n-@InterfaceAudience.Private\n-public class LimitIterator<T> implements Iterator<T> {\n-\n-  private final Iterator<T> delegate;\n-  private final int limit;\n-  private int count;\n-\n-  LimitIterator(final Iterator<T> delegate, final int limit) {\n-    this.delegate = delegate;\n-    this.limit = limit;\n-    this.count = 0;\n-  }\n-\n-  /**\n-   * @return {@code true} when {@code delegate} has more entries, {@code false} otherwise.\n-   */\n-  public boolean delegateHasMore() {\n-    return delegate.hasNext();\n-  }\n-\n-  @Override\n-  public boolean hasNext() {\n-    if (count < limit) {\n-      return delegate.hasNext();\n-    }\n-    return false;\n-  }\n-\n-  @Override\n-  public T next() {\n-    if (!hasNext()) { throw new NoSuchElementException(); }\n-    count++;\n-    return delegate.next();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080164", "bodyText": "Can I page through a 100k hbase:meta ?", "author": "saintstack", "createdAt": "2020-01-13T23:17:33Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Mjg1NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366092854", "bodyText": "Yes, but not in a single page. A user can view up to SCAN_LIMIT_MAX rows in a single request. If there are more results, the next page link is rendered. When clicked, the next page worth of results are rendered. This pagination continues until all results are exhausted.\nThe scanner is not held across button clicks, so the result set is not stable. If new entries are inserted before the \"lastRow\" pagination token, then the user will no see them. There's probably also a bug around region replicas, because I don't track how many replicas have been rendered by the page, only the last row rendered. Thus it's possible to have up to all non-primary replicas for one region slip through the cracks.", "author": "ndimiduk", "createdAt": "2020-01-14T00:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwNjQzMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366106432", "bodyText": "I think some form of this could be captured in the javadoc class comment. Otherwise, one needs to understand how this class paginates the result by setting the start row intelligently..", "author": "bharathv", "createdAt": "2020-01-14T00:56:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUxMzYzNg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366513636", "bodyText": "Big fat javadoc applied.", "author": "ndimiduk", "createdAt": "2020-01-14T18:54:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDM0NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080344", "bodyText": "Above all have to public?", "author": "saintstack", "createdAt": "2020-01-13T23:18:15Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Mjk4Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366092986", "bodyText": "To be accessed from table.jsp, yes, I believe so.", "author": "ndimiduk", "createdAt": "2020-01-14T00:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDM0NA=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080981", "bodyText": "Scan all each time we page?", "author": "saintstack", "createdAt": "2020-01-13T23:20:24Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MTgwNw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366081807", "bodyText": "MetaTable Visitor no good to you in here?", "author": "saintstack", "createdAt": "2020-01-13T23:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5NDAxOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366094019", "bodyText": "Scan all each time we page?\n\nScan all of meta? No. Scanning starts at the row represented by SCAN_START_PARAM and is limited by SCAN_LIMIT_PARAM and the other filter parameters. See the TestMetaBrowser#paginate* tests.\n\nMetaTable Visitor no good to you in here?\n\nIs that a class? I figured out how to do away with scanAll; moving to the old fashioned ResultScanner.", "author": "ndimiduk", "createdAt": "2020-01-14T00:06:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex 0b4a1d75aa..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -26,10 +26,7 @@ import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import javax.servlet.http.HttpServletRequest;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082204", "bodyText": "How's this differ from a RegionInfo?", "author": "saintstack", "createdAt": "2020-01-13T23:24:17Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjQxNA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082414", "bodyText": "It has state and serverName? A location has RI and SN? Should this subclass one of them then?", "author": "saintstack", "createdAt": "2020-01-13T23:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5NjcyMA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366096720", "bodyText": "How's this differ from a RegionInfo?\n\nI guess it's an HRegionLocation + meta table's rowkey + RegionState with best-effort accessors. I don't think subclassing buys us much.", "author": "ndimiduk", "createdAt": "2020-01-14T00:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\nindex b00820bd68..554d49bfc3 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n\n@@ -19,7 +19,6 @@ package org.apache.hadoop.hbase.master.webapp;\n \n import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n import java.util.stream.Collectors;\n import java.util.stream.StreamSupport;\n import org.apache.commons.lang3.builder.EqualsBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjY0NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082645", "bodyText": "These have to be public?", "author": "saintstack", "createdAt": "2020-01-13T23:25:50Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);\n+    this.regionInfo = maybeRegionInfo.orElse(null);\n+    this.regionName = maybeRegionInfo.map(RegionInfo::getRegionName).orElse(null);\n+    this.startKey = maybeRegionInfo.map(RegionInfo::getStartKey).orElse(null);\n+    this.endKey = maybeRegionInfo.map(RegionInfo::getEndKey).orElse(null);\n+    this.replicaId = maybeRegionInfo.map(RegionInfo::getReplicaId).orElse(null);\n+    this.regionState = result != null && maybeRegionInfo.isPresent()\n+      ? RegionStateStore.getRegionState(result, maybeRegionInfo.get())\n+      : null;\n+    this.serverName = maybeLocation.map(HRegionLocation::getServerName).orElse(null);\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {\n+    return regionName;\n+  }\n+\n+  public byte[] getStartKey() {\n+    return startKey;\n+  }\n+\n+  public byte[] getEndKey() {\n+    return endKey;\n+  }\n+\n+  public Integer getReplicaId() {\n+    return replicaId;\n+  }\n+\n+  public RegionState.State getRegionState() {\n+    return regionState;\n+  }\n+\n+  public ServerName getServerName() {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Njk2NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366096964", "bodyText": "For the same reason as the accessors in MetaBrowser: they need to be reachable by table.jsp.", "author": "ndimiduk", "createdAt": "2020-01-14T00:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjY0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\nindex b00820bd68..554d49bfc3 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java\n\n@@ -19,7 +19,6 @@ package org.apache.hadoop.hbase.master.webapp;\n \n import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n import java.util.stream.Collectors;\n import java.util.stream.StreamSupport;\n import org.apache.commons.lang3.builder.EqualsBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4Mzc1MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083750", "bodyText": "One thought, why cleanup after a test? What if it takes a bunch of work cleaning up, more than just start fresh?", "author": "saintstack", "createdAt": "2020-01-13T23:29:17Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))\n+      .thenCompose(_void -> admin.listTableNames(false))\n+      .thenApply(tableNames -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        tableNames.stream().map(TableName::getNameAsString).forEach(joiner::add);\n+        logger.info(\"found existing tables {}\", joiner.toString());\n+        return tableNames;\n+      });\n+  }\n+\n+  private CompletableFuture<Boolean> isTableEnabled(final TableName tableName) {\n+    return admin.isTableEnabled(tableName)\n+      .thenApply(isEnabled -> {\n+        logger.info(\"table {} is enabled.\", tableName);\n+        return isEnabled;\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableIfEnabled(final TableName tableName) {\n+    return isTableEnabled(tableName)\n+      .thenCompose(isEnabled -> {\n+        if (isEnabled) { return disableTable(tableName); }\n+        return CompletableFuture.completedFuture(null);\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"disabling enabled table {}\", tableName))\n+      .thenCompose(_void -> admin.disableTable(tableName));\n+  }\n+\n+  private CompletableFuture<Void> deleteTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting disabled table {}\", tableName))\n+      .thenCompose(_void -> admin.deleteTable(tableName));\n+  }\n+\n+  private CompletableFuture<List<String>> listUserNamespaces() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing namespaces\"))\n+      .thenCompose(_void -> admin.listNamespaceDescriptors())\n+      .thenApply(namespaceDescriptors -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        final List<String> names = namespaceDescriptors.stream()\n+          .map(NamespaceDescriptor::getName)\n+          .peek(joiner::add)\n+          .collect(Collectors.toList());\n+        logger.info(\"found existing namespaces {}\", joiner.toString());\n+        return names;\n+      })\n+      .thenApply(namespaces -> namespaces.stream()\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.SYSTEM_NAMESPACE.getName()))\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.DEFAULT_NAMESPACE.getName()))\n+        .collect(Collectors.toList()));\n+  }\n+\n+  private CompletableFuture<Void> deleteNamespace(final String namespace) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting namespace {}\", namespace))\n+      .thenCompose(_void -> admin.deleteNamespace(namespace));\n+  }\n+\n+  private CompletableFuture<Void> deleteUserNamespaces() {\n+    return listUserNamespaces()\n+      .thenCompose(namespaces -> CompletableFuture.allOf(\n+        namespaces.stream()\n+          .map(this::deleteNamespace)\n+          .toArray(CompletableFuture[]::new)));\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5ODYwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366098608", "bodyText": "I'm writing tests over the content of meta. So I can\n\nclean up after each test method; or\ntear down and spin up a new mini cluster for each test method; or\ncreate a fixed set of tables @BeforeClass and write all test methods around those tables.\n\nI chose the first option because (2) would make for a really slow test run and (3) is very inflexible in terms of how each test method is written, and renders all other test methods brittle to tweaks of any one of them.", "author": "ndimiduk", "createdAt": "2020-01-14T00:23:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4Mzc1MA=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\nindex 26ba171c08..6400eb8553 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java\n\n@@ -25,13 +25,39 @@ import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import org.apache.hadoop.hbase.client.AsyncAdmin;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n+import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ * A {@link TestRule} that clears all user namespaces and tables\n+ * {@link ExternalResource#before() before} the test executes. Can be used in either the\n+ * {@link Rule} or {@link ClassRule} positions. Lazily realizes the provided\n+ * {@link AsyncConnection} so as to avoid initialization races with other {@link Rule Rules}.\n+ * <b>Does not</b> {@link AsyncConnection#close() close()} provided connection instance when\n+ * finished.\n+ * </p>\n+ * Use in combination with {@link MiniClusterRule} and {@link ConnectionRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     @ClassRule\n+ *     public static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *\n+ *     private final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *     private final ClearUserNamespacesAndTablesRule clearUserNamespacesAndTablesRule =\n+ *       new ClearUserNamespacesAndTablesRule(connectionRule::getConnection);\n+ *\n+ *     @Rule\n+ *     public TestRule rule = RuleChain\n+ *       .outerRule(connectionRule)\n+ *       .around(clearUserNamespacesAndTablesRule);\n+ *   }\n+ * }</pre>\n  */\n public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n   private static final Logger logger =\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MzgwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083808", "bodyText": "Fun", "author": "saintstack", "createdAt": "2020-01-13T23:29:33Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of {@link AsyncConnection}.\n+ */\n+public class ConnectionRule extends ExternalResource {\n+\n+  private final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier;\n+  private AsyncConnection connection;\n+\n+  public ConnectionRule(final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  public AsyncConnection getConnection() {\n+    return connection;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    this.connection = connectionSupplier.get().join();\n+  }\n+\n+  @Override\n+  protected void after() {\n+    if (this.connection != null) {\n+      try {\n+        connection.close();\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java\nindex 29e8ca31a5..bf4c5aa020 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java\n\n@@ -21,11 +21,28 @@ import java.io.IOException;\n import java.util.concurrent.CompletableFuture;\n import java.util.function.Supplier;\n import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.ClassRule;\n import org.junit.Rule;\n import org.junit.rules.ExternalResource;\n \n /**\n- * A {@link Rule} that manages an instance of {@link AsyncConnection}.\n+ * A {@link Rule} that manages the lifecycle of an instance of {@link AsyncConnection}. Can be used\n+ * in either the {@link Rule} or {@link ClassRule} positions.\n+ * </p>\n+ * Use in combination with {@link MiniClusterRule}, for example:\n+ *\n+ * <pre>{@code\n+ *   public class TestMyClass {\n+ *     private static final MiniClusterRule miniClusterRule = new MiniClusterRule();\n+ *     private static final ConnectionRule connectionRule =\n+ *       new ConnectionRule(miniClusterRule::createConnection);\n+ *\n+ *     @ClassRule\n+ *     public static final TestRule rule = RuleChain\n+ *       .outerRule(connectionRule)\n+ *       .around(connectionRule);\n+ *   }\n+ * }</pre>\n  */\n public class ConnectionRule extends ExternalResource {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366084083", "bodyText": "This only reason for import?", "author": "saintstack", "createdAt": "2020-01-13T23:30:28Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.client.hamcrest;\n+\n+import static org.hamcrest.core.Is.is;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5OTIwOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366099209", "bodyText": "I don't follow the question. This class comes out of hamcrest-core.", "author": "ndimiduk", "createdAt": "2020-01-14T00:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE1NzQzMQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366157431", "bodyText": "Is this the only class we use from hamcrest? Are we adding a new dependency to modules just for one class? Its a good one I presume.", "author": "saintstack", "createdAt": "2020-01-14T05:25:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2MzY3Nw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366463677", "bodyText": "is is provided by the existing dependency, hamcrest-core. I've added the dependency on hamcrest-library for the the collection- and bean-related matchers that it provides. This patch makes use of contains and hasProperty, but I look forward to writing more tests that make use of others. It's quite unusual to depend only on hamcrest-core; usually -core and -library come as a pair in user applications. I believe -core is stand-alone so that 3rd party extension libraries (such as Spotify's java-hamcrest can pull in the minimal set if they so choose.", "author": "ndimiduk", "createdAt": "2020-01-14T17:09:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIyMzg2NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r367223865", "bodyText": "ok", "author": "saintstack", "createdAt": "2020-01-16T04:17:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java\nindex 092ec77b30..581ac9fa39 100644\n--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java\n+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java\n\n@@ -38,7 +38,9 @@ public final class BytesMatchers {\n     return new TypeSafeDiagnosingMatcher<byte[]>() {\n       @Override protected boolean matchesSafely(byte[] item, Description mismatchDescription) {\n         final String binary = Bytes.toStringBinary(item);\n-        if (matcher.matches(binary)) { return true; }\n+        if (matcher.matches(binary)) {\n+          return true;\n+        }\n         mismatchDescription.appendText(\"was a byte[] with a Bytes.toStringBinary value \");\n         matcher.describeMismatch(binary, mismatchDescription);\n         return false;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwODE2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366108162", "bodyText": "nit: If toString() is the only needed call, no need to use generics / templatize it? Just pass an object?", "author": "bharathv", "createdAt": "2020-01-14T01:03:05Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2ODU0OA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366468548", "bodyText": "Meh, I suppose.", "author": "ndimiduk", "createdAt": "2020-01-14T17:19:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwODE2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex e9aa0edb5a..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -50,8 +50,53 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n \n /**\n- * A support class for the \"Meta Entries\" section in\n- * {@code resources/hbase-webapps/master/table.jsp}.\n+ * <p>\n+ * Support class for the \"Meta Entries\" section in {@code resources/hbase-webapps/master/table.jsp}.\n+ * </p>\n+ * <p>\n+ * <b>Interface</b>. This class's intended consumer is {@code table.jsp}. As such, it's primary\n+ * interface is the active {@link HttpServletRequest}, from which it uses the {@code scan_*}\n+ * request parameters. This class supports paging through an optionally filtered view of the\n+ * contents of {@code hbase:meta}. Those filters and the pagination offset are specified via these\n+ * request parameters. It provides helper methods for constructing pagination links.\n+ * <ul>\n+ *   <li>{@value #NAME_PARAM} - the name of the table requested. The only table of our concern here\n+ *   is {@code hbase:meta}; any other value is effectively ignored by the giant conditional in the\n+ *   jsp.</li>\n+ *   <li>{@value #SCAN_LIMIT_PARAM} - specifies a limit on the number of region (replicas) rendered\n+ *   on the by the table in a single request -- a limit on page size. This corresponds to the\n+ *   number of {@link RegionReplicaInfo} objects produced by {@link Results#iterator()}. When a\n+ *   value for {@code scan_limit} is invalid or not specified, the default value of\n+ *   {@value #SCAN_LIMIT_DEFAULT} is used. In order to avoid excessive resource consumption, a\n+ *   maximum value of {@value #SCAN_LIMIT_MAX} is enforced.</li>\n+ *   <li>{@value #SCAN_REGION_STATE_PARAM} - an optional filter on {@link RegionState.State}.</li>\n+ *   <li>{@value #SCAN_START_PARAM} - specifies the rowkey at which a scan should start. For usage\n+ *   details, see the below section on <b>Pagination</b>.</li>\n+ *   <li>{@value #SCAN_TABLE_PARAM} - specifies a filter on the values returned, limiting them to\n+ *   regions from a specified table. This parameter is implemented as a prefix filter on the\n+ *   {@link Scan}, so in effect it can be used for simple namespace and multi-table matches.</li>\n+ * </ul>\n+ * </p>\n+ * <p>\n+ * <b>Pagination</b>. A single page of results are made available via {@link #getResults()} / an\n+ * instance of {@link Results}. Callers use its {@link Iterator} consume the page of\n+ * {@link RegionReplicaInfo} instances, each of which represents a region or region replica. Helper\n+ * methods are provided for building page navigation controls preserving the user's selected filter\n+ * set: {@link #buildFirstPageUrl()}, {@link #buildNextPageUrl(byte[])}. Pagination is implemented\n+ * using a simple offset + limit system. Offset is provided by the {@value #SCAN_START_PARAM},\n+ * limit via {@value #SCAN_LIMIT_PARAM}. Under the hood, the {@link Scan} is constructed with\n+ * {@link Scan#setMaxResultSize(long)} set to ({@value SCAN_LIMIT_PARAM} +1), while the\n+ * {@link Results} {@link Iterator} honors {@value #SCAN_LIMIT_PARAM}. The +1 allows the caller to\n+ * know if a \"next page\" is available via {@link Results#hasMoreResults()}. Note that this\n+ * pagination strategy is incomplete when it comes to region replicas and can potentially omit\n+ * rendering replicas that fall between the last rowkey offset and {@code replicaCount % page size}.\n+ * </p>\n+ * <p>\n+ * <b>Error Messages</b>. Any time there's an error parsing user input, a message will be populated\n+ * in {@link #getErrorMessages()}. Any fields which produce an error will have their filter values\n+ * set to the default, except for a value of {@value  #SCAN_LIMIT_PARAM} that exceeds\n+ * {@value #SCAN_LIMIT_MAX}, in which case {@value #SCAN_LIMIT_MAX} is used.\n+ * </p>\n  */\n @InterfaceAudience.Private\n public class MetaBrowser {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366109181", "bodyText": "nit: return SCAN_LIMIT_DEFAULT here and simplify callers?", "author": "bharathv", "createdAt": "2020-01-14T01:07:07Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2ODM5Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366468396", "bodyText": "I think you're right. Now that I've implemented a protocol for error handling, I should be able to clean up this case. Let me review all the accessors under the same lens.", "author": "ndimiduk", "createdAt": "2020-01-14T17:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUwNTMxMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366505312", "bodyText": "Oh, no we cannot. We need to differentiate between a value specified or not.", "author": "ndimiduk", "createdAt": "2020-01-14T18:37:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex e9aa0edb5a..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -50,8 +50,53 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n \n /**\n- * A support class for the \"Meta Entries\" section in\n- * {@code resources/hbase-webapps/master/table.jsp}.\n+ * <p>\n+ * Support class for the \"Meta Entries\" section in {@code resources/hbase-webapps/master/table.jsp}.\n+ * </p>\n+ * <p>\n+ * <b>Interface</b>. This class's intended consumer is {@code table.jsp}. As such, it's primary\n+ * interface is the active {@link HttpServletRequest}, from which it uses the {@code scan_*}\n+ * request parameters. This class supports paging through an optionally filtered view of the\n+ * contents of {@code hbase:meta}. Those filters and the pagination offset are specified via these\n+ * request parameters. It provides helper methods for constructing pagination links.\n+ * <ul>\n+ *   <li>{@value #NAME_PARAM} - the name of the table requested. The only table of our concern here\n+ *   is {@code hbase:meta}; any other value is effectively ignored by the giant conditional in the\n+ *   jsp.</li>\n+ *   <li>{@value #SCAN_LIMIT_PARAM} - specifies a limit on the number of region (replicas) rendered\n+ *   on the by the table in a single request -- a limit on page size. This corresponds to the\n+ *   number of {@link RegionReplicaInfo} objects produced by {@link Results#iterator()}. When a\n+ *   value for {@code scan_limit} is invalid or not specified, the default value of\n+ *   {@value #SCAN_LIMIT_DEFAULT} is used. In order to avoid excessive resource consumption, a\n+ *   maximum value of {@value #SCAN_LIMIT_MAX} is enforced.</li>\n+ *   <li>{@value #SCAN_REGION_STATE_PARAM} - an optional filter on {@link RegionState.State}.</li>\n+ *   <li>{@value #SCAN_START_PARAM} - specifies the rowkey at which a scan should start. For usage\n+ *   details, see the below section on <b>Pagination</b>.</li>\n+ *   <li>{@value #SCAN_TABLE_PARAM} - specifies a filter on the values returned, limiting them to\n+ *   regions from a specified table. This parameter is implemented as a prefix filter on the\n+ *   {@link Scan}, so in effect it can be used for simple namespace and multi-table matches.</li>\n+ * </ul>\n+ * </p>\n+ * <p>\n+ * <b>Pagination</b>. A single page of results are made available via {@link #getResults()} / an\n+ * instance of {@link Results}. Callers use its {@link Iterator} consume the page of\n+ * {@link RegionReplicaInfo} instances, each of which represents a region or region replica. Helper\n+ * methods are provided for building page navigation controls preserving the user's selected filter\n+ * set: {@link #buildFirstPageUrl()}, {@link #buildNextPageUrl(byte[])}. Pagination is implemented\n+ * using a simple offset + limit system. Offset is provided by the {@value #SCAN_START_PARAM},\n+ * limit via {@value #SCAN_LIMIT_PARAM}. Under the hood, the {@link Scan} is constructed with\n+ * {@link Scan#setMaxResultSize(long)} set to ({@value SCAN_LIMIT_PARAM} +1), while the\n+ * {@link Results} {@link Iterator} honors {@value #SCAN_LIMIT_PARAM}. The +1 allows the caller to\n+ * know if a \"next page\" is available via {@link Results#hasMoreResults()}. Note that this\n+ * pagination strategy is incomplete when it comes to region replicas and can potentially omit\n+ * rendering replicas that fall between the last rowkey offset and {@code replicaCount % page size}.\n+ * </p>\n+ * <p>\n+ * <b>Error Messages</b>. Any time there's an error parsing user input, a message will be populated\n+ * in {@link #getErrorMessages()}. Any fields which produce an error will have their filter values\n+ * set to the default, except for a value of {@value  #SCAN_LIMIT_PARAM} that exceeds\n+ * {@value #SCAN_LIMIT_MAX}, in which case {@value #SCAN_LIMIT_MAX} is used.\n+ * </p>\n  */\n @InterfaceAudience.Private\n public class MetaBrowser {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMDY2MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366110661", "bodyText": "I was about to suggest something like this.. wrap the output in some auto-closeable implementation that can be used in callers.. neat...", "author": "bharathv", "createdAt": "2020-01-14T01:13:53Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {\n+    if (lastRow == null) {\n+      return null;\n+    }\n+    return urlEncode(Bytes.toStringBinary(lastRow));\n+  }\n+\n+  public String buildNextPageUrl(final byte[] lastRow) {\n+    final QueryStringEncoder encoder = buildFirstPageEncoder();\n+    final String startRow = buildStartParamFrom(lastRow);\n+    addParam(encoder, SCAN_START_PARAM, startRow);\n+    return encoder.toString();\n+  }\n+\n+  private static String urlEncode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLEncoder.encode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String urlDecode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLDecoder.decode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static Integer tryParseInt(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return Integer.parseInt(val);\n+    } catch (NumberFormatException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,\n+    final String value) {\n+    if (clazz == null || value == null) {\n+      return null;\n+    }\n+    try {\n+      return T.valueOf(clazz, value);\n+    } catch (IllegalArgumentException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String buildScanLimitExceededErrorMessage(final int requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value %d exceeds maximum value %d.\", requestValue, SCAN_LIMIT_MAX);\n+  }\n+\n+  private static String buildScanLimitMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value '%s' cannot be parsed as an integer.\", requestValue);\n+  }\n+\n+  private static String buildScanLimitLTEZero(final int requestValue) {\n+    return String.format(\"Requested SCAN_LIMIT value %d is <= 0.\", requestValue);\n+  }\n+\n+  private static String buildScanRegionStateMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_REGION_STATE value '%s' cannot be parsed as a RegionState.\", requestValue);\n+  }\n+\n+  /**\n+   * Encapsulates the results produced by this {@link MetaBrowser} instance.\n+   */\n+  public final class Results implements AutoCloseable, Iterable<RegionReplicaInfo> {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex e9aa0edb5a..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -50,8 +50,53 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n \n /**\n- * A support class for the \"Meta Entries\" section in\n- * {@code resources/hbase-webapps/master/table.jsp}.\n+ * <p>\n+ * Support class for the \"Meta Entries\" section in {@code resources/hbase-webapps/master/table.jsp}.\n+ * </p>\n+ * <p>\n+ * <b>Interface</b>. This class's intended consumer is {@code table.jsp}. As such, it's primary\n+ * interface is the active {@link HttpServletRequest}, from which it uses the {@code scan_*}\n+ * request parameters. This class supports paging through an optionally filtered view of the\n+ * contents of {@code hbase:meta}. Those filters and the pagination offset are specified via these\n+ * request parameters. It provides helper methods for constructing pagination links.\n+ * <ul>\n+ *   <li>{@value #NAME_PARAM} - the name of the table requested. The only table of our concern here\n+ *   is {@code hbase:meta}; any other value is effectively ignored by the giant conditional in the\n+ *   jsp.</li>\n+ *   <li>{@value #SCAN_LIMIT_PARAM} - specifies a limit on the number of region (replicas) rendered\n+ *   on the by the table in a single request -- a limit on page size. This corresponds to the\n+ *   number of {@link RegionReplicaInfo} objects produced by {@link Results#iterator()}. When a\n+ *   value for {@code scan_limit} is invalid or not specified, the default value of\n+ *   {@value #SCAN_LIMIT_DEFAULT} is used. In order to avoid excessive resource consumption, a\n+ *   maximum value of {@value #SCAN_LIMIT_MAX} is enforced.</li>\n+ *   <li>{@value #SCAN_REGION_STATE_PARAM} - an optional filter on {@link RegionState.State}.</li>\n+ *   <li>{@value #SCAN_START_PARAM} - specifies the rowkey at which a scan should start. For usage\n+ *   details, see the below section on <b>Pagination</b>.</li>\n+ *   <li>{@value #SCAN_TABLE_PARAM} - specifies a filter on the values returned, limiting them to\n+ *   regions from a specified table. This parameter is implemented as a prefix filter on the\n+ *   {@link Scan}, so in effect it can be used for simple namespace and multi-table matches.</li>\n+ * </ul>\n+ * </p>\n+ * <p>\n+ * <b>Pagination</b>. A single page of results are made available via {@link #getResults()} / an\n+ * instance of {@link Results}. Callers use its {@link Iterator} consume the page of\n+ * {@link RegionReplicaInfo} instances, each of which represents a region or region replica. Helper\n+ * methods are provided for building page navigation controls preserving the user's selected filter\n+ * set: {@link #buildFirstPageUrl()}, {@link #buildNextPageUrl(byte[])}. Pagination is implemented\n+ * using a simple offset + limit system. Offset is provided by the {@value #SCAN_START_PARAM},\n+ * limit via {@value #SCAN_LIMIT_PARAM}. Under the hood, the {@link Scan} is constructed with\n+ * {@link Scan#setMaxResultSize(long)} set to ({@value SCAN_LIMIT_PARAM} +1), while the\n+ * {@link Results} {@link Iterator} honors {@value #SCAN_LIMIT_PARAM}. The +1 allows the caller to\n+ * know if a \"next page\" is available via {@link Results#hasMoreResults()}. Note that this\n+ * pagination strategy is incomplete when it comes to region replicas and can potentially omit\n+ * rendering replicas that fall between the last rowkey offset and {@code replicaCount % page size}.\n+ * </p>\n+ * <p>\n+ * <b>Error Messages</b>. Any time there's an error parsing user input, a message will be populated\n+ * in {@link #getErrorMessages()}. Any fields which produce an error will have their filter values\n+ * set to the default, except for a value of {@value  #SCAN_LIMIT_PARAM} that exceeds\n+ * {@value #SCAN_LIMIT_MAX}, in which case {@value #SCAN_LIMIT_MAX} is used.\n+ * </p>\n  */\n @InterfaceAudience.Private\n public class MetaBrowser {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMTA0MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366111041", "bodyText": "Thanks for the clean up.. this is what I had in mind.", "author": "bharathv", "createdAt": "2020-01-14T01:15:46Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    this.row = result != null ? result.getRow() : null;\n+    this.regionInfo = location != null ? location.getRegion() : null;\n+    this.regionState = (result != null && regionInfo != null)\n+      ? RegionStateStore.getRegionState(result, regionInfo)\n+      : null;\n+    this.serverName = location != null ? location.getServerName() : null;\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjU2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366112562", "bodyText": "I think, the +1 part is pretty subtle.. add a comment that it is needed for pagination hasNext() support?", "author": "bharathv", "createdAt": "2020-01-14T01:22:28Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2Nzk4OA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366467988", "bodyText": "Rgr.", "author": "ndimiduk", "createdAt": "2020-01-14T17:18:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjU2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex e9aa0edb5a..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -50,8 +50,53 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n \n /**\n- * A support class for the \"Meta Entries\" section in\n- * {@code resources/hbase-webapps/master/table.jsp}.\n+ * <p>\n+ * Support class for the \"Meta Entries\" section in {@code resources/hbase-webapps/master/table.jsp}.\n+ * </p>\n+ * <p>\n+ * <b>Interface</b>. This class's intended consumer is {@code table.jsp}. As such, it's primary\n+ * interface is the active {@link HttpServletRequest}, from which it uses the {@code scan_*}\n+ * request parameters. This class supports paging through an optionally filtered view of the\n+ * contents of {@code hbase:meta}. Those filters and the pagination offset are specified via these\n+ * request parameters. It provides helper methods for constructing pagination links.\n+ * <ul>\n+ *   <li>{@value #NAME_PARAM} - the name of the table requested. The only table of our concern here\n+ *   is {@code hbase:meta}; any other value is effectively ignored by the giant conditional in the\n+ *   jsp.</li>\n+ *   <li>{@value #SCAN_LIMIT_PARAM} - specifies a limit on the number of region (replicas) rendered\n+ *   on the by the table in a single request -- a limit on page size. This corresponds to the\n+ *   number of {@link RegionReplicaInfo} objects produced by {@link Results#iterator()}. When a\n+ *   value for {@code scan_limit} is invalid or not specified, the default value of\n+ *   {@value #SCAN_LIMIT_DEFAULT} is used. In order to avoid excessive resource consumption, a\n+ *   maximum value of {@value #SCAN_LIMIT_MAX} is enforced.</li>\n+ *   <li>{@value #SCAN_REGION_STATE_PARAM} - an optional filter on {@link RegionState.State}.</li>\n+ *   <li>{@value #SCAN_START_PARAM} - specifies the rowkey at which a scan should start. For usage\n+ *   details, see the below section on <b>Pagination</b>.</li>\n+ *   <li>{@value #SCAN_TABLE_PARAM} - specifies a filter on the values returned, limiting them to\n+ *   regions from a specified table. This parameter is implemented as a prefix filter on the\n+ *   {@link Scan}, so in effect it can be used for simple namespace and multi-table matches.</li>\n+ * </ul>\n+ * </p>\n+ * <p>\n+ * <b>Pagination</b>. A single page of results are made available via {@link #getResults()} / an\n+ * instance of {@link Results}. Callers use its {@link Iterator} consume the page of\n+ * {@link RegionReplicaInfo} instances, each of which represents a region or region replica. Helper\n+ * methods are provided for building page navigation controls preserving the user's selected filter\n+ * set: {@link #buildFirstPageUrl()}, {@link #buildNextPageUrl(byte[])}. Pagination is implemented\n+ * using a simple offset + limit system. Offset is provided by the {@value #SCAN_START_PARAM},\n+ * limit via {@value #SCAN_LIMIT_PARAM}. Under the hood, the {@link Scan} is constructed with\n+ * {@link Scan#setMaxResultSize(long)} set to ({@value SCAN_LIMIT_PARAM} +1), while the\n+ * {@link Results} {@link Iterator} honors {@value #SCAN_LIMIT_PARAM}. The +1 allows the caller to\n+ * know if a \"next page\" is available via {@link Results#hasMoreResults()}. Note that this\n+ * pagination strategy is incomplete when it comes to region replicas and can potentially omit\n+ * rendering replicas that fall between the last rowkey offset and {@code replicaCount % page size}.\n+ * </p>\n+ * <p>\n+ * <b>Error Messages</b>. Any time there's an error parsing user input, a message will be populated\n+ * in {@link #getErrorMessages()}. Any fields which produce an error will have their filter values\n+ * set to the default, except for a value of {@value  #SCAN_LIMIT_PARAM} that exceeds\n+ * {@value #SCAN_LIMIT_MAX}, in which case {@value #SCAN_LIMIT_MAX} is used.\n+ * </p>\n  */\n @InterfaceAudience.Private\n public class MetaBrowser {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ4MzgzMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366483832", "bodyText": "this can be package-protected as it's only used internally and within the unit test.", "author": "ndimiduk", "createdAt": "2020-01-14T17:51:08Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\nindex e9aa0edb5a..b1cacc5c58 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java\n\n@@ -50,8 +50,53 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n \n /**\n- * A support class for the \"Meta Entries\" section in\n- * {@code resources/hbase-webapps/master/table.jsp}.\n+ * <p>\n+ * Support class for the \"Meta Entries\" section in {@code resources/hbase-webapps/master/table.jsp}.\n+ * </p>\n+ * <p>\n+ * <b>Interface</b>. This class's intended consumer is {@code table.jsp}. As such, it's primary\n+ * interface is the active {@link HttpServletRequest}, from which it uses the {@code scan_*}\n+ * request parameters. This class supports paging through an optionally filtered view of the\n+ * contents of {@code hbase:meta}. Those filters and the pagination offset are specified via these\n+ * request parameters. It provides helper methods for constructing pagination links.\n+ * <ul>\n+ *   <li>{@value #NAME_PARAM} - the name of the table requested. The only table of our concern here\n+ *   is {@code hbase:meta}; any other value is effectively ignored by the giant conditional in the\n+ *   jsp.</li>\n+ *   <li>{@value #SCAN_LIMIT_PARAM} - specifies a limit on the number of region (replicas) rendered\n+ *   on the by the table in a single request -- a limit on page size. This corresponds to the\n+ *   number of {@link RegionReplicaInfo} objects produced by {@link Results#iterator()}. When a\n+ *   value for {@code scan_limit} is invalid or not specified, the default value of\n+ *   {@value #SCAN_LIMIT_DEFAULT} is used. In order to avoid excessive resource consumption, a\n+ *   maximum value of {@value #SCAN_LIMIT_MAX} is enforced.</li>\n+ *   <li>{@value #SCAN_REGION_STATE_PARAM} - an optional filter on {@link RegionState.State}.</li>\n+ *   <li>{@value #SCAN_START_PARAM} - specifies the rowkey at which a scan should start. For usage\n+ *   details, see the below section on <b>Pagination</b>.</li>\n+ *   <li>{@value #SCAN_TABLE_PARAM} - specifies a filter on the values returned, limiting them to\n+ *   regions from a specified table. This parameter is implemented as a prefix filter on the\n+ *   {@link Scan}, so in effect it can be used for simple namespace and multi-table matches.</li>\n+ * </ul>\n+ * </p>\n+ * <p>\n+ * <b>Pagination</b>. A single page of results are made available via {@link #getResults()} / an\n+ * instance of {@link Results}. Callers use its {@link Iterator} consume the page of\n+ * {@link RegionReplicaInfo} instances, each of which represents a region or region replica. Helper\n+ * methods are provided for building page navigation controls preserving the user's selected filter\n+ * set: {@link #buildFirstPageUrl()}, {@link #buildNextPageUrl(byte[])}. Pagination is implemented\n+ * using a simple offset + limit system. Offset is provided by the {@value #SCAN_START_PARAM},\n+ * limit via {@value #SCAN_LIMIT_PARAM}. Under the hood, the {@link Scan} is constructed with\n+ * {@link Scan#setMaxResultSize(long)} set to ({@value SCAN_LIMIT_PARAM} +1), while the\n+ * {@link Results} {@link Iterator} honors {@value #SCAN_LIMIT_PARAM}. The +1 allows the caller to\n+ * know if a \"next page\" is available via {@link Results#hasMoreResults()}. Note that this\n+ * pagination strategy is incomplete when it comes to region replicas and can potentially omit\n+ * rendering replicas that fall between the last rowkey offset and {@code replicaCount % page size}.\n+ * </p>\n+ * <p>\n+ * <b>Error Messages</b>. Any time there's an error parsing user input, a message will be populated\n+ * in {@link #getErrorMessages()}. Any fields which produce an error will have their filter values\n+ * set to the default, except for a value of {@value  #SCAN_LIMIT_PARAM} that exceeds\n+ * {@value #SCAN_LIMIT_MAX}, in which case {@value #SCAN_LIMIT_MAX} is used.\n+ * </p>\n  */\n @InterfaceAudience.Private\n public class MetaBrowser {\n"}}, {"oid": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "url": "https://github.com/apache/hbase/commit/18d86a4d85b8a70a36131dd02f9e06753b05cad7", "message": "PR feedback", "committedDate": "2020-01-14T18:53:18Z", "type": "forcePushed"}, {"oid": "f2d794831fa53849c437c27143844a20e338ab92", "url": "https://github.com/apache/hbase/commit/f2d794831fa53849c437c27143844a20e338ab92", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:37:11Z", "type": "forcePushed"}, {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:47:49Z", "type": "commit"}, {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:47:49Z", "type": "forcePushed"}]}