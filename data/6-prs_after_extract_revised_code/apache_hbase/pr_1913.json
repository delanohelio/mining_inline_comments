{"pr_number": 1913, "pr_title": "HBASE-24577 Doc WALSplitter classes", "pr_createdAt": "2020-06-16T23:12:03Z", "pr_url": "https://github.com/apache/hbase/pull/1913", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwODUwMw==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441208503", "bodyText": "Log => WAL?", "author": "infraio", "createdAt": "2020-06-17T00:08:20Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -233,7 +247,7 @@ public static boolean splitLogFile(Path walDir, FileStatus logfile, FileSystem w\n   }\n \n   /**\n-   * log splitting implementation, splits one log file.\n+   * Log splitting implementation, splits one log file.", "originalCommit": "9b22ca97fcc775401b0a330b7a014df26107c13b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTE4OA==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441721188", "bodyText": "Let me change.", "author": "saintstack", "createdAt": "2020-06-17T17:47:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwODUwMw=="}], "type": "inlineReview", "revised_code": {"commit": "dc3f2c477d0573abeebe3fc0fe6c7f8207625121", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\nindex 79351b7502..fdcaf2d821 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\n\n@@ -247,7 +247,7 @@ public class WALSplitter {\n   }\n \n   /**\n-   * Log splitting implementation, splits one log file.\n+   * WAL splitting implementation, splits one log file.\n    * @param logfile should be an actual log file.\n    */\n   @VisibleForTesting\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441311696", "bodyText": "Per one WAL file split, every region will have one recovered edits file. If another WAL file also get split, we will have one more right?", "author": "anoopsjohn", "createdAt": "2020-06-17T06:35:19Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -26,20 +26,20 @@\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n-\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.MultipleIOException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n import org.apache.hbase.thirdparty.com.google.common.collect.Lists;\n \n /**\n  * Class that manages the output streams from the log splitting process.\n- * Every region only has one recovered edits.\n+ * Every region only has one recovered edits file.", "originalCommit": "9b22ca97fcc775401b0a330b7a014df26107c13b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxNDQ3NA==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441314474", "bodyText": "Yes. Right.", "author": "infraio", "createdAt": "2020-06-17T06:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTEyNw==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441721127", "bodyText": "Let me add qualification here.", "author": "saintstack", "createdAt": "2020-06-17T17:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMTY5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "dc3f2c477d0573abeebe3fc0fe6c7f8207625121", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java\nindex 950e9ed98f..798c716cfe 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java\n\n@@ -37,7 +37,9 @@ import org.apache.hbase.thirdparty.com.google.common.collect.Lists;\n \n /**\n  * Class that manages the output streams from the log splitting process.\n- * Every region only has one recovered edits file.\n+ * Every region only has one recovered edits file PER split WAL (if we split\n+ * multiple WALs during a log-splitting session, on open, a Region may\n+ * have multiple recovered.edits files to replay -- one per split WAL).\n  * @see BoundedRecoveredEditsOutputSink which is like this class but imposes upper bound on\n  *   the number of writers active at one time (makes for better throughput).\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441312577", "bodyText": "This defaults to false right?", "author": "anoopsjohn", "createdAt": "2020-06-17T06:37:31Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java", "diffHunk": "@@ -115,7 +119,18 @@\n \n   private final String tmpDirName;\n \n+  /**\n+   * Split WAL directly to hfiles instead of into intermediary 'recovered.edits' files.\n+   */\n+  public static final String WAL_SPLIT_TO_HFILE = \"hbase.wal.split.to.hfile\";\n+  public static final boolean DEFAULT_WAL_SPLIT_TO_HFILE = false;\n+\n+  /**\n+   * True if we are to run with bounded amount of writers rather than let the count blossom.\n+   * Bounded writing tends to have higher throughput.\n+   */\n   public final static String SPLIT_WRITER_CREATION_BOUNDED = \"hbase.split.writer.creation.bounded\";", "originalCommit": "9b22ca97fcc775401b0a330b7a014df26107c13b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMDk1Mg==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441720952", "bodyText": "Let me add this.", "author": "saintstack", "createdAt": "2020-06-17T17:47:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyNTM2OQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441725369", "bodyText": "I note that it defaults to 'false' but man, this should be 'true'.  Let me test.", "author": "saintstack", "createdAt": "2020-06-17T17:54:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMxMjU3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "dc3f2c477d0573abeebe3fc0fe6c7f8207625121", "chunk": "diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\nindex 79351b7502..fdcaf2d821 100644\n--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\n+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java\n\n@@ -127,7 +127,7 @@ public class WALSplitter {\n \n   /**\n    * True if we are to run with bounded amount of writers rather than let the count blossom.\n-   * Bounded writing tends to have higher throughput.\n+   * Default is 'false'. Bounded writing tends to have higher throughput.\n    */\n   public final static String SPLIT_WRITER_CREATION_BOUNDED = \"hbase.split.writer.creation.bounded\";\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441447205", "bodyText": "Good to guard with LOG.isTraceEnabled()?", "author": "virajjasani", "createdAt": "2020-06-17T10:30:57Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -99,12 +102,13 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n       }\n     }\n \n-    // The key point is create a new writer for each column family, write edits then close writer.\n+    // Create a new hfile writer for each column family, write edits then close writer.\n     String regionName = Bytes.toString(buffer.encodedRegionName);\n     for (Map.Entry<String, CellSet> cellsEntry : familyCells.entrySet()) {\n       String familyName = cellsEntry.getKey();\n       StoreFileWriter writer = createRecoveredHFileWriter(buffer.tableName, regionName,\n         familySeqIds.get(familyName), familyName, isMetaTable);\n+      LOG.trace(\"Created {}\", writer.getPath());", "originalCommit": "9b22ca97fcc775401b0a330b7a014df26107c13b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMDY1NQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441720655", "bodyText": "Why you say? The slf4j doesn't do any work creating log string if we are not at log TRACE level.", "author": "saintstack", "createdAt": "2020-06-17T17:46:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0NTg1NQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441745855", "bodyText": "This is to avoid the cost of parameter construction: http://logging.apache.org/log4j/1.2/manual.html#performance", "author": "virajjasani", "createdAt": "2020-06-17T18:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc0Njg3Ng==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441746876", "bodyText": "My bad, this is old doc for apache log4j, not slf4j", "author": "virajjasani", "createdAt": "2020-06-17T18:32:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTc1MDU5OQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441750599", "bodyText": "You are right, I just looked into one of the implementors of slf4j (Log4jLoggerAdapter) and it already covers isTraceEnabled check:\n    public void trace(String format, Object arg) {\n        if (isTraceEnabled()) {\n            FormattingTuple ft = MessageFormatter.format(format, arg);\n            logger.log(FQCN, traceCapable ? Level.TRACE : Level.DEBUG, ft.getMessage(), ft.getThrowable());\n        }\n    }\n\nWe are following explicit guarding for the cases of Trace and Debug level log at multiple places, all are redundant checks since library already does it for us.", "author": "virajjasani", "createdAt": "2020-06-17T18:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTg0MDU2MQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441840561", "bodyText": "Thanks @virajjasani", "author": "saintstack", "createdAt": "2020-06-17T21:19:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkyOTE5Nw==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441929197", "bodyText": "IIRC, this extra check could be used only if the parameter self will cause a heavy call.", "author": "bsglz", "createdAt": "2020-06-18T02:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzIwNQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ0NzI5Mg==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441447292", "bodyText": "same here", "author": "virajjasani", "createdAt": "2020-06-17T10:31:06Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.java", "diffHunk": "@@ -118,6 +122,7 @@ public void append(RegionEntryBuffer buffer) throws IOException {\n         openingWritersNum.decrementAndGet();\n       } finally {\n         writer.close();\n+        LOG.trace(\"Closed {}, edits={}\", writer.getPath(), familyCells.size());", "originalCommit": "9b22ca97fcc775401b0a330b7a014df26107c13b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "dc3f2c477d0573abeebe3fc0fe6c7f8207625121", "url": "https://github.com/apache/hbase/commit/dc3f2c477d0573abeebe3fc0fe6c7f8207625121", "message": "HBASE-24577 Doc WALSplitter classes\n\nSigned-off-by: Anoop Sam John <anoopsamjohn@apache.org>\nSigned-off-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Guanghao Zhang <zghao@apache.org>", "committedDate": "2020-06-17T17:59:30Z", "type": "forcePushed"}, {"oid": "36d85465ab24abea1fd98ffdf9adb95712364fd6", "url": "https://github.com/apache/hbase/commit/36d85465ab24abea1fd98ffdf9adb95712364fd6", "message": "HBASE-24577 Doc WALSplitter classes\n\nSigned-off-by: Anoop Sam John <anoopsamjohn@apache.org>\nSigned-off-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Guanghao Zhang <zghao@apache.org>", "committedDate": "2020-06-17T18:49:15Z", "type": "commit"}, {"oid": "36d85465ab24abea1fd98ffdf9adb95712364fd6", "url": "https://github.com/apache/hbase/commit/36d85465ab24abea1fd98ffdf9adb95712364fd6", "message": "HBASE-24577 Doc WALSplitter classes\n\nSigned-off-by: Anoop Sam John <anoopsamjohn@apache.org>\nSigned-off-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Guanghao Zhang <zghao@apache.org>", "committedDate": "2020-06-17T18:49:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk2NjM1NQ==", "url": "https://github.com/apache/hbase/pull/1913#discussion_r441966355", "bodyText": "Just asking. this can be in DEBUG mode right rather than trace?", "author": "ramkrish86", "createdAt": "2020-06-18T04:49:34Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.java", "diffHunk": "@@ -81,6 +83,7 @@ private RecoveredEditsWriter getRecoveredEditsWriter(TableName tableName, byte[]\n     if (ret == null) {\n       return null;\n     }\n+    LOG.trace(\"Created {}\", ret.path);", "originalCommit": "36d85465ab24abea1fd98ffdf9adb95712364fd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}