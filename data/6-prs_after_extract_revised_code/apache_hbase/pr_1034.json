{"pr_number": 1034, "pr_title": "HBASE-23622 Reduced the number of Checkstyle violations in hbase-common", "pr_createdAt": "2020-01-14T08:50:17Z", "pr_url": "https://github.com/apache/hbase/pull/1034", "timeline": [{"oid": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "url": "https://github.com/apache/hbase/commit/ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "message": "HBASE-23622 Reduced the number of Checkstyle violations in hbase-common", "committedDate": "2020-01-14T08:48:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MjIzMA==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366382230", "bodyText": "this is a binary incompatible change and matters because ByteRangeUtils is IA.Public\nref: JLS 13.4.2 \"final classes\"\nhttps://docs.oracle.com/javase/specs/jls/se13/html/jls-13.html#jls-13.4.2", "author": "busbey", "createdAt": "2020-01-14T14:52:19Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java", "diffHunk": "@@ -33,12 +33,16 @@\n  */\n @InterfaceAudience.Public\n @InterfaceStability.Evolving\n-public class ByteRangeUtils {\n+public final class ByteRangeUtils {", "originalCommit": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ0MzI0Mg==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366443242", "bodyText": "I reverted this change. As this one is already pushed to higher versions, let me also revert it there.", "author": "HorizonNet", "createdAt": "2020-01-14T16:33:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MjIzMA=="}], "type": "inlineReview", "revised_code": {"commit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\nindex cac05b725d..f33e3ab4fd 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\n\n@@ -26,17 +27,12 @@ import java.util.Collection;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n \n-import com.google.common.collect.Lists;\n-\n /**\n  * Utility methods for working with {@link ByteRange}.\n  */\n @InterfaceAudience.Public\n @InterfaceStability.Evolving\n-public final class ByteRangeUtils {\n-  private ByteRangeUtils() {\n-  }\n-\n+public class ByteRangeUtils {\n   public static int numEqualPrefixBytes(ByteRange left, ByteRange right, int rightInnerOffset) {\n     int maxCompares = Math.min(left.getLength(), right.getLength() - rightInnerOffset);\n     final byte[] lbytes = left.getBytes();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NTY0Ng==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366385646", "bodyText": "This is a binary incompatible change and matters because ByteRangeUtils is IA.Public\nref JLS 13.4.12\nhttps://docs.oracle.com/javase/specs/jls/se13/html/jls-13.html#jls-13.4.12\nBecause there were no constructors defined and this class is marked public, this class implicitly had a public ByteRangeUtils() constructor. So changing it to private here will break compatibility.", "author": "busbey", "createdAt": "2020-01-14T14:57:46Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java", "diffHunk": "@@ -33,12 +33,16 @@\n  */\n @InterfaceAudience.Public\n @InterfaceStability.Evolving\n-public class ByteRangeUtils {\n+public final class ByteRangeUtils {\n+  private ByteRangeUtils() {", "originalCommit": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ0MzQyMA==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366443420", "bodyText": "Reverted this change and put it onto the suppressions list.", "author": "HorizonNet", "createdAt": "2020-01-14T16:33:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NTY0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\nindex cac05b725d..f33e3ab4fd 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteRangeUtils.java\n\n@@ -26,17 +27,12 @@ import java.util.Collection;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n \n-import com.google.common.collect.Lists;\n-\n /**\n  * Utility methods for working with {@link ByteRange}.\n  */\n @InterfaceAudience.Public\n @InterfaceStability.Evolving\n-public final class ByteRangeUtils {\n-  private ByteRangeUtils() {\n-  }\n-\n+public class ByteRangeUtils {\n   public static int numEqualPrefixBytes(ByteRange left, ByteRange right, int rightInnerOffset) {\n     int maxCompares = Math.min(left.getLength(), right.getLength() - rightInnerOffset);\n     final byte[] lbytes = left.getBytes();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5NjQ4OA==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366396488", "bodyText": "The method getDefaultBlockSize(Path) has been a supported API in all the Hadoop versions we support for a very longs time. We should skip the reflection here.", "author": "busbey", "createdAt": "2020-01-14T15:16:16Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java", "diffHunk": "@@ -157,7 +156,7 @@ public static long getDefaultBlockSize(final FileSystem fs, final Path path) thr\n     Method m = null;\n     Class<? extends FileSystem> cls = fs.getClass();\n     try {\n-      m = cls.getMethod(\"getDefaultBlockSize\", new Class<?>[] { Path.class });\n+      m = cls.getMethod(\"getDefaultBlockSize\", Path.class);", "originalCommit": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ0MzYxOA==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366443618", "bodyText": "Removed the reflection part.", "author": "HorizonNet", "createdAt": "2020-01-14T16:33:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5NjQ4OA=="}], "type": "inlineReview", "revised_code": {"commit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\nindex ff15d35c1c..95b3439d3a 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n\n@@ -145,69 +145,22 @@ public abstract class CommonFSUtils {\n    * Return the number of bytes that large input files should be optimally\n    * be split into to minimize i/o time.\n    *\n-   * use reflection to search for getDefaultBlockSize(Path f)\n-   * if the method doesn't exist, fall back to using getDefaultBlockSize()\n-   *\n    * @param fs filesystem object\n    * @return the default block size for the path's filesystem\n-   * @throws IOException e\n    */\n-  public static long getDefaultBlockSize(final FileSystem fs, final Path path) throws IOException {\n-    Method m = null;\n-    Class<? extends FileSystem> cls = fs.getClass();\n-    try {\n-      m = cls.getMethod(\"getDefaultBlockSize\", Path.class);\n-    } catch (NoSuchMethodException e) {\n-      LOG.info(\"FileSystem doesn't support getDefaultBlockSize\");\n-    } catch (SecurityException e) {\n-      LOG.info(\"Doesn't have access to getDefaultBlockSize on FileSystems\", e);\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m == null) {\n-      return fs.getDefaultBlockSize(path);\n-    } else {\n-      try {\n-        Object ret = m.invoke(fs, path);\n-        return ((Long)ret).longValue();\n-      } catch (Exception e) {\n-        throw new IOException(e);\n-      }\n-    }\n+  public static long getDefaultBlockSize(final FileSystem fs, final Path path) {\n+    return fs.getDefaultBlockSize(path);\n   }\n \n   /*\n    * Get the default replication.\n    *\n-   * use reflection to search for getDefaultReplication(Path f)\n-   * if the method doesn't exist, fall back to using getDefaultReplication()\n-   *\n    * @param fs filesystem object\n    * @param f path of file\n    * @return default replication for the path's filesystem\n-   * @throws IOException e\n    */\n-  public static short getDefaultReplication(final FileSystem fs, final Path path)\n-      throws IOException {\n-    Method m = null;\n-    Class<? extends FileSystem> cls = fs.getClass();\n-    try {\n-      m = cls.getMethod(\"getDefaultReplication\", Path.class);\n-    } catch (NoSuchMethodException e) {\n-      LOG.info(\"FileSystem doesn't support getDefaultReplication\");\n-    } catch (SecurityException e) {\n-      LOG.info(\"Doesn't have access to getDefaultReplication on FileSystems\", e);\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m == null) {\n-      return fs.getDefaultReplication(path);\n-    } else {\n-      try {\n-        Object ret = m.invoke(fs, path);\n-        return ((Number)ret).shortValue();\n-      } catch (Exception e) {\n-        throw new IOException(e);\n-      }\n-    }\n+  public static short getDefaultReplication(final FileSystem fs, final Path path) {\n+    return fs.getDefaultReplication(path);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5NjYwMQ==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366396601", "bodyText": "The method getDefaultReplication(Path) has been a supported API in all the Hadoop versions we support for a very longs time. We should skip the reflection here.", "author": "busbey", "createdAt": "2020-01-14T15:16:29Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java", "diffHunk": "@@ -192,7 +191,7 @@ public static short getDefaultReplication(final FileSystem fs, final Path path)\n     Method m = null;\n     Class<? extends FileSystem> cls = fs.getClass();\n     try {\n-      m = cls.getMethod(\"getDefaultReplication\", new Class<?>[] { Path.class });\n+      m = cls.getMethod(\"getDefaultReplication\", Path.class);", "originalCommit": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ0Mzc2NQ==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366443765", "bodyText": "Removed the reflection part.", "author": "HorizonNet", "createdAt": "2020-01-14T16:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5NjYwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\nindex ff15d35c1c..95b3439d3a 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n\n@@ -145,69 +145,22 @@ public abstract class CommonFSUtils {\n    * Return the number of bytes that large input files should be optimally\n    * be split into to minimize i/o time.\n    *\n-   * use reflection to search for getDefaultBlockSize(Path f)\n-   * if the method doesn't exist, fall back to using getDefaultBlockSize()\n-   *\n    * @param fs filesystem object\n    * @return the default block size for the path's filesystem\n-   * @throws IOException e\n    */\n-  public static long getDefaultBlockSize(final FileSystem fs, final Path path) throws IOException {\n-    Method m = null;\n-    Class<? extends FileSystem> cls = fs.getClass();\n-    try {\n-      m = cls.getMethod(\"getDefaultBlockSize\", Path.class);\n-    } catch (NoSuchMethodException e) {\n-      LOG.info(\"FileSystem doesn't support getDefaultBlockSize\");\n-    } catch (SecurityException e) {\n-      LOG.info(\"Doesn't have access to getDefaultBlockSize on FileSystems\", e);\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m == null) {\n-      return fs.getDefaultBlockSize(path);\n-    } else {\n-      try {\n-        Object ret = m.invoke(fs, path);\n-        return ((Long)ret).longValue();\n-      } catch (Exception e) {\n-        throw new IOException(e);\n-      }\n-    }\n+  public static long getDefaultBlockSize(final FileSystem fs, final Path path) {\n+    return fs.getDefaultBlockSize(path);\n   }\n \n   /*\n    * Get the default replication.\n    *\n-   * use reflection to search for getDefaultReplication(Path f)\n-   * if the method doesn't exist, fall back to using getDefaultReplication()\n-   *\n    * @param fs filesystem object\n    * @param f path of file\n    * @return default replication for the path's filesystem\n-   * @throws IOException e\n    */\n-  public static short getDefaultReplication(final FileSystem fs, final Path path)\n-      throws IOException {\n-    Method m = null;\n-    Class<? extends FileSystem> cls = fs.getClass();\n-    try {\n-      m = cls.getMethod(\"getDefaultReplication\", Path.class);\n-    } catch (NoSuchMethodException e) {\n-      LOG.info(\"FileSystem doesn't support getDefaultReplication\");\n-    } catch (SecurityException e) {\n-      LOG.info(\"Doesn't have access to getDefaultReplication on FileSystems\", e);\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m == null) {\n-      return fs.getDefaultReplication(path);\n-    } else {\n-      try {\n-        Object ret = m.invoke(fs, path);\n-        return ((Number)ret).shortValue();\n-      } catch (Exception e) {\n-        throw new IOException(e);\n-      }\n-    }\n+  public static short getDefaultReplication(final FileSystem fs, final Path path) {\n+    return fs.getDefaultReplication(path);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5ODk2Mg==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366398962", "bodyText": "branch-1's minimum hadoop version is 2.8.5, so for that branch we can remove this reflection as well. Note to potential further backporters: the method is not in Hadoop 2.7, we'll need this reflection in branch-1.4 and branch-1.3.", "author": "busbey", "createdAt": "2020-01-14T15:20:22Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java", "diffHunk": "@@ -560,8 +558,7 @@ private static void invokeSetStoragePolicy(final FileSystem fs, final Path path,\n     Method m = null;\n     Exception toThrow = null;\n     try {\n-      m = fs.getClass().getDeclaredMethod(\"setStoragePolicy\",\n-        new Class<?>[] { Path.class, String.class });\n+      m = fs.getClass().getDeclaredMethod(\"setStoragePolicy\", Path.class, String.class);", "originalCommit": "ef7f8762da23fcbc8d96df3b35f89fb56ac7865f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ0NDMwOQ==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366444309", "bodyText": "Removed the reflection part. Will create additional backports for branch-1.4 and branch-1.3.", "author": "HorizonNet", "createdAt": "2020-01-14T16:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM5ODk2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\nindex ff15d35c1c..95b3439d3a 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n\n@@ -555,64 +508,40 @@ public abstract class CommonFSUtils {\n    */\n   private static void invokeSetStoragePolicy(final FileSystem fs, final Path path,\n       final String storagePolicy) throws IOException {\n-    Method m = null;\n     Exception toThrow = null;\n+\n     try {\n-      m = fs.getClass().getDeclaredMethod(\"setStoragePolicy\", Path.class, String.class);\n-      m.setAccessible(true);\n-    } catch (NoSuchMethodException e) {\n-      toThrow = e;\n-      final String msg = \"FileSystem doesn't support setStoragePolicy; HDFS-6584 not available\";\n-      if (!warningMap.containsKey(fs)) {\n-        warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n-      } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+      fs.setStoragePolicy(path, storagePolicy);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Set storagePolicy=\" + storagePolicy + \" for path=\" + path);\n       }\n-      m = null;\n-    } catch (SecurityException e) {\n+    } catch (Exception e) {\n       toThrow = e;\n-      final String msg = \"No access to setStoragePolicy on FileSystem; HDFS-6584 not available\";\n+      // This swallows FNFE, should we be throwing it? seems more likely to indicate dev\n+      // misuse than a runtime problem with HDFS.\n       if (!warningMap.containsKey(fs)) {\n         warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n+        LOG.warn(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+        LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       }\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m != null) {\n-      try {\n-        m.invoke(fs, path, storagePolicy);\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Set storagePolicy=\" + storagePolicy + \" for path=\" + path);\n-        }\n-      } catch (Exception e) {\n-        toThrow = e;\n-        // This swallows FNFE, should we be throwing it? seems more likely to indicate dev\n-        // misuse than a runtime problem with HDFS.\n-        if (!warningMap.containsKey(fs)) {\n-          warningMap.put(fs, true);\n-          LOG.warn(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n-        } else if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n-        }\n-        // check for lack of HDFS-7228\n-        if (e instanceof InvocationTargetException) {\n-          final Throwable exception = e.getCause();\n-          if (exception instanceof RemoteException &&\n-              HadoopIllegalArgumentException.class.getName().equals(\n-                ((RemoteException)exception).getClassName())) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n-                \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n-                \"trying to use SSD related policies then you're likely missing HDFS-7228. For \" +\n-                \"more information see the 'ArchivalStorage' docs for your Hadoop release.\");\n-            }\n+      // check for lack of HDFS-7228\n+      if (e instanceof InvocationTargetException) {\n+        final Throwable exception = e.getCause();\n+        if (exception instanceof RemoteException &&\n+            HadoopIllegalArgumentException.class.getName().equals(\n+              ((RemoteException)exception).getClassName())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n+              \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n+              \"trying to use SSD related policies then you're likely missing HDFS-7228. For \" +\n+              \"more information see the 'ArchivalStorage' docs for your Hadoop release.\");\n           }\n         }\n       }\n     }\n+\n     if (toThrow != null) {\n       throw new IOException(toThrow);\n     }\n"}}, {"oid": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "url": "https://github.com/apache/hbase/commit/f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "message": "HBASE-23622 Fixed review comments", "committedDate": "2020-01-14T16:31:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2NDI5NQ==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366464295", "bodyText": "Now that we're not using reflection, this should be a direct check for RemoteException because it won't be wrapped in the reflection invocation exception.", "author": "busbey", "createdAt": "2020-01-14T17:10:34Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java", "diffHunk": "@@ -557,71 +508,45 @@ static void setStoragePolicy(final FileSystem fs, final Path path, final String\n    */\n   private static void invokeSetStoragePolicy(final FileSystem fs, final Path path,\n       final String storagePolicy) throws IOException {\n-    Method m = null;\n     Exception toThrow = null;\n+\n     try {\n-      m = fs.getClass().getDeclaredMethod(\"setStoragePolicy\",\n-        new Class<?>[] { Path.class, String.class });\n-      m.setAccessible(true);\n-    } catch (NoSuchMethodException e) {\n-      toThrow = e;\n-      final String msg = \"FileSystem doesn't support setStoragePolicy; HDFS-6584 not available\";\n-      if (!warningMap.containsKey(fs)) {\n-        warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n-      } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+      fs.setStoragePolicy(path, storagePolicy);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Set storagePolicy=\" + storagePolicy + \" for path=\" + path);\n       }\n-      m = null;\n-    } catch (SecurityException e) {\n+    } catch (Exception e) {\n       toThrow = e;\n-      final String msg = \"No access to setStoragePolicy on FileSystem; HDFS-6584 not available\";\n+      // This swallows FNFE, should we be throwing it? seems more likely to indicate dev\n+      // misuse than a runtime problem with HDFS.\n       if (!warningMap.containsKey(fs)) {\n         warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n+        LOG.warn(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+        LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       }\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m != null) {\n-      try {\n-        m.invoke(fs, path, storagePolicy);\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Set storagePolicy=\" + storagePolicy + \" for path=\" + path);\n-        }\n-      } catch (Exception e) {\n-        toThrow = e;\n-        // This swallows FNFE, should we be throwing it? seems more likely to indicate dev\n-        // misuse than a runtime problem with HDFS.\n-        if (!warningMap.containsKey(fs)) {\n-          warningMap.put(fs, true);\n-          LOG.warn(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n-        } else if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n-        }\n-        // check for lack of HDFS-7228\n-        if (e instanceof InvocationTargetException) {\n-          final Throwable exception = e.getCause();\n-          if (exception instanceof RemoteException &&\n-              HadoopIllegalArgumentException.class.getName().equals(\n-                ((RemoteException)exception).getClassName())) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n-                \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n-                \"trying to use SSD related policies then you're likely missing HDFS-7228. For \" +\n-                \"more information see the 'ArchivalStorage' docs for your Hadoop release.\");\n-            }\n+      // check for lack of HDFS-7228\n+      if (e instanceof InvocationTargetException) {", "originalCommit": "f9c61bdfb552b0915dd47c2b740b7d8d2e3bf9c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ4MzIyMg==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366483222", "bodyText": "Updated the changes to unwrap the InvocationTargetException check.", "author": "HorizonNet", "createdAt": "2020-01-14T17:49:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2NDI5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "e002f59b486ba101a7c928c34c0a55ed519e1aa3", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\nindex 95b3439d3a..3e9bb53e19 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n\n@@ -526,18 +526,17 @@ public abstract class CommonFSUtils {\n       } else if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       }\n+\n       // check for lack of HDFS-7228\n-      if (e instanceof InvocationTargetException) {\n-        final Throwable exception = e.getCause();\n-        if (exception instanceof RemoteException &&\n-            HadoopIllegalArgumentException.class.getName().equals(\n-              ((RemoteException)exception).getClassName())) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n-              \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n-              \"trying to use SSD related policies then you're likely missing HDFS-7228. For \" +\n-              \"more information see the 'ArchivalStorage' docs for your Hadoop release.\");\n-          }\n+      final Throwable exception = e.getCause();\n+      if (exception instanceof RemoteException &&\n+          HadoopIllegalArgumentException.class.getName().equals(\n+            ((RemoteException)exception).getClassName())) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n+            \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n+            \"trying to use SSD related policies then you're likely missing HDFS-7228. For \" +\n+            \"more information see the 'ArchivalStorage' docs for your Hadoop release.\");\n         }\n       }\n     }\n"}}, {"oid": "e002f59b486ba101a7c928c34c0a55ed519e1aa3", "url": "https://github.com/apache/hbase/commit/e002f59b486ba101a7c928c34c0a55ed519e1aa3", "message": "HBASE-23622 Fixed review comment", "committedDate": "2020-01-14T17:48:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMTY1OA==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366521658", "bodyText": "I don't think this is correct. wouldn't e be the RemoteException?", "author": "busbey", "createdAt": "2020-01-14T19:11:46Z", "path": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java", "diffHunk": "@@ -557,71 +508,44 @@ static void setStoragePolicy(final FileSystem fs, final Path path, final String\n    */\n   private static void invokeSetStoragePolicy(final FileSystem fs, final Path path,\n       final String storagePolicy) throws IOException {\n-    Method m = null;\n     Exception toThrow = null;\n+\n     try {\n-      m = fs.getClass().getDeclaredMethod(\"setStoragePolicy\",\n-        new Class<?>[] { Path.class, String.class });\n-      m.setAccessible(true);\n-    } catch (NoSuchMethodException e) {\n-      toThrow = e;\n-      final String msg = \"FileSystem doesn't support setStoragePolicy; HDFS-6584 not available\";\n-      if (!warningMap.containsKey(fs)) {\n-        warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n-      } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+      fs.setStoragePolicy(path, storagePolicy);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Set storagePolicy=\" + storagePolicy + \" for path=\" + path);\n       }\n-      m = null;\n-    } catch (SecurityException e) {\n+    } catch (Exception e) {\n       toThrow = e;\n-      final String msg = \"No access to setStoragePolicy on FileSystem; HDFS-6584 not available\";\n+      // This swallows FNFE, should we be throwing it? seems more likely to indicate dev\n+      // misuse than a runtime problem with HDFS.\n       if (!warningMap.containsKey(fs)) {\n         warningMap.put(fs, true);\n-        LOG.warn(msg, e);\n+        LOG.warn(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(msg, e);\n+        LOG.debug(\"Unable to set storagePolicy=\" + storagePolicy + \" for path=\" + path, e);\n       }\n-      m = null; // could happen on setAccessible()\n-    }\n-    if (m != null) {\n-      try {\n-        m.invoke(fs, path, storagePolicy);\n+\n+      // check for lack of HDFS-7228\n+      final Throwable exception = e.getCause();", "originalCommit": "e002f59b486ba101a7c928c34c0a55ed519e1aa3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzODQxNQ==", "url": "https://github.com/apache/hbase/pull/1034#discussion_r366538415", "bodyText": "Yes, you're right. I removed the part of getting the cause.", "author": "HorizonNet", "createdAt": "2020-01-14T19:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMTY1OA=="}], "type": "inlineReview", "revised_code": {"commit": "f71018c3abd53fc0d520a0f218075c90e644182a", "chunk": "diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\nindex 3e9bb53e19..ef6d48908a 100644\n--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java\n\n@@ -528,10 +528,9 @@ public abstract class CommonFSUtils {\n       }\n \n       // check for lack of HDFS-7228\n-      final Throwable exception = e.getCause();\n-      if (exception instanceof RemoteException &&\n+      if (e instanceof RemoteException &&\n           HadoopIllegalArgumentException.class.getName().equals(\n-            ((RemoteException)exception).getClassName())) {\n+            ((RemoteException)e).getClassName())) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Given storage policy, '\" +storagePolicy +\"', was rejected and probably \" +\n             \"isn't a valid policy for the version of Hadoop you're running. I.e. if you're \" +\n"}}, {"oid": "f71018c3abd53fc0d520a0f218075c90e644182a", "url": "https://github.com/apache/hbase/commit/f71018c3abd53fc0d520a0f218075c90e644182a", "message": "HBASE-23622 Fixed review comment", "committedDate": "2020-01-14T19:45:31Z", "type": "commit"}]}