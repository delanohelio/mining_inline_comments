{"pr_number": 789, "pr_title": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "pr_createdAt": "2020-12-17T05:22:15Z", "pr_url": "https://github.com/linkedin/brooklin/pull/789", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTA1Nw==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275057", "bodyText": "nit: If this datastream wants", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T17:37:01Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -161,15 +161,19 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n \n-    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n-    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n     // upon first poll, to be handled by seeking to teh startOffsets\n-    if (_startOffsets.isPresent()) {\n-      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n           _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTM1Mg==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275352", "bodyText": "nit: typo on the.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T17:37:25Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMzNDQ0MA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545334440", "bodyText": "Nit: Another typo one the exception name", "author": "nisargthakkar", "createdAt": "2020-12-17T19:07:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTM1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -161,15 +161,19 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n \n-    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n-    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n     // upon first poll, to be handled by seeking to teh startOffsets\n-    if (_startOffsets.isPresent()) {\n-      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n           _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NjY3MQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545276671", "bodyText": "unrelated to your change, but should we make _startOffsets not Optional? It is a map and we are wrapping it in a Nullable only to be using isPresent everywhere if present needing to access it with the get call which is not serving the purpose of Optional at all.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T17:39:26Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ2NTk2NQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545465965", "bodyText": "makes sense. I made _startOffsets not Optional", "author": "shenodaguirguis", "createdAt": "2020-12-17T23:12:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NjY3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -161,15 +161,19 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n \n-    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n-    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n     // upon first poll, to be handled by seeking to teh startOffsets\n-    if (_startOffsets.isPresent()) {\n-      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n           _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545283096", "bodyText": "nit: I was originally conflicted about the change because both start position and reset strategy are overridable values and we are basically giving higher precedence to start position by adjusting the reset strategy. But on more thought I do feel like this is the right thing to do, given that reset strategy isn't something a client will provide when they have already specified the start position. Although, it would be confusing is if they provided both as it wouldn't be possible to figure out which one they really wanted to set, I do feel like start position somehow feels like a more granular thing to specify and so should probably take precedence.\nCan we however fetch offset reset strategy first so that in the logs it can be made clear that that was ignored because startPosition was also set?", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T17:48:53Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMjM4NQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545302385", "bodyText": "+1\nWe could perhaps add some kind of validation that start positions cannot be present if the auto.offset.reset strategy is set to anything other than \"none\" if we feel the need to be careful, but I'll leave the decision to add this or not to you.", "author": "somandal", "createdAt": "2020-12-17T18:18:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMyNjk3NQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545326975", "bodyText": "I was thinking about validation, but then the stream task would just die, right? I wasn't terribly sure if this was a good idea since clients are not necessarily good at figuring out what's happening on the server, but may be in such cases they are, and may be SREs should in fact get alerted for bad configs.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T18:54:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMyODk5MA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545328990", "bodyText": "@DEEPTHIKORAT what about checking this during a) initialize datastream and b) datastream update? This way both stream creation/update will fail.\nAgain don't have a strong opinion on whether we should add this or not, but it's just a thought if we're worried about explaining the behavior when both metadata fields are present.", "author": "somandal", "createdAt": "2020-12-17T18:58:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ5NjQxMg==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545496412", "bodyText": "I agree with Deepthi that failing might not be the right thing to do. And that was exactly my thinking @DEEPTHIKORAT  that when users set the start offsets, they mean to use it. I am also doing exactly as you suggest: I am logging the original value before the update. Here is a sample log from the test:\n2020-12-17 16:27:41 INFO  KafkaConnectorTask:176 - Datastream contains startOffsets, override auto.offset.reset with value none (was earliest in the provided configs)\nI will attempt to reword to make it clearer", "author": "shenodaguirguis", "createdAt": "2020-12-18T00:35:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg0MTI3MQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546841271", "bodyText": "I think that is a nice idea @somandal. @shenodaguirguis feel free to check in this version since Sonam has approved it as well, but we can consider making the change Sonam suggested in the future perhaps. At least then it is a user-facing failure and they won't be caught unaware.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-21T17:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg1NTQ4MQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546855481", "bodyText": "we can have a separate change for this", "author": "shenodaguirguis", "createdAt": "2020-12-21T18:13:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -161,15 +161,19 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n \n-    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n-    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n     // upon first poll, to be handled by seeking to teh startOffsets\n-    if (_startOffsets.isPresent()) {\n-      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n           _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545289355", "bodyText": "Can we add a specific test case to try the use-cases out? Many cases crop up since it is 2 configs, but may be we need at least the one you are fixing, with start position specified and no reset strategy.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-17T17:58:16Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMTM0MA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545301340", "bodyText": "+1, I'd go as far as to make sure that we see the expected behavior when start positions are present, and when they aren't.", "author": "somandal", "createdAt": "2020-12-17T18:16:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUwNDA4OQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545504089", "bodyText": "testConsumeBaseCase tests when no startOffsets are present,\ntestConsumeWithTheStartingOffset tests the case with startOffsets are present. It was testing with no auto.offset.rest set, I changed it to set auto.offset.reset set to earliest, and it already verifies the expected behavior (in terms of number of records consumed)\nThe actual value of the auto.offset.reset is irrelevant, we only need to test if it is set that we do override. Whether it was earliest,  latest or none, is orthogonal.", "author": "shenodaguirguis", "createdAt": "2020-12-18T00:58:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAyMDc1NA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546020754", "bodyText": "I added a test case for when the auto offset reset strategy is set in metadata, to verify startOffsets takes precedence", "author": "shenodaguirguis", "createdAt": "2020-12-18T18:44:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\nindex 40c015d7..12004522 100644\n--- a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n+++ b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n\n@@ -170,11 +170,12 @@ public class TestKafkaConnectorTask extends BaseKafkaZkTest {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n \n     // validate auto.offset.reset config is overridden to none (given the start offsets)\n     Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n \n+\n     LOG.info(\"Sending third set of events\");\n \n     //send 100 more msgs\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5ODUxMw==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545298513", "bodyText": "nit: NoOffsetForPartiionException -> NoOffsetForPartitionException", "author": "somandal", "createdAt": "2020-12-17T18:11:57Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -161,15 +161,19 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n \n-    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n-    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n     // upon first poll, to be handled by seeking to teh startOffsets\n-    if (_startOffsets.isPresent()) {\n-      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n           _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5OTk4OA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545299988", "bodyText": "Can you return a null string instead of \"NOT SET\"?", "author": "somandal", "createdAt": "2020-12-17T18:14:19Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -1035,4 +1047,9 @@ public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor gro\n       throw e;\n     }\n   }\n+\n+  @VisibleForTesting\n+  public String getConsumerAutoOffsetResetConfig() {\n+    return _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"NOT SET\");", "originalCommit": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05823e9c923f1bef307323c09f3efa840c7faeae", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 9c70a001..46e3e506 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -1050,6 +1054,6 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n \n   @VisibleForTesting\n   public String getConsumerAutoOffsetResetConfig() {\n-    return _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"NOT SET\");\n+    return _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, null);\n   }\n }\n\\ No newline at end of file\n"}}, {"oid": "05823e9c923f1bef307323c09f3efa840c7faeae", "url": "https://github.com/linkedin/brooklin/commit/05823e9c923f1bef307323c09f3efa840c7faeae", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "committedDate": "2020-12-18T01:41:32Z", "type": "forcePushed"}, {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "url": "https://github.com/linkedin/brooklin/commit/94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "committedDate": "2020-12-18T02:25:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NzY0OQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545987649", "bodyText": "nit:  teh -> the", "author": "somandal", "createdAt": "2020-12-18T17:42:26Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "originalCommit": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2df71d42261f8e3eefffb82899b21daf417a2818", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 81ab91ba..b87feea4 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -171,12 +171,20 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n \n     // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n     // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n-    // upon first poll, to be handled by seeking to teh startOffsets\n+    // upon first poll, to be handled by seeking to the startOffsets\n     if (!_startOffsets.isEmpty()) {\n+      if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n+        _logger.info(\"Datastream contains metadata for both {} and {}, which are mutually exclusive. Ignoring the \"\n+                + \"{}=\\\"{}\\\" metadata, as start offsets require it to be set to \\\"none\\\"\",\n+            DatastreamMetadataConstants.START_POSITION,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY));\n+      }\n       _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n               + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n-          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n+          getConsumerAutoOffsetResetConfig());\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n     } else if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n         String strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDE0Mg==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990142", "bodyText": "In case someone adds the datastream metadata too, should we print that here too? It'll be helpful in debugging if someone sets this to something else and is wondering why it isn't taking effect.\nString strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\nThis log is only printing the config.", "author": "somandal", "createdAt": "2020-12-18T17:47:05Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"", "originalCommit": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2df71d42261f8e3eefffb82899b21daf417a2818", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 81ab91ba..b87feea4 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -171,12 +171,20 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n \n     // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n     // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n-    // upon first poll, to be handled by seeking to teh startOffsets\n+    // upon first poll, to be handled by seeking to the startOffsets\n     if (!_startOffsets.isEmpty()) {\n+      if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n+        _logger.info(\"Datastream contains metadata for both {} and {}, which are mutually exclusive. Ignoring the \"\n+                + \"{}=\\\"{}\\\" metadata, as start offsets require it to be set to \\\"none\\\"\",\n+            DatastreamMetadataConstants.START_POSITION,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY));\n+      }\n       _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n               + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n-          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n+          getConsumerAutoOffsetResetConfig());\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n     } else if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n         String strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDQ4OQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990489", "bodyText": "can we modify getConsumerAutoOffsetResetConfig to set an empty string \"\" instead of null, and call that here? Then instead of null checks we can use isBlank checks.", "author": "somandal", "createdAt": "2020-12-18T17:47:46Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n+          ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n+          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));", "originalCommit": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2df71d42261f8e3eefffb82899b21daf417a2818", "chunk": "diff --git a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\nindex 81ab91ba..b87feea4 100644\n--- a/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n+++ b/datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java\n\n@@ -171,12 +171,20 @@ abstract public class AbstractKafkaBasedConnectorTask implements Runnable, Consu\n \n     // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n     // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n-    // upon first poll, to be handled by seeking to teh startOffsets\n+    // upon first poll, to be handled by seeking to the startOffsets\n     if (!_startOffsets.isEmpty()) {\n+      if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n+        _logger.info(\"Datastream contains metadata for both {} and {}, which are mutually exclusive. Ignoring the \"\n+                + \"{}=\\\"{}\\\" metadata, as start offsets require it to be set to \\\"none\\\"\",\n+            DatastreamMetadataConstants.START_POSITION,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY,\n+            _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY));\n+      }\n       _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n               + \"(overriding the configs value of \\\"{}\\\")\",\n           ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n-          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));\n+          getConsumerAutoOffsetResetConfig());\n       _consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE);\n     } else if (_datastream.getMetadata().containsKey(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY)) {\n         String strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MTAyOA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545991028", "bodyText": "nit: add an empty line after this", "author": "somandal", "createdAt": "2020-12-18T17:48:46Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -448,6 +452,13 @@ static Datastream getDatastream(String broker, String topic) {\n     return datastream;\n   }\n \n+  private KafkaConnectorTask createKafkaConnectorTaskWithAutoOffsetResetConfig(DatastreamTaskImpl task,\n+      String autoOffsetResetStrategy) throws InterruptedException {\n+    Properties consumerProperties = new Properties();\n+    consumerProperties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetStrategy);\n+    return createKafkaConnectorTask(task, new KafkaBasedConnectorConfigBuilder()\n+        .setConsumerProps(consumerProperties).build());\n+  }", "originalCommit": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2df71d42261f8e3eefffb82899b21daf417a2818", "chunk": "diff --git a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\nindex 12004522..d29bd657 100644\n--- a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n+++ b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n\n@@ -459,6 +507,7 @@ public class TestKafkaConnectorTask extends BaseKafkaZkTest {\n     return createKafkaConnectorTask(task, new KafkaBasedConnectorConfigBuilder()\n         .setConsumerProps(consumerProperties).build());\n   }\n+\n   private KafkaConnectorTask createKafkaConnectorTask(DatastreamTaskImpl task) throws InterruptedException {\n     return createKafkaConnectorTask(task, new KafkaBasedConnectorConfigBuilder().build());\n   }\n"}}, {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818", "url": "https://github.com/linkedin/brooklin/commit/2df71d42261f8e3eefffb82899b21daf417a2818", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "committedDate": "2020-12-18T18:27:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODI1MQ==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898251", "bodyText": "was the choice of 100L intentional for start timestamp?", "author": "DEEPTHIKORAT", "createdAt": "2020-12-21T19:48:39Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);", "originalCommit": "2df71d42261f8e3eefffb82899b21daf417a2818", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTQ3OA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546911478", "bodyText": "yes intentional, and it is a start offset, not timestamp.\nwe produce 100 events, set startOffsets to 100, produce 200 more and make sure we consume 200 only to verify start offset metadata took effect", "author": "shenodaguirguis", "createdAt": "2020-12-21T20:20:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODI1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "ae7498afad70fdad4efa704519465d1471d20ff4", "chunk": "diff --git a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\nindex d29bd657..ef5498c9 100644\n--- a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n+++ b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n\n@@ -210,10 +210,9 @@ public class TestKafkaConnectorTask extends BaseKafkaZkTest {\n     //start\n     MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n     Datastream datastream = getDatastream(_broker, topic);\n-\n-    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.\n+    // set system.start.position in the metadata\n     datastream.getMetadata().put(DatastreamMetadataConstants.START_POSITION, JsonUtils.toJson(startOffsets));\n-    // set auto.offset.reset strategy in metadata to ensure it is ignored in presence of start offsets\n+    // set system.auto.offset.reset strategy in metadata to ensure it is ignored in presence of start offsets\n     datastream.getMetadata().put(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY, \"earliest\");\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898458", "bodyText": "The name start position unfortunately is confusing since it is a timestamp. Since you have set it to 100 I am guessing it is returning null? And so what happens when it returns null? What is the code flow when this happens?", "author": "DEEPTHIKORAT", "createdAt": "2020-12-21T19:49:04Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);\n+\n+    LOG.info(\"Sending second set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 100, 100);\n+\n+    //start\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    Datastream datastream = getDatastream(_broker, topic);\n+\n+    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.", "originalCommit": "2df71d42261f8e3eefffb82899b21daf417a2818", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMjgwNA==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546912804", "bodyText": "a) I copied and modfied testConsumeWithStartOffset, so not sure what this comment refers to, I removed it.\nb) it is not a timestamp, it is an offset. The metadata key is: DatastreamMetadataConstants.START_POSITION = \"system.start.position\"", "author": "shenodaguirguis", "createdAt": "2020-12-21T20:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMzk1Nw==", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546913957", "bodyText": "I see. So it does seem to be offset in the implementation and there are some other connectors storing time. I was guessing that is where the comment about offsetForTimestamp was coming from. Good to know it actually causes no issues.", "author": "DEEPTHIKORAT", "createdAt": "2020-12-21T20:26:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA=="}], "type": "inlineReview", "revised_code": {"commit": "ae7498afad70fdad4efa704519465d1471d20ff4", "chunk": "diff --git a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\nindex d29bd657..ef5498c9 100644\n--- a/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n+++ b/datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java\n\n@@ -210,10 +210,9 @@ public class TestKafkaConnectorTask extends BaseKafkaZkTest {\n     //start\n     MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n     Datastream datastream = getDatastream(_broker, topic);\n-\n-    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.\n+    // set system.start.position in the metadata\n     datastream.getMetadata().put(DatastreamMetadataConstants.START_POSITION, JsonUtils.toJson(startOffsets));\n-    // set auto.offset.reset strategy in metadata to ensure it is ignored in presence of start offsets\n+    // set system.auto.offset.reset strategy in metadata to ensure it is ignored in presence of start offsets\n     datastream.getMetadata().put(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY, \"earliest\");\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n"}}, {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4", "url": "https://github.com/linkedin/brooklin/commit/ae7498afad70fdad4efa704519465d1471d20ff4", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "committedDate": "2020-12-21T20:24:13Z", "type": "commit"}, {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4", "url": "https://github.com/linkedin/brooklin/commit/ae7498afad70fdad4efa704519465d1471d20ff4", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "committedDate": "2020-12-21T20:24:13Z", "type": "forcePushed"}]}