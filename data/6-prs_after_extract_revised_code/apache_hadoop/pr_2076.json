{"pr_number": 2076, "pr_title": "Hadoop 16961. ABFS: Adding metrics to AbfsInputStream", "pr_createdAt": "2020-06-16T07:45:28Z", "pr_url": "https://github.com/apache/hadoop/pull/2076", "timeline": [{"oid": "7d1f920f4c4d13f7271c115eca3dcd04384875f9", "url": "https://github.com/apache/hadoop/commit/7d1f920f4c4d13f7271c115eca3dcd04384875f9", "message": "HADOOP-16961. ABFS: Adding metrics to AbfsInputStream\n\nChange-Id: I034b771533b8314364a3762034439e323758ee09", "committedDate": "2020-06-14T03:15:24Z", "type": "commit"}, {"oid": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "url": "https://github.com/apache/hadoop/commit/eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "message": "HADOOP-16961. AbfsInputStreamStatistics via AbfsInputStreamContext and adding tests.", "committedDate": "2020-06-16T07:41:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MTkxNQ==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440791915", "bodyText": "needs check for stats == null", "author": "steveloughran", "createdAt": "2020-06-16T11:53:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -443,4 +488,26 @@ protected void setCachedSasToken(final CachedSASToken cachedSasToken) {\n     this.cachedSasToken = cachedSasToken;\n   }\n \n+  /**\n+   * Getter for AbfsInputStreamStatistics.\n+   *\n+   * @return an instance of AbfsInputStreamStatistics.\n+   */\n+  @VisibleForTesting\n+  public AbfsInputStreamStatistics getStreamStatistics() {\n+    return streamStatistics;\n+  }\n+\n+  /**\n+   * Get the statistics of the stream.\n+   * @return a string value.\n+   */\n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(super.toString());\n+    sb.append(\"AbfsInputStream@(\").append(this.hashCode()).append(\"){\");\n+    sb.append(streamStatistics.toString());", "originalCommit": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\nindex 6128ca77b9c..788dda8ef7d 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n\n@@ -505,9 +505,11 @@ public AbfsInputStreamStatistics getStreamStatistics() {\n   @Override\n   public String toString() {\n     final StringBuilder sb = new StringBuilder(super.toString());\n-    sb.append(\"AbfsInputStream@(\").append(this.hashCode()).append(\"){\");\n-    sb.append(streamStatistics.toString());\n-    sb.append(\"}\");\n+    if (streamStatistics != null) {\n+      sb.append(\"AbfsInputStream@(\").append(this.hashCode()).append(\"){\");\n+      sb.append(streamStatistics.toString());\n+      sb.append(\"}\");\n+    }\n     return sb.toString();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5MjMyNw==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440792327", "bodyText": "not needed; remove", "author": "steveloughran", "createdAt": "2020-06-16T11:54:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Interface for statistics for the AbfsInputStream.\n+ */\n+@InterfaceStability.Unstable\n+public interface AbfsInputStreamStatistics {\n+  /**\n+   * Seek backwards, incrementing the seek and backward seek counters.\n+   *\n+   * @param negativeOffset how far was the seek?\n+   *                       This is expected to be negative.\n+   */\n+  void seekBackwards(long negativeOffset);\n+\n+  /**\n+   * Record a forward seek, adding a seek operation, a forward\n+   * seek operation, and any bytes skipped.\n+   *\n+   * @param skipped number of bytes skipped by reading from the stream.\n+   *                If the seek was implemented by a close + reopen, set this to zero.\n+   */\n+  void seekForwards(long skipped);\n+\n+  /**\n+   * Record a forward or backward seek, adding a seek operation, a forward or\n+   * a backward seek operation, and number of bytes skipped.\n+   *\n+   * @param seekTo     seek to the position.\n+   * @param currentPos current position.\n+   */\n+  void seek(long seekTo, long currentPos);\n+\n+  /**\n+   * Increment the bytes read counter by the number of bytes;\n+   * no-op if the argument is negative.\n+   *\n+   * @param bytes number of bytes read.\n+   */\n+  void bytesRead(long bytes);\n+\n+  /**\n+   * Record the total bytes read from buffer.\n+   *\n+   * @param bytes number of bytes that are read from buffer.\n+   */\n+  void bytesReadFromBuffer(long bytes);\n+\n+  /**\n+   * Records the total number of seeks done in the buffer.\n+   */\n+  void seekInBuffer();\n+\n+  /**\n+   * A {@code read(byte[] buf, int off, int len)} operation has started.\n+   *\n+   * @param pos starting position of the read.\n+   * @param len length of bytes to read.\n+   */\n+  void readOperationStarted(long pos, long len);\n+\n+  /**\n+   * Records a successful remote read operation.\n+   */\n+  void remoteReadOperation();\n+\n+  /**\n+   * Makes the string of all the AbfsInputStream statistics.\n+   * @return the string with all the statistics.\n+   */\n+  @Override\n+  String toString();", "originalCommit": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5Mzg3Mw==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440793873", "bodyText": "close() the stream and ask for the stats again, to verify they are still readable\ncall toString on opened and closed streams.", "author": "steveloughran", "createdAt": "2020-06-16T11:57:25Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+\n+public class ITestAbfsInputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int OPERATIONS = 10;\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsInputStreamStatistics.class);\n+  private static final int ONE_MB = 1024 * 1024;\n+  private byte[] defBuffer = new byte[ONE_MB];\n+\n+  public ITestAbfsInputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Test to check the initial values of the AbfsInputStream statistics.\n+   */\n+  @Test\n+  public void testInitValues() throws IOException {\n+    describe(\"Testing the initial values of AbfsInputStream Statistics\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path initValuesPath = path(getMethodName());\n+    AbfsOutputStream outputStream = null;\n+    AbfsInputStream inputStream = null;\n+\n+    try {\n+\n+      outputStream = createAbfsOutputStreamWithFlushEnabled(fs, initValuesPath);\n+      inputStream = abfss.openFileForRead(initValuesPath, fs.getFsStatistics());\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) inputStream.getStreamStatistics();\n+\n+      checkInitValue(stats.getSeekOperations(), \"seekOps\");\n+      checkInitValue(stats.getForwardSeekOperations(), \"forwardSeekOps\");\n+      checkInitValue(stats.getBackwardSeekOperations(), \"backwardSeekOps\");\n+      checkInitValue(stats.getBytesRead(), \"bytesRead\");\n+      checkInitValue(stats.getBytesSkippedOnSeek(), \"bytesSkippedOnSeek\");\n+      checkInitValue(stats.getBytesBackwardsOnSeek(), \"bytesBackwardsOnSeek\");\n+      checkInitValue(stats.getSeekInBuffer(), \"seekInBuffer\");\n+      checkInitValue(stats.getReadOperations(), \"readOps\");\n+      checkInitValue(stats.getBytesReadFromBuffer(), \"bytesReadFromBuffer\");\n+      checkInitValue(stats.getRemoteReadOperations(), \"remoteReadOps\");\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, outputStream, inputStream);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics from seek operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testSeekStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from seek operations in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path seekStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, seekStatPath);\n+\n+      //Writing a default buffer in a file.\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(seekStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Writing 1MB buffer to the file, this would make the fCursor(Current\n+       * position of cursor) to the end of file.\n+       */\n+      int result = in.read(defBuffer, 0, ONE_MB);\n+      LOG.info(\"Result of read : {}\", result);\n+\n+      /*\n+       * Seeking to start of file and then back to end would result in a\n+       * backward and a forward seek respectively 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.seek(0);\n+        in.seek(ONE_MB);\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+      /*\n+       * seekOps - Since we are doing backward and forward seek OPERATIONS\n+       * times, total seeks would be 2 * OPERATIONS.\n+       *\n+       * backwardSeekOps - Since we are doing a backward seek inside a loop\n+       * for OPERATION times, total backward seeks would be OPERATIONS.\n+       *\n+       * forwardSeekOps - Since we are doing a forward seek inside a loop\n+       * for OPERATION times, total forward seeks would be OPERATIONS.\n+       *\n+       * bytesBackwardsOnSeek - Since we are doing backward seeks from end of\n+       * file in a ONE_MB file each time, this would mean the bytes from\n+       * backward seek would be OPERATIONS * ONE_MB. Since this is backward\n+       * seek this value is expected be to be negative.\n+       *\n+       * bytesSkippedOnSeek - Since, we move from start to end in seek, but\n+       * our fCursor(position of cursor) always remain at end of file, this\n+       * would mean no bytes were skipped on seek. Since, all forward seeks\n+       * are in buffer.\n+       *\n+       * seekInBuffer - Since all seeks were in buffer, the seekInBuffer\n+       * would be equal to 2 * OPERATIONS.\n+       *\n+       */\n+      assertEquals(\"Mismatch in seekOps value\", 2 * OPERATIONS,\n+          stats.getSeekOperations());\n+      assertEquals(\"Mismatch in backwardSeekOps value\", OPERATIONS,\n+          stats.getBackwardSeekOperations());\n+      assertEquals(\"Mismatch in forwardSeekOps value\", OPERATIONS,\n+          stats.getForwardSeekOperations());\n+      assertEquals(\"Mismatch in bytesBackwardsOnSeek value\",\n+          -1 * OPERATIONS * ONE_MB, stats.getBytesBackwardsOnSeek());\n+      assertEquals(\"Mismatch in bytesSkippedOnSeek value\",\n+          0, stats.getBytesSkippedOnSeek());\n+      assertEquals(\"Mismatch in seekInBuffer value\", 2 * OPERATIONS,\n+          stats.getSeekInBuffer());\n+", "originalCommit": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM5OTM4NA==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r441399384", "bodyText": "So, should I use toString() to basically verify these stats are readable? or should I assert them again?", "author": "mehakmeet", "createdAt": "2020-06-17T09:10:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5Mzg3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU1ODAxNg==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r441558016", "bodyText": "hmm. how about just calling toString, which queries them anyway -doesn't it?", "author": "steveloughran", "createdAt": "2020-06-17T13:47:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5Mzg3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU2MDE1NA==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r441560154", "bodyText": "Yes, it does.", "author": "mehakmeet", "createdAt": "2020-06-17T13:49:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5Mzg3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\nindex 39d0583b381..ebdb53036f3 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\n\n@@ -26,8 +26,10 @@\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamContext;\n import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n import org.apache.hadoop.io.IOUtils;\n \n public class ITestAbfsInputStreamStatistics\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NDUyNg==", "url": "https://github.com/apache/hadoop/pull/2076#discussion_r440794526", "bodyText": "close() the stream and ask for the stats again, to verify they are still readable\ncall toString on opened and closed streams.", "author": "steveloughran", "createdAt": "2020-06-16T11:58:37Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs;\n+\n+import java.io.IOException;\n+\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.io.IOUtils;\n+\n+public class ITestAbfsInputStreamStatistics\n+    extends AbstractAbfsIntegrationTest {\n+  private static final int OPERATIONS = 10;\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ITestAbfsInputStreamStatistics.class);\n+  private static final int ONE_MB = 1024 * 1024;\n+  private byte[] defBuffer = new byte[ONE_MB];\n+\n+  public ITestAbfsInputStreamStatistics() throws Exception {\n+  }\n+\n+  /**\n+   * Test to check the initial values of the AbfsInputStream statistics.\n+   */\n+  @Test\n+  public void testInitValues() throws IOException {\n+    describe(\"Testing the initial values of AbfsInputStream Statistics\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path initValuesPath = path(getMethodName());\n+    AbfsOutputStream outputStream = null;\n+    AbfsInputStream inputStream = null;\n+\n+    try {\n+\n+      outputStream = createAbfsOutputStreamWithFlushEnabled(fs, initValuesPath);\n+      inputStream = abfss.openFileForRead(initValuesPath, fs.getFsStatistics());\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) inputStream.getStreamStatistics();\n+\n+      checkInitValue(stats.getSeekOperations(), \"seekOps\");\n+      checkInitValue(stats.getForwardSeekOperations(), \"forwardSeekOps\");\n+      checkInitValue(stats.getBackwardSeekOperations(), \"backwardSeekOps\");\n+      checkInitValue(stats.getBytesRead(), \"bytesRead\");\n+      checkInitValue(stats.getBytesSkippedOnSeek(), \"bytesSkippedOnSeek\");\n+      checkInitValue(stats.getBytesBackwardsOnSeek(), \"bytesBackwardsOnSeek\");\n+      checkInitValue(stats.getSeekInBuffer(), \"seekInBuffer\");\n+      checkInitValue(stats.getReadOperations(), \"readOps\");\n+      checkInitValue(stats.getBytesReadFromBuffer(), \"bytesReadFromBuffer\");\n+      checkInitValue(stats.getRemoteReadOperations(), \"remoteReadOps\");\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, outputStream, inputStream);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics from seek operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testSeekStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from seek operations in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path seekStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, seekStatPath);\n+\n+      //Writing a default buffer in a file.\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(seekStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Writing 1MB buffer to the file, this would make the fCursor(Current\n+       * position of cursor) to the end of file.\n+       */\n+      int result = in.read(defBuffer, 0, ONE_MB);\n+      LOG.info(\"Result of read : {}\", result);\n+\n+      /*\n+       * Seeking to start of file and then back to end would result in a\n+       * backward and a forward seek respectively 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.seek(0);\n+        in.seek(ONE_MB);\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+      /*\n+       * seekOps - Since we are doing backward and forward seek OPERATIONS\n+       * times, total seeks would be 2 * OPERATIONS.\n+       *\n+       * backwardSeekOps - Since we are doing a backward seek inside a loop\n+       * for OPERATION times, total backward seeks would be OPERATIONS.\n+       *\n+       * forwardSeekOps - Since we are doing a forward seek inside a loop\n+       * for OPERATION times, total forward seeks would be OPERATIONS.\n+       *\n+       * bytesBackwardsOnSeek - Since we are doing backward seeks from end of\n+       * file in a ONE_MB file each time, this would mean the bytes from\n+       * backward seek would be OPERATIONS * ONE_MB. Since this is backward\n+       * seek this value is expected be to be negative.\n+       *\n+       * bytesSkippedOnSeek - Since, we move from start to end in seek, but\n+       * our fCursor(position of cursor) always remain at end of file, this\n+       * would mean no bytes were skipped on seek. Since, all forward seeks\n+       * are in buffer.\n+       *\n+       * seekInBuffer - Since all seeks were in buffer, the seekInBuffer\n+       * would be equal to 2 * OPERATIONS.\n+       *\n+       */\n+      assertEquals(\"Mismatch in seekOps value\", 2 * OPERATIONS,\n+          stats.getSeekOperations());\n+      assertEquals(\"Mismatch in backwardSeekOps value\", OPERATIONS,\n+          stats.getBackwardSeekOperations());\n+      assertEquals(\"Mismatch in forwardSeekOps value\", OPERATIONS,\n+          stats.getForwardSeekOperations());\n+      assertEquals(\"Mismatch in bytesBackwardsOnSeek value\",\n+          -1 * OPERATIONS * ONE_MB, stats.getBytesBackwardsOnSeek());\n+      assertEquals(\"Mismatch in bytesSkippedOnSeek value\",\n+          0, stats.getBytesSkippedOnSeek());\n+      assertEquals(\"Mismatch in seekInBuffer value\", 2 * OPERATIONS,\n+          stats.getSeekInBuffer());\n+\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, out, in);\n+    }\n+  }\n+\n+  /**\n+   * Test to check statistics value from read operation in AbfsInputStream.\n+   */\n+  @Test\n+  public void testReadStatistics() throws IOException {\n+    describe(\"Testing the values of statistics from read operation in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readStatPath = path(getMethodName());\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readStatPath);\n+\n+      /*\n+       * Writing 1MB buffer to the file.\n+       */\n+      out.write(defBuffer);\n+      out.hflush();\n+      in = abfss.openFileForRead(readStatPath, fs.getFsStatistics());\n+\n+      /*\n+       * Doing file read 10 times.\n+       */\n+      for (int i = 0; i < OPERATIONS; i++) {\n+        in.read();\n+      }\n+\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * bytesRead - Since each time a single byte is read, total\n+       * bytes read would be equal to OPERATIONS.\n+       *\n+       * readOps - Since each time read operation is performed OPERATIONS\n+       * times, total number of read operations would be equal to OPERATIONS.\n+       *\n+       * remoteReadOps - Only a single remote read operation is done. Hence,\n+       * total remote read ops is 1.\n+       *\n+       */\n+      assertEquals(\"Mismatch in bytesRead value\", OPERATIONS,\n+          stats.getBytesRead());\n+      assertEquals(\"Mismatch in readOps value\", OPERATIONS,\n+          stats.getReadOperations());\n+      assertEquals(\"Mismatch in remoteReadOps value\", 1,\n+          stats.getRemoteReadOperations());\n+", "originalCommit": "eee59b8d8fc92e1a6abc7cab4942fcbd02924586", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\nindex 39d0583b381..ebdb53036f3 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java\n\n@@ -26,8 +26,10 @@\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamContext;\n import org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl;\n import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation;\n import org.apache.hadoop.io.IOUtils;\n \n public class ITestAbfsInputStreamStatistics\n"}}, {"oid": "e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "url": "https://github.com/apache/hadoop/commit/e07bd4a942b6f4b307fdc75a7bd3d31f0b0bad0f", "message": "HADOOP-16961. Review Comments, Statistics Readability with closed stream and null statistics test", "committedDate": "2020-06-22T07:10:43Z", "type": "commit"}, {"oid": "da482cafc62570ca84f55711d6a692d05cb31255", "url": "https://github.com/apache/hadoop/commit/da482cafc62570ca84f55711d6a692d05cb31255", "message": "HADOOP-16961. null statistics test with no errors and bytesReadFromBuffer at correct place.", "committedDate": "2020-06-26T04:53:19Z", "type": "commit"}]}