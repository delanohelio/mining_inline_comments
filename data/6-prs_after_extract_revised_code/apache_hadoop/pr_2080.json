{"pr_number": 2080, "pr_title": "HDFS-15417. RBF: Get the datanode report from cache for federation WebHDFS operations", "pr_createdAt": "2020-06-17T21:53:20Z", "pr_url": "https://github.com/apache/hadoop/pull/2080", "timeline": [{"oid": "a1ad88e6d458a14be26736183894ed0ed2a31017", "url": "https://github.com/apache/hadoop/commit/a1ad88e6d458a14be26736183894ed0ed2a31017", "message": "Update RouterWebHdfsMethods.java", "committedDate": "2020-06-17T21:45:04Z", "type": "commit"}, {"oid": "3369ba7b5eea4355487d8961a1f89fee15650407", "url": "https://github.com/apache/hadoop/commit/3369ba7b5eea4355487d8961a1f89fee15650407", "message": "Update RouterWebHdfsMethods.java", "committedDate": "2020-06-17T21:48:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NDI3NQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442384275", "bodyText": "nit: this is always true.", "author": "sunchao", "createdAt": "2020-06-18T17:20:55Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -502,12 +479,27 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n+          if (excludeDatanodes != null) {\n+            Collection<String> collection =\n+                getTrimmedStringCollection(excludeDatanodes);\n+            dns = getDatanodeReport(router);\n+            for (DatanodeInfo dn : dns) {\n+              if (collection.contains(dn.getName())) {\n+                excludes.add(dn);\n+              }\n+            }\n+          }\n+          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n+    if (dns == null) {", "originalCommit": "3369ba7b5eea4355487d8961a1f89fee15650407", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyOTQzMA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443029430", "bodyText": "Thanks. I follow the previous logic now with cached DN report", "author": "NickyYe", "createdAt": "2020-06-19T20:24:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NDI3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\nindex a1dcf237d86..39f06a3b66f 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n\n@@ -479,27 +495,12 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n-          if (excludeDatanodes != null) {\n-            Collection<String> collection =\n-                getTrimmedStringCollection(excludeDatanodes);\n-            dns = getDatanodeReport(router);\n-            for (DatanodeInfo dn : dns) {\n-              if (collection.contains(dn.getName())) {\n-                excludes.add(dn);\n-              }\n-            }\n-          }\n-          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n-    if (dns == null) {\n-      dns = getDatanodeReport(router);\n-    }\n-    \n     return getRandomDatanode(dns, excludes);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NTEyOQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442385129", "bodyText": "In this case, excludes is always empty. We need to compute it using the dns obtained.", "author": "sunchao", "createdAt": "2020-06-18T17:22:21Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -502,12 +479,27 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n+          if (excludeDatanodes != null) {\n+            Collection<String> collection =\n+                getTrimmedStringCollection(excludeDatanodes);\n+            dns = getDatanodeReport(router);\n+            for (DatanodeInfo dn : dns) {\n+              if (collection.contains(dn.getName())) {\n+                excludes.add(dn);\n+              }\n+            }\n+          }\n+          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n+    if (dns == null) {\n+      dns = getDatanodeReport(router);\n+    }\n+    \n     return getRandomDatanode(dns, excludes);", "originalCommit": "3369ba7b5eea4355487d8961a1f89fee15650407", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\nindex a1dcf237d86..39f06a3b66f 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n\n@@ -479,27 +495,12 @@ private DatanodeInfo chooseDatanode(final Router router,\n         final LocatedBlocks locations = cp.getBlockLocations(path, offset, 1);\n         final int count = locations.locatedBlockCount();\n         if (count > 0) {\n-          if (excludeDatanodes != null) {\n-            Collection<String> collection =\n-                getTrimmedStringCollection(excludeDatanodes);\n-            dns = getDatanodeReport(router);\n-            for (DatanodeInfo dn : dns) {\n-              if (collection.contains(dn.getName())) {\n-                excludes.add(dn);\n-              }\n-            }\n-          }\n-          \n           LocatedBlock location0 = locations.get(0);\n           return bestNode(location0.getLocations(), excludes);\n         }\n       }\n     }\n \n-    if (dns == null) {\n-      dns = getDatanodeReport(router);\n-    }\n-    \n     return getRandomDatanode(dns, excludes);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4NTMxOQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442385319", "bodyText": "nit: leave a blank line before the @param line.", "author": "sunchao", "createdAt": "2020-06-18T17:22:38Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -564,4 +556,28 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n+  \n+    /**\n+   * Get the datanode report from all namespaces that are registered\n+   * and active in the federation.\n+   * @param router Router from which to get the report.", "originalCommit": "3369ba7b5eea4355487d8961a1f89fee15650407", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\nindex a1dcf237d86..39f06a3b66f 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n\n@@ -556,28 +557,4 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n-  \n-    /**\n-   * Get the datanode report from all namespaces that are registered\n-   * and active in the federation.\n-   * @param router Router from which to get the report.\n-   * @return List of datanodes.\n-   * @throws IOException If it cannot get the RPC Server.\n-   */\n-  private static DatanodeInfo[] getDatanodeReport(\n-      final Router router) throws IOException {\n-    // We need to get the DNs as a privileged user\n-    final RouterRpcServer rpcServer = getRPCServer(router);\n-    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n-    RouterRpcServer.setCurrentUser(loginUser);\n-\n-    try {\n-      return rpcServer.getDatanodeReport(DatanodeReportType.LIVE);\n-    } catch (IOException e) {\n-      LOG.error(\"Cannot get the datanodes from the RPC server\", e);\n-    } finally {\n-      // Reset ugi to remote user for remaining operations.\n-      RouterRpcServer.resetCurrentUser();\n-    }\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4ODkxMQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r442388911", "bodyText": "should we wrap these in the try .. catch block? and what is the return value if rpcServer.getDatanodeReport fail? null?", "author": "sunchao", "createdAt": "2020-06-18T17:28:43Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -564,4 +556,28 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n+  \n+    /**\n+   * Get the datanode report from all namespaces that are registered\n+   * and active in the federation.\n+   * @param router Router from which to get the report.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the RPC Server.\n+   */\n+  private static DatanodeInfo[] getDatanodeReport(\n+      final Router router) throws IOException {\n+    // We need to get the DNs as a privileged user\n+    final RouterRpcServer rpcServer = getRPCServer(router);", "originalCommit": "3369ba7b5eea4355487d8961a1f89fee15650407", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\nindex a1dcf237d86..39f06a3b66f 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java\n\n@@ -556,28 +557,4 @@ public Credentials createCredentials(\n         renewer != null? renewer: ugi.getShortUserName());\n     return c;\n   }\n-  \n-    /**\n-   * Get the datanode report from all namespaces that are registered\n-   * and active in the federation.\n-   * @param router Router from which to get the report.\n-   * @return List of datanodes.\n-   * @throws IOException If it cannot get the RPC Server.\n-   */\n-  private static DatanodeInfo[] getDatanodeReport(\n-      final Router router) throws IOException {\n-    // We need to get the DNs as a privileged user\n-    final RouterRpcServer rpcServer = getRPCServer(router);\n-    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n-    RouterRpcServer.setCurrentUser(loginUser);\n-\n-    try {\n-      return rpcServer.getDatanodeReport(DatanodeReportType.LIVE);\n-    } catch (IOException e) {\n-      LOG.error(\"Cannot get the datanodes from the RPC server\", e);\n-    } finally {\n-      // Reset ugi to remote user for remaining operations.\n-      RouterRpcServer.resetCurrentUser();\n-    }\n-  }\n }\n"}}, {"oid": "712835aa8d579f6b9d2c3b4d4798b393d2941657", "url": "https://github.com/apache/hadoop/commit/712835aa8d579f6b9d2c3b4d4798b393d2941657", "message": "Update RouterWebHdfsMethods.java", "committedDate": "2020-06-19T20:15:50Z", "type": "commit"}, {"oid": "53723474f41259315f1f9919b8c989b73a8b20a3", "url": "https://github.com/apache/hadoop/commit/53723474f41259315f1f9919b8c989b73a8b20a3", "message": "Update RouterRpcServer.java", "committedDate": "2020-06-19T20:21:58Z", "type": "commit"}, {"oid": "66b2b33ab275a6819ca1494ec3b272392134d5e1", "url": "https://github.com/apache/hadoop/commit/66b2b33ab275a6819ca1494ec3b272392134d5e1", "message": "Update RouterRpcServer.java", "committedDate": "2020-06-19T20:45:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3ODUyOA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443078528", "bodyText": "This will cause compilation error. Also I'm wondering whether it makes sense to move the DN cache logic from  NamenodeBeanMetrics to here and have the former to depend on this. This way we don't have to keep two copies of cache.", "author": "sunchao", "createdAt": "2020-06-19T23:40:33Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -18,6 +18,8 @@\n package org.apache.hadoop.hdfs.server.federation.router;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION;\n+import static org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics.DN_REPORT_CACHE_EXPIRE;", "originalCommit": "66b2b33ab275a6819ca1494ec3b272392134d5e1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzgxOTQxNQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443819415", "bodyText": "Thanks for catching this. I was working on an old branch, now it is fixed. I prefer to move the NamenodeBeanMetrics logic by a separate change. Let's make this change cohesive. NamenodeBeanMetrics cached report needs a different API or at least some extra work since it needs a String.", "author": "NickyYe", "createdAt": "2020-06-22T20:52:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3ODUyOA=="}], "type": "inlineReview", "revised_code": {"commit": "801216bc6b2ab05b0be9f4fc82bb6714c8120200", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\nindex e74c04721d5..7af5e3fde2c 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n\n@@ -18,8 +18,6 @@\n package org.apache.hadoop.hdfs.server.federation.router;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION;\n-import static org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics.DN_REPORT_CACHE_EXPIRE;\n-import static org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics.DN_REPORT_CACHE_EXPIRE_DEFAULT;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.DFS_ROUTER_HANDLER_COUNT_DEFAULT;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.DFS_ROUTER_HANDLER_COUNT_KEY;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.DFS_ROUTER_HANDLER_QUEUE_SIZE_DEFAULT;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3OTYwMA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443079600", "bodyText": "Question from me and not quite related to this JIRA: seems this is getting DNs for all the clusters while in the CREATE case we should only choose DN from a particular sub-cluster. Where is this logic implemented?", "author": "sunchao", "createdAt": "2020-06-19T23:48:44Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java", "diffHunk": "@@ -454,19 +454,12 @@ private URI redirectURI(final Router router, final UserGroupInformation ugi,\n   private DatanodeInfo chooseDatanode(final Router router,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final String excludeDatanodes) throws IOException {\n-    // We need to get the DNs as a privileged user\n     final RouterRpcServer rpcServer = getRPCServer(router);\n-    UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n-    RouterRpcServer.setCurrentUser(loginUser);\n-\n     DatanodeInfo[] dns = null;\n     try {\n-      dns = rpcServer.getDatanodeReport(DatanodeReportType.LIVE);\n+      dns = rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);", "originalCommit": "66b2b33ab275a6819ca1494ec3b272392134d5e1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTgzNA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443081834", "bodyText": "You are correct. I found this issue also. We could be redirected to a datanode from a different subcluster where the path is acutually mounted, but the file would be written to the correct subcluster. We may need to open a new JIRA for this.", "author": "NickyYe", "createdAt": "2020-06-20T00:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3OTYwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA5MTM5OA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r443091398", "bodyText": "Thanks. I filed https://issues.apache.org/jira/browse/HDFS-15423 to track this.", "author": "sunchao", "createdAt": "2020-06-20T01:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3OTYwMA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "801216bc6b2ab05b0be9f4fc82bb6714c8120200", "url": "https://github.com/apache/hadoop/commit/801216bc6b2ab05b0be9f4fc82bb6714c8120200", "message": "Update RouterRpcServer.java", "committedDate": "2020-06-22T19:01:57Z", "type": "commit"}, {"oid": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "url": "https://github.com/apache/hadoop/commit/6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "message": "Remove whitespace-eol", "committedDate": "2020-06-22T20:47:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDQ3Mg==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664472", "bodyText": "Hmm, have you considered using\nthis.dnCache = CacheBuilder.newBuilder()\n         .refreshAfterWrite(dnCacheExpire, TimeUnit.MILLISECONDS)\n         .build(new DatanodeReportCacheLoader());\nThis will also automatically refresh the caches. Also it only refreshes a key iff 1) it becomes stale, and 2) there is a request on it. So this will save some calls for those infrequent DN report types.", "author": "sunchao", "createdAt": "2020-06-24T06:08:05Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -361,6 +380,23 @@ public RouterRpcServer(Configuration configuration, Router router,\n     this.nnProto = new RouterNamenodeProtocol(this);\n     this.clientProto = new RouterClientProtocol(conf, this);\n     this.routerProto = new RouterUserProtocol(this);\n+\n+    long dnCacheExpire = conf.getTimeDuration(\n+        DN_REPORT_CACHE_EXPIRE,\n+        DN_REPORT_CACHE_EXPIRE_MS_DEFAULT, TimeUnit.MILLISECONDS);\n+    this.dnCache = CacheBuilder.newBuilder()\n+        .build(new DatanodeReportCacheLoader());\n+\n+    // Actively refresh the dn cache in a configured interval\n+    Executors", "originalCommit": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2ODE3OA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444668178", "bodyText": "Yes. The point here is, with refreshAfterWrite, you will only get the previously value in this call, but the result will be refreshed in the background for next retreival. If we only have 1 request per hour, you will only get the datanode report 1 hour ago, unless you make the call sync, which is slow. Given it is already a background thread and not that heavy with an interval, current design is better.", "author": "NickyYe", "createdAt": "2020-06-24T06:18:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDQ3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY3MzExNw==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444673117", "bodyText": "I see. Makes sense.", "author": "sunchao", "createdAt": "2020-06-24T06:31:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDQ3Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDU5NA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664594", "bodyText": "nit: this can be package-private?", "author": "sunchao", "createdAt": "2020-06-24T06:08:29Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -868,6 +904,50 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n     return clientProto.getDatanodeReport(type);\n   }\n \n+  /**\n+   * Get the datanode report from cache.\n+   *\n+   * @param type Type of the datanode.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the report.\n+   */\n+  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)", "originalCommit": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33f104b75cdb61cd453c07cc356116e4231edb42", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\nindex a358b6e0a97..e5f7796f5e0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n\n@@ -911,7 +909,7 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n    * @return List of datanodes.\n    * @throws IOException If it cannot get the report.\n    */\n-  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n+  DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n       throws IOException {\n     try {\n       DatanodeInfo[] dns = this.dnCache.get(type);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NDY2NQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444664665", "bodyText": "nit: space after {", "author": "sunchao", "createdAt": "2020-06-24T06:08:44Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -868,6 +904,50 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n     return clientProto.getDatanodeReport(type);\n   }\n \n+  /**\n+   * Get the datanode report from cache.\n+   *\n+   * @param type Type of the datanode.\n+   * @return List of datanodes.\n+   * @throws IOException If it cannot get the report.\n+   */\n+  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n+      throws IOException {\n+    try {\n+      DatanodeInfo[] dns = this.dnCache.get(type);\n+      if (dns == null) {\n+        LOG.debug(\"Get null DN report from cache\");\n+        dns = getCachedDatanodeReportImpl(type);\n+        this.dnCache.put(type, dns);\n+      }\n+      return dns;\n+    } catch (ExecutionException e) {\n+      LOG.error(\"Cannot get the DN report for {}\", type, e);\n+      Throwable cause = e.getCause();\n+      if (cause instanceof IOException) {\n+        throw (IOException) cause;\n+      } else {\n+        throw new IOException(cause);\n+      }\n+    }\n+  }\n+\n+  private DatanodeInfo[] getCachedDatanodeReportImpl\n+      (final DatanodeReportType type) throws IOException{", "originalCommit": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33f104b75cdb61cd453c07cc356116e4231edb42", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\nindex a358b6e0a97..e5f7796f5e0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n\n@@ -911,7 +909,7 @@ public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n    * @return List of datanodes.\n    * @throws IOException If it cannot get the report.\n    */\n-  public DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n+  DatanodeInfo[] getCachedDatanodeReport(DatanodeReportType type)\n       throws IOException {\n     try {\n       DatanodeInfo[] dns = this.dnCache.get(type);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NTE1NQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444665155", "bodyText": "hmm can we just use:\nexecutorService = MoreExecutors.listeningDecorator(\n    Executors.newSingleThreadExecutor());\n?", "author": "sunchao", "createdAt": "2020-06-24T06:10:10Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -1748,4 +1828,58 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n   public String[] getGroupsForUser(String user) throws IOException {\n     return routerProto.getGroupsForUser(user);\n   }\n-}\n\\ No newline at end of file\n+\n+  /**\n+   * Deals with loading datanode report into the cache and refresh.\n+   */\n+  private class DatanodeReportCacheLoader\n+      extends CacheLoader<DatanodeReportType, DatanodeInfo[]> {\n+\n+    private ListeningExecutorService executorService;\n+\n+    DatanodeReportCacheLoader() {\n+      ThreadFactory threadFactory = new ThreadFactoryBuilder()", "originalCommit": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33f104b75cdb61cd453c07cc356116e4231edb42", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\nindex a358b6e0a97..e5f7796f5e0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n\n@@ -1843,19 +1841,8 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n           .setDaemon(true)\n           .build();\n \n-      // Only use 1 thread to refresh cache.\n-      // With coreThreadCount == maxThreadCount we effectively\n-      // create a fixed size thread pool. As allowCoreThreadTimeOut\n-      // has been set, all threads will die after 60 seconds of non use.\n-      ThreadPoolExecutor parentExecutor = new ThreadPoolExecutor(\n-          1,\n-          1,\n-          60,\n-          TimeUnit.SECONDS,\n-          new LinkedBlockingQueue<Runnable>(),\n-          threadFactory);\n-      parentExecutor.allowCoreThreadTimeOut(true);\n-      executorService = MoreExecutors.listeningDecorator(parentExecutor);\n+      executorService = MoreExecutors.listeningDecorator(\n+          Executors.newSingleThreadExecutor(threadFactory));\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NTUyMQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444665521", "bodyText": "nit: variable listenableFuture is redundant - you can just return from submit call.", "author": "sunchao", "createdAt": "2020-06-24T06:11:16Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java", "diffHunk": "@@ -1748,4 +1828,58 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n   public String[] getGroupsForUser(String user) throws IOException {\n     return routerProto.getGroupsForUser(user);\n   }\n-}\n\\ No newline at end of file\n+\n+  /**\n+   * Deals with loading datanode report into the cache and refresh.\n+   */\n+  private class DatanodeReportCacheLoader\n+      extends CacheLoader<DatanodeReportType, DatanodeInfo[]> {\n+\n+    private ListeningExecutorService executorService;\n+\n+    DatanodeReportCacheLoader() {\n+      ThreadFactory threadFactory = new ThreadFactoryBuilder()\n+          .setNameFormat(\"DatanodeReport-Cache-Reload\")\n+          .setDaemon(true)\n+          .build();\n+\n+      // Only use 1 thread to refresh cache.\n+      // With coreThreadCount == maxThreadCount we effectively\n+      // create a fixed size thread pool. As allowCoreThreadTimeOut\n+      // has been set, all threads will die after 60 seconds of non use.\n+      ThreadPoolExecutor parentExecutor = new ThreadPoolExecutor(\n+          1,\n+          1,\n+          60,\n+          TimeUnit.SECONDS,\n+          new LinkedBlockingQueue<Runnable>(),\n+          threadFactory);\n+      parentExecutor.allowCoreThreadTimeOut(true);\n+      executorService = MoreExecutors.listeningDecorator(parentExecutor);\n+    }\n+\n+    @Override\n+    public DatanodeInfo[] load(DatanodeReportType type) throws Exception {\n+      return getCachedDatanodeReportImpl(type);\n+    }\n+\n+    /**\n+     * Override the reload method to provide an asynchronous implementation,\n+     * so that the query will not be slowed down by the cache refresh. It\n+     * will return the old cache value and schedule a background refresh.\n+     */\n+    @Override\n+    public ListenableFuture<DatanodeInfo[]> reload(\n+        final DatanodeReportType type, DatanodeInfo[] oldValue)\n+        throws Exception {\n+      ListenableFuture<DatanodeInfo[]> listenableFuture =", "originalCommit": "6bbbf46eb96ab315daf6d00d87c716e9543d04b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4NTcwMg==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r444685702", "bodyText": "Thank you for the comments. I've addressed all of them.", "author": "NickyYe", "createdAt": "2020-06-24T07:02:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY2NTUyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "33f104b75cdb61cd453c07cc356116e4231edb42", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\nindex a358b6e0a97..e5f7796f5e0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java\n\n@@ -1843,19 +1841,8 @@ public void refreshSuperUserGroupsConfiguration() throws IOException {\n           .setDaemon(true)\n           .build();\n \n-      // Only use 1 thread to refresh cache.\n-      // With coreThreadCount == maxThreadCount we effectively\n-      // create a fixed size thread pool. As allowCoreThreadTimeOut\n-      // has been set, all threads will die after 60 seconds of non use.\n-      ThreadPoolExecutor parentExecutor = new ThreadPoolExecutor(\n-          1,\n-          1,\n-          60,\n-          TimeUnit.SECONDS,\n-          new LinkedBlockingQueue<Runnable>(),\n-          threadFactory);\n-      parentExecutor.allowCoreThreadTimeOut(true);\n-      executorService = MoreExecutors.listeningDecorator(parentExecutor);\n+      executorService = MoreExecutors.listeningDecorator(\n+          Executors.newSingleThreadExecutor(threadFactory));\n     }\n \n     @Override\n"}}, {"oid": "33f104b75cdb61cd453c07cc356116e4231edb42", "url": "https://github.com/apache/hadoop/commit/33f104b75cdb61cd453c07cc356116e4231edb42", "message": "Address comments", "committedDate": "2020-06-24T07:01:23Z", "type": "commit"}, {"oid": "8970a0ed39d645fe47c2ec31ce5a39a490f7989e", "url": "https://github.com/apache/hadoop/commit/8970a0ed39d645fe47c2ec31ce5a39a490f7989e", "message": "Fix checkstyle", "committedDate": "2020-06-24T18:00:20Z", "type": "commit"}, {"oid": "7eeb2b527dbce451e3006b5f3b0698132ad8b2ad", "url": "https://github.com/apache/hadoop/commit/7eeb2b527dbce451e3006b5f3b0698132ad8b2ad", "message": "Add testGetCachedDatanodeReport", "committedDate": "2020-07-05T08:19:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkwNzE2NQ==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r449907165", "bodyText": "I think we need to clear state after the test case so that it won't affect others like testNamenodeMetrics.\nAlso long line.", "author": "sunchao", "createdAt": "2020-07-05T18:48:49Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "diffHunk": "@@ -1777,6 +1777,43 @@ public void testgetGroupsForUser() throws IOException {\n     assertArrayEquals(group, result);\n   }\n \n+  @Test\n+  public void testGetCachedDatanodeReport() throws Exception {\n+    final DatanodeInfo[] datanodeReport =\n+        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+\n+    // We should have 12 nodes in total\n+    assertEquals(12, datanodeReport.length);\n+\n+    // We should be caching this information\n+    DatanodeInfo[] datanodeReport1 =\n+        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+    assertArrayEquals(datanodeReport1, datanodeReport);\n+\n+    // Add one datanode\n+    getCluster().getCluster().startDataNodes(getCluster().getCluster().getConfiguration(0),", "originalCommit": "7eeb2b527dbce451e3006b5f3b0698132ad8b2ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDAzNjE2NA==", "url": "https://github.com/apache/hadoop/pull/2080#discussion_r450036164", "bodyText": "Done. Thanks. @sunchao", "author": "NickyYe", "createdAt": "2020-07-06T07:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkwNzE2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4d4b9ac945f5983c7d7bb9f5295594b39959980", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\nindex 05254358a9a..154fd01369d 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\n\n@@ -1779,20 +1786,22 @@ public void testgetGroupsForUser() throws IOException {\n \n   @Test\n   public void testGetCachedDatanodeReport() throws Exception {\n+    RouterRpcServer rpcServer = router.getRouter().getRpcServer();\n     final DatanodeInfo[] datanodeReport =\n-        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+        rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\n \n     // We should have 12 nodes in total\n     assertEquals(12, datanodeReport.length);\n \n     // We should be caching this information\n     DatanodeInfo[] datanodeReport1 =\n-        routerProtocol.getDatanodeReport(DatanodeReportType.ALL);\n+        rpcServer.getCachedDatanodeReport(DatanodeReportType.LIVE);\n     assertArrayEquals(datanodeReport1, datanodeReport);\n \n     // Add one datanode\n-    getCluster().getCluster().startDataNodes(getCluster().getCluster().getConfiguration(0),\n-        1, true, null, null, null);\n+    MiniDFSCluster cluster = getCluster().getCluster();\n+    cluster.startDataNodes(\n+        cluster.getConfiguration(0), 1, true, null, null, null);\n \n     // We wait until the cached value is updated\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n"}}, {"oid": "f4d4b9ac945f5983c7d7bb9f5295594b39959980", "url": "https://github.com/apache/hadoop/commit/f4d4b9ac945f5983c7d7bb9f5295594b39959980", "message": "Update TestRouterRpc.java", "committedDate": "2020-07-06T01:38:54Z", "type": "commit"}, {"oid": "dfb68119968e1601c7e0dbc0e7701f49d37c88a9", "url": "https://github.com/apache/hadoop/commit/dfb68119968e1601c7e0dbc0e7701f49d37c88a9", "message": "Update TestRouterRpc.java", "committedDate": "2020-07-06T05:54:51Z", "type": "commit"}]}