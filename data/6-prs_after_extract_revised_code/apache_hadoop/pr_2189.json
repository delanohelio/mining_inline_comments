{"pr_number": 2189, "pr_title": "HDFS-15025. Applying NVDIMM storage media to HDFS", "pr_createdAt": "2020-08-04T03:23:29Z", "pr_url": "https://github.com/apache/hadoop/pull/2189", "timeline": [{"oid": "9f6dca326d6887ddcd2368a40a8f689edbb74153", "url": "https://github.com/apache/hadoop/commit/9f6dca326d6887ddcd2368a40a8f689edbb74153", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.", "committedDate": "2020-08-04T09:02:39Z", "type": "forcePushed"}, {"oid": "a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "url": "https://github.com/apache/hadoop/commit/a1caf08b54a05c7891d6f2b4806965cee01c2cd5", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang yywangyayun@163.com", "committedDate": "2020-08-04T09:24:30Z", "type": "forcePushed"}, {"oid": "b8286dc50be8f0335a7a68e937d598758d47335a", "url": "https://github.com/apache/hadoop/commit/b8286dc50be8f0335a7a68e937d598758d47335a", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>", "committedDate": "2020-08-04T09:39:57Z", "type": "forcePushed"}, {"oid": "bb02bd8718d81805aeba6b758772c326037bd77a", "url": "https://github.com/apache/hadoop/commit/bb02bd8718d81805aeba6b758772c326037bd77a", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>", "committedDate": "2020-08-20T03:38:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r475372926", "bodyText": "nit: I see other variables are using lower case, could we change this name to allnvdimmId", "author": "liuml07", "createdAt": "2020-08-24T06:40:28Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java", "diffHunk": "@@ -63,6 +63,12 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n+    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();", "originalCommit": "bb02bd8718d81805aeba6b758772c326037bd77a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjEwOTI3Mw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476109273", "bodyText": "\u201callnvdimmId\u201d is more applicable in the situation.", "author": "YaYun-Wang", "createdAt": "2020-08-25T03:20:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjE0NDk2NQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476144965", "bodyText": "The overall change looks good to me, thanks! I will finish the review of testing in 1/2 days and provide more input.\nI also will check which isTransient() will need to be replaced with isRAM(). It seems case by case for all usages. One simple question is for this NVDIMM storage type, we save the checksum file right?\n\nThe calculation of checksum in hadoop is the responsibility of clients and datanodes, each storage media including NVDIMM participates in the checksum of data.", "author": "YaYun-Wang", "createdAt": "2020-08-25T04:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjIxNDk3MA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476214970", "bodyText": "Yes I agree the checksum calculation and read is built-in hadoop. I'm thinking of more about: do we need checksum for this storage type? For example, the RAM_DISK does not need checksum as far as I remember. This is RAM, but this also survives the service restarts, so checksum makes sense for data integrity. The code so far looks good regarding this.", "author": "liuml07", "createdAt": "2020-08-25T06:48:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI2ODE5Ng==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477268196", "bodyText": "Yes I agree the checksum calculation and read is built-in hadoop. I'm thinking of more about: do we need checksum for this storage type? For example, the RAM_DISK does not need checksum as far as I remember. This is RAM, but this also survives the service restarts, so checksum makes sense for data integrity. The code so far looks good regarding this.\n\nYes, NVDIMM, like ordinary persistent storage type, such as DISK, SSD, etc. requires  checksum too.", "author": "YaYun-Wang", "createdAt": "2020-08-26T12:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM3MjkyNg=="}], "type": "inlineReview", "revised_code": {"commit": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java\nindex 28a538f8815..7151934c652 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java\n\n@@ -63,8 +63,8 @@ public static BlockStoragePolicySuite createDefaultSuite(\n         new StorageType[]{StorageType.DISK},\n         new StorageType[]{StorageType.DISK},\n         true);    // Cannot be changed on regular files, but inherited.\n-    final byte allNVDIMMId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();\n-    policies[allNVDIMMId] = new BlockStoragePolicy(allNVDIMMId,\n+    final byte allnvdimmId = HdfsConstants.StoragePolicy.ALL_NVDIMM.value();\n+    policies[allnvdimmId] = new BlockStoragePolicy(allnvdimmId,\n         HdfsConstants.StoragePolicy.ALL_NVDIMM.name(),\n         new StorageType[]{StorageType.NVDIMM},\n         new StorageType[]{StorageType.DISK},\n"}}, {"oid": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "url": "https://github.com/apache/hadoop/commit/84c30785e64d6419d702c468a0a6b22b6c855bc6", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>", "committedDate": "2020-08-25T08:22:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476644846", "bodyText": "will RAM_DISK and NVDIMM co-exist..? if co-exist's, why can't we name NVDIM itself..?", "author": "brahmareddybattula", "createdAt": "2020-08-25T18:13:14Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),\n+  SSD(false, false),\n+  DISK(false, false),\n+  ARCHIVE(false, false),\n+  PROVIDED(false, false);\n \n   private final boolean isTransient;\n+  private final boolean isRAM;\n \n   public static final StorageType DEFAULT = DISK;\n \n   public static final StorageType[] EMPTY_ARRAY = {};\n \n   private static final StorageType[] VALUES = values();\n \n-  StorageType(boolean isTransient) {\n+  StorageType(boolean isTransient, boolean isRAM) {\n     this.isTransient = isTransient;\n+    this.isRAM = isRAM;\n   }\n \n   public boolean isTransient() {\n     return isTransient;\n   }\n \n+  public boolean isRAM() {\n+    return isRAM;\n+  }", "originalCommit": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY3OTk3Nw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476679977", "bodyText": "why can't we name NVDIM itself..?\n\nI don't think I get the question totally. Yes they can co-exist. You can have some folders being RAM_DISK storage and some folders being NVDIMM storage. RAM_DISK will get data lost after process restart, so it is transient. NVDIMM is not transient.", "author": "liuml07", "createdAt": "2020-08-25T19:16:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0OTYyMw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482249623", "bodyText": "As this will be co-exists. I was suggesting to change the method name to something like \"isNvdimm\" which was revlant here", "author": "brahmareddybattula", "createdAt": "2020-09-02T17:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM0MTIyOA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482341228", "bodyText": "No. Both RAM_DISK and NVDIMM will return true here.", "author": "liuml07", "createdAt": "2020-09-02T19:30:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEyNTYyMw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483125623", "bodyText": "Balancer and mover will not move the blocks based on the isTransient ( they call getMovableTypes(..))..The blocks which are in NVDIMM shouldn't moved I feel(as this also exists in RAM and no need to move),but as per this change it will move.", "author": "brahmareddybattula", "createdAt": "2020-09-03T16:59:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzE5NDEyNQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483194125", "bodyText": "Oh, I was thinking that allowing Balancer to move the NVDIMM data is by design since they are not volatile. But if that is not case, then we can update Balancer code by replacing isTransient() call with isRAM() call. Not sure if this makes more sense?", "author": "liuml07", "createdAt": "2020-09-03T19:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQxMDE4Mw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483410183", "bodyText": "NVDIMM is special RAM, the data above can be stored persistently. It can be regarded as a general hardware device. We don't have to consider what storage type it is, balancer and mover can be applied on NVDIMM, therefore, I think it is better to use isRAM to determine whether to use mover . In addition, neither RAM nor nvdimm need FsDatasetCache, and isTransient() used to determine whether FsDatasetCache is needed", "author": "YaYun-Wang", "createdAt": "2020-09-04T06:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQxNDc1NQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483414755", "bodyText": "neither RAM nor nvdimm need FsDatasetCache, and isTransient() used to determine whether FsDatasetCache is needed\n\nI think we have agreed on this.\n\nI think it is better to use isRAM to determine whether to use mover\n\nSo just to be clear, if we use isRAM() to determine, both RAM_DISK and NVDIMM will return true and thus will simply disable Balancer and Mover. I was proposing that only when we do not want to move those NVDIMM data replicas around (disk/node) ever. Is that the design here? I'm fine with that but I think this contradicts with the statement that \"It can be regarded as a general hardware device. \"", "author": "liuml07", "createdAt": "2020-09-04T06:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzUxNTUyMQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483515521", "bodyText": "Sorry, I made a big mistake in the above reply. Our design idea is: NVDIMM supports mover and balancer, and isTransient()  applied in the case. isRAM() is only used to \u201cFsDatasetCache\u201d judgment. So the current code is reasonable, i think it\u2018s not necessary to modify the code.", "author": "YaYun-Wang", "createdAt": "2020-09-04T09:54:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc4MDc1OA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r483780758", "bodyText": "Thanks for clarification, @YaYun-Wang I think now we both are on the same page. @brahmareddybattula Does this make sense to you? Thanks", "author": "liuml07", "createdAt": "2020-09-04T18:16:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQ2MzQ0MQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r485463441", "bodyText": "@brahmareddybattula  would you please have a look for this? Thanks.", "author": "huangtianhua", "createdAt": "2020-09-09T09:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTYzOQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486535639", "bodyText": "ok, By design if you dn't want to move.", "author": "brahmareddybattula", "createdAt": "2020-09-10T18:06:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0OTY4Mg==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486549682", "bodyText": "My final query then, why can't have one NVDIMM like one SSD as this also movable and peristent..?", "author": "brahmareddybattula", "createdAt": "2020-09-10T18:27:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjcxNjQ3MQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486716471", "bodyText": "My final query then, why can't have one NVDIMM like one SSD as this also movable and peristent..?\n\nConsidering NVDIMM is faster, so NVDIMM does not  use FsDatasetCache() which SSD needs in the design.", "author": "YaYun-Wang", "createdAt": "2020-09-11T01:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAzNzc1MQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r494037751", "bodyText": "ok", "author": "brahmareddybattula", "createdAt": "2020-09-24T04:53:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NDg0Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645089", "bodyText": "Let's add only this line", "author": "brahmareddybattula", "createdAt": "2020-08-25T18:13:39Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java", "diffHunk": "@@ -34,28 +34,35 @@\n @InterfaceStability.Unstable\n public enum StorageType {\n   // sorted by the speed of the storage types, from fast to slow\n-  RAM_DISK(true),\n-  SSD(false),\n-  DISK(false),\n-  ARCHIVE(false),\n-  PROVIDED(false);\n+  RAM_DISK(true, true),\n+  NVDIMM(false, true),", "originalCommit": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2NjU0Ng==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477166546", "bodyText": "Sorry, I don't understand.", "author": "huangtianhua", "createdAt": "2020-08-26T09:30:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0ODMzMA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r482248330", "bodyText": "As we are adding only \"NVDIMM\",So I expect one line change here but I missed that you added one more param to enum.", "author": "brahmareddybattula", "createdAt": "2020-09-02T17:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTA4OQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r476645634", "bodyText": "is this will not valid anymore.?", "author": "brahmareddybattula", "createdAt": "2020-08-25T18:14:45Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java", "diffHunk": "@@ -77,6 +77,9 @@\n   /** Returns true if the volume is NOT backed by persistent storage. */\n   boolean isTransientStorage();", "originalCommit": "84c30785e64d6419d702c468a0a6b22b6c855bc6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzMwNDQ3OA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r477304478", "bodyText": "The \"isTransientStorage\" method is still available.\nIn the original code, isTransient() and isTransientStorage methods are used to determine whether to support FsDatasetCache, Persistent, Quota, and Movable.\nFsDatasetCache will be used When the storage type is persistent. NVDIMM is RAM to some extent, which is fast. However, NVDIMM is a persistent storage type.  Then, isTransient() and isTransientStorage()  used to determine whether to support FsDatasetCache can't meet the requirements. Therefore, we add  isRAM() and isRAMStorage()  methods to decide whether cache is supported or not. And the other functions, such as, Persistent,  Quota, and Movable judged byisTransient() and isTransientStorage()  methods.", "author": "YaYun-Wang", "createdAt": "2020-08-26T13:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0NTExMQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486545111", "bodyText": "Ok. Got it.", "author": "brahmareddybattula", "createdAt": "2020-09-10T18:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU0ODA1Mw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486548053", "bodyText": "So, NVDIMM is peristent storage and RAM.", "author": "brahmareddybattula", "createdAt": "2020-09-10T18:24:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjcxNjgxMA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r486716810", "bodyText": "So, NVDIMM is peristent storage and RAM.\n\nyes, that\u2019s right.", "author": "YaYun-Wang", "createdAt": "2020-09-11T01:14:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY0NTYzNA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM4NzY4Mw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r487387683", "bodyText": "nit: we can make it clear what the volume is.\n LOG.warn(\"Caching not supported on block with id {} since the volume \"\n    + \"is backed by {} which is RAM.\", blockId, volume);", "author": "liuml07", "createdAt": "2020-09-12T08:57:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java", "diffHunk": "@@ -2298,9 +2298,9 @@ private void cacheBlock(String bpid, long blockId) {\n               \": volume was not an instance of FsVolumeImpl.\");\n           return;\n         }\n-        if (volume.isTransientStorage()) {\n+        if (volume.isRAMStorage()) {\n           LOG.warn(\"Caching not supported on block with id \" + blockId +\n-              \" since the volume is backed by RAM.\");\n+              \" since the volume is backed by RAM_DISK or NVDIMM.\");", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "075ee8a07ec7c1f994e478824cdee17b93dad959", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java\nindex c86ff606287..837cc363731 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java\n\n@@ -2299,8 +2300,8 @@ private void cacheBlock(String bpid, long blockId) {\n           return;\n         }\n         if (volume.isRAMStorage()) {\n-          LOG.warn(\"Caching not supported on block with id \" + blockId +\n-              \" since the volume is backed by RAM_DISK or NVDIMM.\");\n+          LOG.warn(\"Caching not supported on block with id {} since the volume \" +\n+              \"is backed by {} which is RAM.\", blockId, volume);\n           return;\n         }\n         success = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTQyMg==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661422", "bodyText": "nit: Add a blank line before every new rack, aka\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\ncan be\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n  \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n  \n  \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n\nSame to the hosts, please  make the last three hosts in a new line so that racks, hosts, and types can be easily read with eyeballs.", "author": "liuml07", "createdAt": "2020-09-20T07:03:08Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -64,37 +64,42 @@ public void setupDatanodes() {\n     final String[] racks = {\n         \"/l1/d1/r1\", \"/l1/d1/r1\", \"/l1/d1/r2\", \"/l1/d1/r2\", \"/l1/d1/r2\",\n \n-        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n+        \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\", \"/l1/d2/r3\",\n \n         \"/l2/d3/r1\", \"/l2/d3/r2\", \"/l2/d3/r3\", \"/l2/d3/r4\", \"/l2/d3/r5\",\n \n         \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n-        \"/l2/d4/r1\", \"/l2/d4/r1\"};\n+        \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n+        \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "180cc036a21a4263eb030ca19f7347e5718f0a8b", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\nindex ba1265eebab..9d7c86002c2 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n\n@@ -70,6 +70,7 @@ public void setupDatanodes() {\n \n         \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r1\",\n         \"/l2/d4/r1\", \"/l2/d4/r1\", \"/l2/d4/r2\",\n+\n         \"/l3/d5/r1\", \"/l3/d5/r1\", \"/l3/d5/r2\"};\n     final String[] hosts = {\n         \"host1\", \"host2\", \"host3\", \"host4\", \"host5\",\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MTU3Mg==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491661572", "bodyText": "nit: replace with assertEquals(4, d2info.get(\"r3\").size());", "author": "liuml07", "createdAt": "2020-09-20T07:05:17Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -120,10 +125,11 @@ public void testGetStorageTypeInfo() throws Exception {\n     HashMap<String, EnumMap<StorageType, Integer>> d2info =\n         d2.getChildrenStorageInfo();\n     assertEquals(1, d2info.keySet().size());\n-    assertTrue(d2info.get(\"r3\").size() == 3);\n+    assertTrue(d2info.get(\"r3\").size() == 4);", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0d753adac011ddf898b8565515998bfe775dd35f", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\nindex ba1265eebab..e4f18ccd118 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n\n@@ -125,7 +126,7 @@ public void testGetStorageTypeInfo() throws Exception {\n     HashMap<String, EnumMap<StorageType, Integer>> d2info =\n         d2.getChildrenStorageInfo();\n     assertEquals(1, d2info.keySet().size());\n-    assertTrue(d2info.get(\"r3\").size() == 4);\n+    assertEquals(4, d2info.get(\"r3\").size());\n     assertEquals(1, (int)d2info.get(\"r3\").get(StorageType.DISK));\n     assertEquals(1, (int)d2info.get(\"r3\").get(StorageType.RAM_DISK));\n     assertEquals(1, (int)d2info.get(\"r3\").get(StorageType.SSD));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI0NQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663245", "bodyText": "nit: why not use 34.34.34.34?", "author": "liuml07", "createdAt": "2020-09-20T07:27:27Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0d753adac011ddf898b8565515998bfe775dd35f", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\nindex ba1265eebab..e4f18ccd118 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n\n@@ -216,12 +217,12 @@ public void testAddAndRemoveTopology() throws Exception {\n         \"/l1/d3/r4\"};\n     String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\", \"33.33.33.34\"};\n+        \"33.33.33.33\", \"34.34.34.34\"};\n     StorageType[] newTypes = {StorageType.DISK, StorageType.SSD,\n         StorageType.SSD, StorageType.SSD, StorageType.NVDIMM};\n     DatanodeDescriptor[] newDD = new DatanodeDescriptor[5];\n \n-    for (int i = 0; i<5; i++) {\n+    for (int i = 0; i < 5; i++) {\n       DatanodeStorageInfo dsi = DFSTestUtil.createDatanodeStorageInfo(\n           \"s\" + newHost[i], newips[i], newRack[i], newHost[i],\n           newTypes[i], null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzI4MA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663280", "bodyText": "nit: let's have space before and after < operator", "author": "liuml07", "createdAt": "2020-09-20T07:27:57Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java", "diffHunk": "@@ -182,15 +212,16 @@ public void testGetStorageTypeInfo() throws Exception {\n    */\n   @Test\n   public void testAddAndRemoveTopology() throws Exception {\n-    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\"};\n-    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\"};\n+    String[] newRack = {\"/l1/d1/r1\", \"/l1/d1/r3\", \"/l1/d3/r3\", \"/l1/d3/r3\",\n+        \"/l1/d3/r4\"};\n+    String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\"};\n+        \"33.33.33.33\", \"33.33.33.34\"};\n     StorageType[] newTypes = {StorageType.DISK, StorageType.SSD,\n-        StorageType.SSD, StorageType.SSD};\n-    DatanodeDescriptor[] newDD = new DatanodeDescriptor[4];\n+        StorageType.SSD, StorageType.SSD, StorageType.NVDIMM};\n+    DatanodeDescriptor[] newDD = new DatanodeDescriptor[5];\n \n-    for (int i = 0; i<4; i++) {\n+    for (int i = 0; i<5; i++) {", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0d753adac011ddf898b8565515998bfe775dd35f", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\nindex ba1265eebab..e4f18ccd118 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java\n\n@@ -216,12 +217,12 @@ public void testAddAndRemoveTopology() throws Exception {\n         \"/l1/d3/r4\"};\n     String[] newHost = {\"nhost1\", \"nhost2\", \"nhost3\", \"nhost4\", \"nhost5\"};\n     String[] newips = {\"30.30.30.30\", \"31.31.31.31\", \"32.32.32.32\",\n-        \"33.33.33.33\", \"33.33.33.34\"};\n+        \"33.33.33.33\", \"34.34.34.34\"};\n     StorageType[] newTypes = {StorageType.DISK, StorageType.SSD,\n         StorageType.SSD, StorageType.SSD, StorageType.NVDIMM};\n     DatanodeDescriptor[] newDD = new DatanodeDescriptor[5];\n \n-    for (int i = 0; i<5; i++) {\n+    for (int i = 0; i < 5; i++) {\n       DatanodeStorageInfo dsi = DFSTestUtil.createDatanodeStorageInfo(\n           \"s\" + newHost[i], newips[i], newRack[i], newHost[i],\n           newTypes[i], null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663549", "bodyText": "As this if-else if-else if-else getting longer, let's use switch case?", "author": "liuml07", "createdAt": "2020-09-20T07:31:21Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java", "diffHunk": "@@ -145,9 +150,11 @@ public void testStorageTypeStatsJMX() throws Exception {\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n       if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3l, storageTypeStats.get(\"nodesInService\"));\n+        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1MjQ3MQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491852471", "bodyText": "storageType is a parameter of \"java.lang.String\" , and switch()  does not support \"java.lang.String\" before java 1.7. So, will if-else  be more appropriate here?", "author": "YaYun-Wang", "createdAt": "2020-09-21T07:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1Nzc5Nw==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491857797", "bodyText": "I have not used Java 7 for a while, but I remember vaguely this is actually supported?\nhttps://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html", "author": "liuml07", "createdAt": "2020-09-21T08:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg1OTczOQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491859739", "bodyText": "Hadoop releases before 2.10 are all end of life (EoL). Hadoop 2.10 is the only version using Java 7. We do not need any support, compile or runtime, for Java versions before Java 7.\nHadoop 3.x are all using Java 8+. We do not need any Java 7 support in Hadoop 3.", "author": "liuml07", "createdAt": "2020-09-21T08:11:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzU0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "842f8c72d17b55ebf318a0d19f7d4a3b710f4f0c", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java\nindex 619331efa7c..1742c8db037 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java\n\n@@ -149,15 +149,19 @@ public void testStorageTypeStatsJMX() throws Exception {\n       String storageType = (String)entry.get(\"key\");\n       Map<String,Object> storageTypeStats = (Map<String,Object>)entry.get(\"value\");\n       typesPresent.add(storageType);\n-      if (storageType.equals(\"ARCHIVE\") || storageType.equals(\"DISK\") ) {\n-        assertEquals(3L, storageTypeStats.get(\"nodesInService\"));\n-      } else if (storageType.equals(\"RAM_DISK\")) {\n-        assertEquals(7L, storageTypeStats.get(\"nodesInService\"));\n-      } else if (storageType.equals(\"NVDIMM\")) {\n-        assertEquals(1L, storageTypeStats.get(\"nodesInService\"));\n-      }\n-      else {\n-        fail();\n+      switch (storageType) {\n+          case \"ARCHIVE\":\n+          case \"DISK\":\n+              assertEquals(3L, storageTypeStats.get(\"nodesInService\"));\n+              break;\n+          case \"RAM_DISK\":\n+              assertEquals(7L, storageTypeStats.get(\"nodesInService\"));\n+              break;\n+          case \"NVDIMM\":\n+              assertEquals(1L, storageTypeStats.get(\"nodesInService\"));\n+              break;\n+          default:\n+              fail();\n       }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzgxMQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663811", "bodyText": "The invalid 8th URI has ending space deliberately for testing. Let's keep it, aka\n\"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] , [nvdimm]/dir7\";", "author": "liuml07", "createdAt": "2020-09-20T07:34:55Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java", "diffHunk": "@@ -43,14 +43,15 @@ public void testDataDirParsing() throws Throwable {\n \n     File dir5 = new File(\"/dir5\");\n     File dir6 = new File(\"/dir6\");\n+    File dir7 = new File(\"/dir7\");\n     // Verify that a valid string is correctly parsed, and that storage\n     // type is not case-sensitive and we are able to handle white-space between\n     // storage type and URI.\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,\" +\n-            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] \";\n+            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk], [nvdimm]/dir7\";", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8b75c799db9157ae3263d07b678878cad69560ef", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\nindex 885e531ecb9..7d8734c8f34 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\n\n@@ -48,7 +48,7 @@ public void testDataDirParsing() throws Throwable {\n     // type is not case-sensitive and we are able to handle white-space between\n     // storage type and URI.\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,\" +\n-            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk], [nvdimm]/dir7\";\n+            \"[ram_disk]/dir4,[disk]/dir5, [disk] /dir6, [disk] , [nvdimm]/dir7\";\n     conf.set(DFS_DATANODE_DATA_DIR_KEY, locations1);\n     locations = DataNode.getStorageLocations(conf);\n     assertThat(locations.size(), is(9));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491663910", "bodyText": "nit: Let's use better assertion statement assertEquals(3L, volume5.getReserved());", "author": "liuml07", "createdAt": "2020-09-20T07:35:59Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java", "diffHunk": "@@ -202,6 +205,14 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setConf(conf)\n         .build();\n     assertEquals(\"\", 100L, volume4.getReserved());\n+    FsVolumeImpl volume5 = new FsVolumeImplBuilder().setDataset(dataset)\n+        .setStorageDirectory(\n+            new StorageDirectory(\n+                StorageLocation.parse(\"[NVDIMM]\"+volDir.getPath())))\n+        .setStorageID(\"storage-id\")\n+        .setConf(conf)\n+        .build();\n+    assertEquals(\"\", 3L, volume5.getReserved());", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU5ODgwNQ==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r492598805", "bodyText": "In order to be consistent with the original code, the assertEquals()  here has three parameters, such as, lines  196 and 204 of the original code.", "author": "YaYun-Wang", "createdAt": "2020-09-22T09:32:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg2NjI2Ng==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r492866266", "bodyText": "Usually we can, but following original code style here is bad. When it fails, the original code gives up empty string. My code shows your expected value and actual value so you can debug. Please change it.\nWe can also file a JIRA to update all such cases where assertEquals can be improved.", "author": "liuml07", "createdAt": "2020-09-22T16:17:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2MzkxMA=="}], "type": "inlineReview", "revised_code": {"commit": "7771df7d4968641fac6f2652717cce3d8cb6a0e5", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java\nindex 43395dc53c2..6c58a2e9852 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsVolumeList.java\n\n@@ -212,7 +212,7 @@ public void testDfsReservedForDifferentStorageTypes() throws IOException {\n         .setStorageID(\"storage-id\")\n         .setConf(conf)\n         .build();\n-    assertEquals(\"\", 3L, volume5.getReserved());\n+    assertEquals(3L, volume5.getReserved());\n   }\n \n   @Test\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY2NDEyNg==", "url": "https://github.com/apache/hadoop/pull/2189#discussion_r491664126", "bodyText": "Also add a few happy test?", "author": "liuml07", "createdAt": "2020-09-20T07:39:17Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java", "diffHunk": "@@ -1103,6 +1103,8 @@ public void testSetQuota() throws Exception {\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.SSD, -100));\n     LambdaTestUtils.intercept(IllegalArgumentException.class,\n         () -> webHdfs.setQuotaByStorageType(path, StorageType.RAM_DISK, 100));\n+    LambdaTestUtils.intercept(IllegalArgumentException.class,\n+        () -> webHdfs.setQuotaByStorageType(path, StorageType.NVDIMM, -100));", "originalCommit": "dea6c63a91a7d0815dbdb405f8cf1c219365aeca", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "43bf6723c7c47e13be801781ae99724f1e6725e5", "url": "https://github.com/apache/hadoop/commit/43bf6723c7c47e13be801781ae99724f1e6725e5", "message": "HDFS-15025. Applying NVDIMM storage media to HDFS\n\nThe non-volatile memory NVDIMM is faster than SSD,\nit can be used simultaneously with RAM, DISK and SSD.\nStoring the data of HDFS on NVDIMM directly will get\nbetter response rate and reliability.\n\nCo-authored-by: YaYun-Wang <yywangyayun@163.com>", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "17fd047e62c287fbf605916f6676561df1d6e8ca", "url": "https://github.com/apache/hadoop/commit/17fd047e62c287fbf605916f6676561df1d6e8ca", "message": "Update ArchivalStorage.md", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "76d2716777e822a3c975879f4c28b2c6d9c298c2", "url": "https://github.com/apache/hadoop/commit/76d2716777e822a3c975879f4c28b2c6d9c298c2", "message": "Update ArchivalStorage.md\n\nput NVDIMM to the end of all storage types", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "075ee8a07ec7c1f994e478824cdee17b93dad959", "url": "https://github.com/apache/hadoop/commit/075ee8a07ec7c1f994e478824cdee17b93dad959", "message": "Update FsDatasetImpl.java", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "180cc036a21a4263eb030ca19f7347e5718f0a8b", "url": "https://github.com/apache/hadoop/commit/180cc036a21a4263eb030ca19f7347e5718f0a8b", "message": "Update TestDFSNetworkTopology.java\n\nadd a blank line before every new rack", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "0d753adac011ddf898b8565515998bfe775dd35f", "url": "https://github.com/apache/hadoop/commit/0d753adac011ddf898b8565515998bfe775dd35f", "message": "Update TestDFSNetworkTopology.java", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "8f9ace07121f69aa55f426169474ab5c33f4cb5e", "url": "https://github.com/apache/hadoop/commit/8f9ace07121f69aa55f426169474ab5c33f4cb5e", "message": "Update TestDFSNetworkTopology.java", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "842f8c72d17b55ebf318a0d19f7d4a3b710f4f0c", "url": "https://github.com/apache/hadoop/commit/842f8c72d17b55ebf318a0d19f7d4a3b710f4f0c", "message": "Update TestBlockStatsMXBean.java", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "8b75c799db9157ae3263d07b678878cad69560ef", "url": "https://github.com/apache/hadoop/commit/8b75c799db9157ae3263d07b678878cad69560ef", "message": "Update TestDataDirs.java\n\nadd one space after 'disk'", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "241faf4eb72b6870607f6a6fe9e2a1d9a9c4752d", "url": "https://github.com/apache/hadoop/commit/241faf4eb72b6870607f6a6fe9e2a1d9a9c4752d", "message": "Update TestWebHDFS.java\n\nadd  NVDIMM test for setQuotaByStorageType method", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "7771df7d4968641fac6f2652717cce3d8cb6a0e5", "url": "https://github.com/apache/hadoop/commit/7771df7d4968641fac6f2652717cce3d8cb6a0e5", "message": "Update TestFsVolumeList.java\n\nupdate assertEquals() with two parameters", "committedDate": "2020-09-24T01:23:35Z", "type": "commit"}, {"oid": "0a87e83673394ded5f0104d4ec295262a1b673a8", "url": "https://github.com/apache/hadoop/commit/0a87e83673394ded5f0104d4ec295262a1b673a8", "message": "Fix checkstyle errors", "committedDate": "2020-09-24T01:44:54Z", "type": "commit"}, {"oid": "0a87e83673394ded5f0104d4ec295262a1b673a8", "url": "https://github.com/apache/hadoop/commit/0a87e83673394ded5f0104d4ec295262a1b673a8", "message": "Fix checkstyle errors", "committedDate": "2020-09-24T01:44:54Z", "type": "forcePushed"}]}