{"pr_number": 2520, "pr_title": "HADOOP-17290. ABFS: Add Identifiers to Client Request Header", "pr_createdAt": "2020-12-04T11:26:11Z", "pr_url": "https://github.com/apache/hadoop/pull/2520", "timeline": [{"oid": "ce05bd0627e1320945067227884fdc22a26a971c", "url": "https://github.com/apache/hadoop/commit/ce05bd0627e1320945067227884fdc22a26a971c", "message": "correlation-id : req-id : retry-count", "committedDate": "2020-09-29T20:09:06Z", "type": "commit"}, {"oid": "da6c02538686015881cbc95f817472d0a01cefc5", "url": "https://github.com/apache/hadoop/commit/da6c02538686015881cbc95f817472d0a01cefc5", "message": "adding IDs", "committedDate": "2020-10-06T08:43:06Z", "type": "commit"}, {"oid": "e21f7c6b1604b515c1359532294b5f49552f36fc", "url": "https://github.com/apache/hadoop/commit/e21f7c6b1604b515c1359532294b5f49552f36fc", "message": "add op id", "committedDate": "2020-10-07T06:57:53Z", "type": "commit"}, {"oid": "67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "url": "https://github.com/apache/hadoop/commit/67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "message": "undo formatting", "committedDate": "2020-10-07T11:01:35Z", "type": "commit"}, {"oid": "0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "url": "https://github.com/apache/hadoop/commit/0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "message": "to pc", "committedDate": "2020-10-07T22:50:04Z", "type": "commit"}, {"oid": "20c916d962fb10e9c9209c332a2aa00818c3f1aa", "url": "https://github.com/apache/hadoop/commit/20c916d962fb10e9c9209c332a2aa00818c3f1aa", "message": "tc -> ops", "committedDate": "2020-10-08T02:15:33Z", "type": "commit"}, {"oid": "5e97c55b77905e441037b0af76db85c69da5f810", "url": "https://github.com/apache/hadoop/commit/5e97c55b77905e441037b0af76db85c69da5f810", "message": "other IDs", "committedDate": "2020-10-08T10:30:11Z", "type": "commit"}, {"oid": "2df54bfb9008324ebee052c052b1c85f88f37417", "url": "https://github.com/apache/hadoop/commit/2df54bfb9008324ebee052c052b1c85f88f37417", "message": "debug", "committedDate": "2020-10-08T16:05:41Z", "type": "commit"}, {"oid": "d3ba55099abebafe7b6624f8aff17710c0b54fe9", "url": "https://github.com/apache/hadoop/commit/d3ba55099abebafe7b6624f8aff17710c0b54fe9", "message": "debug", "committedDate": "2020-10-09T03:18:48Z", "type": "commit"}, {"oid": "70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "url": "https://github.com/apache/hadoop/commit/70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "message": "builds", "committedDate": "2020-10-09T05:31:46Z", "type": "commit"}, {"oid": "3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "url": "https://github.com/apache/hadoop/commit/3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "message": "primary req id", "committedDate": "2020-10-12T04:05:10Z", "type": "commit"}, {"oid": "c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "url": "https://github.com/apache/hadoop/commit/c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "message": "readahead", "committedDate": "2020-10-12T15:23:11Z", "type": "commit"}, {"oid": "2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "url": "https://github.com/apache/hadoop/commit/2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "message": "dependent & client req id (readaheads)", "committedDate": "2020-10-13T12:24:14Z", "type": "commit"}, {"oid": "eeae13ee9a93dab4ea6713333842b543065529b3", "url": "https://github.com/apache/hadoop/commit/eeae13ee9a93dab4ea6713333842b543065529b3", "message": "liststatus ok", "committedDate": "2020-10-13T13:13:43Z", "type": "commit"}, {"oid": "d4343fae35638e25952a3ce1cc8b472fb726a83b", "url": "https://github.com/apache/hadoop/commit/d4343fae35638e25952a3ce1cc8b472fb726a83b", "message": "create overwrite case ok", "committedDate": "2020-10-13T13:54:26Z", "type": "commit"}, {"oid": "0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "url": "https://github.com/apache/hadoop/commit/0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "message": "fixed some errors", "committedDate": "2020-10-15T03:09:17Z", "type": "commit"}, {"oid": "107df060d5bca839cb0e266d6631d862e2fcfe0a", "url": "https://github.com/apache/hadoop/commit/107df060d5bca839cb0e266d6631d862e2fcfe0a", "message": "tc changes", "committedDate": "2020-10-15T19:14:22Z", "type": "commit"}, {"oid": "166f99cd6239cab03076e6dd02192eebd3f79165", "url": "https://github.com/apache/hadoop/commit/166f99cd6239cab03076e6dd02192eebd3f79165", "message": "fix errors", "committedDate": "2020-10-19T01:44:34Z", "type": "commit"}, {"oid": "d762d864206f3b88af01d1558b4101ce8746dedd", "url": "https://github.com/apache/hadoop/commit/d762d864206f3b88af01d1558b4101ce8746dedd", "message": "test", "committedDate": "2020-10-20T10:03:27Z", "type": "commit"}, {"oid": "d905c93c79a3db9d55505bbd022e9d8ae62f8050", "url": "https://github.com/apache/hadoop/commit/d905c93c79a3db9d55505bbd022e9d8ae62f8050", "message": "1 test draft", "committedDate": "2020-10-20T13:47:23Z", "type": "commit"}, {"oid": "56acf8059ff53d9a833df3e45951d2d6e64182db", "url": "https://github.com/apache/hadoop/commit/56acf8059ff53d9a833df3e45951d2d6e64182db", "message": "test IDs", "committedDate": "2020-10-23T05:47:35Z", "type": "commit"}, {"oid": "0a96fbe57d115c80277de7c11510bcbad9de3d55", "url": "https://github.com/apache/hadoop/commit/0a96fbe57d115c80277de7c11510bcbad9de3d55", "message": "clear()", "committedDate": "2020-10-23T07:02:41Z", "type": "commit"}, {"oid": "ba8d98837dc0c5d291a9d1bcde096c09327a024d", "url": "https://github.com/apache/hadoop/commit/ba8d98837dc0c5d291a9d1bcde096c09327a024d", "message": "minor edits", "committedDate": "2020-10-27T17:52:22Z", "type": "commit"}, {"oid": "6be948e986a26c754ba9306a5f6a740e42f25bdd", "url": "https://github.com/apache/hadoop/commit/6be948e986a26c754ba9306a5f6a740e42f25bdd", "message": "minor edits", "committedDate": "2020-10-27T18:27:35Z", "type": "commit"}, {"oid": "5af42f61947953fd879f6a3db96206be88cf9db1", "url": "https://github.com/apache/hadoop/commit/5af42f61947953fd879f6a3db96206be88cf9db1", "message": "minor edits", "committedDate": "2020-10-27T18:31:32Z", "type": "commit"}, {"oid": "077b5bd26503e998071c3b4289d6ddbeab79c63c", "url": "https://github.com/apache/hadoop/commit/077b5bd26503e998071c3b4289d6ddbeab79c63c", "message": "minor edits", "committedDate": "2020-10-27T18:40:31Z", "type": "commit"}, {"oid": "ee07bae1eb71781b11ae52aed473fa22f6095038", "url": "https://github.com/apache/hadoop/commit/ee07bae1eb71781b11ae52aed473fa22f6095038", "message": "minor edits/whitespc", "committedDate": "2020-10-27T19:12:35Z", "type": "commit"}, {"oid": "9457a04dbcac6da42253c4f9bdc0c98758e608a2", "url": "https://github.com/apache/hadoop/commit/9457a04dbcac6da42253c4f9bdc0c98758e608a2", "message": "merge conflict", "committedDate": "2020-11-02T10:48:06Z", "type": "commit"}, {"oid": "16811384244eeb61895c7d04037000b646e5d717", "url": "https://github.com/apache/hadoop/commit/16811384244eeb61895c7d04037000b646e5d717", "message": "pr changes + dummyTC", "committedDate": "2020-11-03T10:32:00Z", "type": "commit"}, {"oid": "a4755553881a1c8d39bdff737bf4a3d273238328", "url": "https://github.com/apache/hadoop/commit/a4755553881a1c8d39bdff737bf4a3d273238328", "message": "test ns() + remove extra changes", "committedDate": "2020-11-03T14:06:56Z", "type": "commit"}, {"oid": "644293231d6eb34245742c1fde75fc022d7dae66", "url": "https://github.com/apache/hadoop/commit/644293231d6eb34245742c1fde75fc022d7dae66", "message": "revert httpop formatting", "committedDate": "2020-11-03T14:25:18Z", "type": "commit"}, {"oid": "ec92f2576698dcb702898f1af61cfb71fb9d54b0", "url": "https://github.com/apache/hadoop/commit/ec92f2576698dcb702898f1af61cfb71fb9d54b0", "message": "revert httpop formatting", "committedDate": "2020-11-03T14:32:59Z", "type": "commit"}, {"oid": "fecc00a2962009f17e4eca57da4b448f25136b4a", "url": "https://github.com/apache/hadoop/commit/fecc00a2962009f17e4eca57da4b448f25136b4a", "message": "move tc init near usage", "committedDate": "2020-11-03T15:08:44Z", "type": "commit"}, {"oid": "1cead5325d825d9076063fef209b07e2f2ae9ca3", "url": "https://github.com/apache/hadoop/commit/1cead5325d825d9076063fef209b07e2f2ae9ca3", "message": "minor change", "committedDate": "2020-11-04T05:52:05Z", "type": "commit"}, {"oid": "5dbc7830221092d91562723320e271901ba4eb82", "url": "https://github.com/apache/hadoop/commit/5dbc7830221092d91562723320e271901ba4eb82", "message": "pr changes", "committedDate": "2020-11-05T02:40:38Z", "type": "commit"}, {"oid": "6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "url": "https://github.com/apache/hadoop/commit/6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "message": "enum, opnames", "committedDate": "2020-11-05T06:23:28Z", "type": "commit"}, {"oid": "d304123654e1cbe18c6617b317727c7e76a900a5", "url": "https://github.com/apache/hadoop/commit/d304123654e1cbe18c6617b317727c7e76a900a5", "message": "enum", "committedDate": "2020-11-08T08:46:42Z", "type": "commit"}, {"oid": "1e4f46db77273da9ec37770aa2923eb4c8350df2", "url": "https://github.com/apache/hadoop/commit/1e4f46db77273da9ec37770aa2923eb4c8350df2", "message": "format changes", "committedDate": "2020-11-12T04:57:37Z", "type": "commit"}, {"oid": "15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "url": "https://github.com/apache/hadoop/commit/15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "message": "test code (#3)\n\n* adding callback structure\r\n\r\n* testListPath correlation header\r\n\r\n* validate IDs; readahead/streamid\r\n\r\n* add common tests + other changes\r\n\r\n* remove stream/extra stuff\r\n\r\n* handle parallel requests\r\n\r\n* clear\r\n\r\n* testTC, retryNum\r\n\r\n* rebase on HADOOP-17290", "committedDate": "2020-11-12T07:35:27Z", "type": "commit"}, {"oid": "e44c64c720485a058d8acd60844848980396ac97", "url": "https://github.com/apache/hadoop/commit/e44c64c720485a058d8acd60844848980396ac97", "message": "other tests (main)", "committedDate": "2020-11-12T12:15:40Z", "type": "commit"}, {"oid": "725c98415030f856ce333b605cbf3de82bb2b028", "url": "https://github.com/apache/hadoop/commit/725c98415030f856ce333b605cbf3de82bb2b028", "message": "all tests", "committedDate": "2020-11-16T05:02:22Z", "type": "commit"}, {"oid": "9b4f55809d54a4f7348859cede4c44a68ccd16ec", "url": "https://github.com/apache/hadoop/commit/9b4f55809d54a4f7348859cede4c44a68ccd16ec", "message": "test tc for appendblob=true", "committedDate": "2020-11-17T07:50:35Z", "type": "commit"}, {"oid": "858695bd6dba12363a7a57e4fe406697f3ce6f8e", "url": "https://github.com/apache/hadoop/commit/858695bd6dba12363a7a57e4fe406697f3ce6f8e", "message": "clean up code", "committedDate": "2020-11-23T05:39:08Z", "type": "commit"}, {"oid": "a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "url": "https://github.com/apache/hadoop/commit/a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "message": "simplify preq test", "committedDate": "2020-11-23T09:19:05Z", "type": "commit"}, {"oid": "e18bab62749da71b56ecb6a4a3ec86983cdc7139", "url": "https://github.com/apache/hadoop/commit/e18bab62749da71b56ecb6a4a3ec86983cdc7139", "message": "fix matchers error", "committedDate": "2020-11-25T05:45:26Z", "type": "commit"}, {"oid": "1553cf653a5c503aea310e9980c52925be0c05fa", "url": "https://github.com/apache/hadoop/commit/1553cf653a5c503aea310e9980c52925be0c05fa", "message": "merge conflicts", "committedDate": "2020-11-25T06:01:00Z", "type": "commit"}, {"oid": "22c3a8411e623bac3e2909620a13e666184e85cf", "url": "https://github.com/apache/hadoop/commit/22c3a8411e623bac3e2909620a13e666184e85cf", "message": "pr revw changes", "committedDate": "2020-11-25T11:48:45Z", "type": "commit"}, {"oid": "3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "url": "https://github.com/apache/hadoop/commit/3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "message": "fix some test failures", "committedDate": "2020-11-26T02:49:03Z", "type": "commit"}, {"oid": "472d0903972128c0634702712ab4d7fb84442b3d", "url": "https://github.com/apache/hadoop/commit/472d0903972128c0634702712ab4d7fb84442b3d", "message": "code cleanup", "committedDate": "2020-11-27T04:06:06Z", "type": "commit"}, {"oid": "21e2a8682ca9c9684f68e4c06185f16c991f85d7", "url": "https://github.com/apache/hadoop/commit/21e2a8682ca9c9684f68e4c06185f16c991f85d7", "message": "fix sastoken matcher", "committedDate": "2020-11-27T06:40:32Z", "type": "commit"}, {"oid": "8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "url": "https://github.com/apache/hadoop/commit/8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "message": "access test, formatting", "committedDate": "2020-11-30T09:07:17Z", "type": "commit"}, {"oid": "db8d89586959545ff69e1c9ab95260759c8bf1a4", "url": "https://github.com/apache/hadoop/commit/db8d89586959545ff69e1c9ab95260759c8bf1a4", "message": "format PR diff", "committedDate": "2020-11-30T10:41:03Z", "type": "commit"}, {"oid": "0521969b26ead9397ff6856bba4b7f1feaca5394", "url": "https://github.com/apache/hadoop/commit/0521969b26ead9397ff6856bba4b7f1feaca5394", "message": "more formatting", "committedDate": "2020-11-30T12:20:53Z", "type": "commit"}, {"oid": "907fc1b987d2006ebde2783eca73b0b0391526dc", "url": "https://github.com/apache/hadoop/commit/907fc1b987d2006ebde2783eca73b0b0391526dc", "message": "merge conflict", "committedDate": "2020-12-03T06:35:52Z", "type": "commit"}, {"oid": "f3f91f4390a365f85ed64b34cd667163fa583c1b", "url": "https://github.com/apache/hadoop/commit/f3f91f4390a365f85ed64b34cd667163fa583c1b", "message": "merge conflict", "committedDate": "2020-12-03T06:36:37Z", "type": "commit"}, {"oid": "18ea7f0445555dfbf43aa175a0aa642521ff39f9", "url": "https://github.com/apache/hadoop/commit/18ea7f0445555dfbf43aa175a0aa642521ff39f9", "message": "pr changes", "committedDate": "2020-12-03T07:44:19Z", "type": "commit"}, {"oid": "af74d8e464bfd736614dabbdd54cd37113d40126", "url": "https://github.com/apache/hadoop/commit/af74d8e464bfd736614dabbdd54cd37113d40126", "message": "stream id test", "committedDate": "2020-12-03T09:28:17Z", "type": "commit"}, {"oid": "3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "url": "https://github.com/apache/hadoop/commit/3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "message": "documentation md", "committedDate": "2020-12-04T08:50:24Z", "type": "commit"}, {"oid": "dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "url": "https://github.com/apache/hadoop/commit/dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2020-12-04T10:26:30Z", "type": "commit"}, {"oid": "f90bbb399437d461a9bc4ee614c27a3d03bae227", "url": "https://github.com/apache/hadoop/commit/f90bbb399437d461a9bc4ee614c27a3d03bae227", "message": "fix yetus bugs", "committedDate": "2020-12-05T06:57:23Z", "type": "commit"}, {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "url": "https://github.com/apache/hadoop/commit/a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "message": "fix randomread getTC failure", "committedDate": "2020-12-07T10:11:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA4OTQwOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543089409", "bodyText": "This will only be used by test code ? If so, lower accessibility to private and set VisibleForTesting.", "author": "snvijaya", "createdAt": "2020-12-15T06:51:37Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1202,6 +1287,10 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  public void setListenerOperation(String operation) {", "originalCommit": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM4MzE1NQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r548383155", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2020-12-24T05:02:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA4OTQwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "d2bf54c1423aea99a21fa8194debea985f6ced01", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex 1e78cd1ffad..accaed4ad5e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -1287,7 +1287,8 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n-  public void setListenerOperation(String operation) {\n+  @VisibleForTesting\n+  void setListenerOperation(String operation) {\n     listener.setOperation(operation);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5ODg5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543098890", "bodyText": "Same as before. Methods used by test need to be private and annotated with VisibleForTesting", "author": "snvijaya", "createdAt": "2020-12-15T07:11:52Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -107,6 +122,15 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n+  }\n+\n+  public void registerListener(Listener listener1) {", "originalCommit": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM4NDUxOA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r548384518", "bodyText": "Cannot make it private since the test classes using it are in a different package. Have moved to the end of file where other VisibleForTesting methods are written (which are also public due to the same reason).", "author": "sumangala-patki", "createdAt": "2020-12-24T05:09:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5ODg5MA=="}], "type": "inlineReview", "revised_code": {"commit": "d2bf54c1423aea99a21fa8194debea985f6ced01", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\nindex bda5cd55bff..5a9ecde665a 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n\n@@ -126,11 +126,6 @@ private String getInputStreamID() {\n     return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n   }\n \n-  public void registerListener(Listener listener1) {\n-    listener = listener1;\n-    tracingContext.setListener(listener);\n-  }\n-\n   @Override\n   public int read() throws IOException {\n     byte[] b = new byte[1];\n"}}, {"oid": "6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "url": "https://github.com/apache/hadoop/commit/6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2020-12-24T04:50:05Z", "type": "commit"}, {"oid": "d2bf54c1423aea99a21fa8194debea985f6ced01", "url": "https://github.com/apache/hadoop/commit/d2bf54c1423aea99a21fa8194debea985f6ced01", "message": "addressing pr comments", "committedDate": "2020-12-24T04:59:00Z", "type": "commit"}, {"oid": "84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "url": "https://github.com/apache/hadoop/commit/84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "message": "merge conflicts", "committedDate": "2021-02-23T07:09:50Z", "type": "commit"}, {"oid": "98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "url": "https://github.com/apache/hadoop/commit/98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "message": "new file conflicts", "committedDate": "2021-02-23T07:17:35Z", "type": "commit"}, {"oid": "9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "url": "https://github.com/apache/hadoop/commit/9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-02T09:29:59Z", "type": "commit"}, {"oid": "c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "url": "https://github.com/apache/hadoop/commit/c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-03T04:41:03Z", "type": "commit"}, {"oid": "ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "url": "https://github.com/apache/hadoop/commit/ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "message": "imports", "committedDate": "2021-03-03T05:46:03Z", "type": "commit"}, {"oid": "fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "url": "https://github.com/apache/hadoop/commit/fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-09T06:14:28Z", "type": "commit"}, {"oid": "22e9a40330fae359dc839b8062efe8f0e841a548", "url": "https://github.com/apache/hadoop/commit/22e9a40330fae359dc839b8062efe8f0e841a548", "message": "import", "committedDate": "2021-03-10T10:24:01Z", "type": "commit"}, {"oid": "29f59f84ee6db71ed342a800282c53035c38b584", "url": "https://github.com/apache/hadoop/commit/29f59f84ee6db71ed342a800282c53035c38b584", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-10T10:25:01Z", "type": "commit"}, {"oid": "a42e5b19a39e6858c790ced0a39edf43a2a262da", "url": "https://github.com/apache/hadoop/commit/a42e5b19a39e6858c790ced0a39edf43a2a262da", "message": "use write code for out", "committedDate": "2021-03-10T10:57:40Z", "type": "commit"}, {"oid": "51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "url": "https://github.com/apache/hadoop/commit/51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "message": "handle invocation ex + write tests", "committedDate": "2021-03-11T04:34:25Z", "type": "commit"}, {"oid": "4ed465f8ce965343be63194a5034070ef932dfd2", "url": "https://github.com/apache/hadoop/commit/4ed465f8ce965343be63194a5034070ef932dfd2", "message": "checkstyle", "committedDate": "2021-03-11T04:45:47Z", "type": "commit"}, {"oid": "0e9770052dd12cd551c581e1b382b28ec1712147", "url": "https://github.com/apache/hadoop/commit/0e9770052dd12cd551c581e1b382b28ec1712147", "message": "minor chkstyle", "committedDate": "2021-03-11T06:33:05Z", "type": "commit"}, {"oid": "1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "url": "https://github.com/apache/hadoop/commit/1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-15T06:27:54Z", "type": "commit"}, {"oid": "1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "url": "https://github.com/apache/hadoop/commit/1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "message": "fix merge conflict", "committedDate": "2021-04-11T04:21:35Z", "type": "commit"}, {"oid": "302fc06772362b0d108f3aab42cc7b353d7a7f58", "url": "https://github.com/apache/hadoop/commit/302fc06772362b0d108f3aab42cc7b353d7a7f58", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-15T12:11:31Z", "type": "commit"}, {"oid": "3447977af7444bdd033ba091124bc70b130954e3", "url": "https://github.com/apache/hadoop/commit/3447977af7444bdd033ba091124bc70b130954e3", "message": "lease rm acquire op", "committedDate": "2021-04-15T18:06:00Z", "type": "commit"}, {"oid": "979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "url": "https://github.com/apache/hadoop/commit/979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-26T06:18:01Z", "type": "commit"}, {"oid": "c9217d9492486f401cbcb9320e8d473f98595c1b", "url": "https://github.com/apache/hadoop/commit/c9217d9492486f401cbcb9320e8d473f98595c1b", "message": "part of merge fix", "committedDate": "2021-04-26T07:01:52Z", "type": "commit"}, {"oid": "fd631219582b5bc6db47b69a4aa496682ac335a4", "url": "https://github.com/apache/hadoop/commit/fd631219582b5bc6db47b69a4aa496682ac335a4", "message": "add active lease fn tests", "committedDate": "2021-04-26T10:05:03Z", "type": "commit"}, {"oid": "78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "url": "https://github.com/apache/hadoop/commit/78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-26T11:29:10Z", "type": "commit"}, {"oid": "d52ee795da47e98178b2498357fb9f55a4e78ef1", "url": "https://github.com/apache/hadoop/commit/d52ee795da47e98178b2498357fb9f55a4e78ef1", "message": "javadoc", "committedDate": "2021-04-26T11:32:07Z", "type": "commit"}, {"oid": "6966924728e4234af175f839b27535166ab1b6d0", "url": "https://github.com/apache/hadoop/commit/6966924728e4234af175f839b27535166ab1b6d0", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-05-30T09:06:51Z", "type": "commit"}, {"oid": "d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "url": "https://github.com/apache/hadoop/commit/d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "message": "merge", "committedDate": "2021-05-30T09:36:46Z", "type": "commit"}, {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "url": "https://github.com/apache/hadoop/commit/d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "message": "typo", "committedDate": "2021-05-30T09:40:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NjEwNjI5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r646106290", "bodyText": "Can you give me the example of possible value ?\nWhat if in same jvm two FS objects are there ?", "author": "surendralilhore", "createdAt": "2021-06-06T09:51:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";", "originalCommit": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MzQ2NjMyNg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r653466326", "bodyText": "The client correlation ID can be any client-provided string within the specified length/character constraints (eg, a GUID like \"faaffc18-5a5d-426d-a259-c1a25c442898\", or a simple string such as \"correlation-id\"). If two or more filesystem instances are created using the same Configuration, or using configurations with the same value of fs.azure.client.correlationid, the identifier will be common for those objects.\nThis allows us to correlate requests across different FS and stream instances (for debugging/analysis purposes) as it is passed over a configuration. To filter among filesystem objects, we can use the unique filesystemId (GUID, generated per AzureBlobFileSystem instance)", "author": "sumangala-patki", "createdAt": "2021-06-17T11:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NjEwNjI5MA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\nindex 4e3afed5112..df9845130bc 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n\n@@ -114,7 +114,7 @@\n    * x-ms-client-request-Id header. Defaults to empty string if the length and\n    * character constraints are not satisfied. **/\n   public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";\n-  public static final String FS_AZURE_TRACINGCONTEXT_FORMAT = \"fs.azure.tracingcontext.format\";\n+  public static final String FS_AZURE_TRACINGHEADER_FORMAT = \"fs.azure.tracingheader.format\";\n   public static final String FS_AZURE_CLUSTER_NAME = \"fs.azure.cluster.name\";\n   public static final String FS_AZURE_CLUSTER_TYPE = \"fs.azure.cluster.type\";\n   public static final String FS_AZURE_SSL_CHANNEL_MODE_KEY = \"fs.azure.ssl.channel.mode\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MDc4NzA4NQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r650787085", "bodyText": "Why this applicable for only stream id, not for filesystem id ? ?", "author": "surendralilhore", "createdAt": "2021-06-14T09:26:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -142,6 +156,10 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);", "originalCommit": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MzQ3MDUyNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r653470525", "bodyText": "This was done to minimize length of header wherever possible. FilesystemId is applicable to all requests, and hence needs to be a complete GUID to ensure uniqueness. StreamId, on the other hand, appears only for a fraction of the API called (read/write), and we may not require a full GUID for it to remain unique across different streams", "author": "sumangala-patki", "createdAt": "2021-06-17T11:20:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MDc4NzA4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\nindex a5b47c55d90..f7715eb0c0d 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n\n@@ -156,7 +156,7 @@ public String getPath() {\n     return path;\n   }\n \n-  private String getInputStreamID() {\n+  private String getInputStreamId() {\n     return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n   }\n \n"}}, {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378", "url": "https://github.com/apache/hadoop/commit/6250d04f25d1efed187a0835e70f53415f0e1378", "message": "fix merge conflict", "committedDate": "2021-06-23T09:14:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3MTA5Mw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657271093", "bodyText": "clientCorrelationId ?   To be similar as 'userAgentId' etc?  And the getter also", "author": "anoopsjohn", "createdAt": "2021-06-23T16:25:18Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -264,6 +266,10 @@\n       DefaultValue = DEFAULT_VALUE_UNKNOWN)\n   private String clusterType;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_CLIENT_CORRELATIONID,\n+          DefaultValue = EMPTY_STRING)\n+  private String clientCorrelationID;", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIwOTIzOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658209239", "bodyText": "renamed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:03:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3MTA5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\nindex e3c9388bc9c..1261cc2c8c9 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java\n\n@@ -268,7 +268,7 @@\n \n   @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_CLIENT_CORRELATIONID,\n           DefaultValue = EMPTY_STRING)\n-  private String clientCorrelationID;\n+  private String clientCorrelationId;\n \n   @BooleanConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_ENABLE_DELEGATION_TOKEN,\n       DefaultValue = DEFAULT_ENABLE_DELEGATION_TOKEN)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3Njk1OQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657276959", "bodyText": "All places ID to Id?", "author": "anoopsjohn", "createdAt": "2021-06-23T16:32:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -111,10 +116,14 @@\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n   private boolean isClosed;\n+  private final String fileSystemID = UUID.randomUUID().toString();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIwOTY1MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658209651", "bodyText": "yes, done", "author": "sumangala-patki", "createdAt": "2021-06-24T19:03:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3Njk1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -116,13 +116,13 @@\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n   private boolean isClosed;\n-  private final String fileSystemID = UUID.randomUUID().toString();\n+  private final String fileSystemId = UUID.randomUUID().toString();\n \n   private boolean delegationTokenEnabled = false;\n   private AbfsDelegationTokenManager delegationTokenManager;\n   private AbfsCounters abfsCounters;\n-  private String clientCorrelationID;\n-  private TracingContextFormat tracingContextFormat;\n+  private String clientCorrelationId;\n+  private TracingHeaderFormat tracingHeaderFormat;\n   private Listener listener;\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzMzOTgzNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657339834", "bodyText": "Here is a call to getFileStatus(Path) but as part of LISTSTATUS op.  So we should the context created above , during getFileStatus also right?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:00:09Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMDU1Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658210556", "bodyText": "modified getFileStatus to take tracingContext arg", "author": "sumangala-patki", "createdAt": "2021-06-24T19:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzMzOTgzNA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -1130,20 +1137,20 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n-      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n-          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n-          tracingContextFormat, listener);\n+      TracingContext tracingContext = new TracingContext(clientCorrelationId,\n+          fileSystemId, FSOperationType.LISTSTATUS, true, tracingHeaderFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path, tracingContext), abfsStore,\n+              tracingContext);\n       return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\n     } else {\n       return super.listStatusIterator(path);\n     }\n   }\n \n-  private FileStatus tryGetFileStatus(final Path f) {\n+  private FileStatus tryGetFileStatus(final Path f, TracingContext tracingContext) {\n     try {\n-      return getFileStatus(f);\n+      return getFileStatus(f, tracingContext);\n     } catch (IOException ex) {\n       LOG.debug(\"File not found {}\", f);\n       statIncrement(ERROR_IGNORED);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0MTU3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657341578", "bodyText": "Even this call to FileSystem#listStatusIterator() will create ADL gen2  calls right?  So should there be way for having a single context(above created) to be used for call from there too?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:02:55Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);\n       return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\n     } else {\n       return super.listStatusIterator(path);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMTM0MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658211341", "bodyText": "the else block eventually calls ABFS liststatus method, which generates its own context. It is equivalent to generating and passing object from here as there is no action (request) before/after the super method call", "author": "sumangala-patki", "createdAt": "2021-06-24T19:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0MTU3OA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -1130,20 +1137,20 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n-      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n-          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n-          tracingContextFormat, listener);\n+      TracingContext tracingContext = new TracingContext(clientCorrelationId,\n+          fileSystemId, FSOperationType.LISTSTATUS, true, tracingHeaderFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path, tracingContext), abfsStore,\n+              tracingContext);\n       return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\n     } else {\n       return super.listStatusIterator(path);\n     }\n   }\n \n-  private FileStatus tryGetFileStatus(final Path f) {\n+  private FileStatus tryGetFileStatus(final Path f, TracingContext tracingContext) {\n     try {\n-      return getFileStatus(f);\n+      return getFileStatus(f, tracingContext);\n     } catch (IOException ex) {\n       LOG.debug(\"File not found {}\", f);\n       statIncrement(ERROR_IGNORED);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NTAyNw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657345027", "bodyText": "Within tryGetFileStatus() there is call to getFileStatus.  We should be using this context created here.\ntryGetFileStatus() been called by createNonRecursive API also.\nHave to handle these.", "author": "anoopsjohn", "createdAt": "2021-06-23T18:07:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -335,7 +361,10 @@ public boolean rename(final Path src, final Path dst) throws IOException {\n     }\n \n     // Non-HNS account need to check dst status on driver side.\n-    if (!abfsStore.getIsNamespaceEnabled() && dstFileStatus == null) {\n+    TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+        fileSystemID, HdfsOperationConstants.RENAME, true, tracingContextFormat,\n+        listener);\n+    if (!abfsStore.getIsNamespaceEnabled(tracingContext) && dstFileStatus == null) {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMTcxNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658211715", "bodyText": "refactored the getFileStatus methods in AzureBlobFileSystem class to accommodate these. Changes done:\n\noverload getFileStatus to use context passed by other methods\ntryGetFileStatus to take context as arg\npvt method createFileSystem signature change to avoid overloading tryGetFileStatus", "author": "sumangala-patki", "createdAt": "2021-06-24T19:07:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NTAyNw=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -353,7 +360,7 @@ public boolean rename(final Path src, final Path dst) throws IOException {\n       // - if it doesn't exist, return false\n       // - if it is file, return true\n       // - if it is dir, return false.\n-      dstFileStatus = tryGetFileStatus(qualifiedDstPath);\n+      dstFileStatus = tryGetFileStatus(qualifiedDstPath, tracingContext);\n       if (dstFileStatus == null) {\n         return false;\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NjMzMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657346332", "bodyText": "GET_FILESTATUS op?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:08:22Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1071,7 +1155,10 @@ private boolean fileSystemExists() throws IOException {\n     LOG.debug(\n             \"AzureBlobFileSystem.fileSystemExists uri: {}\", uri);\n     try {\n-      abfsStore.getFilesystemProperties();\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.GET_FILESTATUS,", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjMyNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212324", "bodyText": "This is a private method that does not implement any Hadoop function, and is never used in the Driver. However, we need to pass in a dummy tracing object for syntax. Modified to use test operation type", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NjMzMg=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -1155,9 +1162,8 @@ private boolean fileSystemExists() throws IOException {\n     LOG.debug(\n             \"AzureBlobFileSystem.fileSystemExists uri: {}\", uri);\n     try {\n-      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n-          fileSystemID, HdfsOperationConstants.GET_FILESTATUS,\n-          tracingContextFormat, listener);\n+      TracingContext tracingContext = new TracingContext(clientCorrelationId,\n+          fileSystemId, FSOperationType.TEST_OP, tracingHeaderFormat, listener);\n       abfsStore.getFilesystemProperties(tracingContext);\n     } catch (AzureBlobFileSystemException ex) {\n       try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NzQ5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657347490", "bodyText": "What is this operation been set on Listener?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:09:26Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1283,6 +1373,11 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  @VisibleForTesting\n+  void setListenerOperation(String operation) {\n+    listener.setOperation(operation);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjUxOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212519", "bodyText": "this is to reset and verify the 2-letter operation type (along with other IDs) when header tests are triggered using callback after header construction", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NzQ5MA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\nindex d3c3ef8ec47..9206efeae1e 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java\n\n@@ -1374,7 +1377,7 @@ public String getCanonicalServiceName() {\n   }\n \n   @VisibleForTesting\n-  void setListenerOperation(String operation) {\n+  void setListenerOperation(FSOperationType operation) {\n     listener.setOperation(operation);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1NTcwMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657355703", "bodyText": "This is the tracing header format right?  Will that be a better name?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:16:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";\n+  public static final String FS_AZURE_TRACINGCONTEXT_FORMAT = \"fs.azure.tracingcontext.format\";", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjU1Mw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212553", "bodyText": "Yes, renamed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1NTcwMw=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\nindex 4e3afed5112..df9845130bc 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n\n@@ -114,7 +114,7 @@\n    * x-ms-client-request-Id header. Defaults to empty string if the length and\n    * character constraints are not satisfied. **/\n   public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";\n-  public static final String FS_AZURE_TRACINGCONTEXT_FORMAT = \"fs.azure.tracingcontext.format\";\n+  public static final String FS_AZURE_TRACINGHEADER_FORMAT = \"fs.azure.tracingheader.format\";\n   public static final String FS_AZURE_CLUSTER_NAME = \"fs.azure.cluster.name\";\n   public static final String FS_AZURE_CLUSTER_TYPE = \"fs.azure.cluster.type\";\n   public static final String FS_AZURE_SSL_CHANNEL_MODE_KEY = \"fs.azure.ssl.channel.mode\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657359784", "bodyText": "Can be Enum?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:21:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.constants;\n+\n+public final class HdfsOperationConstants {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzY0MDA3MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657640070", "bodyText": "Can we avoid the hdfs in this ?  Its after all Hadoop common FS API types", "author": "anoopsjohn", "createdAt": "2021-06-24T05:38:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjY5Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212696", "bodyText": "switched to enum; renamed class to FSOperationType", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java\ndeleted file mode 100644\nindex cb2fc81ab0d..00000000000\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java\n+++ /dev/null\n\n@@ -1,50 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.fs.azurebfs.constants;\n-\n-public final class HdfsOperationConstants {\n-    public static final String ACCESS = \"AS\";\n-    public static final String APPEND = \"AP\";\n-    public static final String BREAK_LEASE = \"BL\";\n-    public static final String CREATE = \"CR\";\n-    public static final String CREATE_FILESYSTEM = \"CF\";\n-    public static final String DELETE = \"DL\";\n-    public static final String GET_ACL_STATUS = \"GA\";\n-    public static final String GET_ATTR = \"GR\";\n-    public static final String GET_FILESTATUS = \"GF\";\n-    public static final String LISTSTATUS = \"LS\";\n-    public static final String MKDIR = \"MK\";\n-    public static final String MODIFY_ACL = \"MA\";\n-    public static final String OPEN = \"OP\";\n-    public static final String HAS_PATH_CAPABILITY = \"PC\";\n-    public static final String SET_PERMISSION = \"SP\";\n-    public static final String READ = \"RE\";\n-    public static final String RELEASE_LEASE = \"RL\";\n-    public static final String REMOVE_ACL = \"RA\";\n-    public static final String REMOVE_ACL_ENTRIES = \"RT\";\n-    public static final String REMOVE_DEFAULT_ACL = \"RD\";\n-    public static final String RENAME = \"RN\";\n-    public static final String SET_ATTR = \"SR\";\n-    public static final String SET_OWNER = \"SO\";\n-    public static final String SET_ACL = \"SA\";\n-    public static final String TEST_OP = \"TS\";\n-    public static final String WRITE = \"WR\";\n-\n-    private HdfsOperationConstants() {}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2NDE1MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657364150", "bodyText": "Only used for tests?  Should be returning List only?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:28:28Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -166,6 +166,11 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  @VisibleForTesting\n+  public String getRequestHeader(String httpHeader) {\n+    return connection.getRequestProperties().get(httpHeader).toString();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjc3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212778", "bodyText": "method not required, removed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2NDE1MA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\nindex 92cc53f958e..09c5fe549d1 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\n\n@@ -166,11 +166,6 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n-  @VisibleForTesting\n-  public String getRequestHeader(String httpHeader) {\n-    return connection.getRequestProperties().get(httpHeader).toString();\n-  }\n-\n   // Returns a trace message for the request\n   @Override\n   public String toString() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2ODM3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657368378", "bodyText": "Can avoid this way of explicit call which for sure should get executed before setting of this header.\nCan we have a better method name than toString() for generation of required header value?  This has to consider the format and generate.", "author": "anoopsjohn", "createdAt": "2021-06-23T18:34:45Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestID();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjk1Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212957", "bodyText": "Generating the clientRequestId in the toString() method of tracingContext is not a good option as it will re-generate it every time the method is called. Also, we need tracingContext to store the id for consistency\nHave renamed toString to constructHeader. Format is passed over config and already available to tracingContext object", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2ODM3OA=="}], "type": "inlineReview", "revised_code": {"commit": "1567e3e82240a242a6f11970d6d313db575a3787", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\nindex f89c8bf3705..3547ba8acb3 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n\n@@ -228,10 +228,10 @@ private void completeExecute(TracingContext tracingContext)\n \n   private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n       TracingContext tracingContext) {\n-    tracingContext.generateClientRequestID();\n+    tracingContext.generateClientRequestId();\n     httpOperation.getConnection()\n         .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n-            tracingContext.toString());\n+            tracingContext.constructHeader());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM3NTUyMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657375523", "bodyText": "Why passing 'tracingContext' when its set as instance member?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:45:50Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -451,15 +472,15 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length);\n+      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n-      return readRemote(position, b, offset, length);\n+      return readRemote(position, b, offset, length, new TracingContext(tracingContext));\n     }\n   }\n \n-  int readRemote(long position, byte[] b, int offset, int length) throws IOException {\n+  int readRemote(long position, byte[] b, int offset, int length, TracingContext tracingContext) throws IOException {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMzA3NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658213074", "bodyText": "The readRemote function is also used by the readahead worker threads (in ReadBufferWorker). The tracingContext passed from the readahead buffers may correspond to independent read requests", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM3NTUyMw=="}], "type": "inlineReview", "revised_code": null}, {"oid": "1567e3e82240a242a6f11970d6d313db575a3787", "url": "https://github.com/apache/hadoop/commit/1567e3e82240a242a6f11970d6d313db575a3787", "message": "address review comments", "committedDate": "2021-06-24T19:01:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxMzYxMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658613610", "bodyText": "This clone of Context needed here?  What gets changed?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:07:10Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -114,13 +119,15 @@ public AbfsLease(AbfsClient client, String path, int acquireMaxRetries,\n     LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n   }\n \n-  private void acquireLease(RetryPolicy retryPolicy, int numRetries, int retryInterval, long delay)\n+  private void acquireLease(RetryPolicy retryPolicy, int numRetries,\n+      int retryInterval, long delay)\n       throws LeaseException {\n     LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n     if (future != null && !future.isDone()) {\n       throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n     }\n-    future = client.schedule(() -> client.acquireLease(path, INFINITE_LEASE_DURATION),\n+    future = client.schedule(() -> client.acquireLease(path,\n+        INFINITE_LEASE_DURATION, new TracingContext(tracingContext)),", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4MzkwOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660083909", "bodyText": "not needed; moved to calling method (constructor) where it is cloned only once, instead of per retry", "author": "sumangala-patki", "createdAt": "2021-06-28T20:09:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxMzYxMA=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java\nindex 7421bc2e282..2e97598ef04 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java\n\n@@ -120,14 +121,14 @@ public AbfsLease(AbfsClient client, String path, int acquireMaxRetries,\n   }\n \n   private void acquireLease(RetryPolicy retryPolicy, int numRetries,\n-      int retryInterval, long delay)\n+      int retryInterval, long delay, TracingContext tracingContext)\n       throws LeaseException {\n     LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n     if (future != null && !future.isDone()) {\n       throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n     }\n     future = client.schedule(() -> client.acquireLease(path,\n-        INFINITE_LEASE_DURATION, new TracingContext(tracingContext)),\n+        INFINITE_LEASE_DURATION, tracingContext),\n         delay, TimeUnit.SECONDS);\n     client.addCallback(future, new FutureCallback<AbfsRestOperation>() {\n       @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615501", "bodyText": "getOutputStreamId() and getStreamID() -> Both create some confusion.  Normally the Getter just return a already available value.  getStreamID() make sense.\nYou can use createOutputStreamId() instead?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:10:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -160,6 +170,14 @@ public AbfsOutputStream(\n     if (outputStreamStatistics != null) {\n       this.ioStatistics = outputStreamStatistics.getIOStatistics();\n     }\n+    this.outputStreamId = getOutputStreamId();\n+    this.tracingContext = new TracingContext(tracingContext);\n+    this.tracingContext.setStreamID(outputStreamId);\n+    this.tracingContext.setOperation(FSOperationType.WRITE);\n+  }\n+\n+  private String getOutputStreamId() {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTY5NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615694", "bodyText": "The same thing in ABFSInputStream also", "author": "anoopsjohn", "createdAt": "2021-06-25T09:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4Mzk3NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660083974", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex b9d57a0aab1..91b068a78c9 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -170,13 +170,13 @@ public AbfsOutputStream(\n     if (outputStreamStatistics != null) {\n       this.ioStatistics = outputStreamStatistics.getIOStatistics();\n     }\n-    this.outputStreamId = getOutputStreamId();\n+    this.outputStreamId = createOutputStreamId();\n     this.tracingContext = new TracingContext(tracingContext);\n     this.tracingContext.setStreamID(outputStreamId);\n     this.tracingContext.setOperation(FSOperationType.WRITE);\n   }\n \n-  private String getOutputStreamId() {\n+  private String createOutputStreamId() {\n     return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjMzNw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616337", "bodyText": "Ok the context been passed here might get changed at least wrt the retryCount.  that is why been cloned here?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:11:22Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -385,7 +412,9 @@ private void writeAppendBlobCurrentBufferToService() throws IOException {\n             \"writeCurrentBufferToService\", \"append\")) {\n       AppendRequestParameters reqParams = new AppendRequestParameters(offset, 0,\n           bytesLength, APPEND_MODE, true, leaseId);\n-      AbfsRestOperation op = client.append(path, bytes, reqParams, cachedSasToken.get());\n+      AbfsRestOperation op = client\n+          .append(path, bytes, reqParams, cachedSasToken.get(),\n+              new TracingContext(tracingContext));", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDAwNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084004", "bodyText": "yes", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjMzNw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjY5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616690", "bodyText": "Yaaa here.", "author": "anoopsjohn", "createdAt": "2021-06-25T09:11:54Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -202,9 +206,10 @@ private void completeExecute() throws AzureBlobFileSystemException {\n \n     retryCount = 0;\n     LOG.debug(\"First execution of REST operation - {}\", operationType);\n-    while (!executeHttpOperation(retryCount)) {\n+    while (!executeHttpOperation(retryCount, tracingContext)) {\n       try {\n         ++retryCount;\n+        tracingContext.setRetryCount(retryCount);", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODIzMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618232", "bodyText": "generateClientRequestId() can be done internally within constructHeader()?  Else its upto callee to set it proper before.  Actually the UUID clientReqId generation should be an internal responsibility of this tracingContext right?\nAll other methods are like setter.", "author": "anoopsjohn", "createdAt": "2021-06-25T09:14:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDIzMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084233", "bodyText": "constructHeader() may be called simply to display IDs, in which case it should not regenerate new clientReqId each time\ncallee setting it would mean adding code to all methods to call this, and retry case would not be handled\nCan be done internally within TC: have a generateClientRequestId function to create guid, and call it in constructor as well as the setRetryCount which is invoked per retry. However, had avoided this since we are populating ID variables only as and when they pass through the corresponding ABFS layers.", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODIzMg=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\nindex 3547ba8acb3..c6582b0653f 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n\n@@ -226,7 +226,7 @@ private void completeExecute(TracingContext tracingContext)\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n-  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+  private void setClientRequestHeader(AbfsHttpOperation httpOperation,\n       TracingContext tracingContext) {\n     tracingContext.generateClientRequestId();\n     httpOperation.getConnection()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODg0Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618847", "bodyText": "Its actually a fresh AbfsHttpOperation operation and here by we set the header.  Can we call it setXXX  than updateXXX?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:15:17Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());\n+  }\n+\n   /**\n    * Executes a single HTTP operation to complete the REST operation.  If it\n    * fails, there may be a retry.  The retryCount is incremented with each\n    * attempt.\n    */\n-  private boolean executeHttpOperation(final int retryCount) throws AzureBlobFileSystemException {\n+  private boolean executeHttpOperation(final int retryCount,\n+    TracingContext tracingContext) throws AzureBlobFileSystemException {\n     AbfsHttpOperation httpOperation = null;\n     try {\n       // initialize the HTTP request and open the connection\n       httpOperation = new AbfsHttpOperation(url, method, requestHeaders);\n       incrementCounter(AbfsStatistic.CONNECTIONS_MADE, 1);\n+      updateClientRequestHeader(httpOperation, tracingContext);", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDMwNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084305", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODg0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\nindex 3547ba8acb3..c6582b0653f 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n\n@@ -226,7 +226,7 @@ private void completeExecute(TracingContext tracingContext)\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n-  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+  private void setClientRequestHeader(AbfsHttpOperation httpOperation,\n       TracingContext tracingContext) {\n     tracingContext.generateClientRequestId();\n     httpOperation.getConnection()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYyMDA0MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658620041", "bodyText": "This is for testability only?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:17:10Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/Listener.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+\n+/**\n+ * Interface for testing identifiers tracked via TracingContext\n+ * Implemented in TracingHeaderValidator\n+ */\n+\n+public interface Listener {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDQxNg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084416", "bodyText": "yes, it's an interface to trigger header tests through callback when header is constructed. The tests are run only when listener is registered (not null), which is done across existing tests for different methods", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYyMDA0MQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMDYwMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658630602", "bodyText": "primaryRequestID will be used in case of Read ahead?  Worth adding come comments about its use here", "author": "anoopsjohn", "createdAt": "2021-06-25T09:34:03Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDUzMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084530", "bodyText": "primaryRequestId is applicable for any method call that triggers more than one http request. For example, methods using continuation logic like listStatus and rename\nHave added comments explaining use of tracingContext and its members at the beginning of file", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMDYwMg=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\nindex 0df792c2852..2b0e381e8ed 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n\n@@ -39,7 +39,7 @@\n  *\n  * Add new operations to HdfsOperationConstants file.\n  *\n- * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * PrimaryRequestId can be enabled for individual Hadoop API that invoke\n  * multiple Store calls.\n  *\n  * Testing:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMTEzMQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658631131", "bodyText": "call it opType only?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:34:52Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDU4NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084584", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMTEzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\nindex 0df792c2852..2b0e381e8ed 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n\n@@ -39,7 +39,7 @@\n  *\n  * Add new operations to HdfsOperationConstants file.\n  *\n- * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * PrimaryRequestId can be enabled for individual Hadoop API that invoke\n  * multiple Store calls.\n  *\n  * Testing:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMzQ1Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658633456", "bodyText": "A regex matching call is not that cheap. We will end up calling this for every object creation of this TracingContext.  Can we limit this check only at the time of FS instantiation?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:38:32Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDY0OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084648", "bodyText": "true. Changed the validate method to be static, calling from ABFS constructor only now", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMzQ1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\nindex 0df792c2852..2b0e381e8ed 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n\n@@ -39,7 +39,7 @@\n  *\n  * Add new operations to HdfsOperationConstants file.\n  *\n- * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * PrimaryRequestId can be enabled for individual Hadoop API that invoke\n  * multiple Store calls.\n  *\n  * Testing:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NDAzMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658644030", "bodyText": "In any kind of op clientCorrelationID , clientRequestId , fileSystemID , hadoopOpName and retryCount will be present\nOptional things are primaryRequestID and streamID .  Correct?\nWorth detailing in some comments here.\nWhen these 2 are not there it will come like ::::...  ?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:55:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {\n+      LOG.debug(\n+          \"Invalid config provided; correlation id not included in header.\");\n+      return EMPTY_STRING;\n+    }\n+    return clientCorrelationID;\n+  }\n+\n+  public void generateClientRequestId() {\n+    clientRequestId = UUID.randomUUID().toString();\n+  }\n+\n+  public void setPrimaryRequestID() {\n+    primaryRequestID = UUID.randomUUID().toString();\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public void setStreamID(String stream) {\n+    streamID = stream;\n+  }\n+\n+  public void setOperation(FSOperationType operation) {\n+    this.hadoopOpName = operation;\n+  }\n+\n+  public void setRetryCount(int retryCount) {\n+    this.retryCount = retryCount;\n+  }\n+\n+  public void setListener(Listener listener) {\n+    this.listener = listener;\n+  }\n+\n+  public String constructHeader() {\n+    String header;\n+    switch (format) {\n+    case ALL_ID_FORMAT:\n+      header =", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDc0Mg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084742", "bodyText": "Added description to member declaration and added note in the fn\nYes the number of separators (:) is kept constant for parsing", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NDAzMA=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\nindex 0df792c2852..2b0e381e8ed 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java\n\n@@ -39,7 +39,7 @@\n  *\n  * Add new operations to HdfsOperationConstants file.\n  *\n- * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * PrimaryRequestId can be enabled for individual Hadoop API that invoke\n  * multiple Store calls.\n  *\n  * Testing:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NTEyOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658645129", "bodyText": "Will the below way will be more consistent?\nSINGLE_ID_FORMAT,  // client-req-id\nTWO_ID_FORMAT; // client-req-id:client-correlation-id\nALL_ID_FORMAT,  // client-req-id:client-correlation-id:filesystem-id:primary-req-id:stream-id:hdfs-operation:retry-count", "author": "anoopsjohn", "createdAt": "2021-06-25T09:57:09Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderFormat {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDc4Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084787", "bodyText": "switched enum, but keeping correlationId at the start would be easier to distinguish it as a custom id of variable length", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NTEyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "ecee1809de89ed5b4fc76df79dd64895537aab99", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java\nindex 93af796f880..3f23ae3ed7c 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java\n\n@@ -21,8 +21,8 @@\n public enum TracingHeaderFormat {\n   SINGLE_ID_FORMAT,  // <client-req-id>\n \n-  ALL_ID_FORMAT,  // <client-correlation-id>:<client-req-id>:<filesystem-id>\n-  // :<primary-req-id>:<stream-id>:<hdfs-operation>:<retry-count>\n+  TWO_ID_FORMAT,  // <correlation-id>:<client-req-id>\n \n-  TWO_ID_FORMAT; // <correlation-id>:<client-req-id>\n+  ALL_ID_FORMAT;  // <client-correlation-id>:<client-req-id>:<filesystem-id>\n+  // :<primary-req-id>:<stream-id>:<hdfs-operation>:<retry-count>\n }\n"}}, {"oid": "ecee1809de89ed5b4fc76df79dd64895537aab99", "url": "https://github.com/apache/hadoop/commit/ecee1809de89ed5b4fc76df79dd64895537aab99", "message": "revw comments", "committedDate": "2021-06-28T20:02:10Z", "type": "commit"}, {"oid": "b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "url": "https://github.com/apache/hadoop/commit/b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "message": "correction", "committedDate": "2021-06-30T09:18:17Z", "type": "commit"}, {"oid": "8cf0fe32c89351a55c475cdd91261335591129f8", "url": "https://github.com/apache/hadoop/commit/8cf0fe32c89351a55c475cdd91261335591129f8", "message": "checkstyle", "committedDate": "2021-07-01T09:00:24Z", "type": "commit"}, {"oid": "0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "url": "https://github.com/apache/hadoop/commit/0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "message": "set header in tc", "committedDate": "2021-07-02T12:03:53Z", "type": "commit"}]}