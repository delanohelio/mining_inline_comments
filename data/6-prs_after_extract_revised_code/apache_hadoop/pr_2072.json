{"pr_number": 2072, "pr_title": "HADOOP-17058. ABFS: Support for AppendBlob in Hadoop ABFS Driver", "pr_createdAt": "2020-06-12T17:19:16Z", "pr_url": "https://github.com/apache/hadoop/pull/2072", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846153", "bodyText": "nit: the 2 if here can be combined with &&", "author": "bilaharith", "createdAt": "2020-06-14T16:27:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0Njg2Mg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444446862", "bodyText": "code is removed.", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\nindex bbab2d47b98..e9c000ac6e8 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n\n@@ -1352,28 +1354,6 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n-  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n-\n-    for (String dir : dirSet) {\n-      if (dir.isEmpty() || key.startsWith(dir)) {\n-        return true;\n-      }\n-\n-      try {\n-        URI uri = new URI(dir);\n-        if (null == uri.getAuthority()) {\n-          if (key.startsWith(dir + \"/\")){\n-            return true;\n-          }\n-        }\n-      } catch (URISyntaxException e) {\n-        LOG.info(\"URI syntax error creating URI for {}\", dir);\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n   private boolean isKeyForDirectorySet(String key, Set<String> dirSet) {\n \n     for (String dir : dirSet) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjIxMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846213", "bodyText": "nit: Use the constant for forward slash", "author": "bilaharith", "createdAt": "2020-06-14T16:27:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\nindex bbab2d47b98..e9c000ac6e8 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n\n@@ -1352,28 +1354,6 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n-  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n-\n-    for (String dir : dirSet) {\n-      if (dir.isEmpty() || key.startsWith(dir)) {\n-        return true;\n-      }\n-\n-      try {\n-        URI uri = new URI(dir);\n-        if (null == uri.getAuthority()) {\n-          if (key.startsWith(dir + \"/\")){\n-            return true;\n-          }\n-        }\n-      } catch (URISyntaxException e) {\n-        LOG.info(\"URI syntax error creating URI for {}\", dir);\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n   private boolean isKeyForDirectorySet(String key, Set<String> dirSet) {\n \n     for (String dir : dirSet) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847515", "bodyText": "How about throwing an AzureBlobFileSystemException from here. So that the customer will get to know that the config is not correct.", "author": "bilaharith", "createdAt": "2020-06-14T16:44:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){\n+            return true;\n+          }\n+        }\n+      } catch (URISyntaxException e) {\n+        LOG.info(\"URI syntax error creating URI for {}\", dir);", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMTIwMQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440001201", "bodyText": "this is used for every file being created, returning true or false. Raising an exception can be a problem.", "author": "ishaniahuja", "createdAt": "2020-06-15T08:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA5NTkwMA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440095900", "bodyText": "but what if only one comma separated value is incorrect, we would be throwing an exception adn crashing the app/jvm?", "author": "ishaniahuja", "createdAt": "2020-06-15T10:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\nindex bbab2d47b98..e9c000ac6e8 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n\n@@ -1352,28 +1354,6 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n-  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n-\n-    for (String dir : dirSet) {\n-      if (dir.isEmpty() || key.startsWith(dir)) {\n-        return true;\n-      }\n-\n-      try {\n-        URI uri = new URI(dir);\n-        if (null == uri.getAuthority()) {\n-          if (key.startsWith(dir + \"/\")){\n-            return true;\n-          }\n-        }\n-      } catch (URISyntaxException e) {\n-        LOG.info(\"URI syntax error creating URI for {}\", dir);\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n   private boolean isKeyForDirectorySet(String key, Set<String> dirSet) {\n \n     for (String dir : dirSet) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847645", "bodyText": "fs.azure.appendblob.key config name doesn't look good. Wouldsomething like fs.azure.appendblob.directories convey the meaning better? (The way it is named in the FileSystemConfiguarations)", "author": "bilaharith", "createdAt": "2020-06-14T16:45:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -59,6 +59,9 @@\n   public static final String FS_AZURE_ENABLE_AUTOTHROTTLING = \"fs.azure.enable.autothrottling\";\n   public static final String FS_AZURE_ALWAYS_USE_HTTPS = \"fs.azure.always.use.https\";\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n+  /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n+   *  Default is empty. **/\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMjQ0Ng==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440002446", "bodyText": "I have made is similar to FS_AZURE_ATOMIC_RENAME_KEY which also provide a set of directories. Further please note for appendblob, this is actually a prefix for the path (and not necessarily the directories). This is done so that the test suite (which all runs on a container with a randon guid can run) can run on appendblob based files. Let me know ur comments/thoughts here.", "author": "ishaniahuja", "createdAt": "2020-06-15T08:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzA0Nw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447047", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\nindex e4430d7d4dd..b5feee64ab4 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java\n\n@@ -61,7 +61,7 @@\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n   /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n    *  Default is empty. **/\n-  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.directories\";\n   public static final String FS_AZURE_READ_AHEAD_QUEUE_DEPTH = \"fs.azure.readaheadqueue.depth\";\n   /** Provides a config control to enable or disable ABFS Flush operations -\n    *  HFlush and HSync. Default is true. **/\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848105", "bodyText": "Could you move this append blob handling to a separate method and call the same from here.", "author": "bilaharith", "createdAt": "2020-06-14T16:51:02Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -323,6 +328,35 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n     final long offset = position;\n     position += bytesLength;\n \n+    if (this.isAppendBlob) {", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzE5OA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447198", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 230488c5c13..67d4f99d6b4 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -328,34 +328,51 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n     final long offset = position;\n     position += bytesLength;\n \n-    if (this.isAppendBlob) {\n-      try {\n-        AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n-        try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker,\n-                \"writeCurrentBufferToService\", \"append\")) {\n-          AbfsRestOperation op = client.append(path, offset, bytes, 0,\n-              bytesLength, cachedSasToken.get(), this.isAppendBlob);\n-          cachedSasToken.update(op.getSasToken());\n-          outputStreamStatistics.uploadSuccessful(bytesLength);\n-          perfInfo.registerResult(op.getResult());\n-          byteBufferPool.putBuffer(ByteBuffer.wrap(bytes));\n-          perfInfo.registerSuccess(true);\n-        }\n-        return;\n-      } catch (Exception ex) {\n-        if (ex instanceof AbfsRestOperationException) {\n-          if (((AbfsRestOperationException) ex).getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n-            throw new FileNotFoundException(ex.getMessage());\n-          }\n-        }\n-        if (ex instanceof AzureBlobFileSystemException) {\n-          ex = (AzureBlobFileSystemException) ex;\n+    try {\n+      AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n+      try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker,\n+              \"writeCurrentBufferToService\", \"append\")) {\n+        AbfsRestOperation op = client.append(path, offset, bytes, 0,\n+            bytesLength, cachedSasToken.get(), this.isAppendBlob);\n+        cachedSasToken.update(op.getSasToken());\n+        outputStreamStatistics.uploadSuccessful(bytesLength);\n+        perfInfo.registerResult(op.getResult());\n+        byteBufferPool.putBuffer(ByteBuffer.wrap(bytes));\n+        perfInfo.registerSuccess(true);\n+      }\n+      return;\n+    } catch (Exception ex) {\n+      if (ex instanceof AbfsRestOperationException) {\n+        if (((AbfsRestOperationException) ex).getStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {\n+          throw new FileNotFoundException(ex.getMessage());\n         }\n-        lastError = new IOException(ex);\n-        throw lastError;\n       }\n+      if (ex instanceof AzureBlobFileSystemException) {\n+        ex = (AzureBlobFileSystemException) ex;\n+      }\n+      lastError = new IOException(ex);\n+      throw lastError;\n+    }\n+  }\n+\n+  private synchronized void writeCurrentBufferToService() throws IOException {\n+    if (this.isAppendBlob) {\n+      writeAppendBlobCurrentBufferToService();\n+      return;\n+    }\n+\n+    if (bufferIndex == 0) {\n+      return;\n     }\n+    outputStreamStatistics.writeCurrentBuffer();\n \n+    final byte[] bytes = buffer;\n+    final int bytesLength = bufferIndex;\n+    outputStreamStatistics.bytesToUpload(bytesLength);\n+    buffer = byteBufferPool.getBuffer(false, bufferSize).array();\n+    bufferIndex = 0;\n+    final long offset = position;\n+    position += bytesLength;\n \n     if (threadExecutor.getQueue().size() >= maxConcurrentRequestCount * 2) {\n       long start = System.currentTimeMillis();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848257", "bodyText": "nit: Should we have a debug log here?", "author": "bilaharith", "createdAt": "2020-06-14T16:52:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +423,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+\n+    // flush is not called for appendblob as is not needed\n+    if (this.isAppendBlob) {\n+      return;", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA4MTQ0OA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440081448", "bodyText": "this can lead to frequent log lines, every time a flush(), hflush(), hsync() is called. will that be ok?", "author": "ishaniahuja", "createdAt": "2020-06-15T10:30:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NzQxMg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445367412", "bodyText": "Resolved", "author": "bilaharith", "createdAt": "2020-06-25T07:43:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 230488c5c13..67d4f99d6b4 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -424,8 +441,8 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n \n-    // flush is not called for appendblob as is not needed\n-    if (this.isAppendBlob) {\n+    // flush is called for appendblob only on close\n+    if (this.isAppendBlob && !isClose) {\n       return;\n     }\n \n"}}, {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "url": "https://github.com/apache/hadoop/commit/d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "message": "checkstyle fixes", "committedDate": "2020-06-13T03:11:28Z", "type": "forcePushed"}, {"oid": "2efbe0d0b93255e7cc9e8af771609d99dff8caed", "url": "https://github.com/apache/hadoop/commit/2efbe0d0b93255e7cc9e8af771609d99dff8caed", "message": "changes for appendblo", "committedDate": "2020-06-20T18:31:04Z", "type": "commit"}, {"oid": "cda6d67702d59acff8bca4aa062be108f2f33967", "url": "https://github.com/apache/hadoop/commit/cda6d67702d59acff8bca4aa062be108f2f33967", "message": "yetus fixes", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "89d836deab495b4be559402003c49e795b888e32", "url": "https://github.com/apache/hadoop/commit/89d836deab495b4be559402003c49e795b888e32", "message": "checkstyle fixes", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "url": "https://github.com/apache/hadoop/commit/99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "message": "incorporated feedbacks", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "commit"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "forcePushed"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1MzY0OQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445353649", "bodyText": "Minor. Fix comment.", "author": "snvijaya", "createdAt": "2020-06-25T07:16:58Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -47,6 +47,7 @@\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n+  public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 8 MB", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java\nindex 4e4dac1236a..a367daf6ee5 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java\n\n@@ -47,7 +47,7 @@\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n-  public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 8 MB\n+  public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n   public static final int DEFAULT_READ_BUFFER_SIZE = 4 * ONE_MB;  // 4 MB\n   public static final int MIN_BUFFER_SIZE = 16 * ONE_KB;  // 16 KB\n   public static final int MAX_BUFFER_SIZE = 100 * ONE_MB;  // 100 MB\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDEyMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354123", "bodyText": "Minor. boolean flag better named as isAppendBlob", "author": "snvijaya", "createdAt": "2020-06-25T07:17:49Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -272,7 +272,8 @@ public AbfsRestOperation deleteFilesystem() throws AzureBlobFileSystemException\n   }\n \n   public AbfsRestOperation createPath(final String path, final boolean isFile, final boolean overwrite,\n-                                      final String permission, final String umask) throws AzureBlobFileSystemException {\n+                                      final String permission, final String umask,\n+                                      final boolean appendBlob) throws AzureBlobFileSystemException {", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java\nindex da5bedfb202..3cdc9816be3 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java\n\n@@ -273,7 +273,7 @@ public AbfsRestOperation deleteFilesystem() throws AzureBlobFileSystemException\n \n   public AbfsRestOperation createPath(final String path, final boolean isFile, final boolean overwrite,\n                                       final String permission, final String umask,\n-                                      final boolean appendBlob) throws AzureBlobFileSystemException {\n+                                      final boolean isAppendBlob) throws AzureBlobFileSystemException {\n     final List<AbfsHttpHeader> requestHeaders = createDefaultHeaders();\n     if (!overwrite) {\n       requestHeaders.add(new AbfsHttpHeader(IF_NONE_MATCH, AbfsHttpConstants.STAR));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDY0NA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354644", "bodyText": "Undo. newline needed after a block.", "author": "snvijaya", "createdAt": "2020-06-25T07:18:50Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -337,7 +344,6 @@ public void processResponse(final byte[] buffer, final int offset, final int len\n     if (this.isTraceEnabled) {\n       startTime = System.nanoTime();\n     }\n-", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\nindex a904ed0b375..a63c98261f1 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java\n\n@@ -344,6 +344,7 @@ public void processResponse(final byte[] buffer, final int offset, final int len\n     if (this.isTraceEnabled) {\n       startTime = System.nanoTime();\n     }\n+\n     if (statusCode >= HttpURLConnection.HTTP_BAD_REQUEST) {\n       processStorageErrorResponse();\n       if (this.isTraceEnabled) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTA0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355045", "bodyText": "Undo. new line needed.", "author": "snvijaya", "createdAt": "2020-06-25T07:19:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -367,7 +418,6 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 67d4f99d6b4..a70e13c3d17 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -418,6 +418,7 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n+\n         if (ex.getCause() instanceof AzureBlobFileSystemException) {\n           ex = (AzureBlobFileSystemException) ex.getCause();\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTIyNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355225", "bodyText": "unnecessary additional new line.", "author": "snvijaya", "createdAt": "2020-06-25T07:19:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -378,6 +428,7 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n     flushWrittenBytesToServiceInternal(position, false, isClose);\n   }\n \n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 67d4f99d6b4..a70e13c3d17 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -428,7 +429,6 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n     flushWrittenBytesToServiceInternal(position, false, isClose);\n   }\n \n-\n   private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n     shrinkWriteOperationQueue();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTcwNg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355706", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:20:46Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +440,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 67d4f99d6b4..a70e13c3d17 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -440,7 +440,6 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n-\n     // flush is called for appendblob only on close\n     if (this.isAppendBlob && !isClose) {\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTgxMQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355811", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:21:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -430,10 +487,15 @@ private synchronized void shrinkWriteOperationQueue() throws IOException {\n   }\n \n   private void waitForTaskToComplete() throws IOException {\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\nindex 67d4f99d6b4..a70e13c3d17 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java\n\n@@ -487,7 +486,6 @@ private synchronized void shrinkWriteOperationQueue() throws IOException {\n   }\n \n   private void waitForTaskToComplete() throws IOException {\n-\n     boolean completed;\n     for (completed = false; completionService.poll() != null; completed = true) {\n       // keep polling until there is no data\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NjQxMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445356413", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:22:07Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\nindex 2d89db52c2e..aaab1364436 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n\n@@ -189,7 +189,6 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n-\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445357550", "bodyText": "Why is it that for Http Status code 400 and above, exception is suppressed ? Can you please add code comments for the reason.", "author": "snvijaya", "createdAt": "2020-06-25T07:24:17Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n       }\n     }\n \n     if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\n+      if (this.isAppendBlobAppend && retryCount > 0 && result.getStorageErrorCode().equals(\"InvalidQueryParameterValue\")) {", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NDk0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r446094945", "bodyText": "This isnt the right place to handle a case specific to appendblob. HttpOperation returned to specific AbfsClient method can take the call on what to return. Similar is done for rename.\nYou can check:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java#L358", "author": "snvijaya", "createdAt": "2020-06-26T10:11:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA=="}], "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\nindex 2d89db52c2e..aaab1364436 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java\n\n@@ -189,7 +189,6 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n-\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445364600", "bodyText": "Would be better to add a new line since the next line is a constructor.", "author": "bilaharith", "createdAt": "2020-06-25T07:37:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -144,6 +145,10 @@\n   private final IdentityTransformerInterface identityTransformer;\n   private final AbfsPerfTracker abfsPerfTracker;\n \n+  /**\n+   * The set of directories where we should store files as append blobs.\n+   */\n+  private Set<String> appendBlobDirSet;", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc0NTI5MQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445745291", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-25T18:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA=="}], "type": "inlineReview", "revised_code": {"commit": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\nindex e9c000ac6e8..34edc8aa704 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java\n\n@@ -149,6 +149,7 @@\n    * The set of directories where we should store files as append blobs.\n    */\n   private Set<String> appendBlobDirSet;\n+\n   public AzureBlobFileSystemStore(URI uri, boolean isSecureScheme,\n                                   Configuration configuration,\n                                   AbfsCounters abfsCounters) throws IOException {\n"}}, {"oid": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "url": "https://github.com/apache/hadoop/commit/3b7a65bf066fe41f5a1674584ce3665532fe96ed", "message": "incorporated feedbacks", "committedDate": "2020-06-25T18:42:45Z", "type": "commit"}, {"oid": "8e089333aa058a09bfcfb402293523057edf013b", "url": "https://github.com/apache/hadoop/commit/8e089333aa058a09bfcfb402293523057edf013b", "message": "support for idempotency check on appendblob append", "committedDate": "2020-06-29T08:47:19Z", "type": "commit"}, {"oid": "8e089333aa058a09bfcfb402293523057edf013b", "url": "https://github.com/apache/hadoop/commit/8e089333aa058a09bfcfb402293523057edf013b", "message": "support for idempotency check on appendblob append", "committedDate": "2020-06-29T08:47:19Z", "type": "forcePushed"}, {"oid": "26c91b3e97ed1b003b405b70653f66796daedb8a", "url": "https://github.com/apache/hadoop/commit/26c91b3e97ed1b003b405b70653f66796daedb8a", "message": "incorporated feedbacks", "committedDate": "2020-06-29T11:55:34Z", "type": "commit"}]}