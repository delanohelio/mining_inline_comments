{"pr_number": 2223, "pr_title": "MAPREDUCE-7294. Only application master should upload resource to Yarn Shared Cache", "pr_createdAt": "2020-08-13T04:33:45Z", "pr_url": "https://github.com/apache/hadoop/pull/2223", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0MTYxOQ==", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487341619", "bodyText": "The bug is when the policies are empty it won't clean up the existing policies.", "author": "JohnZZGithub", "createdAt": "2020-09-12T00:20:56Z", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {", "originalCommit": "a15e59c06c9cd5e2e9f52d48d3693592255b7465", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b356f239e3559b5bf41a950a33e903b553f8c38d", "chunk": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\nindex 2a6e77401ec..9a998dacd98 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n\n@@ -1453,30 +1453,18 @@ private static void setSharedCacheUploadPolicies(Configuration conf,\n     String confParam = areFiles ?\n         MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n         MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n-  }\n-\n-  private static String populateSharedCacheUploadPolicies(\n-      Map<String, Boolean> policies) {\n-    // If policies are an empty map or null, we will set EMPTY_STRING.\n-    // In other words, cleaning up existing policies. This is useful when we\n-    // try to clean up shared cache upload policies for non-application\n-    // master tasks. See YARN-10398 for details.\n+    // If no policy is provided, we will reset the config by setting an empty\n+    // string value. In other words, cleaning up existing policies. This is\n+    // useful when we try to clean up shared cache upload policies for\n+    // non-application master tasks. See MAPREDUCE-7294 for details.\n     if (policies == null || policies.size() == 0) {\n-      return \"\";\n+      conf.set(confParam, \"\");\n+      return;\n     }\n     StringBuilder sb = new StringBuilder();\n-    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-    Map.Entry<String, Boolean> e;\n-    if (it.hasNext()) {\n-      e = it.next();\n-      sb.append(e.getKey() + DELIM + e.getValue());\n-    }\n-    while (it.hasNext()) {\n-      e = it.next();\n-      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-    }\n-    return sb.toString();\n+    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n+    sb.deleteCharAt(sb.length() - 1);\n+    conf.set(confParam, sb.toString());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzc3MA==", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487367770", "bodyText": "I know following code was mostly borrowed from the existing code, but since we are in Java 8 for Hadoop 3, should we simplify this a bit using this chance?\n    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n    Map.Entry<String, Boolean> e;\n    if (it.hasNext()) {\n      e = it.next();\n      sb.append(e.getKey() + DELIM + e.getValue());\n    }\n    while (it.hasNext()) {\n      e = it.next();\n      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n    }\n\ncan be shorter and clearer statement, for e.g.\n    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n    sb.deleteCharAt(sb.length() - 1); // do we need this, or it is just fine?\n\nThoughts?", "author": "liuml07", "createdAt": "2020-09-12T04:49:01Z", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.\n+    // In other words, cleaning up existing policies. This is useful when we\n+    // try to clean up shared cache upload policies for non-application\n+    // master tasks. See YARN-10398 for details.\n+    if (policies == null || policies.size() == 0) {\n+      return \"\";\n+    }\n+    StringBuilder sb = new StringBuilder();", "originalCommit": "a15e59c06c9cd5e2e9f52d48d3693592255b7465", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzk1OA==", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487367958", "bodyText": "Also, given the code is so simple, maybe we can save one extra private helper method, and move the logic back to setSharedCacheUploadPolicies() method, which itself has only several lines of code.", "author": "liuml07", "createdAt": "2020-09-12T04:51:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2Nzc3MA=="}], "type": "inlineReview", "revised_code": {"commit": "b356f239e3559b5bf41a950a33e903b553f8c38d", "chunk": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\nindex 2a6e77401ec..9a998dacd98 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n\n@@ -1453,30 +1453,18 @@ private static void setSharedCacheUploadPolicies(Configuration conf,\n     String confParam = areFiles ?\n         MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n         MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n-  }\n-\n-  private static String populateSharedCacheUploadPolicies(\n-      Map<String, Boolean> policies) {\n-    // If policies are an empty map or null, we will set EMPTY_STRING.\n-    // In other words, cleaning up existing policies. This is useful when we\n-    // try to clean up shared cache upload policies for non-application\n-    // master tasks. See YARN-10398 for details.\n+    // If no policy is provided, we will reset the config by setting an empty\n+    // string value. In other words, cleaning up existing policies. This is\n+    // useful when we try to clean up shared cache upload policies for\n+    // non-application master tasks. See MAPREDUCE-7294 for details.\n     if (policies == null || policies.size() == 0) {\n-      return \"\";\n+      conf.set(confParam, \"\");\n+      return;\n     }\n     StringBuilder sb = new StringBuilder();\n-    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-    Map.Entry<String, Boolean> e;\n-    if (it.hasNext()) {\n-      e = it.next();\n-      sb.append(e.getKey() + DELIM + e.getValue());\n-    }\n-    while (it.hasNext()) {\n-      e = it.next();\n-      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-    }\n-    return sb.toString();\n+    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n+    sb.deleteCharAt(sb.length() - 1);\n+    conf.set(confParam, sb.toString());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2ODA1Nw==", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487368057", "bodyText": "Since this JIRA has been moved from YARN to MAPREDUCE project, should we replace the YARN-10398 in comment with the new JIRA number MAPREDUCE-7294?", "author": "liuml07", "createdAt": "2020-09-12T04:52:14Z", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.\n+    // In other words, cleaning up existing policies. This is useful when we\n+    // try to clean up shared cache upload policies for non-application\n+    // master tasks. See YARN-10398 for details.", "originalCommit": "a15e59c06c9cd5e2e9f52d48d3693592255b7465", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b356f239e3559b5bf41a950a33e903b553f8c38d", "chunk": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\nindex 2a6e77401ec..9a998dacd98 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n\n@@ -1453,30 +1453,18 @@ private static void setSharedCacheUploadPolicies(Configuration conf,\n     String confParam = areFiles ?\n         MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n         MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n-  }\n-\n-  private static String populateSharedCacheUploadPolicies(\n-      Map<String, Boolean> policies) {\n-    // If policies are an empty map or null, we will set EMPTY_STRING.\n-    // In other words, cleaning up existing policies. This is useful when we\n-    // try to clean up shared cache upload policies for non-application\n-    // master tasks. See YARN-10398 for details.\n+    // If no policy is provided, we will reset the config by setting an empty\n+    // string value. In other words, cleaning up existing policies. This is\n+    // useful when we try to clean up shared cache upload policies for\n+    // non-application master tasks. See MAPREDUCE-7294 for details.\n     if (policies == null || policies.size() == 0) {\n-      return \"\";\n+      conf.set(confParam, \"\");\n+      return;\n     }\n     StringBuilder sb = new StringBuilder();\n-    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-    Map.Entry<String, Boolean> e;\n-    if (it.hasNext()) {\n-      e = it.next();\n-      sb.append(e.getKey() + DELIM + e.getValue());\n-    }\n-    while (it.hasNext()) {\n-      e = it.next();\n-      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-    }\n-    return sb.toString();\n+    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n+    sb.deleteCharAt(sb.length() - 1);\n+    conf.set(confParam, sb.toString());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM2ODE4OQ==", "url": "https://github.com/apache/hadoop/pull/2223#discussion_r487368189", "bodyText": "nit: this sentence can be:\n// If no policy is provided, we will reset the config by setting an empty string value.", "author": "liuml07", "createdAt": "2020-09-12T04:54:14Z", "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java", "diffHunk": "@@ -1450,26 +1450,33 @@ public static void setArchiveSharedCacheUploadPolicies(Configuration conf,\n    */\n   private static void setSharedCacheUploadPolicies(Configuration conf,\n       Map<String, Boolean> policies, boolean areFiles) {\n-    if (policies != null) {\n-      StringBuilder sb = new StringBuilder();\n-      Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-      Map.Entry<String, Boolean> e;\n-      if (it.hasNext()) {\n-        e = it.next();\n-        sb.append(e.getKey() + DELIM + e.getValue());\n-      } else {\n-        // policies is an empty map, just skip setting the parameter\n-        return;\n-      }\n-      while (it.hasNext()) {\n-        e = it.next();\n-        sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-      }\n-      String confParam =\n-          areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES\n-              : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-      conf.set(confParam, sb.toString());\n+    String confParam = areFiles ?\n+        MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n+        MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n+    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n+  }\n+\n+  private static String populateSharedCacheUploadPolicies(\n+      Map<String, Boolean> policies) {\n+    // If policies are an empty map or null, we will set EMPTY_STRING.", "originalCommit": "a15e59c06c9cd5e2e9f52d48d3693592255b7465", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b356f239e3559b5bf41a950a33e903b553f8c38d", "chunk": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\nindex 2a6e77401ec..9a998dacd98 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java\n\n@@ -1453,30 +1453,18 @@ private static void setSharedCacheUploadPolicies(Configuration conf,\n     String confParam = areFiles ?\n         MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES :\n         MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;\n-    conf.set(confParam, populateSharedCacheUploadPolicies(policies));\n-  }\n-\n-  private static String populateSharedCacheUploadPolicies(\n-      Map<String, Boolean> policies) {\n-    // If policies are an empty map or null, we will set EMPTY_STRING.\n-    // In other words, cleaning up existing policies. This is useful when we\n-    // try to clean up shared cache upload policies for non-application\n-    // master tasks. See YARN-10398 for details.\n+    // If no policy is provided, we will reset the config by setting an empty\n+    // string value. In other words, cleaning up existing policies. This is\n+    // useful when we try to clean up shared cache upload policies for\n+    // non-application master tasks. See MAPREDUCE-7294 for details.\n     if (policies == null || policies.size() == 0) {\n-      return \"\";\n+      conf.set(confParam, \"\");\n+      return;\n     }\n     StringBuilder sb = new StringBuilder();\n-    Iterator<Map.Entry<String, Boolean>> it = policies.entrySet().iterator();\n-    Map.Entry<String, Boolean> e;\n-    if (it.hasNext()) {\n-      e = it.next();\n-      sb.append(e.getKey() + DELIM + e.getValue());\n-    }\n-    while (it.hasNext()) {\n-      e = it.next();\n-      sb.append(\",\" + e.getKey() + DELIM + e.getValue());\n-    }\n-    return sb.toString();\n+    policies.forEach((k,v) -> sb.append(k).append(DELIM).append(v).append(\",\"));\n+    sb.deleteCharAt(sb.length() - 1);\n+    conf.set(confParam, sb.toString());\n   }\n \n   /**\n"}}, {"oid": "9428c1ef6bb1acde2625a6634a144a88ad300857", "url": "https://github.com/apache/hadoop/commit/9428c1ef6bb1acde2625a6634a144a88ad300857", "message": "YARN-10398. Fix the bug to make sure only application master upload resource to yarn shared cache manager.", "committedDate": "2020-09-15T06:34:42Z", "type": "commit"}, {"oid": "b356f239e3559b5bf41a950a33e903b553f8c38d", "url": "https://github.com/apache/hadoop/commit/b356f239e3559b5bf41a950a33e903b553f8c38d", "message": "Adress comments and rebase code", "committedDate": "2020-09-15T06:52:58Z", "type": "commit"}, {"oid": "b356f239e3559b5bf41a950a33e903b553f8c38d", "url": "https://github.com/apache/hadoop/commit/b356f239e3559b5bf41a950a33e903b553f8c38d", "message": "Adress comments and rebase code", "committedDate": "2020-09-15T06:52:58Z", "type": "forcePushed"}]}