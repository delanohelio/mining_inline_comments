{"pr_number": 2166, "pr_title": "HDFS-15488. Add a command to list all snapshots for a snaphottable root with snapshot Ids.", "pr_createdAt": "2020-07-22T16:05:26Z", "pr_url": "https://github.com/apache/hadoop/pull/2166", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4ODI5Ng==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459688296", "bodyText": "Should relative path be resolved as well?\n    Path absF = fixRelativePart(path);\nShould Increment Read Statistics as well.\n    statistics.incrementReadOps(1);", "author": "ayushtkn", "createdAt": "2020-07-23T19:47:50Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java", "diffHunk": "@@ -2148,6 +2149,15 @@ public Void next(final FileSystem fs, final Path p)\n     return dfs.getSnapshottableDirListing();\n   }\n \n+  /**\n+   * @return all the snapshots for a snapshottable directory\n+   * @throws IOException\n+   */\n+  public SnapshotStatus[] getSnapshotListing(Path snapshotRoot)\n+      throws IOException {\n+    return dfs.getSnapshotListing(getPathName(snapshotRoot));", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java\nindex 01a80d15275..37d0226a3a3 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java\n\n@@ -2155,7 +2155,11 @@ public Void next(final FileSystem fs, final Path p)\n    */\n   public SnapshotStatus[] getSnapshotListing(Path snapshotRoot)\n       throws IOException {\n-    return dfs.getSnapshotListing(getPathName(snapshotRoot));\n+    Path absF = fixRelativePart(snapshotRoot);\n+    statistics.incrementReadOps(1);\n+    storageStatistics\n+        .incrementOpCounter(OpType.GET_SNAPSHOT_LIST);\n+    return dfs.getSnapshotListing(getPathName(absF));\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5MDk0MQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459690941", "bodyText": "Should be :\nClientProtocol#getSnapshotListing(String)", "author": "ayushtkn", "createdAt": "2020-07-23T19:53:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "diffHunk": "@@ -2190,6 +2191,24 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n     }\n   }\n \n+  /**\n+   * Get listing of all the snapshots for a snapshottable directory\n+   *\n+   * @return Information about all the snapshots for a snapshottable directory\n+   * @throws IOException If an I/O error occurred\n+   * @see ClientProtocol#getSnapshotListing()", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nindex 7df03807e8c..d781dd9ac45 100755\n--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\n\n@@ -2192,16 +2192,16 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n   }\n \n   /**\n-   * Get listing of all the snapshots for a snapshottable directory\n+   * Get listing of all the snapshots for a snapshottable directory.\n    *\n    * @return Information about all the snapshots for a snapshottable directory\n    * @throws IOException If an I/O error occurred\n-   * @see ClientProtocol#getSnapshotListing()\n+   * @see ClientProtocol#getSnapshotListing(String)\n    */\n   public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n       throws IOException {\n     checkOpen();\n-    try (TraceScope ignored = tracer.newScope(\"getSnapshottableDirListing\")) {\n+    try (TraceScope ignored = tracer.newScope(\"getSnapshotListing\")) {\n       return namenode.getSnapshotListing(snapshotRoot);\n     } catch (RemoteException re) {\n       throw re.unwrapRemoteException();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5MTgxNQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459691815", "bodyText": "Seems Copy-paste error, Change to\ntry (TraceScope ignored = tracer.newScope(\"getSnapshotListing\")) {", "author": "ayushtkn", "createdAt": "2020-07-23T19:54:54Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java", "diffHunk": "@@ -2190,6 +2191,24 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n     }\n   }\n \n+  /**\n+   * Get listing of all the snapshots for a snapshottable directory\n+   *\n+   * @return Information about all the snapshots for a snapshottable directory\n+   * @throws IOException If an I/O error occurred\n+   * @see ClientProtocol#getSnapshotListing()\n+   */\n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    checkOpen();\n+    try (TraceScope ignored = tracer.newScope(\"getSnapshottableDirListing\")) {", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nindex 7df03807e8c..d781dd9ac45 100755\n--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\n\n@@ -2192,16 +2192,16 @@ public void renameSnapshot(String snapshotDir, String snapshotOldName,\n   }\n \n   /**\n-   * Get listing of all the snapshots for a snapshottable directory\n+   * Get listing of all the snapshots for a snapshottable directory.\n    *\n    * @return Information about all the snapshots for a snapshottable directory\n    * @throws IOException If an I/O error occurred\n-   * @see ClientProtocol#getSnapshotListing()\n+   * @see ClientProtocol#getSnapshotListing(String)\n    */\n   public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n       throws IOException {\n     checkOpen();\n-    try (TraceScope ignored = tracer.newScope(\"getSnapshottableDirListing\")) {\n+    try (TraceScope ignored = tracer.newScope(\"getSnapshotListing\")) {\n       return namenode.getSnapshotListing(snapshotRoot);\n     } catch (RemoteException re) {\n       throw re.unwrapRemoteException();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5NjE3Ng==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459696176", "bodyText": "Keep the argument name consistent, In ClientProtocol & Router its snapshotRoot, better keep same everywhere", "author": "ayushtkn", "createdAt": "2020-07-23T20:03:16Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java", "diffHunk": "@@ -2004,6 +2005,16 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n     return status;\n   }\n \n+  @Override // Client Protocol\n+  public SnapshotStatus[] getSnapshotListing(String path)", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java\nindex c0161dc11fa..b42252d9aa9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java\n\n@@ -2006,11 +2006,11 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n   }\n \n   @Override // Client Protocol\n-  public SnapshotStatus[] getSnapshotListing(String path)\n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n       throws IOException {\n     checkNNStartup();\n     SnapshotStatus[] status = namesystem\n-        .getSnapshotListing(path);\n+        .getSnapshotListing(snapshotRoot);\n     metrics.incrListSnapshotsOps();\n     return status;\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY5ODAyNw==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459698027", "bodyText": "Should put the path as well in the audit log too\n      logAuditEvent(success, \"listSnapshots\", snapshotRoot);", "author": "ayushtkn", "createdAt": "2020-07-23T20:06:48Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -7001,7 +7002,33 @@ void renameSnapshot(\n     logAuditEvent(true, operationName, null, null, null);\n     return status;\n   }\n-  \n+\n+  /**\n+   * Get the list of snapshots for a given snapshottable directory.\n+   *\n+   * @return The list of all the snapshots for a snapshottable directory\n+   * @throws IOException\n+   */\n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    SnapshotStatus[] status = null;\n+    checkOperation(OperationCategory.READ);\n+    boolean success = false;\n+    readLock();\n+    try {\n+      checkOperation(OperationCategory.READ);\n+      status = FSDirSnapshotOp.getSnapshotListing(dir, snapshotManager,\n+          snapshotRoot);\n+      success = true;\n+    } catch (AccessControlException ace) {\n+      logAuditEvent(success, \"listSnapshots\", null, null, null);", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\nindex 34f4bcd63bf..9efcab28727 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n\n@@ -7011,22 +7011,27 @@ void renameSnapshot(\n    */\n   public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n       throws IOException {\n-    SnapshotStatus[] status = null;\n+    final String operationName = \"listSnapshotDirectory\";\n+    SnapshotStatus[] status;\n     checkOperation(OperationCategory.READ);\n     boolean success = false;\n-    readLock();\n+    final FSPermissionChecker pc = getPermissionChecker();\n+    FSPermissionChecker.setOperationType(operationName);\n     try {\n-      checkOperation(OperationCategory.READ);\n-      status = FSDirSnapshotOp.getSnapshotListing(dir, snapshotManager,\n-          snapshotRoot);\n-      success = true;\n+      readLock();\n+      try {\n+        checkOperation(OperationCategory.READ);\n+        status = FSDirSnapshotOp.getSnapshotListing(dir, pc, snapshotManager,\n+            snapshotRoot);\n+        success = true;\n+      } finally {\n+        readUnlock(operationName, getLockReportInfoSupplier(null));\n+      }\n     } catch (AccessControlException ace) {\n-      logAuditEvent(success, \"listSnapshots\", null, null, null);\n+      logAuditEvent(success, \"listSnapshots\", snapshotRoot);\n       throw ace;\n-    } finally {\n-      readUnlock();\n     }\n-    logAuditEvent(success, \"listSnapshots\", null, null, null);\n+    logAuditEvent(success, \"listSnapshots\", snapshotRoot);\n     return status;\n   }\n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwMjE4OQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459702189", "bodyText": "Can get pc out of fsn lock and pass on from FsNamesystem by getting before taking lock, Would save lock retention time, the FsDirectory lock below is just dummy just asserts whether FSN lock is there or not", "author": "ayushtkn", "createdAt": "2020-07-23T20:15:16Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -155,6 +156,23 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n     }\n   }\n \n+  static SnapshotStatus[] getSnapshotListing(\n+      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      throws IOException {\n+    FSPermissionChecker pc = fsd.getPermissionChecker();", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\nindex ac9af68549c..f264dc34063 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\n\n@@ -157,15 +156,14 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n   }\n \n   static SnapshotStatus[] getSnapshotListing(\n-      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      FSDirectory fsd, FSPermissionChecker pc, SnapshotManager snapshotManager,\n+      String path)\n       throws IOException {\n-    FSPermissionChecker pc = fsd.getPermissionChecker();\n     fsd.readLock();\n     try {\n       INodesInPath iip = fsd.getINodesInPath(path, DirOp.READ);\n       if (fsd.isPermissionEnabled()) {\n-        fsd.checkPermission(pc, iip, false, null, null, FsAction.READ,\n-            FsAction.READ);\n+        fsd.checkPathAccess(pc, iip, FsAction.READ);\n       }\n       return snapshotManager.getSnapshotListing(iip);\n     } finally {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwMjc5Ng==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459702796", "bodyText": "Can use instead :\nfsd.checkPathAccess(pc,iip,FsAction.READ);", "author": "ayushtkn", "createdAt": "2020-07-23T20:16:27Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java", "diffHunk": "@@ -155,6 +156,23 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n     }\n   }\n \n+  static SnapshotStatus[] getSnapshotListing(\n+      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      throws IOException {\n+    FSPermissionChecker pc = fsd.getPermissionChecker();\n+    fsd.readLock();\n+    try {\n+      INodesInPath iip = fsd.getINodesInPath(path, DirOp.READ);\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPermission(pc, iip, false, null, null, FsAction.READ,", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\nindex ac9af68549c..f264dc34063 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSnapshotOp.java\n\n@@ -157,15 +156,14 @@ static void renameSnapshot(FSDirectory fsd, FSPermissionChecker pc,\n   }\n \n   static SnapshotStatus[] getSnapshotListing(\n-      FSDirectory fsd, SnapshotManager snapshotManager, String path)\n+      FSDirectory fsd, FSPermissionChecker pc, SnapshotManager snapshotManager,\n+      String path)\n       throws IOException {\n-    FSPermissionChecker pc = fsd.getPermissionChecker();\n     fsd.readLock();\n     try {\n       INodesInPath iip = fsd.getINodesInPath(path, DirOp.READ);\n       if (fsd.isPermissionEnabled()) {\n-        fsd.checkPermission(pc, iip, false, null, null, FsAction.READ,\n-            FsAction.READ);\n+        fsd.checkPathAccess(pc, iip, FsAction.READ);\n       }\n       return snapshotManager.getSnapshotListing(iip);\n     } finally {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNTkxMg==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459705912", "bodyText": "Isn't the size of the list always be equal to the size of snapshotList?, if so and size is already known then no need of having a list and then converting to array, can directly take an array of size same as that of snapshotList", "author": "ayushtkn", "createdAt": "2020-07-23T20:22:14Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -471,7 +473,35 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n+        getSnapshotList();\n+    if (snapshotList.isEmpty()) {\n+      return null;\n+    }\n+    List<SnapshotStatus> statusList =\n+        new ArrayList<>();", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex c7c2198a4bf..7aaae3cfa6e 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n\n@@ -480,26 +516,22 @@ public void write(DataOutput out) throws IOException {\n   public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n       throws IOException {\n     INodeDirectory srcRoot = getSnapshottableRoot(iip);\n-    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n-        getSnapshotList();\n-    if (snapshotList.isEmpty()) {\n-      return null;\n-    }\n-    List<SnapshotStatus> statusList =\n-        new ArrayList<>();\n-    for (Snapshot s : snapshotList) {\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n       Snapshot.Root dir = s.getRoot();\n-      SnapshotStatus status = new SnapshotStatus(dir.getModificationTime()\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime()\n           , dir.getAccessTime(), dir.getFsPermission(),\n           EnumSet.noneOf(HdfsFileStatus.Flags.class),\n           dir.getUserName(), dir.getGroupName(),\n           dir.getLocalNameBytes(), dir.getId(),\n           dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),\n           s.getId(), DFSUtil.string2Bytes(dir.getParent().getFullPathName()));\n-      statusList.add(status);\n+\n     }\n-    return statusList.toArray(\n-        new SnapshotStatus[statusList.size()]);\n+    return statuses;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNzQ2OQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459707469", "bodyText": "Do you want to pass null, if there is no snapshots, Can't we pass an empty list?\nIt might create problem for some client checking the size of the array to conclude if there are snapshots or not or performing directly some operations without being too smart of having a null check, his code would break will a NPE. If there is no strong opposition, we may pass an empty array, I think, but I will keep the ball in your court", "author": "ayushtkn", "createdAt": "2020-07-23T20:25:25Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -471,7 +473,35 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n+        getSnapshotList();\n+    if (snapshotList.isEmpty()) {\n+      return null;", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex c7c2198a4bf..7aaae3cfa6e 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n\n@@ -480,26 +516,22 @@ public void write(DataOutput out) throws IOException {\n   public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n       throws IOException {\n     INodeDirectory srcRoot = getSnapshottableRoot(iip);\n-    ReadOnlyList<Snapshot> snapshotList = srcRoot.getDirectorySnapshottableFeature().\n-        getSnapshotList();\n-    if (snapshotList.isEmpty()) {\n-      return null;\n-    }\n-    List<SnapshotStatus> statusList =\n-        new ArrayList<>();\n-    for (Snapshot s : snapshotList) {\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n       Snapshot.Root dir = s.getRoot();\n-      SnapshotStatus status = new SnapshotStatus(dir.getModificationTime()\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime()\n           , dir.getAccessTime(), dir.getFsPermission(),\n           EnumSet.noneOf(HdfsFileStatus.Flags.class),\n           dir.getUserName(), dir.getGroupName(),\n           dir.getLocalNameBytes(), dir.getId(),\n           dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),\n           s.getId(), DFSUtil.string2Bytes(dir.getParent().getFullPathName()));\n-      statusList.add(status);\n+\n     }\n-    return statusList.toArray(\n-        new SnapshotStatus[statusList.size()]);\n+    return statuses;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNzg0NQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459717845", "bodyText": "This is messed up.\n\nlocations need to be passed, as of now you are ignoring the passed parameter itself.\ninvokeConcurrent only in case if the Path is of type isAll, you can use rpcServer.isInvokeConcurrent(snapshotRoot)\nOnce you make this change I think RouterRpcServer.merge(.) won't work, you need to write your own util to aggregate.\nThis needs to be covered by a UT as well, TestRouterRpc or TestRouterRPCMultipleDestinationMountTableResolver could be a good place to add one.\n\n** If you have any issue with RBF, let me know, will try to get you the code. :-)", "author": "ayushtkn", "createdAt": "2020-07-23T20:45:36Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -157,6 +158,22 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     return RouterRpcServer.merge(ret, SnapshottableDirectoryStatus.class);\n   }\n \n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n+    final List<RemoteLocation> locations =\n+        rpcServer.getLocationsForPath(snapshotRoot, true, false);\n+    RemoteMethod method = new RemoteMethod(\"getSnapshotListing\",\n+        new Class<?>[] {String.class},\n+        new RemoteParam());\n+    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n+    Map<FederationNamespaceInfo, SnapshotStatus[]> ret =\n+        rpcClient.invokeConcurrent(\n+            nss, method, true, false, SnapshotStatus[].class);\n+\n+    return RouterRpcServer.merge(ret, SnapshotStatus.class);\n+  }", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\nindex 63c7514efa8..056f92a77bf 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n\n@@ -163,15 +163,17 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     rpcServer.checkOperation(NameNode.OperationCategory.READ);\n     final List<RemoteLocation> locations =\n         rpcServer.getLocationsForPath(snapshotRoot, true, false);\n-    RemoteMethod method = new RemoteMethod(\"getSnapshotListing\",\n-        new Class<?>[] {String.class},\n+    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n+        new Class<?>[]{String.class},\n         new RemoteParam());\n-    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, SnapshotStatus[]> ret =\n-        rpcClient.invokeConcurrent(\n-            nss, method, true, false, SnapshotStatus[].class);\n-\n-    return RouterRpcServer.merge(ret, SnapshotStatus.class);\n+    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n+      Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n+          locations, remoteMethod, true, false, SnapshotStatus[].class);\n+      return ret.values().iterator().next();\n+    } else {\n+      return rpcClient.invokeSequential(\n+          locations, remoteMethod, SnapshotStatus[].class, null);\n+    }\n   }\n \n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxOTQ2OA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459719468", "bodyText": "Would be good if there is a error message as well, Something like invalid number of arguments...", "author": "ayushtkn", "createdAt": "2020-07-23T20:49:02Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\nindex f089f9550bb..65b38afe416 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n\n@@ -17,16 +17,13 @@\n  */\n package org.apache.hadoop.hdfs.tools.snapshot;\n \n-import java.io.IOException;\n \n import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n-import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n-import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.tools.AdminHelper;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMjc3Ng==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459722776", "bodyText": "Instead can use\n    DistributedFileSystem dfs = AdminHelper.getDFS(getConf());\nThis shall handle ViewFsOverloadScheme as well.\nThis would throw IllegalArgumentException so it should be inside the try block", "author": "ayushtkn", "createdAt": "2020-07-23T20:55:41Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);\n+      return 1;\n+    }\n+\n+    FileSystem fs = FileSystem.get(getConf());\n+    if (! (fs instanceof DistributedFileSystem)) {\n+      System.err.println(\n+          \"lsSnapshot can only be used in DistributedFileSystem\");\n+      return 1;\n+    }\n+    DistributedFileSystem dfs = (DistributedFileSystem) fs;", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\nindex f089f9550bb..65b38afe416 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n\n@@ -17,16 +17,13 @@\n  */\n package org.apache.hadoop.hdfs.tools.snapshot;\n \n-import java.io.IOException;\n \n import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n-import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n-import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.tools.AdminHelper;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjE1MA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459726150", "bodyText": "I don't think we need to print the stack trace on the CLI? Just a line of error should work? Mostly it should trigger for FNF or for SnapshotException if the directory is not snapshottable. if required we can have the exception with trace in the debug log\nApart we should catch Exception as well for any runtime exceptions, propagating the exception in CLI won't look good", "author": "ayushtkn", "createdAt": "2020-07-23T21:02:09Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools.snapshot;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+\n+/**\n+ * A tool used to list all snapshottable directories that are owned by the\n+ * current user. The tool returns all the snapshottable directories if the user\n+ * is a super user.\n+ */\n+@InterfaceAudience.Private\n+public class LsSnapshot extends Configured implements Tool {\n+  @Override\n+  public int run(String[] argv) throws Exception {\n+    String description = \"hdfs lsSnapshot <snapshotDir>: \\n\" +\n+        \"\\tGet the list of snapshots for a snapshottable directory.\\n\";\n+\n+    if(argv.length != 1) {\n+      System.err.println(\"Usage: \\n\" + description);\n+      return 1;\n+    }\n+\n+    FileSystem fs = FileSystem.get(getConf());\n+    if (! (fs instanceof DistributedFileSystem)) {\n+      System.err.println(\n+          \"lsSnapshot can only be used in DistributedFileSystem\");\n+      return 1;\n+    }\n+    DistributedFileSystem dfs = (DistributedFileSystem) fs;\n+    Path snapshotRoot = new Path(argv[0]);\n+\n+    try {\n+      SnapshotStatus[] stats = dfs.getSnapshotListing(snapshotRoot);\n+      SnapshotStatus.print(stats, System.out);\n+    } catch (IOException e) {\n+      String[] content = e.getLocalizedMessage().split(\"\\n\");\n+      System.err.println(\"lsSnapshot: \" + content[0]);\n+      e.printStackTrace(System.err);\n+      return 1;", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\nindex f089f9550bb..65b38afe416 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshot.java\n\n@@ -17,16 +17,13 @@\n  */\n package org.apache.hadoop.hdfs.tools.snapshot;\n \n-import java.io.IOException;\n \n import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n-import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n-import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.tools.AdminHelper;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNzA4OQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459727089", "bodyText": "fsn is already there, can change to:\nfsn.getSnapshotManager().setAllowNestedSnapshots(true);", "author": "ayushtkn", "createdAt": "2020-07-23T21:04:06Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\nindex 9f73202f242..ed942884a4e 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n\n@@ -21,17 +21,20 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n \n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.*;\n \n+/**\n+ * Tests listSnapshot.\n+ */\n public class TestListSnapshot {\n \n   static final short REPLICATION = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyOTc0NA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r459729744", "bodyText": "Can use LambdaTestUtils :\n``LambdaTestUtils.intercept(SnapshotException.class,\n    \"Directory is not a \" + \"snapshottable directory\",\n    () -> hdfs.getSnapshotListing(dir1)); ``", "author": "ayushtkn", "createdAt": "2020-07-23T21:09:46Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);\n+\n+    // Initially there is no snapshottable directories in the system\n+    SnapshotStatus[] snapshotStatuses = null;\n+    SnapshottableDirectoryStatus[] dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    try {\n+      hdfs.getSnapshotListing(dir1);\n+    } catch (Exception e) {\n+      assertTrue(e.getMessage().contains(\n+          \"Directory is not a snapshottable directory\"));\n+    }", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\nindex 9f73202f242..ed942884a4e 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n\n@@ -21,17 +21,20 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n \n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.*;\n \n+/**\n+ * Tests listSnapshot.\n+ */\n public class TestListSnapshot {\n \n   static final short REPLICATION = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNDAzNQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r460634035", "bodyText": "SnapshotStatus should extend HDFSFileStatus, so that this can also be used as a status object if required,", "author": "mukul1987", "createdAt": "2020-07-27T03:57:15Z", "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.protocol;\n+\n+import java.io.PrintStream;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.EnumSet;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSUtilClient;\n+\n+/**\n+ * Metadata about a snapshottable directory\n+ */\n+public class SnapshotStatus {", "originalCommit": "51fc498d69c0e86fb7cf2bb217ed04ea36100991", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY1NDY4Mw==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r460654683", "bodyText": "I would prefer to not extend the status class as it doesn't inherently displays/constructs the filestatus object correctly regarding the children list info as well as permissions. The approach here is similar to what has been addresed in SnapshottableDirectoryStatus.", "author": "bshashikant", "createdAt": "2020-07-27T05:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNDAzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java\nindex 72bb05f14b4..42953fb2397 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotStatus.java\n\n@@ -27,16 +27,16 @@\n import org.apache.hadoop.hdfs.DFSUtilClient;\n \n /**\n- * Metadata about a snapshottable directory\n+ * Metadata about a snapshottable directory.\n  */\n public class SnapshotStatus {\n   /**\n-   * Basic information of the snapshot directory\n+   * Basic information of the snapshot directory.\n    */\n   private final HdfsFileStatus dirStatus;\n \n   /**\n-   * Snapshot ID for the snapshot\n+   * Snapshot ID for the snapshot.\n    */\n   private final int snapshotID;\n \n"}}, {"oid": "989158ab65079d530aa83e5c9b8fab5f0873e818", "url": "https://github.com/apache/hadoop/commit/989158ab65079d530aa83e5c9b8fab5f0873e818", "message": "HDFS-15488. Add a command to list all snapshots for a snaphottable root with snapshot Ids.", "committedDate": "2020-07-27T06:08:36Z", "type": "commit"}, {"oid": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "url": "https://github.com/apache/hadoop/commit/04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "message": "Addressed checkstyle/findbug as well as review comments.", "committedDate": "2020-07-27T06:31:07Z", "type": "commit"}, {"oid": "04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "url": "https://github.com/apache/hadoop/commit/04f02d2d9820ce67abc3fe3b918f7dd637a56c51", "message": "Addressed checkstyle/findbug as well as review comments.", "committedDate": "2020-07-27T06:31:07Z", "type": "forcePushed"}, {"oid": "69902c9e9bfb6adc16d5976f101877f2a3f2ff9a", "url": "https://github.com/apache/hadoop/commit/69902c9e9bfb6adc16d5976f101877f2a3f2ff9a", "message": "Added documentaion.", "committedDate": "2020-07-27T06:49:17Z", "type": "commit"}, {"oid": "5bc83f1f685017c7182366e1c3ed3dd8cfa38bd6", "url": "https://github.com/apache/hadoop/commit/5bc83f1f685017c7182366e1c3ed3dd8cfa38bd6", "message": "Fixed xml error.", "committedDate": "2020-07-27T12:39:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA3MjkxNw==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461072917", "bodyText": "Here the SnapshotStatus contains parentpath as well, the path will be the one wrt the actual namespace not with respect to mount table. You need to replace the parentpath with path corresponding to the mount entry.\nif mount entry is : /mnt -> /dir in ns0, then if you trigger call on ns0 the path would be something like /dir/sub.. that you need to change to /mnt/sub..\nIn case of InvokeConcurrent you would be able to get Src and Dst from the ret and maybe something like this may work  -\n      response = ret.values().iterator().next();\n      String src = ret.keySet().iterator().next().getSrc();\n      String dst = ret.keySet().iterator().next().getDest();\n      for (SnapshotStatus s : response) {\n        String mountPath =\n            new String(s.getParentFullPath()).replaceFirst(dst, src);\n        s.setParentFullPath(mountPath.getBytes());\n      }\n\nFor the invokeSequential one you won't be having the detail on which location did the call got success, For that you have to get it returned back from RouterRpcClient#L858, I guess to get the location returned you would require a new InvokeSequential method which returns the location as well, may be can refactor and reuse this one...\nThis problem would be there I think in getSnapshottableDirListing() as well. If you want, you can put a TODO and handle the location stuff in a separate follow up jira for both API's.", "author": "ayushtkn", "createdAt": "2020-07-27T18:04:52Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -157,6 +158,24 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     return RouterRpcServer.merge(ret, SnapshottableDirectoryStatus.class);\n   }\n \n+  public SnapshotStatus[] getSnapshotListing(String snapshotRoot)\n+      throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.READ);\n+    final List<RemoteLocation> locations =\n+        rpcServer.getLocationsForPath(snapshotRoot, true, false);\n+    RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n+        new Class<?>[]{String.class},\n+        new RemoteParam());\n+    if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n+      Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n+          locations, remoteMethod, true, false, SnapshotStatus[].class);\n+      return ret.values().iterator().next();\n+    } else {\n+      return rpcClient.invokeSequential(\n+          locations, remoteMethod, SnapshotStatus[].class, null);", "originalCommit": "5bc83f1f685017c7182366e1c3ed3dd8cfa38bd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\nindex 056f92a77bf..e7ab64369ab 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n\n@@ -166,14 +167,30 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n         new Class<?>[]{String.class},\n         new RemoteParam());\n+    SnapshotStatus[] response;\n     if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n       Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n           locations, remoteMethod, true, false, SnapshotStatus[].class);\n-      return ret.values().iterator().next();\n+      response = ret.values().iterator().next();\n+      String src = ret.keySet().iterator().next().getSrc();\n+      String dst = ret.keySet().iterator().next().getDest();\n+      for (SnapshotStatus s : response) {\n+        String mountPath =\n+            new String(s.getParentFullPath()).replaceFirst(src, dst);\n+        s.setParentFullPath(mountPath.getBytes());\n+      }\n     } else {\n-      return rpcClient.invokeSequential(\n+      response = rpcClient.invokeSequential(\n           locations, remoteMethod, SnapshotStatus[].class, null);\n+      RemoteLocation loc = locations.get(0);\n+      for (SnapshotStatus s : response) {\n+        String mountPath =\n+            new String(s.getParentFullPath()).replaceFirst(loc.getDest(),\n+                loc.getSrc());\n+        s.setParentFullPath(mountPath.getBytes());\n+      }\n     }\n+    return response;\n   }\n \n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n"}}, {"oid": "972ab57f8c6769f4291456eb37b880b836d29ce2", "url": "https://github.com/apache/hadoop/commit/972ab57f8c6769f4291456eb37b880b836d29ce2", "message": "Addressed review comments.", "committedDate": "2020-07-28T05:38:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM1MDI5Mg==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461350292", "bodyText": "The actual location won't be always the first one in the list. RouterRpcClient#L858 iterates over the list and triggers call to them sequentially, if the first location doesn't give the specific response, then second and so on. Adding a line at RouterRpcClient at L872 just before  return ret;, shall make this logic work -\n          Collections.swap(locations, 0, locations.indexOf(loc));\nApart everything seems good here.", "author": "ayushtkn", "createdAt": "2020-07-28T06:36:07Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java", "diffHunk": "@@ -166,14 +167,30 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n     RemoteMethod remoteMethod = new RemoteMethod(\"getSnapshotListing\",\n         new Class<?>[]{String.class},\n         new RemoteParam());\n+    SnapshotStatus[] response;\n     if (rpcServer.isInvokeConcurrent(snapshotRoot)) {\n       Map<RemoteLocation, SnapshotStatus[]> ret = rpcClient.invokeConcurrent(\n           locations, remoteMethod, true, false, SnapshotStatus[].class);\n-      return ret.values().iterator().next();\n+      response = ret.values().iterator().next();\n+      String src = ret.keySet().iterator().next().getSrc();\n+      String dst = ret.keySet().iterator().next().getDest();\n+      for (SnapshotStatus s : response) {\n+        String mountPath =\n+            new String(s.getParentFullPath()).replaceFirst(src, dst);\n+        s.setParentFullPath(mountPath.getBytes());\n+      }\n     } else {\n-      return rpcClient.invokeSequential(\n+      response = rpcClient.invokeSequential(\n           locations, remoteMethod, SnapshotStatus[].class, null);\n+      RemoteLocation loc = locations.get(0);", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMzU1NQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461413555", "bodyText": "Thanks @ayushtkn . I tried doing the similar change but it breaks many existing unit tests.\nMoreover, i see a similar path has been followed in createSnapshot as well.\nDo you think, createSnapshot/getSnapshottableDirectory Listing is broken as well?\nI am not very familiar with RouterProtocol code. In such a case, i would appreciate if you would fix/help fixing all of these at the same time in a separate jira?", "author": "bshashikant", "createdAt": "2020-07-28T08:36:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM1MDI5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\nindex e7ab64369ab..be28a90d398 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java\n\n@@ -177,7 +177,7 @@ public void renameSnapshot(String snapshotRoot, String oldSnapshotName,\n       for (SnapshotStatus s : response) {\n         String mountPath =\n             new String(s.getParentFullPath()).replaceFirst(src, dst);\n-        s.setParentFullPath(mountPath.getBytes());\n+        s.setParentFullPath(DFSUtil.string2Bytes(mountPath));\n       }\n     } else {\n       response = rpcClient.invokeSequential(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ5NzE2Mw==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461497163", "bodyText": "Expand wildcard imports.", "author": "mukul1987", "createdAt": "2020-07-28T11:03:21Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java", "diffHunk": "@@ -71,29 +71,9 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties;\n import org.apache.hadoop.hdfs.NameNodeProxies;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n-import org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\n-import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;\n-import org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo;\n-import org.apache.hadoop.hdfs.protocol.CachePoolEntry;\n-import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n-import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n-import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n-import org.apache.hadoop.hdfs.protocol.ECBlockGroupStats;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\n-import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState;\n-import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.protocol.*;", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTAyOQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621029", "bodyText": "Sure", "author": "bshashikant", "createdAt": "2020-07-28T14:22:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ5NzE2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\nindex e4ccb82bff4..f4cfd7c7e0b 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java\n\n@@ -71,7 +71,28 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties;\n import org.apache.hadoop.hdfs.NameNodeProxies;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n-import org.apache.hadoop.hdfs.protocol.*;\n+import org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\n+import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;\n+import org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo;\n+import org.apache.hadoop.hdfs.protocol.CachePoolEntry;\n+import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n+import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n+import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n+import org.apache.hadoop.hdfs.protocol.ECBlockGroupStats;\n+import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\n+import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\n+import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;\n+import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n+import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n+import org.apache.hadoop.hdfs.protocol.ReplicatedBlockStats;\n+import org.apache.hadoop.hdfs.protocol.SnapshotDiffReport;\n+import org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants.SafeModeAction;\n import org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461500368", "bodyText": "This snapid should be the id of the snapshot here ?", "author": "mukul1987", "createdAt": "2020-07-28T11:09:47Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java", "diffHunk": "@@ -501,7 +509,31 @@ public void write(DataOutput out) throws IOException {\n     return statusList.toArray(\n         new SnapshottableDirectoryStatus[statusList.size()]);\n   }\n-  \n+\n+  /**\n+   * List all the snapshots under a snapshottable directory.\n+   */\n+  public SnapshotStatus[] getSnapshotListing(INodesInPath iip)\n+      throws IOException {\n+    INodeDirectory srcRoot = getSnapshottableRoot(iip);\n+    ReadOnlyList<Snapshot> snapshotList = srcRoot.\n+        getDirectorySnapshottableFeature().getSnapshotList();\n+    SnapshotStatus[] statuses = new SnapshotStatus[snapshotList.size()];\n+    for (int count = 0; count < snapshotList.size(); count++) {\n+      Snapshot s = snapshotList.get(count);\n+      Snapshot.Root dir = s.getRoot();\n+      statuses[count] = new SnapshotStatus(dir.getModificationTime(),\n+          dir.getAccessTime(), dir.getFsPermission(),\n+          EnumSet.noneOf(HdfsFileStatus.Flags.class),\n+          dir.getUserName(), dir.getGroupName(),\n+          dir.getLocalNameBytes(), dir.getId(),\n+          dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMjQ4Nw==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461502487", "bodyText": "Can we add a comment here on why the ID here should not be snapID ?\nAnd that childrenNumber can be a wrong value.", "author": "mukul1987", "createdAt": "2020-07-28T11:13:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTM1Ng==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621356", "bodyText": "Addressed in the latest patch.", "author": "bshashikant", "createdAt": "2020-07-28T14:22:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwMDM2OA=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\nindex 919754ae536..e9729abd423 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java\n\n@@ -527,6 +527,11 @@ public void write(DataOutput out) throws IOException {\n           EnumSet.noneOf(HdfsFileStatus.Flags.class),\n           dir.getUserName(), dir.getGroupName(),\n           dir.getLocalNameBytes(), dir.getId(),\n+          // the children number is same as the\n+          // live fs as the children count is not cached per snashot.\n+          // It is just used here to construct the HdfsFileStatus object.\n+          // It is expensive to build the snapshot tree for the directory\n+          // and determine the child count.\n           dir.getChildrenNum(Snapshot.CURRENT_STATE_ID),\n           s.getId(), DFSUtil.string2Bytes(dir.getParent().getFullPathName()));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTMyOQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461505329", "bodyText": "wildcard imports here.", "author": "mukul1987", "createdAt": "2020-07-28T11:19:10Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTUwOA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621508", "bodyText": "sure", "author": "bshashikant", "createdAt": "2020-07-28T14:23:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTMyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\nindex ed942884a4e..dcff2f7f7c6 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n\n@@ -30,7 +30,9 @@\n import org.junit.Before;\n import org.junit.Test;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n \n /**\n  * Tests listSnapshot.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTY1MA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461505650", "bodyText": "The flag should be set to false ?", "author": "mukul1987", "createdAt": "2020-07-28T11:19:47Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMDI5OA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461620298", "bodyText": "The flag is dset to true bcoz in the test , first we are making \"/\" snapshottable and then \"/dir1\" snapshottable ensuring it works either ways.", "author": "bshashikant", "createdAt": "2020-07-28T14:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNTY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\nindex ed942884a4e..dcff2f7f7c6 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n\n@@ -30,7 +30,9 @@\n import org.junit.Before;\n import org.junit.Test;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n \n /**\n  * Tests listSnapshot.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNjM4OA==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461506388", "bodyText": "create 2 snapshots for dir1 ?", "author": "mukul1987", "createdAt": "2020-07-28T11:21:22Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode.snapshot;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.protocol.SnapshotException;\n+import org.apache.hadoop.hdfs.protocol.SnapshotStatus;\n+import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests listSnapshot.\n+ */\n+public class TestListSnapshot {\n+\n+  static final short REPLICATION = 3;\n+\n+  private final Path dir1 = new Path(\"/TestSnapshot1\");\n+\n+  Configuration conf;\n+  MiniDFSCluster cluster;\n+  FSNamesystem fsn;\n+  DistributedFileSystem hdfs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new Configuration();\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION)\n+        .build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+    hdfs.mkdirs(dir1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  /**\n+   * Test listing all the snapshottable directories.\n+   */\n+  @Test(timeout = 60000)\n+  public void testListSnapshot() throws Exception {\n+    fsn.getSnapshotManager().setAllowNestedSnapshots(true);\n+\n+    // Initially there is no snapshottable directories in the system\n+    SnapshotStatus[] snapshotStatuses = null;\n+    SnapshottableDirectoryStatus[] dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    LambdaTestUtils.intercept(SnapshotException.class,\n+        \"Directory is not a \" + \"snapshottable directory\",\n+        () -> hdfs.getSnapshotListing(dir1));\n+    // Make root as snapshottable\n+    final Path root = new Path(\"/\");\n+    hdfs.allowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertEquals(1, dirs.length);\n+    assertEquals(\"\", dirs[0].getDirStatus().getLocalName());\n+    assertEquals(root, dirs[0].getFullPath());\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+    // Make root non-snaphsottable\n+    hdfs.disallowSnapshot(root);\n+    dirs = hdfs.getSnapshottableDirListing();\n+    assertNull(dirs);\n+    snapshotStatuses = hdfs.getSnapshotListing(root);\n+    assertTrue(snapshotStatuses.length == 0);\n+\n+    // Make dir1 as snapshottable\n+    hdfs.allowSnapshot(dir1);\n+    hdfs.createSnapshot(dir1, \"s0\");\n+    snapshotStatuses = hdfs.getSnapshotListing(dir1);\n+    assertEquals(1, snapshotStatuses.length);\n+    assertEquals(\"s0\", snapshotStatuses[0].getDirStatus().\n+        getLocalName());\n+    assertEquals(SnapshotTestHelper.getSnapshotRoot(dir1, \"s0\"),\n+        snapshotStatuses[0].getFullPath());\n+    // snapshot id is zero\n+    assertEquals(0, snapshotStatuses[0].getSnapshotID());\n+    // Create a snapshot for dir2", "originalCommit": "972ab57f8c6769f4291456eb37b880b836d29ce2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYyMTcxOQ==", "url": "https://github.com/apache/hadoop/pull/2166#discussion_r461621719", "bodyText": "addressed", "author": "bshashikant", "createdAt": "2020-07-28T14:23:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUwNjM4OA=="}], "type": "inlineReview", "revised_code": {"commit": "81f9119d464e633bbf53681858ae6e2daf4bef04", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\nindex ed942884a4e..dcff2f7f7c6 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestListSnapshot.java\n\n@@ -30,7 +30,9 @@\n import org.junit.Before;\n import org.junit.Test;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n \n /**\n  * Tests listSnapshot.\n"}}, {"oid": "81f9119d464e633bbf53681858ae6e2daf4bef04", "url": "https://github.com/apache/hadoop/commit/81f9119d464e633bbf53681858ae6e2daf4bef04", "message": "Addressed Review comments.", "committedDate": "2020-07-28T14:26:05Z", "type": "commit"}, {"oid": "7f4911ce1f7994a262f924a8a1c37ee565bd4953", "url": "https://github.com/apache/hadoop/commit/7f4911ce1f7994a262f924a8a1c37ee565bd4953", "message": "Addressed checkstyle issue.", "committedDate": "2020-07-29T02:05:15Z", "type": "commit"}, {"oid": "722869410635a0572781488e9dd51b064654013a", "url": "https://github.com/apache/hadoop/commit/722869410635a0572781488e9dd51b064654013a", "message": "Addressed test failure.", "committedDate": "2020-07-29T06:47:56Z", "type": "commit"}, {"oid": "dbfcdafd8cde2ec182572cb7e51d1be5dd74fd3e", "url": "https://github.com/apache/hadoop/commit/dbfcdafd8cde2ec182572cb7e51d1be5dd74fd3e", "message": "Addressed few checkstyle issues.", "committedDate": "2020-07-29T15:58:51Z", "type": "commit"}]}