{"pr_number": 1898, "pr_title": "HADOOP-16852: Report read-ahead error back", "pr_createdAt": "2020-03-17T04:37:10Z", "pr_url": "https://github.com/apache/hadoop/pull/1898", "timeline": [{"oid": "ace73116f1cc1d5e29edf18dafc80a61206c083d", "url": "https://github.com/apache/hadoop/commit/ace73116f1cc1d5e29edf18dafc80a61206c083d", "message": "Report read-ahead error back", "committedDate": "2020-03-17T04:32:28Z", "type": "commit"}, {"oid": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "url": "https://github.com/apache/hadoop/commit/5e29d124a0e41d1553e1c7b58117f8921da6e424", "message": "Trunk merge", "committedDate": "2020-03-17T05:01:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MDkzOQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393790939", "bodyText": "Can you use logger format?\nLOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);", "author": "goiri", "createdAt": "2020-03-17T16:03:16Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -234,6 +242,7 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     final AbfsRestOperation op;\n     AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n     try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"readRemote\", \"read\")) {\n+      LOG.trace(String.format(\"Trigger client.read for path=%s position=%s offset=%s length=%s\", path, position, offset, length));", "originalCommit": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MDM5OQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394940399", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-03-19T10:53:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MDkzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\nindex b42e2019c9b..610256f45bd 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java\n\n@@ -242,7 +238,7 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti\n     final AbfsRestOperation op;\n     AbfsPerfTracker tracker = client.getAbfsPerfTracker();\n     try (AbfsPerfInfo perfInfo = new AbfsPerfInfo(tracker, \"readRemote\", \"read\")) {\n-      LOG.trace(String.format(\"Trigger client.read for path=%s position=%s offset=%s length=%s\", path, position, offset, length));\n+      LOG.trace(\"Trigger client.read for path={} position={} offset={} length={}\", path, position, offset, length);\n       op = client.read(path, position, b, offset, length, tolerateOobAppends ? \"*\" : eTag);\n       perfInfo.registerResult(op.getResult()).registerSuccess(true);\n     } catch (AzureBlobFileSystemException ex) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTUzNw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393791537", "bodyText": "Avoid.", "author": "goiri", "createdAt": "2020-03-17T16:04:05Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -101,6 +107,7 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n       if (isAlreadyQueued(stream, requestedOffset)) {\n         return; // already queued, do not queue again\n       }\n+", "originalCommit": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjQyNg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942426", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-03-19T10:57:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTUzNw=="}], "type": "inlineReview", "revised_code": {"commit": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 373de1005da..c7a789c2ef9 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -107,7 +104,6 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n       if (isAlreadyQueued(stream, requestedOffset)) {\n         return; // already queued, do not queue again\n       }\n-\n       if (freeList.isEmpty() && !tryEvict()) {\n         return; // no buffers available, cannot queue anything\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTYyNQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393791625", "bodyText": "80 char limit", "author": "goiri", "createdAt": "2020-03-17T16:04:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -90,8 +94,10 @@ private ReadBufferManager() {\n    * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n    * @param requestedOffset The offset in the file which shoukd be read\n    * @param requestedLength The length to read\n+   * @param queueReadAheadRequestId unique queue request ID\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength) {\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength", "originalCommit": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MDgwMg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394940802", "bodyText": "Undid any change to method signature as part of the review comments to remove queueRequestId.", "author": "snvijaya", "createdAt": "2020-03-19T10:54:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MTYyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "5870238c3f1b017875fc05b7b709231129c411f1", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 373de1005da..20bf3b5f3c8 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -94,10 +93,8 @@ private ReadBufferManager() {\n    * @param stream          The {@link AbfsInputStream} for which to do the read-ahead\n    * @param requestedOffset The offset in the file which shoukd be read\n    * @param requestedLength The length to read\n-   * @param queueReadAheadRequestId unique queue request ID\n    */\n-  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength\n-      , final UUID queueReadAheadRequestId) {\n+  void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, final int requestedLength) {\n     if (LOGGER.isTraceEnabled()) {\n       LOGGER.trace(\"Start Queueing readAhead for {} offset {} length {}\",\n           stream.getPath(), requestedOffset, requestedLength);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393792745", "bodyText": "What about capturing logs and checking for the messages?", "author": "goiri", "createdAt": "2020-03-17T16:05:49Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,438 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+import java.util.UUID;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static java.util.UUID.randomUUID;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends", "originalCommit": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjAwMg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942002", "bodyText": "If you meant capturing the exception messages from failed read ahead threads, that will not be possible as we can not predict which stub hit which of the parallely running read ahead threads. Hence asserting on the exception and no message.", "author": "snvijaya", "createdAt": "2020-03-19T10:56:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODYyOQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r395128629", "bodyText": "I was referring to check using LogCapturer", "author": "goiri", "createdAt": "2020-03-19T15:48:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyNjIxNw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405026217", "bodyText": "that's always been trouble in the past; major source of maintenance pain in the wasb tests. Better use the assertion", "author": "steveloughran", "createdAt": "2020-04-07T18:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5Mjc0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5870238c3f1b017875fc05b7b709231129c411f1", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 05b091595eb..bd9bd38290f 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -19,7 +19,6 @@\n package org.apache.hadoop.fs.azurebfs.services;\n \n import java.io.IOException;\n-import java.util.UUID;\n \n import org.junit.Assert;\n import org.junit.Test;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MzU5NQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r393793595", "bodyText": "new param", "author": "goiri", "createdAt": "2020-03-17T16:06:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +149,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read", "originalCommit": "5e29d124a0e41d1553e1c7b58117f8921da6e424", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk0MjI1Mw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r394942253", "bodyText": "New param undone as part of latest iteration.", "author": "snvijaya", "createdAt": "2020-03-19T10:56:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzc5MzU5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5870238c3f1b017875fc05b7b709231129c411f1", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 373de1005da..20bf3b5f3c8 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -149,8 +145,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer,\n-      final UUID queueReadAheadRequestId) throws IOException {\n+  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws java.io.IOException {\n     // not synchronized, so have to be careful with locking\n     if (LOGGER.isTraceEnabled()) {\n       LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n"}}, {"oid": "5870238c3f1b017875fc05b7b709231129c411f1", "url": "https://github.com/apache/hadoop/commit/5870238c3f1b017875fc05b7b709231129c411f1", "message": "Incorporating review comments from Vinay", "committedDate": "2020-03-19T10:38:28Z", "type": "commit"}, {"oid": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234", "url": "https://github.com/apache/hadoop/commit/1cb20dbff9e353d7cd8d4bf400721a205ab3e234", "message": "Review comments from Inigo", "committedDate": "2020-03-19T10:51:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r395128849", "bodyText": "These parenthesis are confusing.", "author": "goiri", "createdAt": "2020-03-19T15:48:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -299,11 +327,32 @@ private void clearFromReadAheadQueue(final AbfsInputStream stream, final long re\n   }\n \n   private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long position, final int length,\n-                                         final byte[] buffer) {\n-    ReadBuffer buf = getFromList(completedReadList, stream, position);\n-    if (buf == null || position >= buf.getOffset() + buf.getLength()) {\n+                                         final byte[] buffer) throws IOException {\n+    ReadBuffer buf = getBufferFromCompletedQueue(stream, position);\n+\n+    if (buf == null) {\n       return 0;\n     }\n+\n+    if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n+      // Eviction of a read buffer is triggered only when a queue request comes in\n+      // and each eviction attempt tries to find one eligible buffer.\n+      // Hence there are chances that an old read-ahead buffer with exception is still\n+      // available. To prevent new read requests to fail due to such old buffers,\n+      // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n+      if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {", "originalCommit": "1cb20dbff9e353d7cd8d4bf400721a205ab3e234", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIzMzIxNQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r396233215", "bodyText": "If the buffer was updated with an error in last \"Threshold_age_milliseconds\", then we return the error to client.\nCan you please help me with the cause of confusion.", "author": "snvijaya", "createdAt": "2020-03-23T06:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyNTYwOA==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399725608", "bodyText": "If this is to avoid the failure for new requests, then instead of checking the time window, why not  reset the buffer status before throwing the exception? Then following new requests will not be affected by the old failure.", "author": "DadanielZ", "createdAt": "2020-03-29T00:14:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMjEyMg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400112122", "bodyText": "Aim here is to enforce the read-ahead failure for the threshold time duration (which currently is 30 sec), i.e. any read request for that offset that can be served by the ReadBuffer needs to fail.", "author": "snvijaya", "createdAt": "2020-03-30T11:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMzM3Mg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405033372", "bodyText": "why 30s? Any way to tune this for test runs?", "author": "steveloughran", "createdAt": "2020-04-07T18:44:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEyODg0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "dbf025459034539381857b83b6d6b41eae8cebf2", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex c7a789c2ef9..91e060c12c6 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -335,13 +337,9 @@ private int getBlockFromCompletedQueue(final AbfsInputStream stream, final long\n     }\n \n     if (buf.getStatus() == ReadBufferStatus.READ_FAILED) {\n-      // Eviction of a read buffer is triggered only when a queue request comes in\n-      // and each eviction attempt tries to find one eligible buffer.\n-      // Hence there are chances that an old read-ahead buffer with exception is still\n-      // available. To prevent new read requests to fail due to such old buffers,\n+      // To prevent new read requests to fail due to old read-ahead attempts,\n       // return exception only from buffers that failed within last THRESHOLD_AGE_MILLISECONDS\n       if ((currentTimeMillis() - (buf.getTimeStamp()) < THRESHOLD_AGE_MILLISECONDS)) {\n-        // is read ahead issued from current queue request ID\n         throw buf.getErrException();\n       } else {\n         return 0;\n"}}, {"oid": "dbf025459034539381857b83b6d6b41eae8cebf2", "url": "https://github.com/apache/hadoop/commit/dbf025459034539381857b83b6d6b41eae8cebf2", "message": "Fix for read-ahead length", "committedDate": "2020-03-23T06:19:41Z", "type": "commit"}, {"oid": "4ba43efe00c9d00d9543493ddfb502e02a58dd69", "url": "https://github.com/apache/hadoop/commit/4ba43efe00c9d00d9543493ddfb502e02a58dd69", "message": "Fixing some test comments", "committedDate": "2020-03-27T05:42:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzYxMA==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399723610", "bodyText": "why use java.io.IOException but not IOException?", "author": "DadanielZ", "createdAt": "2020-03-28T23:48:49Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -141,7 +144,8 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @param buffer   the buffer to read data into. Note that the buffer will be written into from offset 0.\n    * @return the number of bytes read\n    */\n-  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer) {\n+  int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n+      throws java.io.IOException {", "originalCommit": "4ba43efe00c9d00d9543493ddfb502e02a58dd69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMzk5Mw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400113993", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-03-30T11:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzYxMA=="}], "type": "inlineReview", "revised_code": {"commit": "772c2757fc62e92f4b81841ee84baf885331f46b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 91e060c12c6..7439558cffb 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -145,7 +145,7 @@ void queueReadAhead(final AbfsInputStream stream, final long requestedOffset, fi\n    * @return the number of bytes read\n    */\n   int getBlock(final AbfsInputStream stream, final long position, final int length, final byte[] buffer)\n-      throws java.io.IOException {\n+      throws IOException {\n     // not synchronized, so have to be careful with locking\n     if (LOGGER.isTraceEnabled()) {\n       LOGGER.trace(\"getBlock for file {}  position {}  thread {}\",\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzcyMw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r399723723", "bodyText": "IOException", "author": "DadanielZ", "createdAt": "2020-03-28T23:50:24Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "diffHunk": "@@ -88,12 +92,23 @@ public void setBufferindex(int bufferindex) {\n     this.bufferindex = bufferindex;\n   }\n \n+  public IOException getErrException() {\n+    return errException;\n+  }\n+\n+  public void setErrException(final java.io.IOException errException) {", "originalCommit": "4ba43efe00c9d00d9543493ddfb502e02a58dd69", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExNDA5NA==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r400114094", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-03-30T11:19:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcyMzcyMw=="}], "type": "inlineReview", "revised_code": {"commit": "772c2757fc62e92f4b81841ee84baf885331f46b", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\nindex f19f99d14f9..71a01b2a1f2 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\n\n@@ -96,7 +96,7 @@ public IOException getErrException() {\n     return errException;\n   }\n \n-  public void setErrException(final java.io.IOException errException) {\n+  public void setErrException(final IOException errException) {\n     this.errException = errException;\n   }\n \n"}}, {"oid": "772c2757fc62e92f4b81841ee84baf885331f46b", "url": "https://github.com/apache/hadoop/commit/772c2757fc62e92f4b81841ee84baf885331f46b", "message": "Fixing full namespace use of IOException", "committedDate": "2020-03-30T11:18:34Z", "type": "commit"}, {"oid": "035fa7aac34fac4098cfe21d74324ae269cfd547", "url": "https://github.com/apache/hadoop/commit/035fa7aac34fac4098cfe21d74324ae269cfd547", "message": "Fixing NoWhiteSpaceBefore checkstyle errors", "committedDate": "2020-04-06T09:39:40Z", "type": "commit"}, {"oid": "0a83ffe9edc1176600765d94c2d2d46c4c926494", "url": "https://github.com/apache/hadoop/commit/0a83ffe9edc1176600765d94c2d2d46c4c926494", "message": "Merge branch 'trunk' into HADOOP-16852", "committedDate": "2020-04-06T10:49:10Z", "type": "commit"}, {"oid": "52fba323269e6272849b89824dc372398ead4894", "url": "https://github.com/apache/hadoop/commit/52fba323269e6272849b89824dc372398ead4894", "message": "Checkstyle LOG field", "committedDate": "2020-04-07T03:59:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyMzk4Mg==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405023982", "bodyText": "add space above this line", "author": "steveloughran", "createdAt": "2020-04-07T18:29:29Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java", "diffHunk": "@@ -17,11 +17,13 @@\n  */\n \n package org.apache.hadoop.fs.azurebfs.services;\n-\n+import java.io.IOException;", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\nindex 71a01b2a1f2..5d55726222d 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBuffer.java\n\n@@ -17,6 +17,7 @@\n  */\n \n package org.apache.hadoop.fs.azurebfs.services;\n+\n import java.io.IOException;\n import java.util.concurrent.CountDownLatch;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyNzQ4MQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405027481", "bodyText": "not needed; just cut it", "author": "steveloughran", "createdAt": "2020-04-07T18:35:18Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 54101110a1b..8c5f9788351 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -43,7 +43,9 @@\n public class TestAbfsInputStream extends\n     AbstractAbfsIntegrationTest {\n \n-  private static final int KILOBYTE = 1024;\n+  private static final int _1KB = 1 * 1024;\n+  private static final int _2KB = 2 * 1024;\n+  private static final int _3KB = 3 * 1024;\n \n   private AbfsRestOperation getMockRestOp() {\n     AbfsRestOperation op = mock(AbfsRestOperation.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODczMw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405028733", "bodyText": "is there any way for test runs to avoid these long sleeps? This might add 30s to the test run, and that adds up over the day.", "author": "steveloughran", "createdAt": "2020-04-07T18:37:21Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3NzI5Ng==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427277296", "bodyText": "Have reduced the sleep that the test will need.", "author": "snvijaya", "createdAt": "2020-05-19T12:52:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODczMw=="}], "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 54101110a1b..8c5f9788351 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -43,7 +43,9 @@\n public class TestAbfsInputStream extends\n     AbstractAbfsIntegrationTest {\n \n-  private static final int KILOBYTE = 1024;\n+  private static final int _1KB = 1 * 1024;\n+  private static final int _2KB = 2 * 1024;\n+  private static final int _3KB = 3 * 1024;\n \n   private AbfsRestOperation getMockRestOp() {\n     AbfsRestOperation op = mock(AbfsRestOperation.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODk2Ng==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405028966", "bodyText": "assertEquals for all equality checks", "author": "steveloughran", "createdAt": "2020-04-07T18:37:42Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock from a new read request should return 0 if there is a failure\n+    // 30 sec before in read ahead buffer for respective offset.\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+    Assert.assertTrue(\"bytesRead should be zero when previously read \"", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njk2Mw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276963", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-05-19T12:51:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODk2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 54101110a1b..8c5f9788351 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -43,7 +43,9 @@\n public class TestAbfsInputStream extends\n     AbstractAbfsIntegrationTest {\n \n-  private static final int KILOBYTE = 1024;\n+  private static final int _1KB = 1 * 1024;\n+  private static final int _2KB = 2 * 1024;\n+  private static final int _3KB = 3 * 1024;\n \n   private AbfsRestOperation getMockRestOp() {\n     AbfsRestOperation op = mock(AbfsRestOperation.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMDg3NQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405030875", "bodyText": "I'd go for replacing 1 * KILOBYTE with just KILOBYTE, or having constants _1KB, _2KB etc, which is what I've done elsewhere", "author": "steveloughran", "createdAt": "2020-04-07T18:40:49Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njg5MQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276891", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-05-19T12:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMDg3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 54101110a1b..8c5f9788351 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -43,7 +43,9 @@\n public class TestAbfsInputStream extends\n     AbstractAbfsIntegrationTest {\n \n-  private static final int KILOBYTE = 1024;\n+  private static final int _1KB = 1 * 1024;\n+  private static final int _2KB = 2 * 1024;\n+  private static final int _3KB = 3 * 1024;\n \n   private AbfsRestOperation getMockRestOp() {\n     AbfsRestOperation op = mock(AbfsRestOperation.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMTYyMw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405031623", "bodyText": "add trailing .", "author": "steveloughran", "createdAt": "2020-04-07T18:41:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -289,6 +298,27 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n     return null;\n   }\n \n+  /**\n+   * Returns buffers that failed or passed from completed queue", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI3Njc0Nw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r427276747", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-05-19T12:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMTYyMw=="}], "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 9dae880ae5e..c551b37392a 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -299,7 +300,7 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n   }\n \n   /**\n-   * Returns buffers that failed or passed from completed queue\n+   * Returns buffers that failed or passed from completed queue.\n    * @param stream\n    * @param requestedOffset\n    * @return\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzMjg1Mw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r405032853", "bodyText": "Should always be for requested length? That is: we don't care about the total length of the buffer, only the amount which was requested from the far end -which may be less?", "author": "steveloughran", "createdAt": "2020-04-07T18:43:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -289,6 +298,27 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n     return null;\n   }\n \n+  /**\n+   * Returns buffers that failed or passed from completed queue\n+   * @param stream\n+   * @param requestedOffset\n+   * @return\n+   */\n+  private ReadBuffer getBufferFromCompletedQueue(final AbfsInputStream stream, final long requestedOffset) {\n+    for (ReadBuffer buffer : completedReadList) {\n+      // Buffer is returned if the requestedOffset is at or above buffer's\n+      // offset but less than buffer's length or the actual requestedLength\n+      if ((buffer.getStream() == stream)\n+          && (requestedOffset >= buffer.getOffset())\n+          && ((requestedOffset < buffer.getOffset() + buffer.getLength())", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\nindex 9dae880ae5e..c551b37392a 100644\n--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java\n\n@@ -299,7 +300,7 @@ private ReadBuffer getFromList(final Collection<ReadBuffer> list, final AbfsInpu\n   }\n \n   /**\n-   * Returns buffers that failed or passed from completed queue\n+   * Returns buffers that failed or passed from completed queue.\n    * @param stream\n    * @param requestedOffset\n    * @return\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTUxNTM3Mw==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r411515373", "bodyText": "new line", "author": "bilaharith", "createdAt": "2020-04-20T16:23:58Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -0,0 +1,433 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.services;\n+\n+import java.io.IOException;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException;\n+import org.apache.hadoop.fs.azurebfs.contracts.exceptions.TimeoutException;\n+\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.FORWARD_SLASH;\n+\n+/**\n+ * Unit test AbfsInputStream.\n+ */\n+public class TestAbfsInputStream extends\n+    AbstractAbfsIntegrationTest {\n+\n+  private static final int KILOBYTE = 1024;\n+\n+  private AbfsRestOperation getMockRestOp() {\n+    AbfsRestOperation op = mock(AbfsRestOperation.class);\n+    AbfsHttpOperation httpOp = mock(AbfsHttpOperation.class);\n+    when(httpOp.getBytesReceived()).thenReturn(1024L);\n+    when(op.getResult()).thenReturn(httpOp);\n+    return op;\n+  }\n+\n+  private AbfsClient getMockAbfsClient() {\n+    // Mock failure for client.read()\n+    AbfsClient client = mock(AbfsClient.class);\n+    AbfsPerfTracker tracker = new AbfsPerfTracker(\n+        \"test\",\n+        this.getAccountName(),\n+        this.getConfiguration());\n+    when(client.getAbfsPerfTracker()).thenReturn(tracker);\n+\n+    return client;\n+  }\n+\n+  private AbfsInputStream getAbfsInputStream(AbfsClient mockAbfsClient, String fileName) {\n+    // Create AbfsInputStream with the client instance\n+    AbfsInputStream inputStream = new AbfsInputStream(\n+        mockAbfsClient,\n+        null,\n+        FORWARD_SLASH + fileName,\n+        3 * KILOBYTE,\n+        1 * KILOBYTE, // Setting read ahead buffer size of 1 KB\n+        this.getConfiguration().getReadAheadQueueDepth(),\n+        this.getConfiguration().getTolerateOobAppends(),\n+        \"eTag\");\n+\n+    return inputStream;\n+  }\n+\n+  private void queueReadAheads(AbfsInputStream inputStream) {\n+    // Mimic AbfsInputStream readAhead queue requests\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 0, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 1 * KILOBYTE, 1 * KILOBYTE);\n+    ReadBufferManager.getBufferManager()\n+        .queueReadAhead(inputStream, 2 * KILOBYTE, 1 * KILOBYTE);\n+  }\n+\n+  private void verifyReadCallCount(AbfsClient client, int count) throws\n+      AzureBlobFileSystemException, InterruptedException {\n+    // ReadAhead threads are triggered asynchronously.\n+    // Wait a second before verifying the number of total calls.\n+    Thread.sleep(1000);\n+    verify(client, times(count)).read(any(String.class), any(Long.class),\n+        any(byte[].class), any(Integer.class), any(Integer.class),\n+        any(String.class));\n+  }\n+\n+  private void checkEvictedStatus(AbfsInputStream inputStream, int position, boolean expectedToThrowException)\n+      throws Exception {\n+    // Sleep for the eviction threshold time\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds() + 1000);\n+\n+    // Eviction is done only when AbfsInputStream tries to queue new items.\n+    // 1 tryEvict will remove 1 eligible item. To ensure that the current test buffer\n+    // will get evicted (considering there could be other tests running in parallel),\n+    // call tryEvict for the number of items that are there in completedReadList.\n+    int numOfCompletedReadListItems = ReadBufferManager.getBufferManager().getCompletedReadListSize();\n+    while (numOfCompletedReadListItems > 0) {\n+      ReadBufferManager.getBufferManager().callTryEvict();\n+      numOfCompletedReadListItems--;\n+    }\n+\n+    if (expectedToThrowException) {\n+      intercept(IOException.class,\n+          () -> inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE));\n+    } else {\n+      inputStream.read(position, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+    }\n+  }\n+\n+  public TestAbfsInputStream() throws Exception {\n+    super();\n+  }\n+\n+  /**\n+   * This test expects AbfsInputStream to throw the exception that readAhead\n+   * thread received on read. The readAhead thread must be initiated from the\n+   * active read request itself.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testFailedReadAhead.txt\");\n+\n+    // Scenario: ReadAhead triggered from current active read call failed\n+    // Before the change to return exception from readahead buffer,\n+    // AbfsInputStream would have triggered an extra readremote on noticing\n+    // data absent in readahead buffers\n+    // In this test, a read should trigger 3 client.read() calls as file is 3 KB\n+    // and readahead buffer size set in AbfsInputStream is 1 KB\n+    // There should only be a total of 3 client.read() in this test.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to initiate a remote read request for\n+   * the request offset and length when previous read ahead on the offset had failed.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range and also that its is an older readahead request.\n+    // So attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testOlderReadAheadFailure.txt\");\n+\n+    // First read request that fails as the readahead triggered from this request failed.\n+    intercept(IOException.class,\n+        () -> inputStream.read(new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Sleep for 30 sec so that the read ahead buffer qualifies for being old.\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Second read request should retry the read (and not issue any new readaheads)\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions. So total number of read\n+    // calls will be one more from earlier (there is a reset mock which will reset the\n+    // count, but the mock stub is erased as well which needs AbsInputStream to be recreated,\n+    // which beats the purpose)\n+    verifyReadCallCount(client, 4);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects AbfsInputStream to utilize any data read ahead for\n+   * requested offset and length.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    // for post eviction check, fail all read aheads\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    // First read request that triggers readAheads.\n+    inputStream.read(new byte[1 * KILOBYTE]);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Another read request whose requested data is already read ahead.\n+    inputStream.read(1 * KILOBYTE, new byte[1 * KILOBYTE], 0, 1 * KILOBYTE);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+\n+  /**\n+   * This test expects ReadAheadManager to throw exception if the read ahead\n+   * thread had failed within the last 30 sec.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForFailedReadAhead() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // Actual read request fails with the failure in readahead thread\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Thread-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error RAH-Thread-Z\"))\n+        .doReturn(successOp) // Any extra calls to read, pass it.\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForFailedReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // if readAhead failed for specific offset, getBlock should\n+    // throw exception from the ReadBuffer that failed within last 30 sec\n+    intercept(IOException.class,\n+        () -> ReadBufferManager.getBufferManager().getBlock(\n+            inputStream,\n+            0,\n+            1 * KILOBYTE,\n+            new byte[1 * KILOBYTE]));\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub returns success for the 4th read request, if ReadBuffers still\n+    // persisted, ReadAheadManager getBlock would have returned exception.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return 0 receivedBytes when previous\n+   * read ahead on the offset had failed and not throw exception received then.\n+   * Also checks that the ReadBuffers are evicted as per the ReadBufferManager\n+   * threshold criteria.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForOlderReadAheadFailure() throws Exception {\n+    AbfsClient client = getMockAbfsClient();\n+    AbfsRestOperation successOp = getMockRestOp();\n+\n+    // Stub :\n+    // First Read request leads to 3 readahead calls: Fail all 3 readahead-client.read()\n+    // A second read request will see that readahead had failed for data in\n+    // the requested offset range but also that its is an older readahead request.\n+    // System issue could have resolved by now, so attempt a new read only for the requested range.\n+    doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\"))\n+        .doReturn(successOp) // pass the read for second read request\n+        .doReturn(successOp) // pass success for post eviction test\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testReadAheadManagerForOlderReadAheadFailure.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for 30 secs so that\n+    // read buffer qualifies for to be an old buffer\n+    Thread.sleep(ReadBufferManager.getBufferManager().getThresholdAgeMilliseconds());\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock from a new read request should return 0 if there is a failure\n+    // 30 sec before in read ahead buffer for respective offset.\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+    Assert.assertTrue(\"bytesRead should be zero when previously read \"\n+        + \"ahead buffer had failed\", bytesRead == 0);\n+\n+    // Stub returns success for the 5th read request, if ReadBuffers still\n+    // persisted request would have failed for position 0.\n+    checkEvictedStatus(inputStream, 0, false);\n+  }\n+\n+  /**\n+   * The test expects ReadAheadManager to return data from previously read\n+   * ahead data of same offset.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n+    // Mock failure for client.read()\n+    AbfsClient client = getMockAbfsClient();\n+\n+    // Success operation mock\n+    AbfsRestOperation op = getMockRestOp();\n+\n+    // Stub :\n+    // Pass all readAheads and fail the post eviction request to\n+    // prove ReadAhead buffer is used\n+    doReturn(op)\n+        .doReturn(op)\n+        .doReturn(op)\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-X\")) // for post eviction request\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Y\"))\n+        .doThrow(new TimeoutException(\"Internal Server error for RAH-Z\"))\n+        .when(client)\n+        .read(any(String.class), any(Long.class), any(byte[].class),\n+            any(Integer.class), any(Integer.class), any(String.class));\n+\n+    AbfsInputStream inputStream = getAbfsInputStream(client, \"testSuccessfulReadAhead.txt\");\n+\n+    queueReadAheads(inputStream);\n+\n+    // AbfsInputStream Read would have waited for the read-ahead for the requested offset\n+    // as we are testing from ReadAheadManager directly, sleep for a sec to\n+    // get the read ahead threads to complete\n+    Thread.sleep(1000);\n+\n+    // Only the 3 readAhead threads should have triggered client.read\n+    verifyReadCallCount(client, 3);\n+\n+    // getBlock for a new read should return the buffer read-ahead\n+    int bytesRead = ReadBufferManager.getBufferManager().getBlock(\n+        inputStream,\n+        1 * KILOBYTE,\n+        1 * KILOBYTE,\n+        new byte[1 * KILOBYTE]);\n+\n+    Assert.assertTrue(\"bytesRead should be non-zero from the \"\n+        + \"buffer that was read-ahead\", bytesRead > 0);\n+\n+    // Once created, mock will remember all interactions.\n+    // As the above read should not have triggered any server calls, total\n+    // number of read calls made at this point will be same as last.\n+    verifyReadCallCount(client, 3);\n+\n+    // Stub will throw exception for client.read() for 4th and later calls\n+    // if not using the read-ahead buffer exception will be thrown on read\n+    checkEvictedStatus(inputStream, 0, true);\n+  }\n+}", "originalCommit": "52fba323269e6272849b89824dc372398ead4894", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "chunk": "diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\nindex 54101110a1b..8c5f9788351 100644\n--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java\n\n@@ -43,7 +43,9 @@\n public class TestAbfsInputStream extends\n     AbstractAbfsIntegrationTest {\n \n-  private static final int KILOBYTE = 1024;\n+  private static final int _1KB = 1 * 1024;\n+  private static final int _2KB = 2 * 1024;\n+  private static final int _3KB = 3 * 1024;\n \n   private AbfsRestOperation getMockRestOp() {\n     AbfsRestOperation op = mock(AbfsRestOperation.class);\n"}}, {"oid": "58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "url": "https://github.com/apache/hadoop/commit/58ac7019dd59b3f56c8ff7c438b45ac6f43421b3", "message": "Review comments", "committedDate": "2020-04-22T10:34:59Z", "type": "commit"}, {"oid": "5d1e427018bbc8d2423ed9c802ade8233faeb482", "url": "https://github.com/apache/hadoop/commit/5d1e427018bbc8d2423ed9c802ade8233faeb482", "message": "Merge with trunk and checkstyle fix", "committedDate": "2020-05-18T14:38:31Z", "type": "commit"}, {"oid": "08c072358e31d31c674dc1aff00916bc6a784ea2", "url": "https://github.com/apache/hadoop/commit/08c072358e31d31c674dc1aff00916bc6a784ea2", "message": "Test updates needed post SAS change", "committedDate": "2020-05-19T12:48:06Z", "type": "commit"}, {"oid": "669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72", "url": "https://github.com/apache/hadoop/commit/669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72", "message": "Fix findbug issue", "committedDate": "2020-05-19T15:50:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686569061", "bodyText": "Hi @snvijaya\nI am unable to understand the significance of this change. I couldn't find in code anywhere where bufferIndex is set to -1 in case of read failure apart from the default value in the class. But when the buffers initialised, they are always set to value from 0 to 15.\nTrying to understand this for #3285. So please review that as well. Thanks.", "author": "mukund-thakur", "createdAt": "2021-08-11T07:26:47Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -253,7 +258,12 @@ private synchronized boolean tryEvict() {\n   }\n \n   private boolean evict(final ReadBuffer buf) {\n-    freeList.push(buf.getBufferindex());\n+    // As failed ReadBuffers (bufferIndx = -1) are saved in completedReadList,\n+    // avoid adding it to freeList.\n+    if (buf.getBufferindex() != -1) {\n+      freeList.push(buf.getBufferindex());\n+    }", "originalCommit": "669f3c18d26b0c29c1c50dcc8e11a039d2fe4a72", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU4MTM4NQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686581385", "bodyText": "Its set to -1 when read fails. You will find the diff for this in ReadBuffer.java line 110.\nThere is an issue with this commit though, for which a hotfix was made. Incase its relevant to your change -> https://issues.apache.org/jira/browse/HADOOP-17301\nWill check on your PR by EOW.", "author": "snvijaya", "createdAt": "2021-08-11T07:44:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4Njc0NTcyNQ==", "url": "https://github.com/apache/hadoop/pull/1898#discussion_r686745725", "bodyText": "Thanks @snvijaya", "author": "mukund-thakur", "createdAt": "2021-08-11T11:36:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY4NjU2OTA2MQ=="}], "type": "inlineReview", "revised_code": null}]}