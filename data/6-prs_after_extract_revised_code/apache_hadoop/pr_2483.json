{"pr_number": 2483, "pr_title": "HDFS-14904. Option to let Balancer prefer top used nodes in each iteration.", "pr_createdAt": "2020-11-23T01:30:14Z", "pr_url": "https://github.com/apache/hadoop/pull/2483", "timeline": [{"oid": "1a293a9a3b2c52716c73087cb8e44db0ff3ec094", "url": "https://github.com/apache/hadoop/commit/1a293a9a3b2c52716c73087cb8e44db0ff3ec094", "message": "Initial code", "committedDate": "2020-11-22T23:46:58Z", "type": "commit"}, {"oid": "3e495b7db13f4c8869aa83b08bfddc7f9d07b1a6", "url": "https://github.com/apache/hadoop/commit/3e495b7db13f4c8869aa83b08bfddc7f9d07b1a6", "message": "Fix style", "committedDate": "2020-11-23T01:17:59Z", "type": "commit"}, {"oid": "8d36dc29ba9ca1a2467272c8461d183598f431fb", "url": "https://github.com/apache/hadoop/commit/8d36dc29ba9ca1a2467272c8461d183598f431fb", "message": "Fix style", "committedDate": "2020-11-23T06:05:54Z", "type": "commit"}, {"oid": "ddd66e0b65e51ca31cb1b628fb05505554078ba6", "url": "https://github.com/apache/hadoop/commit/ddd66e0b65e51ca31cb1b628fb05505554078ba6", "message": "Fix style", "committedDate": "2020-11-23T06:11:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkxODY1Nw==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r532918657", "bodyText": "How about describe the parameter option as: \"sort datanodes based on the utilization so that highly utilized datanodes get scheduled first\"?", "author": "Jing9", "createdAt": "2020-11-30T21:35:38Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "diffHunk": "@@ -199,7 +199,10 @@\n       + \"\\tWhether to run the balancer during an ongoing HDFS upgrade.\"\n       + \"This is usually not desired since it will not affect used space \"\n       + \"on over-utilized machines.\"\n-      + \"\\n\\t[-asService]\\tRun as a long running service.\";\n+      + \"\\n\\t[-asService]\\tRun as a long running service.\"\n+      + \"\\n\\t[-sortTopNodes]\"\n+      + \"\\tSort over-utilized nodes by capacity to\"\n+      + \" bring down top used datanode faster.\";", "originalCommit": "ddd66e0b65e51ca31cb1b628fb05505554078ba6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY0OTg4Nw==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r533649887", "bodyText": "Sounds good, will update", "author": "LeonGao91", "createdAt": "2020-12-01T18:59:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkxODY1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ca36965781af65ee61f753a6afe4f3683a374133", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\nindex 21ca783f78d..e5f9e8c8061 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n\n@@ -201,8 +203,8 @@\n       + \"on over-utilized machines.\"\n       + \"\\n\\t[-asService]\\tRun as a long running service.\"\n       + \"\\n\\t[-sortTopNodes]\"\n-      + \"\\tSort over-utilized nodes by capacity to\"\n-      + \" bring down top used datanode faster.\";\n+      + \"\\tSort datanodes based on the utilization so \"\n+      + \"that highly utilized datanodes get scheduled first.\";\n \n   @VisibleForTesting\n   private static volatile boolean serviceRunning = false;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNTA5MA==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r532925090", "bodyText": "Do we need this \"if\" statement? Maybe use a Preconditions instead?", "author": "Jing9", "createdAt": "2020-11-30T21:48:17Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "diffHunk": "@@ -435,6 +444,22 @@ private long init(List<DatanodeStorageReport> reports) {\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n \n+  private void sortOverUtilizedNodes() {\n+    LOG.info(\"Sorting over-utilized nodes by capacity\" +\n+        \" to bring down top used datanode capacity faster\");\n+\n+    if (overUtilized instanceof List) {", "originalCommit": "ddd66e0b65e51ca31cb1b628fb05505554078ba6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MDQ4Mg==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r533650482", "bodyText": "This is for findbugs, will check if I can get around it with precondition", "author": "LeonGao91", "createdAt": "2020-12-01T19:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNTA5MA=="}], "type": "inlineReview", "revised_code": {"commit": "ca36965781af65ee61f753a6afe4f3683a374133", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\nindex 21ca783f78d..e5f9e8c8061 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n\n@@ -444,20 +449,19 @@ private long init(List<DatanodeStorageReport> reports) {\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n \n-  private void sortOverUtilizedNodes() {\n+  private void sortOverUtilized(Map<Source, Double> overUtilizedPercentage) {\n+    Preconditions.checkState(overUtilized instanceof List,\n+        \"Collection overUtilized is not a List.\");\n+\n     LOG.info(\"Sorting over-utilized nodes by capacity\" +\n         \" to bring down top used datanode capacity faster\");\n \n-    if (overUtilized instanceof List) {\n-      List<Source> list = (List<Source>) overUtilized;\n-      list.sort(\n-          (Source source1, Source source2) ->\n-              (Float.compare(source2.getDatanodeInfo().getDfsUsedPercent(),\n-                  source1.getDatanodeInfo().getDfsUsedPercent()))\n-      );\n-    } else {\n-      LOG.error(\"Collection overUtilized is not a List, skip sorting.\");\n-    }\n+    List<Source> list = (List<Source>) overUtilized;\n+    list.sort(\n+        (Source source1, Source source2) ->\n+            (Double.compare(overUtilizedPercentage.get(source2),\n+                overUtilizedPercentage.get(source1)))\n+    );\n   }\n \n   private static long computeMaxSize2Move(final long capacity, final long remaining,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTMzMw==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r532981333", "bodyText": "Do we need to consider StorageType (which is associated with Source)? E.g., suppose a DN has 2 storage types, one of which is highly utilized and the other is just above average. Do we want to first schedule the movement for the highly-utilized storage type on this node?", "author": "Jing9", "createdAt": "2020-11-30T23:55:58Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "diffHunk": "@@ -435,6 +444,22 @@ private long init(List<DatanodeStorageReport> reports) {\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n \n+  private void sortOverUtilizedNodes() {\n+    LOG.info(\"Sorting over-utilized nodes by capacity\" +\n+        \" to bring down top used datanode capacity faster\");\n+\n+    if (overUtilized instanceof List) {\n+      List<Source> list = (List<Source>) overUtilized;\n+      list.sort(\n+          (Source source1, Source source2) ->", "originalCommit": "ddd66e0b65e51ca31cb1b628fb05505554078ba6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MDg1Nw==", "url": "https://github.com/apache/hadoop/pull/2483#discussion_r533650857", "bodyText": "Good idea, we should use utilization for storage type instead of datanode utilization. Will fix", "author": "LeonGao91", "createdAt": "2020-12-01T19:01:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTMzMw=="}], "type": "inlineReview", "revised_code": {"commit": "ca36965781af65ee61f753a6afe4f3683a374133", "chunk": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\nindex 21ca783f78d..e5f9e8c8061 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n\n@@ -444,20 +449,19 @@ private long init(List<DatanodeStorageReport> reports) {\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n \n-  private void sortOverUtilizedNodes() {\n+  private void sortOverUtilized(Map<Source, Double> overUtilizedPercentage) {\n+    Preconditions.checkState(overUtilized instanceof List,\n+        \"Collection overUtilized is not a List.\");\n+\n     LOG.info(\"Sorting over-utilized nodes by capacity\" +\n         \" to bring down top used datanode capacity faster\");\n \n-    if (overUtilized instanceof List) {\n-      List<Source> list = (List<Source>) overUtilized;\n-      list.sort(\n-          (Source source1, Source source2) ->\n-              (Float.compare(source2.getDatanodeInfo().getDfsUsedPercent(),\n-                  source1.getDatanodeInfo().getDfsUsedPercent()))\n-      );\n-    } else {\n-      LOG.error(\"Collection overUtilized is not a List, skip sorting.\");\n-    }\n+    List<Source> list = (List<Source>) overUtilized;\n+    list.sort(\n+        (Source source1, Source source2) ->\n+            (Double.compare(overUtilizedPercentage.get(source2),\n+                overUtilizedPercentage.get(source1)))\n+    );\n   }\n \n   private static long computeMaxSize2Move(final long capacity, final long remaining,\n"}}, {"oid": "ca36965781af65ee61f753a6afe4f3683a374133", "url": "https://github.com/apache/hadoop/commit/ca36965781af65ee61f753a6afe4f3683a374133", "message": "Resolve comments", "committedDate": "2020-12-01T21:19:22Z", "type": "commit"}, {"oid": "4d22df48c89b94c572958d5a82218d8f83b517c3", "url": "https://github.com/apache/hadoop/commit/4d22df48c89b94c572958d5a82218d8f83b517c3", "message": "Trigger Build", "committedDate": "2020-12-01T23:52:17Z", "type": "commit"}]}