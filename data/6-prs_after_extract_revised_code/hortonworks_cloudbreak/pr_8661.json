{"pr_number": 8661, "pr_title": "CB-5742-EnhanceErrorHandling", "pr_createdAt": "2020-07-29T09:57:13Z", "pr_url": "https://github.com/hortonworks/cloudbreak/pull/8661", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzQwOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462183409", "bodyText": "ClusterProxy is not a user controlled config, hence removed this user facing validation and also clusterProxy is not a cluster level configuration but a deployment level configuration which is required to be enabled. Second  check for clusterProxy is there while polling yarn.\nMultiple load-alerts in a request is already validated in API layer, hence removed duplicate check.", "author": "smaniraju", "createdAt": "2020-07-29T09:59:42Z", "path": "autoscale/src/main/java/com/sequenceiq/periscope/controller/AlertController.java", "diffHunk": "@@ -229,16 +228,6 @@ private void validateLoadAlert(Long clusterId, Optional<Long> alertId, LoadAlert\n                 () -> {\n                     validateAccountEntitlement(cluster);\n                     validateSupportedHostGroup(cluster, json.getScalingPolicy().getHostGroup(), AlertType.LOAD);\n-                    String requestHostGroup = json.getScalingPolicy().getHostGroup();\n-                    cluster.getLoadAlerts().stream().map(LoadAlert::getScalingPolicy).map(ScalingPolicy::getHostGroup)\n-                            .filter(hostGroup -> hostGroup.equalsIgnoreCase(requestHostGroup)).findAny()\n-                            .ifPresent(hostGroup -> {\n-                                throw new BadRequestException(messagesService\n-                                        .getMessage(MessageCode.LOAD_CONFIG_ALREADY_DEFINED, List.of(cluster.getStackName(), requestHostGroup)));\n-                            });\n-                    clusterProxyConfigurationService.getClusterProxyUrl()\n-                            .orElseThrow(() ->  new BadRequestException(\n-                                    messagesService.getMessage(MessageCode.CLUSTER_PROXY_NOT_CONFIGURED, List.of(cluster.getStackName()))));\n                 });\n     }\n ", "originalCommit": "3ed5462dd97bf35eb6983e0d4a9e9cd30c92ff0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYzODkxOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462638918", "bodyText": "Is ClusterProxy always on? (Is there any way to disable it, via the CLI for example?) Cloudbreak still supports DIRECT and CLUSTER_PROXY (Tunnel.java)", "author": "sidseth", "createdAt": "2020-07-29T23:03:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzQwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxODQ1NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462718455", "bodyText": "Yes it is always on. The datahub registration with cluster proxy is always done irrespective of CLI tunnel parameter. I have tested this flow and access via Cluster Proxy is the only approach that works always since private CM IP will not be accessible for Control Plane Services.", "author": "smaniraju", "createdAt": "2020-07-30T03:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzQwOQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzcyNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462183726", "bodyText": "validation need not be within the transaction.", "author": "smaniraju", "createdAt": "2020-07-29T10:00:15Z", "path": "autoscale/src/main/java/com/sequenceiq/periscope/controller/DistroXAutoScaleClusterV1Controller.java", "diffHunk": "@@ -121,11 +121,12 @@ private DistroXAutoscaleClusterResponse createClusterJsonResponse(Cluster cluste\n     private DistroXAutoscaleClusterResponse updateClusterAutoScaleConfig(Long clusterId,\n             DistroXAutoscaleClusterRequest autoscaleClusterRequest) {\n \n+        alertController.validateLoadAlertRequests(clusterId, autoscaleClusterRequest.getLoadAlertRequests());\n+        alertController.validateTimeAlertRequests(clusterId, autoscaleClusterRequest.getTimeAlertRequests());\n+\n         try {\n             transactionService.required(() -> {\n                 clusterService.deleteAlertsForCluster(clusterId);\n-                alertController.validateLoadAlertRequests(clusterId, autoscaleClusterRequest.getLoadAlertRequests());\n-                alertController.validateTimeAlertRequests(clusterId, autoscaleClusterRequest.getTimeAlertRequests());\n                 alertController.createLoadAlerts(clusterId, autoscaleClusterRequest.getLoadAlertRequests());\n                 alertController.createTimeAlerts(clusterId, autoscaleClusterRequest.getTimeAlertRequests());\n                 asClusterCommonService.setAutoscaleState(clusterId, autoscaleClusterRequest.getEnableAutoscaling());", "originalCommit": "3ed5462dd97bf35eb6983e0d4a9e9cd30c92ff0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NDk3Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462184977", "bodyText": "Periscope status should be synced based on both cluster and stack status, since next upscale is supported only when both are in available status. Even though they both are in sync in certain stimulated slow CB scenarios there is small drift between these two status.", "author": "smaniraju", "createdAt": "2020-07-29T10:02:19Z", "path": "autoscale/src/main/java/com/sequenceiq/periscope/monitor/handler/ClusterStatusSyncHandler.java", "diffHunk": "@@ -39,19 +40,22 @@ public void onApplicationEvent(ClusterStatusSyncEvent event) {\n         }\n         MDCBuilder.buildMdcContext(cluster);\n \n-        Status cbClusterStatus = Optional.ofNullable(cloudbreakCommunicator\n-                .getStackStatusByCrn(cluster.getStackCrn()).getClusterStatus()).orElse(Status.AMBIGUOUS);\n-        LOGGER.debug(\"Analysing CBCluster Status '{}' for Cluster '{}' \", cbClusterStatus, cluster.getStackCrn());\n+        StackStatusV4Response statusResponse = cloudbreakCommunicator.getStackStatusByCrn(cluster.getStackCrn());\n+        boolean clusterAvailable = Optional.ofNullable(statusResponse.getStatus()).map(Status::isAvailable).orElse(false)\n+                && Optional.ofNullable(statusResponse.getClusterStatus()).map(Status::isAvailable).orElse(false);\n+        LOGGER.debug(\"Analysing CBCluster Status '{}' for Cluster '{}' \", statusResponse, cluster.getStackCrn());\n \n-        if (DELETE_COMPLETED.equals(cbClusterStatus)) {\n+        if (DELETE_COMPLETED.equals(statusResponse.getStatus())) {\n             clusterService.removeById(autoscaleClusterId);\n-            LOGGER.debug(\"Deleted cluster '{}', CB Cluster status '{}'.\", cluster.getStackCrn(), cbClusterStatus);\n-        } else if (cbClusterStatus.isAvailable() && !RUNNING.equals(cluster.getState())) {\n+            LOGGER.info(\"Deleted cluster '{}', CB Stack Status '{}'.\", cluster.getStackCrn(), statusResponse.getStatus());\n+        } else if (clusterAvailable && !RUNNING.equals(cluster.getState())) {\n             clusterService.setState(cluster.getId(), ClusterState.RUNNING);\n-            LOGGER.debug(\"Updated cluster '{}' to running, CB Cluster status '{}'.\", cluster.getStackCrn(), cbClusterStatus);\n-        } else if (!cbClusterStatus.isAvailable() && RUNNING.equals(cluster.getState())) {\n+            LOGGER.info(\"Updated cluster '{}' to Running, CB Stack Status '{}', CB Cluster Status '{}'.\",\n+                    cluster.getStackCrn(), statusResponse.getStatus(), statusResponse.getClusterStatus());\n+        } else if (!clusterAvailable && RUNNING.equals(cluster.getState())) {\n             clusterService.setState(cluster.getId(), ClusterState.SUSPENDED);\n-            LOGGER.debug(\"Suspended cluster '{}', CB Cluster status '{}'\", cluster.getStackCrn(), cbClusterStatus);\n+            LOGGER.info(\"Suspended cluster '{}', CB Stack Status '{}', CB Cluster Status '{}'\",\n+                    cluster.getStackCrn(), statusResponse.getStatus(), statusResponse.getClusterStatus());\n         }", "originalCommit": "3ed5462dd97bf35eb6983e0d4a9e9cd30c92ff0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0MDI4Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462640282", "bodyText": "Thank you. Was going to ask just this question on how they differ.\nWasn't there a change recently where at least downscale worked even if a previous downscale had failed (or node deleted on provider side).\nThat's the series of jiras Amit had opened. cc @cegganesh84", "author": "sidseth", "createdAt": "2020-07-29T23:07:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NDk3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY2MzMzNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462663336", "bodyText": "Was going to ask just this question on how they differ.\n\nstack -> represents the cloud, cluster -> represents the cm cluster. Yes, there can be drift. The first stack will be scaled to accommodate more nodes. Then the cluster will be scaled by the means of provisioning services. https://jira.cloudera.com/browse/CB-8051?focusedCommentId=2639540&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-2639540. Extending this, will apply for downscale also in the reverse order.", "author": "cegganesh84", "createdAt": "2020-07-30T00:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NDk3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcyMDA5MA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8661#discussion_r462720090", "bodyText": "Wasn't there a change recently where at least downscale worked even if a previous downscale had failed (or node deleted on provider side).\n\namit opened jiras were basically related to CB not identifying node deleted on provider side. It is not related to this change.  The other change in this area is that forced downscaling is integrated with Yarn Recommended Nodes but this PR change is not related to that.", "author": "smaniraju", "createdAt": "2020-07-30T03:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NDk3Nw=="}], "type": "inlineReview", "revised_code": null}, {"oid": "421210a2926c9c840caa1c1d168cc9e4c0de1f7e", "url": "https://github.com/hortonworks/cloudbreak/commit/421210a2926c9c840caa1c1d168cc9e4c0de1f7e", "message": "CB-5742-EnhanceErrorHandling\n\nRemoved non-user facing and duplicate validations.\nUpdate periscope cluster status based on both CB Stack and Cluster Status.", "committedDate": "2020-07-29T10:04:32Z", "type": "forcePushed"}, {"oid": "1813f53b602064baf5528c63db5d787d2b4db272", "url": "https://github.com/hortonworks/cloudbreak/commit/1813f53b602064baf5528c63db5d787d2b4db272", "message": "CB-5742-EnhanceErrorHandling\n\nRemoved user validation for clusterproxy since it is not User Controlled Configuration and duplicate validations.\nUpdate periscope cluster status based on both CB Stack and Cluster Status.", "committedDate": "2020-07-29T10:13:27Z", "type": "commit"}, {"oid": "1813f53b602064baf5528c63db5d787d2b4db272", "url": "https://github.com/hortonworks/cloudbreak/commit/1813f53b602064baf5528c63db5d787d2b4db272", "message": "CB-5742-EnhanceErrorHandling\n\nRemoved user validation for clusterproxy since it is not User Controlled Configuration and duplicate validations.\nUpdate periscope cluster status based on both CB Stack and Cluster Status.", "committedDate": "2020-07-29T10:13:27Z", "type": "forcePushed"}]}