{"pr_number": 1234, "pr_title": "LUCENE-9211 Add compression for Binary doc value fields", "pr_createdAt": "2020-02-03T17:31:55Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1234", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NDg3OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374744879", "bodyText": "Extra space character before implements?", "author": "mikemccand", "createdAt": "2020-02-04T15:34:50Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 808ccbc16fb..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter  implements Closeable {\n+  class CompressedBinaryBlockWriter implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NTgxMQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374745811", "bodyText": "Add space after if before (?", "author": "mikemccand", "createdAt": "2020-02-04T15:36:19Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 808ccbc16fb..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter  implements Closeable {\n+  class CompressedBinaryBlockWriter implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0Njk3OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374746978", "bodyText": "Remove space after ( before blockId?", "author": "mikemccand", "createdAt": "2020-02-04T15:38:02Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if ( blockId != lastBlockId) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex b076a629ec3..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,56 +766,74 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n-    private BytesRef uncompressedBytesRef;\n+    private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n-      if ( blockId != lastBlockId) {\n+      if (blockId != lastBlockId) {\n         lastBlockId = blockId;\n         long blockStartOffset = addresses.get(blockId);\n         compressedData.seek(blockStartOffset);\n         \n-        numDocsInBlock = compressedData.readVInt();\n-        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-        uncompressedDocEnds = new int[numDocsInBlock];\n         uncompressedBlockLength = 0;        \n-        for (int i = 0; i < numDocsInBlock; i++) {\n-          uncompressedBlockLength += compressedData.readVInt();\n-          uncompressedDocEnds[i] = uncompressedBlockLength;\n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < docsPerChunk; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>>1;\n+            if (sameIndicator == 1) {\n+              onlyLength = firstValLength;\n+            }\n+            uncompressedBlockLength += firstValLength;            \n+          } else {\n+            if (onlyLength == -1) {\n+              // Various lengths are stored - read each from disk\n+              uncompressedBlockLength += compressedData.readVInt();            \n+            } else {\n+              // Only one length \n+              uncompressedBlockLength += onlyLength;\n+            }\n+          }\n+          uncompressedDocStarts[i+1] = uncompressedBlockLength;\n         }\n         \n         if (uncompressedBlockLength == 0) {\n-          uncompressedBytesRef = new BytesRef(BytesRef.EMPTY_BYTES);\n-        } else {\n-          assert uncompressedBlockLength <= uncompressedBlock.length;\n-          LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n-          uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+          uncompressedBytesRef.offset = 0;\n+          uncompressedBytesRef.length = 0;\n+          return uncompressedBytesRef;\n         }\n+        \n+        assert uncompressedBlockLength <= uncompressedBlock.length;\n+        LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n       }\n       \n-      // Position the Bytes ref to the relevant part of the uncompressed block\n-      if (docInBlockId > 0) {\n-        uncompressedBytesRef.offset = uncompressedDocEnds[docInBlockId - 1];        \n-      }\n-      uncompressedBytesRef.length = uncompressedDocEnds[docInBlockId] - uncompressedBytesRef.offset;\n+      uncompressedBytesRef.offset = uncompressedDocStarts[docInBlockId];        \n+      uncompressedBytesRef.length = uncompressedDocStarts[docInBlockId +1] - uncompressedBytesRef.offset;\n       return uncompressedBytesRef;\n     }    \n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NzIwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374747203", "bodyText": "s/Bytes ref/BytesRef?", "author": "mikemccand", "createdAt": "2020-02-04T15:38:23Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if ( blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          uncompressedBlockLength += compressedData.readVInt();\n+          uncompressedDocEnds[i] = uncompressedBlockLength;\n+        }\n+        \n+        if (uncompressedBlockLength == 0) {\n+          uncompressedBytesRef = new BytesRef(BytesRef.EMPTY_BYTES);\n+        } else {\n+          assert uncompressedBlockLength <= uncompressedBlock.length;\n+          LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n+          uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+        }\n+      }\n+      \n+      // Position the Bytes ref to the relevant part of the uncompressed block", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex b076a629ec3..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,56 +766,74 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n-    private BytesRef uncompressedBytesRef;\n+    private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n-      if ( blockId != lastBlockId) {\n+      if (blockId != lastBlockId) {\n         lastBlockId = blockId;\n         long blockStartOffset = addresses.get(blockId);\n         compressedData.seek(blockStartOffset);\n         \n-        numDocsInBlock = compressedData.readVInt();\n-        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-        uncompressedDocEnds = new int[numDocsInBlock];\n         uncompressedBlockLength = 0;        \n-        for (int i = 0; i < numDocsInBlock; i++) {\n-          uncompressedBlockLength += compressedData.readVInt();\n-          uncompressedDocEnds[i] = uncompressedBlockLength;\n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < docsPerChunk; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>>1;\n+            if (sameIndicator == 1) {\n+              onlyLength = firstValLength;\n+            }\n+            uncompressedBlockLength += firstValLength;            \n+          } else {\n+            if (onlyLength == -1) {\n+              // Various lengths are stored - read each from disk\n+              uncompressedBlockLength += compressedData.readVInt();            \n+            } else {\n+              // Only one length \n+              uncompressedBlockLength += onlyLength;\n+            }\n+          }\n+          uncompressedDocStarts[i+1] = uncompressedBlockLength;\n         }\n         \n         if (uncompressedBlockLength == 0) {\n-          uncompressedBytesRef = new BytesRef(BytesRef.EMPTY_BYTES);\n-        } else {\n-          assert uncompressedBlockLength <= uncompressedBlock.length;\n-          LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n-          uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+          uncompressedBytesRef.offset = 0;\n+          uncompressedBytesRef.length = 0;\n+          return uncompressedBytesRef;\n         }\n+        \n+        assert uncompressedBlockLength <= uncompressedBlock.length;\n+        LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n       }\n       \n-      // Position the Bytes ref to the relevant part of the uncompressed block\n-      if (docInBlockId > 0) {\n-        uncompressedBytesRef.offset = uncompressedDocEnds[docInBlockId - 1];        \n-      }\n-      uncompressedBytesRef.length = uncompressedDocEnds[docInBlockId] - uncompressedBytesRef.offset;\n+      uncompressedBytesRef.offset = uncompressedDocStarts[docInBlockId];        \n+      uncompressedBytesRef.length = uncompressedDocStarts[docInBlockId +1] - uncompressedBytesRef.offset;\n       return uncompressedBytesRef;\n     }    \n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0ODUxOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374748519", "bodyText": "Remove one of the spaces after void before addDoc?", "author": "mikemccand", "createdAt": "2020-02-04T15:40:31Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 808ccbc16fb..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter  implements Closeable {\n+  class CompressedBinaryBlockWriter implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374749291", "bodyText": "Probably we could (later, separate issue) optimize writing these lengths -- often all docs will have the same length?", "author": "mikemccand", "createdAt": "2020-02-04T15:41:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3MzczNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375273736", "bodyText": "+1", "author": "jpountz", "createdAt": "2020-02-05T14:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk3NDM3MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375974370", "bodyText": "Done", "author": "markharwood", "createdAt": "2020-02-06T17:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 808ccbc16fb..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter  implements Closeable {\n+  class CompressedBinaryBlockWriter implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1MDIwNA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374750204", "bodyText": "Can you include maxPointer and fp in this exception message?", "author": "mikemccand", "createdAt": "2020-02-04T15:43:10Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeInt(totalChunks);\n+      meta.writeInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up\", filePointersIn);", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 808ccbc16fb..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter  implements Closeable {\n+  class CompressedBinaryBlockWriter implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjEzMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252133", "bodyText": "make it final?", "author": "jpountz", "createdAt": "2020-02-05T13:25:54Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -61,11 +66,13 @@\n \n   IndexOutput data, meta;\n   final int maxDoc;\n+  private SegmentWriteState state;", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -66,7 +66,7 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n \n   IndexOutput data, meta;\n   final int maxDoc;\n-  private SegmentWriteState state;\n+  private final SegmentWriteState state;\n \n   /** expert: Creates a new writer */\n   public Lucene80DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjgzNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252836", "bodyText": "we usually don't let spaces between the type of array elements and []\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n          \n          \n            \n                int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];", "author": "jpountz", "createdAt": "2020-02-05T13:27:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; ", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjkwNw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252907", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                byte [] block = new byte [1024 * 16];\n          \n          \n            \n                byte[] block = new byte [1024 * 16];", "author": "jpountz", "createdAt": "2020-02-05T13:27:23Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375274563", "bodyText": "we usually do this like that instead, which helps avoid catching Throwable\nboolean success = false;\ntry {\n  // write header\n} finally {\n  if (success == false) {\n    // close\n  }\n}", "author": "jpountz", "createdAt": "2020-02-05T14:07:52Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyMjM3Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375922373", "bodyText": "What was the \"+1\" comment for line 407 about?\nI've seen encoding elsewhere that have n+1 offsets to record start of each value and the last offset is effectively the end of the last value. In this scenario I'm writing n value lengths.", "author": "markharwood", "createdAt": "2020-02-06T15:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyNzk2Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375927967", "bodyText": "It was about optimizing for the case that all values have the same length. In that case we could still one bit of the first length to mean that all values have the same length for instance?", "author": "jpountz", "createdAt": "2020-02-06T16:07:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375275497", "bodyText": "it looks like we could set blockAddressesStart in the constructor instead?", "author": "jpountz", "createdAt": "2020-02-05T14:09:36Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkwMzM0Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375903347", "bodyText": "I tried that and it didn't work - something else was writing to data in between constructor and addDoc calls", "author": "markharwood", "createdAt": "2020-02-06T15:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NzgwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379297803", "bodyText": "Have you found what this something else is?", "author": "jpountz", "createdAt": "2020-02-14T08:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3Nzg5OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375277898", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n          \n          \n            \n                    LZ4.compress(block, 0, uncompressedBlockLength, data, ht);", "author": "jpountz", "createdAt": "2020-02-05T14:13:59Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375278323", "bodyText": "this only happens if there are no values? when do we run into this condition?", "author": "jpountz", "createdAt": "2020-02-05T14:14:49Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkxNDgzNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375914836", "bodyText": "Looks to be when merges clear out deleted docs leaving no values.", "author": "markharwood", "createdAt": "2020-02-06T15:47:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5OTM4MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379299380", "bodyText": "This makes sense, can you leave a comment about it?", "author": "jpountz", "createdAt": "2020-02-14T08:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw=="}], "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex 04931049ddc..ab0824c9d81 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -365,8 +365,8 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte [] block = new byte [1024 * 16];\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTgyNzM0Ng==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375827346", "bodyText": "we could initialize uncompressedBytesRef from the uncompressed block:\nuncompressedBytesRef = new BytesRef(uncompressedBlock)\nand avoid creating new BytesRefs over and over in decode", "author": "jpountz", "createdAt": "2020-02-06T13:18:18Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      ", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex ec291ad15eb..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,25 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n-    private BytesRef uncompressedBytesRef;\n+    private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNzc1Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376527753", "bodyText": "maybe we could call it uncompressedDocStarts and set the index at i+1 which would then help below to remove the else block of the docInBlockId > 0 condition below?", "author": "jpountz", "createdAt": "2020-02-07T18:02:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>1;\n+            if (sameIndicator == 1) {\n+              onlyLength = firstValLength;\n+            }\n+            uncompressedBlockLength += firstValLength;            \n+          } else {\n+            if (onlyLength == -1) {\n+              // Various lengths are stored - read each from disk\n+              uncompressedBlockLength += compressedData.readVInt();            \n+            } else {\n+              // Only one length \n+              uncompressedBlockLength += onlyLength;\n+            }\n+          }\n+          uncompressedDocEnds[i] = uncompressedBlockLength;", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex c8d2ef05c39..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,26 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n     private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n       uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyODE2OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376528169", "bodyText": "do we really need to record the number of documents in the block? It should be 32 for all blocks except for the last one? Maybe at index-time we could append dummy values to the last block to make sure it has 32 values too, and we wouldn't need this vInt anymore?", "author": "jpountz", "createdAt": "2020-02-07T18:03:15Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex c8d2ef05c39..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,26 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n     private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n       uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyOTE5NQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376529195", "bodyText": "Since you are stealing a bit, we should do an unsigned shift (>>>) instead.\nThis would never be a problem in practice, but imagine than the length was a 31-bits integer. Shifting by one bit on the left at index time would make this number negative. So here we need an unsigned shift rather than a signed shift that preserves the sign.", "author": "jpountz", "createdAt": "2020-02-07T18:05:45Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>1;", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex c8d2ef05c39..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,26 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n     private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n       uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMTk1Mg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376531952", "bodyText": "in the past we've put these constants in the meta file and BinaryEntry so that it's easier to change values over time", "author": "jpountz", "createdAt": "2020-02-07T18:12:12Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU3OTk0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377579943", "bodyText": "@jpountz we should use the same structure while writing the data, in that case you will see all the properties of the class instead of adding comments in the code", "author": "juanka588", "createdAt": "2020-02-11T11:31:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMTk1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex c8d2ef05c39..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,26 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n     private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n       uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMjE4OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376532189", "bodyText": "can we reuse the same array across blocks?", "author": "jpountz", "createdAt": "2020-02-07T18:12:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex c8d2ef05c39..1d8f6b11676 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -764,26 +766,28 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private final int []uncompressedDocStarts;\n     private int uncompressedBlockLength = 0;        \n-    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n     private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n       uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n-      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      int docInBlockId = docNumber % docsPerChunk;\n+      assert docInBlockId < docsPerChunk;\n       \n       \n       // already read and uncompressed?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NDQ3OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377544478", "bodyText": "I think we should use the BinaryEntry object here, and the just make the object \"Writable\" to a given DataOutput and \"Readable\" from a DataInput (which is already the case: readBinaryEntry). This will avoid the comments in the code -2 == docsWithFieldOffset etc.", "author": "juanka588", "createdAt": "2020-02-11T10:18:04Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {\n+              // Write first value shifted and steal a bit to indicate other lengths are to follow\n+              int multipleLengths = (docLengths[0] <<1);\n+              data.writeVInt(multipleLengths);              \n+            } else {\n+              data.writeVInt(docLengths[i]);\n+            }\n+          }\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block, 0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        // Ensure initialized with zeroes because full array is always written\n+        Arrays.fill(docLengths, 0);\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeVInt(totalChunks);\n+      meta.writeVInt(Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK);\n+      meta.writeVInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up (\"+fp+\" vs expected \"+maxPointer+\")\", filePointersIn);\n+          }\n+          filePointers.finish();\n+        } catch (Throwable e) {\n+          priorE = e;\n+        } finally {\n+          CodecUtil.checkFooter(filePointersIn, priorE);\n+        }\n+      }\n+      // Write the length of the DMW block in the data \n+      meta.writeLong(data.getFilePointer() - startDMW);\n+    }\n \n-      final DirectMonotonicWriter writer = DirectMonotonicWriter.getInstance(meta, data, numDocsWithField + 1, DIRECT_MONOTONIC_BLOCK_SHIFT);\n-      long addr = 0;\n-      writer.add(addr);\n-      values = valuesProducer.getBinary(field);\n+    @Override\n+    public void close() throws IOException {\n+      if (tempBinaryOffsets != null) {\n+        IOUtils.close(tempBinaryOffsets);             \n+        state.directory.deleteFile(tempBinaryOffsets.getName());\n+        tempBinaryOffsets = null;\n+      }\n+    }\n+    \n+  }\n+  \n+\n+  @Override\n+  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n+    meta.writeInt(field.number);\n+    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n+\n+    try (CompressedBinaryBlockWriter blockWriter = new CompressedBinaryBlockWriter()){\n+      BinaryDocValues values = valuesProducer.getBinary(field);\n+      long start = data.getFilePointer();\n+      meta.writeLong(start); // dataOffset", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYyMTAwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377621003", "bodyText": "I like the idea but would prefer doing it in a separate PR.", "author": "jpountz", "createdAt": "2020-02-11T13:05:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NDQ3OA=="}], "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NTkwOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377545909", "bodyText": "Currently I'm working in a refactor of this code by having a doc id set iterator serializer capable to provide the correct instance based on the stored metadata. As you might see this is quite repetitive for the other fields", "author": "juanka588", "createdAt": "2020-02-11T10:20:40Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {\n+              // Write first value shifted and steal a bit to indicate other lengths are to follow\n+              int multipleLengths = (docLengths[0] <<1);\n+              data.writeVInt(multipleLengths);              \n+            } else {\n+              data.writeVInt(docLengths[i]);\n+            }\n+          }\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block, 0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        // Ensure initialized with zeroes because full array is always written\n+        Arrays.fill(docLengths, 0);\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeVInt(totalChunks);\n+      meta.writeVInt(Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK);\n+      meta.writeVInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up (\"+fp+\" vs expected \"+maxPointer+\")\", filePointersIn);\n+          }\n+          filePointers.finish();\n+        } catch (Throwable e) {\n+          priorE = e;\n+        } finally {\n+          CodecUtil.checkFooter(filePointersIn, priorE);\n+        }\n+      }\n+      // Write the length of the DMW block in the data \n+      meta.writeLong(data.getFilePointer() - startDMW);\n+    }\n \n-      final DirectMonotonicWriter writer = DirectMonotonicWriter.getInstance(meta, data, numDocsWithField + 1, DIRECT_MONOTONIC_BLOCK_SHIFT);\n-      long addr = 0;\n-      writer.add(addr);\n-      values = valuesProducer.getBinary(field);\n+    @Override\n+    public void close() throws IOException {\n+      if (tempBinaryOffsets != null) {\n+        IOUtils.close(tempBinaryOffsets);             \n+        state.directory.deleteFile(tempBinaryOffsets.getName());\n+        tempBinaryOffsets = null;\n+      }\n+    }\n+    \n+  }\n+  \n+\n+  @Override\n+  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n+    meta.writeInt(field.number);\n+    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n+\n+    try (CompressedBinaryBlockWriter blockWriter = new CompressedBinaryBlockWriter()){\n+      BinaryDocValues values = valuesProducer.getBinary(field);\n+      long start = data.getFilePointer();\n+      meta.writeLong(start); // dataOffset\n+      int numDocsWithField = 0;\n+      int minLength = Integer.MAX_VALUE;\n+      int maxLength = 0;\n       for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-        addr += values.binaryValue().length;\n-        writer.add(addr);\n+        numDocsWithField++;\n+        BytesRef v = values.binaryValue();      \n+        blockWriter.addDoc(doc, v);      \n+        int length = v.length;      \n+        minLength = Math.min(length, minLength);\n+        maxLength = Math.max(length, maxLength);\n       }\n-      writer.finish();\n-      meta.writeLong(data.getFilePointer() - start);\n+      blockWriter.flushData();\n+\n+      assert numDocsWithField <= maxDoc;\n+      meta.writeLong(data.getFilePointer() - start); // dataLength\n+\n+      if (numDocsWithField == 0) {\n+        meta.writeLong(-2); // docsWithFieldOffset\n+        meta.writeLong(0L); // docsWithFieldLength\n+        meta.writeShort((short) -1); // jumpTableEntryCount\n+        meta.writeByte((byte) -1);   // denseRankPower\n+      } else if (numDocsWithField == maxDoc) {\n+        meta.writeLong(-1); // docsWithFieldOffset\n+        meta.writeLong(0L); // docsWithFieldLength\n+        meta.writeShort((short) -1); // jumpTableEntryCount\n+        meta.writeByte((byte) -1);   // denseRankPower\n+      } else {\n+        long offset = data.getFilePointer();\n+        meta.writeLong(offset); // docsWithFieldOffset\n+        values = valuesProducer.getBinary(field);\n+        final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+        meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n+        meta.writeShort(jumpTableEntryCount);\n+        meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+      }", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU2NjU0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377566543", "bodyText": "This could be potentially in the BinaryDocValuesFormat class", "author": "juanka588", "createdAt": "2020-02-11T11:02:17Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesFormat.java", "diffHunk": "@@ -151,7 +151,8 @@ public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOExcepti\n   static final String META_CODEC = \"Lucene80DocValuesMetadata\";\n   static final String META_EXTENSION = \"dvm\";\n   static final int VERSION_START = 0;\n-  static final int VERSION_CURRENT = VERSION_START;\n+  static final int VERSION_BIN_COMPRESSED = 1;  ", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NDc5OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379294799", "bodyText": "can we make ht, tempBinaryOffsets, docLengths final?", "author": "jpountz", "createdAt": "2020-02-14T08:06:54Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NTQzMg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379295432", "bodyText": "Depending on the data that will be indexed it's very hard to know what is the right initial size here. Maybe start with an empty array? This will also give increase confidence that the resizing logic works.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                byte[] block = new byte [1024 * 16];\n          \n          \n            \n                byte[] block = BytesRef.EMPTY_BYTES;", "author": "jpountz", "createdAt": "2020-02-14T08:08:55Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5ODIxMg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379298212", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(i == 0) {\n          \n          \n            \n                        if (i == 0) {", "author": "jpountz", "createdAt": "2020-02-14T08:17:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5ODc2MQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379298761", "bodyText": "The second condition is necessary true given the parent if statement.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n          \n          \n            \n                    boolean allLengthsSame = true;", "author": "jpountz", "createdAt": "2020-02-14T08:18:50Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNDA3NA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379304074", "bodyText": "maybe keep this variable actually, it would help make version final by doing this.version = version; after the try block?", "author": "jpountz", "createdAt": "2020-02-14T08:33:18Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -59,18 +60,18 @@\n   private long ramBytesUsed;\n   private final IndexInput data;\n   private final int maxDoc;\n+  private int version = -1;\n \n   /** expert: instantiates a new reader */\n   Lucene80DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {\n     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);\n     this.maxDoc = state.segmentInfo.maxDoc();\n     ramBytesUsed = RamUsageEstimator.shallowSizeOfInstance(getClass());\n \n-    int version = -1;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNDM2OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379304369", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {\n          \n          \n            \n                if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField > 0) ||  entry.minLength < entry.maxLength) {", "author": "jpountz", "createdAt": "2020-02-14T08:34:03Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -182,10 +183,21 @@ private BinaryEntry readBinary(ChecksumIndexInput meta) throws IOException {\n     entry.numDocsWithField = meta.readInt();\n     entry.minLength = meta.readInt();\n     entry.maxLength = meta.readInt();\n-    if (entry.minLength < entry.maxLength) {\n+    if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex 1d8f6b11676..b076a629ec3 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -190,9 +190,8 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n       long numAddresses = entry.numDocsWithField + 1L;\n       // New count of compressed addresses - the number of compresseed blocks\n       if (version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED) {\n-        entry.numCompressedChunks = meta.readVInt();\n-        entry.docsPerChunk = meta.readVInt();\n-        entry.maxUncompressedChunkSize = meta.readVInt();\n+        entry.numCompressedChunks = meta.readInt();\n+        entry.maxUncompressedChunkSize = meta.readInt();\n         numAddresses = entry.numCompressedChunks;\n       }      \n       \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379305128", "bodyText": "let's use the shift from the BinaryEntry instead of the constant?", "author": "jpountz", "createdAt": "2020-02-14T08:36:08Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +757,125 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private final int []uncompressedDocStarts;\n+    private int uncompressedBlockLength = 0;        \n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; ", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ2MzQ0MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379463440", "bodyText": "I guess that means I should serialize the shift value rather the absolute number of docs per block?", "author": "markharwood", "createdAt": "2020-02-14T14:37:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ3Mzg2MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379473860", "bodyText": "I think so.", "author": "jpountz", "createdAt": "2020-02-14T14:56:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA=="}], "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex 1d8f6b11676..b076a629ec3 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -766,74 +764,56 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n     private final IndexInput compressedData;\n     // Cache of last uncompressed block \n     private long lastBlockId = -1;\n-    private final int []uncompressedDocStarts;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n     private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n     private final byte[] uncompressedBlock;\n-    private final BytesRef uncompressedBytesRef;\n-    private final int docsPerChunk;\n+    private BytesRef uncompressedBytesRef;\n     \n-    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n       super();\n       this.addresses = addresses;\n       this.compressedData = compressedData;\n       // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n       this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n-      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n-      this.docsPerChunk = docsPerChunk;\n-      uncompressedDocStarts = new int[docsPerChunk + 1];\n       \n     }\n \n     BytesRef decode(int docNumber) throws IOException {\n       int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n-      int docInBlockId = docNumber % docsPerChunk;\n-      assert docInBlockId < docsPerChunk;\n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n       \n       \n       // already read and uncompressed?\n-      if (blockId != lastBlockId) {\n+      if ( blockId != lastBlockId) {\n         lastBlockId = blockId;\n         long blockStartOffset = addresses.get(blockId);\n         compressedData.seek(blockStartOffset);\n         \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n         uncompressedBlockLength = 0;        \n-\n-        int onlyLength = -1;\n-        for (int i = 0; i < docsPerChunk; i++) {\n-          if (i == 0) {\n-            // The first length value is special. It is shifted and has a bit to denote if\n-            // all other values are the same length\n-            int lengthPlusSameInd = compressedData.readVInt();\n-            int sameIndicator = lengthPlusSameInd & 1;\n-            int firstValLength = lengthPlusSameInd >>>1;\n-            if (sameIndicator == 1) {\n-              onlyLength = firstValLength;\n-            }\n-            uncompressedBlockLength += firstValLength;            \n-          } else {\n-            if (onlyLength == -1) {\n-              // Various lengths are stored - read each from disk\n-              uncompressedBlockLength += compressedData.readVInt();            \n-            } else {\n-              // Only one length \n-              uncompressedBlockLength += onlyLength;\n-            }\n-          }\n-          uncompressedDocStarts[i+1] = uncompressedBlockLength;\n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          uncompressedBlockLength += compressedData.readVInt();\n+          uncompressedDocEnds[i] = uncompressedBlockLength;\n         }\n         \n         if (uncompressedBlockLength == 0) {\n-          uncompressedBytesRef.offset = 0;\n-          uncompressedBytesRef.length = 0;\n-          return uncompressedBytesRef;\n+          uncompressedBytesRef = new BytesRef(BytesRef.EMPTY_BYTES);\n+        } else {\n+          assert uncompressedBlockLength <= uncompressedBlock.length;\n+          LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n+          uncompressedBytesRef = new BytesRef(uncompressedBlock);\n         }\n-        \n-        assert uncompressedBlockLength <= uncompressedBlock.length;\n-        LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n       }\n       \n-      uncompressedBytesRef.offset = uncompressedDocStarts[docInBlockId];        \n-      uncompressedBytesRef.length = uncompressedDocStarts[docInBlockId +1] - uncompressedBytesRef.offset;\n+      // Position the Bytes ref to the relevant part of the uncompressed block\n+      if (docInBlockId > 0) {\n+        uncompressedBytesRef.offset = uncompressedDocEnds[docInBlockId - 1];        \n+      }\n+      uncompressedBytesRef.length = uncompressedDocEnds[docInBlockId] - uncompressedBytesRef.offset;\n       return uncompressedBytesRef;\n     }    \n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjMyNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379306326", "bodyText": "maybe this should be the \"shift\" instead of the number of docs per chunk, so that you you directly have both the shift (as-is) and the mask ((1 << shift) - 1)", "author": "jpountz", "createdAt": "2020-02-14T08:39:11Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -182,10 +183,21 @@ private BinaryEntry readBinary(ChecksumIndexInput meta) throws IOException {\n     entry.numDocsWithField = meta.readInt();\n     entry.minLength = meta.readInt();\n     entry.maxLength = meta.readInt();\n-    if (entry.minLength < entry.maxLength) {\n+    if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {\n       entry.addressesOffset = meta.readLong();\n+\n+      // Old count of uncompressed addresses \n+      long numAddresses = entry.numDocsWithField + 1L;\n+      // New count of compressed addresses - the number of compresseed blocks\n+      if (version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED) {\n+        entry.numCompressedChunks = meta.readVInt();\n+        entry.docsPerChunk = meta.readVInt();", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ2MzY3NQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379463675", "bodyText": "Ah - ignore my previous comment.", "author": "markharwood", "createdAt": "2020-02-14T14:37:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjMyNg=="}], "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\nindex 1d8f6b11676..b076a629ec3 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java\n\n@@ -190,9 +190,8 @@ final class Lucene80DocValuesProducer extends DocValuesProducer implements Close\n       long numAddresses = entry.numDocsWithField + 1L;\n       // New count of compressed addresses - the number of compresseed blocks\n       if (version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED) {\n-        entry.numCompressedChunks = meta.readVInt();\n-        entry.docsPerChunk = meta.readVInt();\n-        entry.maxUncompressedChunkSize = meta.readVInt();\n+        entry.numCompressedChunks = meta.readInt();\n+        entry.maxUncompressedChunkSize = meta.readInt();\n         numAddresses = entry.numCompressedChunks;\n       }      \n       \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjkwOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379306909", "bodyText": "in general we do a break when setting allLengthsSame = false instead of adding it to the exit condition of the for statement", "author": "jpountz", "createdAt": "2020-02-14T08:40:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNzExNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379307116", "bodyText": "if you're only doing it for i>0, let's make the loop start at i=1?", "author": "jpountz", "createdAt": "2020-02-14T08:41:20Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "chunk": "diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\nindex ab0824c9d81..808ccbc16fb 100644\n--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java\n\n@@ -360,13 +360,13 @@ final class Lucene80DocValuesConsumer extends DocValuesConsumer implements Close\n     }\n   }\n \n-  class CompressedBinaryBlockWriter implements Closeable {\n+  class CompressedBinaryBlockWriter  implements Closeable {\n     FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n     int uncompressedBlockLength = 0;\n     int maxUncompressedBlockLength = 0;\n     int numDocsInCurrentBlock = 0;\n-    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n-    byte[] block = new byte [1024 * 16];\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n     int totalChunks = 0;\n     long maxPointer = 0;\n     long blockAddressesStart = -1; \n"}}, {"oid": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "url": "https://github.com/apache/lucene-solr/commit/a20da17708f17f1b533ff515ad6d7ca249f610a9", "message": "Added variously compressible examples of content to tests as per @mikemccand suggestion", "committedDate": "2020-02-14T09:37:56Z", "type": "forcePushed"}, {"oid": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "url": "https://github.com/apache/lucene-solr/commit/1dcbb2e2ceddd70ec5842c35083a440b267582e9", "message": "Added compression for binary doc values - stores groups of 32 doc values in LZ4 compressed blocks", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "1b0c227412370666a627cfada93ef288fab9111f", "url": "https://github.com/apache/lucene-solr/commit/1b0c227412370666a627cfada93ef288fab9111f", "message": "Formatting fixes (Thanks, Mike!)", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "1b08f9b13194797629a2dea6130c0cacde6a5f4d", "url": "https://github.com/apache/lucene-solr/commit/1b08f9b13194797629a2dea6130c0cacde6a5f4d", "message": "Addressing review comments (thanks, jpountz!)", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "6dd00af56e4eabfd5815f6dae1461185222576ed", "url": "https://github.com/apache/lucene-solr/commit/6dd00af56e4eabfd5815f6dae1461185222576ed", "message": "Optimisation - only write one value if all docs share the same length", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "f69f99001c50294f70400c78f9c5a39c2e5e1018", "url": "https://github.com/apache/lucene-solr/commit/f69f99001c50294f70400c78f9c5a39c2e5e1018", "message": "Addressing @jpountz review comments - make num docs per chunk part of metadata, remove per-chunk storage of num docs and assume always same number of values. Change doc end pointers to be doc starts.", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "289d549bc3003ce1d6a58d9f417123b1631bd7dd", "url": "https://github.com/apache/lucene-solr/commit/289d549bc3003ce1d6a58d9f417123b1631bd7dd", "message": "Added variously compressible examples of content to tests as per @mikemccand suggestion", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "7d0db34b68145dfd0454b2ef5b3f0138aab431df", "url": "https://github.com/apache/lucene-solr/commit/7d0db34b68145dfd0454b2ef5b3f0138aab431df", "message": "Addressing review comments. Fields made final, initialise blockAddressesStart in CompressedBinaryBlockWriter constructor, serialise shift for number of docs in block,", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "7d0db34b68145dfd0454b2ef5b3f0138aab431df", "url": "https://github.com/apache/lucene-solr/commit/7d0db34b68145dfd0454b2ef5b3f0138aab431df", "message": "Addressing review comments. Fields made final, initialise blockAddressesStart in CompressedBinaryBlockWriter constructor, serialise shift for number of docs in block,", "committedDate": "2020-02-18T09:45:32Z", "type": "forcePushed"}, {"oid": "3a3c30fca98e30e1159ecb1707d6874a32212056", "url": "https://github.com/apache/lucene-solr/commit/3a3c30fca98e30e1159ecb1707d6874a32212056", "message": "Added note to changes.txt", "committedDate": "2020-02-18T13:39:29Z", "type": "commit"}]}