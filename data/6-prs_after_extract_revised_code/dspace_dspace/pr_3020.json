{"pr_number": 3020, "pr_title": "Upgrade to latest Guava (v30) and Solr (v8) to resolve security alerts", "pr_createdAt": "2020-10-21T20:09:45Z", "pr_url": "https://github.com/DSpace/DSpace/pull/3020", "timeline": [{"oid": "dd9641a4b6d3f3b62bde386ed11ba8b11c75fdb8", "url": "https://github.com/DSpace/DSpace/commit/dd9641a4b6d3f3b62bde386ed11ba8b11c75fdb8", "message": "Minor cleanup. Comment correction. Remove commented out code", "committedDate": "2021-01-04T17:49:06Z", "type": "forcePushed"}, {"oid": "c75265e582ae9c7465a1bdd0720bf688d37cb471", "url": "https://github.com/DSpace/DSpace/commit/c75265e582ae9c7465a1bdd0720bf688d37cb471", "message": "Upgrade to Solr 8.7.0. Fix dependency convergence issues. Minor fix to MockSolrServer for Solr 8 compatibility", "committedDate": "2021-01-15T15:02:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY3ODE0Mw==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r568678143", "bodyText": "Log4j2's parameter substitution would reduce this expensive multiple-string-concatenation expression to a compile-time constant.", "author": "mwoodiupui", "createdAt": "2021-02-02T15:09:48Z", "path": "dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java", "diffHunk": "@@ -74,32 +79,59 @@ public void writeDocument(Context context, T indexableObject, SolrInputDocument\n      * Write the document to the index under the appropriate unique identifier.\n      *\n      * @param doc     the solr document to be written to the server\n-     * @param streams list of bitstream content streams    DiscoverQueryBuilderTest.java:285\n+     * @param streams list of bitstream content streams\n      * @throws IOException A general class of exceptions produced by failed or interrupted I/O operations.\n      */\n     protected void writeDocument(SolrInputDocument doc, FullTextContentStreams streams)\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n             if (streams != null && !streams.isEmpty()) {\n-                ContentStreamUpdateRequest req = new ContentStreamUpdateRequest(\"/update/extract\");\n-                req.addContentStream(streams);\n-\n-                ModifiableSolrParams params = new ModifiableSolrParams();\n+                // limit full text indexing to first 100,000 characters unless configured otherwise\n+                final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n+                                                           .getIntProperty(\"discovery.solr.fulltext.charLimit\",\n+                                                                           100000);\n+\n+                // Use Tika's Text parser as the streams are always from the TEXT bundle (i.e. already extracted text)\n+                // TODO: We may wish to consider using Tika to extract the text in the future.\n+                TextAndCSVParser tikaParser = new TextAndCSVParser();\n+                BodyContentHandler tikaHandler = new BodyContentHandler(charLimit);\n+                Metadata tikaMetadata = new Metadata();\n+                ParseContext tikaContext = new ParseContext();\n+\n+                // Use Apache Tika to parse the full text stream\n+                try {\n+                    tikaParser.parse(streams.getStream(), tikaHandler, tikaMetadata, tikaContext);\n+                } catch (SAXException saxe) {\n+                    // Check if this SAXException is just a notice that this file was longer than the character limit.\n+                    // Unfortunately there is not a unique, public exception type to catch here. This error is thrown\n+                    // by Tika's WriteOutContentHandler when it encounters a document longer than the char limit\n+                    // https://github.com/apache/tika/blob/main/tika-core/src/main/java/org/apache/tika/sax/WriteOutContentHandler.java\n+                    if (saxe.getMessage().contains(\"limit has been reached\")) {\n+                        // log that we only indexed up to that configured limit\n+                        log.info(\"Full text is larger than the configured limit (discovery.solr.fulltext.charLimit).\"\n+                                     + \" Only the first \" + charLimit + \" characters were indexed.\");", "originalCommit": "95d0cd18e500998f2ce67372d8e0edb66629d745", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "chunk": "diff --git a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\nindex c8ec93b4a..cb500148b 100644\n--- a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n+++ b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n\n@@ -86,6 +86,7 @@ public abstract class IndexFactoryImpl<T extends IndexableObject, S> implements\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n+            // If full text stream(s) were passed in, we'll index them as part of the SolrInputDocument\n             if (streams != null && !streams.isEmpty()) {\n                 // limit full text indexing to first 100,000 characters unless configured otherwise\n                 final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcwNDk5MQ==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r568704991", "bodyText": "Do we really want a hard commit after every document?  The usual advice from the Solr maintainers on committing in the client is \"don't.\"  If this method is ever used in bulk indexing, the bulk operation will be quite slow and make the top-level caches ineffective for all other users.  We should tune our autocommit settings for a good balance of indexing performance and visibility, and trust autocommit.", "author": "mwoodiupui", "createdAt": "2021-02-02T15:42:11Z", "path": "dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java", "diffHunk": "@@ -74,32 +79,59 @@ public void writeDocument(Context context, T indexableObject, SolrInputDocument\n      * Write the document to the index under the appropriate unique identifier.\n      *\n      * @param doc     the solr document to be written to the server\n-     * @param streams list of bitstream content streams    DiscoverQueryBuilderTest.java:285\n+     * @param streams list of bitstream content streams\n      * @throws IOException A general class of exceptions produced by failed or interrupted I/O operations.\n      */\n     protected void writeDocument(SolrInputDocument doc, FullTextContentStreams streams)\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n             if (streams != null && !streams.isEmpty()) {\n-                ContentStreamUpdateRequest req = new ContentStreamUpdateRequest(\"/update/extract\");\n-                req.addContentStream(streams);\n-\n-                ModifiableSolrParams params = new ModifiableSolrParams();\n+                // limit full text indexing to first 100,000 characters unless configured otherwise\n+                final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n+                                                           .getIntProperty(\"discovery.solr.fulltext.charLimit\",\n+                                                                           100000);\n+\n+                // Use Tika's Text parser as the streams are always from the TEXT bundle (i.e. already extracted text)\n+                // TODO: We may wish to consider using Tika to extract the text in the future.\n+                TextAndCSVParser tikaParser = new TextAndCSVParser();\n+                BodyContentHandler tikaHandler = new BodyContentHandler(charLimit);\n+                Metadata tikaMetadata = new Metadata();\n+                ParseContext tikaContext = new ParseContext();\n+\n+                // Use Apache Tika to parse the full text stream\n+                try {\n+                    tikaParser.parse(streams.getStream(), tikaHandler, tikaMetadata, tikaContext);\n+                } catch (SAXException saxe) {\n+                    // Check if this SAXException is just a notice that this file was longer than the character limit.\n+                    // Unfortunately there is not a unique, public exception type to catch here. This error is thrown\n+                    // by Tika's WriteOutContentHandler when it encounters a document longer than the char limit\n+                    // https://github.com/apache/tika/blob/main/tika-core/src/main/java/org/apache/tika/sax/WriteOutContentHandler.java\n+                    if (saxe.getMessage().contains(\"limit has been reached\")) {\n+                        // log that we only indexed up to that configured limit\n+                        log.info(\"Full text is larger than the configured limit (discovery.solr.fulltext.charLimit).\"\n+                                     + \" Only the first \" + charLimit + \" characters were indexed.\");\n+                    } else {\n+                        throw new IOException(\"Tika parsing error. Could not index full text.\", saxe);\n+                    }\n+                } catch (TikaException ex) {\n+                    throw new IOException(\"Tika parsing error. Could not index full text.\", ex);\n+                }\n \n-                //req.setParam(ExtractingParams.EXTRACT_ONLY, \"true\");\n-                for (String name : doc.getFieldNames()) {\n-                    for (Object val : doc.getFieldValues(name)) {\n-                        params.add(ExtractingParams.LITERALS_PREFIX + name, val.toString());\n+                // Write Tika metadata to \"tika_meta_*\" fields.\n+                // This metadata is not very useful right now, but we'll keep it just in case it becomes more useful.\n+                for (String name : tikaMetadata.names()) {\n+                    for (String value : tikaMetadata.getValues(name)) {\n+                        doc.addField(\"tika_meta_\" + name, value);\n                     }\n                 }\n \n-                req.setParams(params);\n-                req.setParam(ExtractingParams.UNKNOWN_FIELD_PREFIX, \"attr_\");\n-                req.setParam(ExtractingParams.MAP_PREFIX + \"content\", \"fulltext\");\n-                req.setParam(ExtractingParams.EXTRACT_FORMAT, \"text\");\n-                req.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n-                req.process(solr);\n+                // Save (parsed) full text to \"fulltext\" field\n+                doc.addField(\"fulltext\", tikaHandler.toString());\n+\n+                // Add document & commit immediately\n+                solr.add(doc);\n+                solr.commit(true, true);", "originalCommit": "95d0cd18e500998f2ce67372d8e0edb66629d745", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODgyODQ3Mg==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r568828472", "bodyText": "Good point.  To be honest, I debated on this one myself...I only added this in because the code it replaced was also doing a hard commit.  So, I did an exact replacement here.  That said, I'm OK with removing this in favor of autocommit.", "author": "tdonohue", "createdAt": "2021-02-02T18:20:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcwNDk5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "chunk": "diff --git a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\nindex c8ec93b4a..cb500148b 100644\n--- a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n+++ b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n\n@@ -86,6 +86,7 @@ public abstract class IndexFactoryImpl<T extends IndexableObject, S> implements\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n+            // If full text stream(s) were passed in, we'll index them as part of the SolrInputDocument\n             if (streams != null && !streams.isEmpty()) {\n                 // limit full text indexing to first 100,000 characters unless configured otherwise\n                 final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcwNzUzNQ==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r568707535", "bodyText": "Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir(), \"solr\");", "author": "mwoodiupui", "createdAt": "2021-02-02T15:45:17Z", "path": "dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java", "diffHunk": "@@ -159,9 +162,9 @@ public void destroy() throws Exception {\n      */\n     private static synchronized void initSolrContainer() {\n         if (container == null) {\n-            String solrDir = AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\";\n-            log.info(\"Initializing SOLR CoreContainer with directory \" + solrDir);\n-            container = new CoreContainer(solrDir);\n+            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\");", "originalCommit": "95d0cd18e500998f2ce67372d8e0edb66629d745", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "chunk": "diff --git a/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java b/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\nindex ba44d45e2..99fda8544 100644\n--- a/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\n+++ b/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\n\n@@ -162,8 +161,8 @@ public class MockSolrServer {\n      */\n     private static synchronized void initSolrContainer() {\n         if (container == null) {\n-            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\");\n-            log.info(\"Initializing SOLR CoreContainer with directory \" + solrDir.toAbsolutePath().toString());\n+            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir(), \"solr\");\n+            log.info(\"Initializing SOLR CoreContainer with directory {}\", solrDir.toAbsolutePath().toString());\n             container = new CoreContainer(solrDir, new Properties());\n             container.load();\n             log.info(\"SOLR CoreContainer initialized\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcwODIzNQ==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r568708235", "bodyText": "Parameter substitution here.\nAlso, another string concatenation in Paths.get.", "author": "mwoodiupui", "createdAt": "2021-02-02T15:46:07Z", "path": "dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java", "diffHunk": "@@ -159,9 +162,9 @@ public void destroy() throws Exception {\n      */\n     private static synchronized void initSolrContainer() {\n         if (container == null) {\n-            String solrDir = AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\";\n-            log.info(\"Initializing SOLR CoreContainer with directory \" + solrDir);\n-            container = new CoreContainer(solrDir);\n+            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\");\n+            log.info(\"Initializing SOLR CoreContainer with directory \" + solrDir.toAbsolutePath().toString());", "originalCommit": "95d0cd18e500998f2ce67372d8e0edb66629d745", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "chunk": "diff --git a/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java b/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\nindex ba44d45e2..99fda8544 100644\n--- a/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\n+++ b/dspace-api/src/test/java/org/dspace/solr/MockSolrServer.java\n\n@@ -162,8 +161,8 @@ public class MockSolrServer {\n      */\n     private static synchronized void initSolrContainer() {\n         if (container == null) {\n-            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir() + File.separator + \"solr\");\n-            log.info(\"Initializing SOLR CoreContainer with directory \" + solrDir.toAbsolutePath().toString());\n+            Path solrDir = Paths.get(AbstractDSpaceIntegrationTest.getDspaceDir(), \"solr\");\n+            log.info(\"Initializing SOLR CoreContainer with directory {}\", solrDir.toAbsolutePath().toString());\n             container = new CoreContainer(solrDir, new Properties());\n             container.load();\n             log.info(\"SOLR CoreContainer initialized\");\n"}}, {"oid": "2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "url": "https://github.com/DSpace/DSpace/commit/2ac127cb792c352c220b5dbfe986f749d6fa6e2d", "message": "Minor refactors based on feedback", "committedDate": "2021-02-02T20:59:32Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDU1MzYxNQ==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r570553615", "bodyText": "Do we need a finally{} clause to close streams?", "author": "mwoodiupui", "createdAt": "2021-02-04T21:25:25Z", "path": "dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java", "diffHunk": "@@ -74,35 +79,60 @@ public void writeDocument(Context context, T indexableObject, SolrInputDocument\n      * Write the document to the index under the appropriate unique identifier.\n      *\n      * @param doc     the solr document to be written to the server\n-     * @param streams list of bitstream content streams    DiscoverQueryBuilderTest.java:285\n+     * @param streams list of bitstream content streams\n      * @throws IOException A general class of exceptions produced by failed or interrupted I/O operations.\n      */\n     protected void writeDocument(SolrInputDocument doc, FullTextContentStreams streams)\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n+            // If full text stream(s) were passed in, we'll index them as part of the SolrInputDocument\n             if (streams != null && !streams.isEmpty()) {\n-                ContentStreamUpdateRequest req = new ContentStreamUpdateRequest(\"/update/extract\");\n-                req.addContentStream(streams);\n-\n-                ModifiableSolrParams params = new ModifiableSolrParams();\n+                // limit full text indexing to first 100,000 characters unless configured otherwise\n+                final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n+                                                           .getIntProperty(\"discovery.solr.fulltext.charLimit\",\n+                                                                           100000);\n+\n+                // Use Tika's Text parser as the streams are always from the TEXT bundle (i.e. already extracted text)\n+                // TODO: We may wish to consider using Tika to extract the text in the future.\n+                TextAndCSVParser tikaParser = new TextAndCSVParser();\n+                BodyContentHandler tikaHandler = new BodyContentHandler(charLimit);\n+                Metadata tikaMetadata = new Metadata();\n+                ParseContext tikaContext = new ParseContext();\n+\n+                // Use Apache Tika to parse the full text stream\n+                try {", "originalCommit": "954a0818930660d5144e97a066ec75bbb911faa5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDU4MTMzMw==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r570581333", "bodyText": "Good catch...I think it is missing.  It's not streams we need to close, but streams.getStream() which creates a new SequenceInputStream that does need to be closed.  https://github.com/DSpace/DSpace/blob/main/dspace-api/src/main/java/org/dspace/discovery/FullTextContentStreams.java#L131\nI had completely overlooked that, so I'm very glad you caught it!  I'll make sure this gets stored to a new variable & closed in a finally.", "author": "tdonohue", "createdAt": "2021-02-04T22:16:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDU1MzYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDU4MzYwMA==", "url": "https://github.com/DSpace/DSpace/pull/3020#discussion_r570583600", "bodyText": "Fixed now in 48f628f.  Thanks again @mwoodiupui !", "author": "tdonohue", "createdAt": "2021-02-04T22:20:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDU1MzYxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "635d3414695191f559a10c5276d7f09d10aa616f", "chunk": "diff --git a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\nindex cb500148b..2e4eb6772 100644\n--- a/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n+++ b/dspace-api/src/main/java/org/dspace/discovery/indexobject/IndexFactoryImpl.java\n\n@@ -79,60 +74,35 @@ public abstract class IndexFactoryImpl<T extends IndexableObject, S> implements\n      * Write the document to the index under the appropriate unique identifier.\n      *\n      * @param doc     the solr document to be written to the server\n-     * @param streams list of bitstream content streams\n+     * @param streams list of bitstream content streams    DiscoverQueryBuilderTest.java:285\n      * @throws IOException A general class of exceptions produced by failed or interrupted I/O operations.\n      */\n     protected void writeDocument(SolrInputDocument doc, FullTextContentStreams streams)\n             throws IOException, SolrServerException {\n         final SolrClient solr = solrSearchCore.getSolr();\n         if (solr != null) {\n-            // If full text stream(s) were passed in, we'll index them as part of the SolrInputDocument\n             if (streams != null && !streams.isEmpty()) {\n-                // limit full text indexing to first 100,000 characters unless configured otherwise\n-                final int charLimit = DSpaceServicesFactory.getInstance().getConfigurationService()\n-                                                           .getIntProperty(\"discovery.solr.fulltext.charLimit\",\n-                                                                           100000);\n-\n-                // Use Tika's Text parser as the streams are always from the TEXT bundle (i.e. already extracted text)\n-                // TODO: We may wish to consider using Tika to extract the text in the future.\n-                TextAndCSVParser tikaParser = new TextAndCSVParser();\n-                BodyContentHandler tikaHandler = new BodyContentHandler(charLimit);\n-                Metadata tikaMetadata = new Metadata();\n-                ParseContext tikaContext = new ParseContext();\n-\n-                // Use Apache Tika to parse the full text stream\n-                try {\n-                    tikaParser.parse(streams.getStream(), tikaHandler, tikaMetadata, tikaContext);\n-                } catch (SAXException saxe) {\n-                    // Check if this SAXException is just a notice that this file was longer than the character limit.\n-                    // Unfortunately there is not a unique, public exception type to catch here. This error is thrown\n-                    // by Tika's WriteOutContentHandler when it encounters a document longer than the char limit\n-                    // https://github.com/apache/tika/blob/main/tika-core/src/main/java/org/apache/tika/sax/WriteOutContentHandler.java\n-                    if (saxe.getMessage().contains(\"limit has been reached\")) {\n-                        // log that we only indexed up to that configured limit\n-                        log.info(\"Full text is larger than the configured limit (discovery.solr.fulltext.charLimit).\"\n-                                     + \" Only the first {} characters were indexed.\", charLimit);\n-                    } else {\n-                        throw new IOException(\"Tika parsing error. Could not index full text.\", saxe);\n-                    }\n-                } catch (TikaException ex) {\n-                    throw new IOException(\"Tika parsing error. Could not index full text.\", ex);\n-                }\n+                ContentStreamUpdateRequest req = new ContentStreamUpdateRequest(\"/update/extract\");\n+                req.addContentStream(streams);\n \n-                // Write Tika metadata to \"tika_meta_*\" fields.\n-                // This metadata is not very useful right now, but we'll keep it just in case it becomes more useful.\n-                for (String name : tikaMetadata.names()) {\n-                    for (String value : tikaMetadata.getValues(name)) {\n-                        doc.addField(\"tika_meta_\" + name, value);\n+                ModifiableSolrParams params = new ModifiableSolrParams();\n+\n+                //req.setParam(ExtractingParams.EXTRACT_ONLY, \"true\");\n+                for (String name : doc.getFieldNames()) {\n+                    for (Object val : doc.getFieldValues(name)) {\n+                        params.add(ExtractingParams.LITERALS_PREFIX + name, val.toString());\n                     }\n                 }\n \n-                // Save (parsed) full text to \"fulltext\" field\n-                doc.addField(\"fulltext\", tikaHandler.toString());\n+                req.setParams(params);\n+                req.setParam(ExtractingParams.UNKNOWN_FIELD_PREFIX, \"attr_\");\n+                req.setParam(ExtractingParams.MAP_PREFIX + \"content\", \"fulltext\");\n+                req.setParam(ExtractingParams.EXTRACT_FORMAT, \"text\");\n+                req.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);\n+                req.process(solr);\n+            } else {\n+                solr.add(doc);\n             }\n-\n-            // Add document to index\n-            solr.add(doc);\n         }\n     }\n \n"}}, {"oid": "635d3414695191f559a10c5276d7f09d10aa616f", "url": "https://github.com/DSpace/DSpace/commit/635d3414695191f559a10c5276d7f09d10aa616f", "message": "Upgrade to latest guava. Remove/replace outdated Lyncode builders & test-support which used old guava version.", "committedDate": "2021-02-25T16:36:02Z", "type": "commit"}, {"oid": "d22a70611c55435c4702ef52554ce30d7e114fc3", "url": "https://github.com/DSpace/DSpace/commit/d22a70611c55435c4702ef52554ce30d7e114fc3", "message": "Minor cleanup. Comment correction. Remove commented out code", "committedDate": "2021-02-25T16:36:02Z", "type": "commit"}, {"oid": "7cabcbabbca808cd22a169d03d2d3f641f6136fb", "url": "https://github.com/DSpace/DSpace/commit/7cabcbabbca808cd22a169d03d2d3f641f6136fb", "message": "Rename to RegistryUpdater so that it comes alphabetically *after* GroupServiceInitializer as Flyway runs callbacks alphabetically", "committedDate": "2021-02-25T16:36:02Z", "type": "commit"}, {"oid": "25b00647b9faf3451c38c949221ce3c2731805d9", "url": "https://github.com/DSpace/DSpace/commit/25b00647b9faf3451c38c949221ce3c2731805d9", "message": "Workaround https://issues.apache.org/jira/browse/SOLR-12858 by ensuring EmbeddedSolrServer always uses GET instead of POST", "committedDate": "2021-02-25T16:36:02Z", "type": "commit"}, {"oid": "022b79cb5e99e0a7872fca8f1c79ca24c178ef62", "url": "https://github.com/DSpace/DSpace/commit/022b79cb5e99e0a7872fca8f1c79ca24c178ef62", "message": "Upgrade to Solr 8.7.0. Fix dependency convergence issues. Minor fix to MockSolrServer for Solr 8 compatibility", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "44aa1025ba5c0941ea827b22b72f1596d149ed63", "url": "https://github.com/DSpace/DSpace/commit/44aa1025ba5c0941ea827b22b72f1596d149ed63", "message": "Update Docker for Solr v8", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "ee466114197dd94af7c77c0195caab094126e183", "url": "https://github.com/DSpace/DSpace/commit/ee466114197dd94af7c77c0195caab094126e183", "message": "Update solrconfig.xml files for Solr v8 (also minor alignment correction)", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "ffb17f0664557941c70ca41e2588ec7b1a4b0c57", "url": "https://github.com/DSpace/DSpace/commit/ffb17f0664557941c70ca41e2588ec7b1a4b0c57", "message": "Fix/Workaround for https://issues.apache.org/jira/browse/SOLR-12798 . Use Tika directly for parsing instead of ContentStreamUpdateRequest (which results in \"URI is too large >8192\" errors in Solr v8)", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "ea0d139c73cf6ae8fbcc86e0ad91e2f725217c78", "url": "https://github.com/DSpace/DSpace/commit/ea0d139c73cf6ae8fbcc86e0ad91e2f725217c78", "message": "Add explicit dependency on Apache Tika to avoid UnsatisfiedDependencyException warnings from Spring Boot during startup", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "891544a49becac227ac8ff52c33bb252d40940ba", "url": "https://github.com/DSpace/DSpace/commit/891544a49becac227ac8ff52c33bb252d40940ba", "message": "Docker bug fix. Ensure new Solr v8 data directories are kept between Docker restarts.", "committedDate": "2021-02-25T16:36:03Z", "type": "commit"}, {"oid": "90050c18fa2fbc2ae5bad8b92deaca799947a4d4", "url": "https://github.com/DSpace/DSpace/commit/90050c18fa2fbc2ae5bad8b92deaca799947a4d4", "message": "Minor refactors based on feedback", "committedDate": "2021-02-25T16:36:04Z", "type": "commit"}, {"oid": "e33b323135d7699743fa51873c05ef2fe5336904", "url": "https://github.com/DSpace/DSpace/commit/e33b323135d7699743fa51873c05ef2fe5336904", "message": "Remove our custom Docker container for Solr. It's unreliable with Solr v8, and all the examples show to use docker-compose directly with official image.", "committedDate": "2021-02-25T16:36:04Z", "type": "commit"}, {"oid": "781456fadb76917b48cfad52ab42e59759f81ce2", "url": "https://github.com/DSpace/DSpace/commit/781456fadb76917b48cfad52ab42e59759f81ce2", "message": "Upgrade Jetty to avoid security warnings from Snyk. Also fixes GitHub security alert", "committedDate": "2021-02-25T16:36:04Z", "type": "commit"}, {"oid": "4394d4bcca489c4c97e7dee6b4f406b36c427aea", "url": "https://github.com/DSpace/DSpace/commit/4394d4bcca489c4c97e7dee6b4f406b36c427aea", "message": "Ensure InputStream is closed", "committedDate": "2021-02-25T16:36:04Z", "type": "commit"}, {"oid": "4394d4bcca489c4c97e7dee6b4f406b36c427aea", "url": "https://github.com/DSpace/DSpace/commit/4394d4bcca489c4c97e7dee6b4f406b36c427aea", "message": "Ensure InputStream is closed", "committedDate": "2021-02-25T16:36:04Z", "type": "forcePushed"}]}