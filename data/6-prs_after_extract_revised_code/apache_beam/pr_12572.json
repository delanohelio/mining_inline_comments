{"pr_number": 12572, "pr_title": "[BEAM-10123] Add Kafka Commit transform.", "pr_createdAt": "2020-08-13T22:16:56Z", "pr_url": "https://github.com/apache/beam/pull/12572", "timeline": [{"oid": "47b4588e40495c00b10ec48d0ee2266efca513d2", "url": "https://github.com/apache/beam/commit/47b4588e40495c00b10ec48d0ee2266efca513d2", "message": "Add commit transform.", "committedDate": "2020-08-27T17:42:08Z", "type": "forcePushed"}, {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4", "url": "https://github.com/apache/beam/commit/1e91d6344a1b91505b581c2ecb80679848f057f4", "message": "Add commit transform.", "committedDate": "2020-09-11T18:05:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwMDA2Ng==", "url": "https://github.com/apache/beam/pull/12572#discussion_r496100066", "bodyText": "Why do we need Reshuffle here?", "author": "aromanenko-dev", "createdAt": "2020-09-28T16:59:34Z", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1620,14 +1635,43 @@ public void processElement(\n       CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n-      Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n-      PCollection<KafkaRecord<K, V>> output =\n-          input.apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this))).setCoder(outputCoder);\n-      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n-      if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n-        throw new IllegalStateException(\"Offset committed is not supported yet\");\n+      Coder<KafkaRecord<K, V>> recordCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+\n+      try {\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> outputWithDescriptor =\n+            input\n+                .apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this)))\n+                .setCoder(\n+                    KvCoder.of(\n+                        input\n+                            .getPipeline()\n+                            .getSchemaRegistry()\n+                            .getSchemaCoder(KafkaSourceDescriptor.class),\n+                        recordCoder));\n+        if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+          outputWithDescriptor =\n+              outputWithDescriptor\n+                  .apply(Reshuffle.viaRandomKey())", "originalCommit": "1e91d6344a1b91505b581c2ecb80679848f057f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzMTExOQ==", "url": "https://github.com/apache/beam/pull/12572#discussion_r497131119", "bodyText": "The Reshuffle is used as a persistent layer which help us to guarantee that we will not re-read records priori to the committed offset.", "author": "boyuanzz", "createdAt": "2020-09-29T23:19:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwMDA2Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NTQ5OA==", "url": "https://github.com/apache/beam/pull/12572#discussion_r496885498", "bodyText": "Is it a possible state that there are no bootstrap servers defined?", "author": "aromanenko-dev", "createdAt": "2020-09-29T16:41:17Z", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);", "originalCommit": "1e91d6344a1b91505b581c2ecb80679848f057f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1Mjc4Mg==", "url": "https://github.com/apache/beam/pull/12572#discussion_r497052782", "bodyText": "Yes, that's possible since we no longer force the user to provide bootstrap servers when constructing transforms. The bootstrap servers can also come from source descriptor.", "author": "boyuanzz", "createdAt": "2020-09-29T20:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NTQ5OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NzA3OA==", "url": "https://github.com/apache/beam/pull/12572#discussion_r496887078", "bodyText": "How many uniq keys per bundle are expected? Only one (because of Max.longsPerKey()) on previous step?", "author": "aromanenko-dev", "createdAt": "2020-09-29T16:43:50Z", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());", "originalCommit": "1e91d6344a1b91505b581c2ecb80679848f057f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA4NDUzMA==", "url": "https://github.com/apache/beam/pull/12572#discussion_r497084530", "bodyText": "How many uniq keys after Max.longsPerKey()  per bundle depends on the runner implementation. For dataflow specific, a bundle may contains many keys. The key I'm using here is a KafkaSourceDescriptor, which represents a unique Kafka connection(a topic + a partition + bootstrap servers).", "author": "boyuanzz", "createdAt": "2020-09-29T21:57:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NzA3OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA==", "url": "https://github.com/apache/beam/pull/12572#discussion_r496889334", "bodyText": "Could you elaborate a bit why Window is hardcoded to 5 mins?", "author": "aromanenko-dev", "createdAt": "2020-09-29T16:47:13Z", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);\n+      Map<String, Object> config = new HashMap<>(currentConfig);\n+      if (description.getBootStrapServers() != null\n+          && description.getBootStrapServers().size() > 0) {\n+        config.put(\n+            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n+            String.join(\",\", description.getBootStrapServers()));\n+      }\n+      return config;\n+    }\n+  }\n+\n+  @Override\n+  public PCollection<Void> expand(PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> input) {\n+    try {\n+      return input\n+          .apply(\n+              MapElements.into(new TypeDescriptor<KV<KafkaSourceDescriptor, Long>>() {})\n+                  .via(element -> KV.of(element.getKey(), element.getValue().getOffset())))\n+          .setCoder(\n+              KvCoder.of(\n+                  input\n+                      .getPipeline()\n+                      .getSchemaRegistry()\n+                      .getSchemaCoder(KafkaSourceDescriptor.class),\n+                  VarLongCoder.of()))\n+          .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))", "originalCommit": "1e91d6344a1b91505b581c2ecb80679848f057f4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyNTMxOQ==", "url": "https://github.com/apache/beam/pull/12572#discussion_r497125319", "bodyText": "That's my expectation of the time interval for committing. The reason for committing offset is to have a good start point when we restart the pipeline, so it not requires a real-time commtting. Do you have any suggestion on this time?", "author": "boyuanzz", "createdAt": "2020-09-29T23:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxNDU3OA==", "url": "https://github.com/apache/beam/pull/12572#discussion_r497414578", "bodyText": "Thanks. Not for now, let's keep it as \"it is\".  I'm just thinking if a user would need to be able to configure this or not.", "author": "aromanenko-dev", "createdAt": "2020-09-30T10:45:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "a087685d0cee3cf940cf819e254b52e4a0aa7ae5", "url": "https://github.com/apache/beam/commit/a087685d0cee3cf940cf819e254b52e4a0aa7ae5", "message": "Add commit transform.", "committedDate": "2020-10-30T19:03:10Z", "type": "forcePushed"}, {"oid": "c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "url": "https://github.com/apache/beam/commit/c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "message": "Add commit transform.", "committedDate": "2020-11-02T18:19:35Z", "type": "commit"}, {"oid": "c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "url": "https://github.com/apache/beam/commit/c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "message": "Add commit transform.", "committedDate": "2020-11-02T18:19:35Z", "type": "forcePushed"}]}