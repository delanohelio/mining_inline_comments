{"pr_number": 13619, "pr_title": "[BEAM-11527] Allow user defined Hadoop ReadSupport flags for ParquetReader", "pr_createdAt": "2020-12-28T09:12:37Z", "pr_url": "https://github.com/apache/beam/pull/13619", "timeline": [{"oid": "a7301b751791308965715df247657a8755e75b13", "url": "https://github.com/apache/beam/commit/a7301b751791308965715df247657a8755e75b13", "message": "[BEAM-11527] Add builder parameter to allow user defined Hadoop ReadSupport flags in Hadoop Configuration.", "committedDate": "2020-12-28T09:07:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTEzMA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345130", "bodyText": "Can you please remove this method and replace its uses with setConfiguration(makeHadoopConfiguration(...))", "author": "iemejia", "createdAt": "2020-12-28T13:18:40Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -311,6 +313,12 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder setHadoopConfigurationFlags(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -315,10 +315,6 @@ public class ParquetIO {\n \n       abstract Builder setConfiguration(SerializableConfiguration configuration);\n \n-      Builder setHadoopConfigurationFlags(Map<String, String> flags) {\n-        return setConfiguration(makeHadoopConfigurationUsingFlags(flags));\n-      }\n-\n       abstract Read build();\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTg4OA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345888", "bodyText": "Please remove all definitions of this method and replace its uses with setConfiguration(makeHadoopConfiguration(...)) in all classes where it appears", "author": "iemejia", "createdAt": "2020-12-28T13:21:12Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -388,6 +402,12 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder<T> setParseFn(SerializableFunction<GenericRecord, T> parseFn);\n \n+      abstract Builder<T> setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder<T> setHadoopConfigurationFlags(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -404,10 +401,6 @@ public class ParquetIO {\n \n       abstract Builder<T> setConfiguration(SerializableConfiguration configuration);\n \n-      Builder<T> setHadoopConfigurationFlags(Map<String, String> flags) {\n-        return setConfiguration(makeHadoopConfigurationUsingFlags(flags));\n-      }\n-\n       abstract Builder<T> setSplittable(boolean splittable);\n \n       abstract Parse<T> build();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NjcxOA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346718", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:24:07Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -623,7 +609,7 @@ public class ParquetIO {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n-      private final SerializableConfiguration hadoopBaseConfig;\n+      private @Nullable final SerializableConfiguration configuration;\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0Njc2Mg==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346762", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:24:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;\n+\n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n       SplitReadFn(\n-          GenericData model, Schema requestSchema, SerializableFunction<GenericRecord, T> parseFn) {\n+          GenericData model,\n+          Schema requestSchema,\n+          SerializableFunction<GenericRecord, T> parseFn,\n+          SerializableConfiguration hadoopBaseConfig) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -623,7 +609,7 @@ public class ParquetIO {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n-      private final SerializableConfiguration hadoopBaseConfig;\n+      private @Nullable final SerializableConfiguration configuration;\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzA1MQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347051", "bodyText": "rename to configuration", "author": "iemejia", "createdAt": "2020-12-28T13:25:13Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -819,9 +884,15 @@ public Progress getProgress() {\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n-      ReadFn(GenericData model, SerializableFunction<GenericRecord, T> parseFn) {\n+      private final SerializableConfiguration hadoopBaseConfig;", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -884,15 +870,15 @@ public class ParquetIO {\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n-      private final SerializableConfiguration hadoopBaseConfig;\n+      private final SerializableConfiguration configuration;\n \n       ReadFn(\n           GenericData model,\n           SerializableFunction<GenericRecord, T> parseFn,\n-          SerializableConfiguration hadoopBaseConfig) {\n+          SerializableConfiguration configuration) {\n         this.modelClass = model != null ? model.getClass() : null;\n         this.parseFn = checkNotNull(parseFn, \"GenericRecord parse function is null\");\n-        this.hadoopBaseConfig = hadoopBaseConfig;\n+        this.configuration = configuration;\n       }\n \n       @ProcessElement\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzUwNQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347505", "bodyText": "\ud83d\udc4d", "author": "iemejia", "createdAt": "2020-12-28T13:26:33Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -920,13 +996,7 @@ public Sink withCompressionCodec(CompressionCodecName compressionCodecName) {\n \n     /** Specifies configuration to be passed into the sink's writer. */\n     public Sink withConfiguration(Map<String, String> configuration) {\n-      Configuration hadoopConfiguration = new Configuration();\n-      for (Map.Entry<String, String> entry : configuration.entrySet()) {\n-        hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-      }\n-      return toBuilder()\n-          .setConfiguration(new SerializableConfiguration(hadoopConfiguration))\n-          .build();\n+      return toBuilder().setConfiguration(makeHadoopConfigurationUsingFlags(configuration)).build();", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -996,7 +981,7 @@ public class ParquetIO {\n \n     /** Specifies configuration to be passed into the sink's writer. */\n     public Sink withConfiguration(Map<String, String> configuration) {\n-      return toBuilder().setConfiguration(makeHadoopConfigurationUsingFlags(configuration)).build();\n+      return toBuilder().setConfiguration(SerializableConfiguration.fromMap(configuration)).build();\n     }\n \n     private transient @Nullable ParquetWriter<GenericRecord> writer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349509", "bodyText": "Can we move this method into the SerializableConfiguration class and make it public static SerializableConfiguration fromMap(Map<String, string> entries) {", "author": "iemejia", "createdAt": "2020-12-28T13:33:18Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n+  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTgzMw==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561833", "bodyText": "Done, added a single test for the new method.\nInitially I was contemplating this alternative but had discarded to reduce touching more files.", "author": "anantdamle", "createdAt": "2020-12-29T04:15:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -1128,18 +1109,6 @@ public class ParquetIO {\n     private GenericRecordPassthroughFn() {}\n   }\n \n-  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n-  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(\n-      Map<String, String> configFlags) {\n-    Configuration hadoopConfiguration = new Configuration();\n-\n-    for (Map.Entry<String, String> entry : configFlags.entrySet()) {\n-      hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-    }\n-\n-    return new SerializableConfiguration(hadoopConfiguration);\n-  }\n-\n   /** Disallow construction of utility class. */\n   private ParquetIO() {}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349655", "bodyText": "test or remove", "author": "iemejia", "createdAt": "2020-12-28T13:33:48Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -416,6 +416,9 @@ public void testWriteAndReadwithSplitUsingReflectDataSchemaWithDataModel() {\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testConfigurationReadFile() {}", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIzNQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562235", "bodyText": "Thanks, Removed.", "author": "anantdamle", "createdAt": "2020-12-29T04:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java b/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java\nindex 6d2c0adfe6..de3abf8147 100644\n--- a/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java\n+++ b/sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java\n\n@@ -416,9 +416,6 @@ public class ParquetIOTest implements Serializable {\n     readPipeline.run().waitUntilFinish();\n   }\n \n-  @Test\n-  public void testConfigurationReadFile() {}\n-\n   /** Returns list of JSON representation of GenericRecords. */\n   private static List<String> convertRecordsToJson(List<GenericRecord> records) {\n     return records.stream().map(ParseGenericRecordAsJsonFn.create()::apply).collect(toList());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MDE0Ng==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549350146", "bodyText": "can you please name the argument of the withConfiguration methods consistently everywhere as configuration instead of flags or hadoopConfigFlags", "author": "iemejia", "createdAt": "2020-12-28T13:35:23Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -332,6 +340,10 @@ public Read withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .build();\n     }\n \n+    public Read withConfiguration(Map<String, String> flags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -340,8 +336,9 @@ public class ParquetIO {\n           .build();\n     }\n \n-    public Read withConfiguration(Map<String, String> flags) {\n-      return toBuilder().setHadoopConfigurationFlags(flags).build();\n+    /** Specify Hadoop configuration for ParquetReader. */\n+    public Read withConfiguration(Map<String, String> configuration) {\n+      return toBuilder().setConfiguration(SerializableConfiguration.fromMap(configuration)).build();\n     }\n \n     /** Enable the Splittable reading. */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MTE2MA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549351160", "bodyText": "Rename to withConfiguration to be consistent with the other methods + s/configurationFlags/configuration", "author": "iemejia", "createdAt": "2020-12-28T13:38:39Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -532,6 +581,12 @@ public ReadFiles withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .setSplittable(true)\n           .build();\n     }\n+\n+    /** Specify Hadoop configuration for ParquetReader. */\n+    public ReadFiles withHadoopConfiguration(Map<String, String> configurationFlags) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -583,8 +569,8 @@ public class ParquetIO {\n     }\n \n     /** Specify Hadoop configuration for ParquetReader. */\n-    public ReadFiles withHadoopConfiguration(Map<String, String> configurationFlags) {\n-      return toBuilder().setHadoopConfigurationFlags(configurationFlags).build();\n+    public ReadFiles withConfiguration(Map<String, String> configuration) {\n+      return toBuilder().setConfiguration(SerializableConfiguration.fromMap(configuration)).build();\n     }\n \n     /** Enable the Splittable reading. */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352679", "bodyText": "We should probably define a default value inside of the builders (read, readFiles, parseGenericRecords, parseFilesGenericRecords)  .setConfiguration(...) and since we define a default value we won't need this if", "author": "iemejia", "createdAt": "2020-12-28T13:43:46Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -835,13 +906,18 @@ public void processElement(ProcessContext processContext) throws Exception {\n \n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n-        AvroParquetReader.Builder builder =\n-            AvroParquetReader.<GenericRecord>builder(new BeamParquetInputFile(seekableByteChannel));\n+        AvroParquetReader.Builder<GenericRecord> builder =\n+            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n+        if (hadoopBaseConfig != null) {", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTY2MA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561660", "bodyText": "looking at the SerializableConfiguration#91 , It seems null is the expected value for building a default configuration.\nI've also replaced all configuration != null checks with SerializableConfiguration.newConfiguration(configuration) for consistency and to avoid NPE. What do you think?", "author": "anantdamle", "createdAt": "2020-12-29T04:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -907,17 +893,16 @@ public class ParquetIO {\n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n         AvroParquetReader.Builder<GenericRecord> builder =\n-            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n+            (AvroParquetReader.Builder<GenericRecord>)\n+                AvroParquetReader.<GenericRecord>builder(\n+                        new BeamParquetInputFile(seekableByteChannel))\n+                    .withConf(SerializableConfiguration.newConfiguration(configuration));\n+\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n-        if (hadoopBaseConfig != null) {\n-          builder =\n-              (AvroParquetReader.Builder<GenericRecord>) builder.withConf(hadoopBaseConfig.get());\n-        }\n-\n         try (ParquetReader<GenericRecord> reader = builder.build()) {\n           GenericRecord read;\n           while ((read = reader.read()) != null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjkwOA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352908", "bodyText": "s/Hadoop {@link Configuration}/{@link SerializableConfiguration}", "author": "iemejia", "createdAt": "2020-12-28T13:44:33Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -1128,18 +1109,6 @@ public class ParquetIO {\n     private GenericRecordPassthroughFn() {}\n   }\n \n-  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n-  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(\n-      Map<String, String> configFlags) {\n-    Configuration hadoopConfiguration = new Configuration();\n-\n-    for (Map.Entry<String, String> entry : configFlags.entrySet()) {\n-      hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-    }\n-\n-    return new SerializableConfiguration(hadoopConfiguration);\n-  }\n-\n   /** Disallow construction of utility class. */\n   private ParquetIO() {}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356033", "bodyText": "Test with new Configuration(), this should not be nullable", "author": "iemejia", "createdAt": "2020-12-28T13:54:44Z", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIwOQ==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562209", "bodyText": "As I'm using SerializableConfiguration#newConfiguration it can be null.\nDo you want to me to add a test where a non-null configuration is tested?", "author": "anantdamle", "createdAt": "2020-12-29T04:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4ODcwNA==", "url": "https://github.com/apache/beam/pull/13619#discussion_r550488704", "bodyText": "I would prefer it to not be Nullable but since this is internal I suppose we can adjust this later, on the other hand if someday Parquet finally gets rid of its Hadoop dependencies probably the null value would align better.", "author": "iemejia", "createdAt": "2020-12-31T13:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjkwNg==", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356906", "bodyText": "\ud83d\udc4d", "author": "iemejia", "createdAt": "2020-12-28T13:57:21Z", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -682,7 +747,7 @@ public void processElement(\n       }\n \n       public Configuration getConfWithModelClass() throws Exception {\n-        Configuration conf = new Configuration();\n+        Configuration conf = SerializableConfiguration.newConfiguration(hadoopBaseConfig);", "originalCommit": "a7301b751791308965715df247657a8755e75b13", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff5a094be93d41e14391600b071bdcb1369391bb", "chunk": "diff --git a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\nindex 1e6bce0be9..681a326e4b 100644\n--- a/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n+++ b/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\n@@ -747,7 +733,7 @@ public class ParquetIO {\n       }\n \n       public Configuration getConfWithModelClass() throws Exception {\n-        Configuration conf = SerializableConfiguration.newConfiguration(hadoopBaseConfig);\n+        Configuration conf = SerializableConfiguration.newConfiguration(configuration);\n         GenericData model = null;\n         if (modelClass != null) {\n           model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n"}}, {"oid": "ff5a094be93d41e14391600b071bdcb1369391bb", "url": "https://github.com/apache/beam/commit/ff5a094be93d41e14391600b071bdcb1369391bb", "message": "Consistency improvements and other fixes", "committedDate": "2020-12-29T04:08:25Z", "type": "commit"}]}