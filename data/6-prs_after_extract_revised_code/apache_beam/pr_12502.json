{"pr_number": 12502, "pr_title": "[BEAM-9891] Added ZetaSQL planner support and uploaded 100G data", "pr_createdAt": "2020-08-07T19:45:45Z", "pr_url": "https://github.com/apache/beam/pull/12502", "timeline": [{"oid": "e10793fd56e8aca40bfe6ad668c4c9158b2f07c5", "url": "https://github.com/apache/beam/commit/e10793fd56e8aca40bfe6ad668c4c9158b2f07c5", "message": "[BEAM-9891] Added ZetaSQL planner support", "committedDate": "2020-08-07T02:40:28Z", "type": "commit"}, {"oid": "5b4dd8de79b40369b0795a9272b274f0b58a46bb", "url": "https://github.com/apache/beam/commit/5b4dd8de79b40369b0795a9272b274f0b58a46bb", "message": "[BEAM-9891] Added ZetaSQL planner support and uploaded 100G data", "committedDate": "2020-08-07T19:34:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI0MTU3Ng==", "url": "https://github.com/apache/beam/pull/12502#discussion_r467241576", "bodyText": "Say there are 3 threads in pool, and 10 queries are queued.\nThen 3 threads pick up 3 queries, and 7 queries queued.\nAt this moment, if executor.shutdown(); is called, will that 7 queries still be executed, or they are cancelled.", "author": "amaliujia", "createdAt": "2020-08-07T19:57:09Z", "path": "sdks/java/testing/tpcds/src/main/java/org/apache/beam/sdk/tpcds/SqlTransformRunner.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.tpcds;\n+\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.extensions.sql.SqlTransform;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamSqlPipelineOptions;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.text.TextTable;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.*;\n+import org.apache.commons.csv.CSVFormat;\n+import java.util.Map;\n+import java.util.concurrent.CompletionService;\n+import java.util.concurrent.ExecutorCompletionService;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+/**\n+ * This class executes jobs using PCollection and SqlTransform, it uses SqlTransform.query to run queries.\n+ */\n+public class SqlTransformRunner {\n+    private static final String dataDirectory = \"gs://beamsql_tpcds_1/data\";\n+    private static final String resultDirectory = \"gs://beamsql_tpcds_1/tpcds_results\";\n+\n+    /**\n+     * Get all tables (in the form of TextTable) needed for a specific query execution\n+     * @param pipeline The pipeline that will be run to execute the query\n+     * @param csvFormat The csvFormat to construct readConverter (CsvToRow) and writeConverter (RowToCsv)\n+     * @param queryName The name of the query which will be executed (for example: query3, query55, query96)\n+     * @return A PCollectionTuple which is constructed by all tables needed for running query.\n+     * @throws Exception\n+     */\n+    private static PCollectionTuple getTables(Pipeline pipeline, CSVFormat csvFormat, String queryName) throws Exception {\n+        Map<String, Schema> schemaMap = TpcdsSchemas.getTpcdsSchemas();\n+        TpcdsOptions tpcdsOptions = pipeline.getOptions().as(TpcdsOptions.class);\n+        String dataSize = TpcdsParametersReader.getAndCheckDataSize(tpcdsOptions);\n+        String queryString = QueryReader.readQuery(queryName);\n+\n+        PCollectionTuple tables = PCollectionTuple.empty(pipeline);\n+        for (Map.Entry<String, Schema> tableSchema : schemaMap.entrySet()) {\n+            String tableName = tableSchema.getKey();\n+\n+            // Only when queryString contains tableName, the table is relevant to this query and will be added. This can avoid reading unnecessary data files.\n+            if (queryString.contains(tableName)) {\n+                // This is location path where the data are stored\n+                String filePattern = dataDirectory + \"/\" + dataSize + \"/\" + tableName + \".dat\";\n+\n+                PCollection<Row> table =\n+                        new TextTable(\n+                                tableSchema.getValue(),\n+                                filePattern,\n+                                new CsvToRow(tableSchema.getValue(), csvFormat),\n+                                new RowToCsv(csvFormat))\n+                                .buildIOReader(pipeline.begin())\n+                                .setCoder(SchemaCoder.of(tableSchema.getValue()))\n+                                .setName(tableSchema.getKey());\n+\n+                tables = tables.and(new TupleTag<>(tableName), table);\n+            }\n+        }\n+        return tables;\n+    }\n+\n+    /**\n+     * This is the default method in BeamTpcds.main method. Run job using SqlTranform.query() method.\n+     * @param args Command line arguments\n+     * @throws Exception\n+     */\n+    public static void runUsingSqlTransform(String[] args) throws Exception {\n+        TpcdsOptions tpcdsOptions = PipelineOptionsFactory.fromArgs(args).withValidation().as(TpcdsOptions.class);\n+\n+        String dataSize = TpcdsParametersReader.getAndCheckDataSize(tpcdsOptions);\n+        String[] queryNameArr = TpcdsParametersReader.getAndCheckQueryNameArray(tpcdsOptions);\n+        int nThreads = TpcdsParametersReader.getAndCheckTpcParallel(tpcdsOptions);\n+\n+        // Using ExecutorService and CompletionService to fulfill multi-threading functionality\n+        ExecutorService executor = Executors.newFixedThreadPool(nThreads);\n+        CompletionService<PipelineResult> completion = new ExecutorCompletionService<>(executor);\n+\n+        // Make an array of pipelines, each pipeline is responsible for running a corresponding query.\n+        Pipeline[] pipelines = new Pipeline[queryNameArr.length];\n+        CSVFormat csvFormat = CSVFormat.MYSQL.withDelimiter('|').withNullString(\"\");\n+\n+        // Execute all queries, transform the each result into a PCollection<String>, write them into the txt file and store in a GCP directory.\n+        for (int i = 0; i < queryNameArr.length; i++) {\n+            // For each query, get a copy of pipelineOptions from command line arguments.\n+            TpcdsOptions tpcdsOptionsCopy = PipelineOptionsFactory.fromArgs(args).withValidation().as(TpcdsOptions.class);\n+\n+            // Cast tpcdsOptions as a BeamSqlPipelineOptions object to read and set queryPlanner (the default one is Calcite, can change to ZetaSQL).\n+            BeamSqlPipelineOptions beamSqlPipelineOptionsCopy = tpcdsOptionsCopy.as(BeamSqlPipelineOptions.class);\n+\n+            // Finally, cast BeamSqlPipelineOptions as a DataflowPipelineOptions object to read and set other required pipeline optionsparameters .\n+            DataflowPipelineOptions dataflowPipelineOptionsCopy = beamSqlPipelineOptionsCopy.as(DataflowPipelineOptions.class);\n+\n+            // Set a unique job name using the time stamp so that multiple different pipelines can run together.\n+            dataflowPipelineOptionsCopy.setJobName(queryNameArr[i] + \"result\" + System.currentTimeMillis());\n+\n+            pipelines[i] = Pipeline.create(dataflowPipelineOptionsCopy);\n+            String queryString = QueryReader.readQuery(queryNameArr[i]);\n+            PCollectionTuple tables = getTables(pipelines[i], csvFormat, queryNameArr[i]);\n+\n+            tables\n+                    .apply(\n+                            SqlTransform.query(queryString))\n+                    .apply(\n+                            MapElements.into(TypeDescriptors.strings()).via((Row row) -> row.toString()))\n+                    .apply(TextIO.write()\n+                            .to(resultDirectory + \"/\" + dataSize + \"/\" + pipelines[i].getOptions().getJobName())\n+                            .withSuffix(\".txt\")\n+                            .withNumShards(1));\n+\n+            completion.submit(new TpcdsRun(pipelines[i]));\n+        }\n+\n+        executor.shutdown();", "originalCommit": "5b4dd8de79b40369b0795a9272b274f0b58a46bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI2Njc1NA==", "url": "https://github.com/apache/beam/pull/12502#discussion_r467266754", "bodyText": "They will be executed, The shutdown() method will allow previously submitted tasks to execute before terminating.\nThis method is the same as previously merged code, I did some tests again and they worked well.", "author": "Imfuyuwei", "createdAt": "2020-08-07T21:00:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI0MTU3Ng=="}], "type": "inlineReview", "revised_code": null}, {"oid": "c0234adb1f3ee1c3a981bc0861477159ad21bf5c", "url": "https://github.com/apache/beam/commit/c0234adb1f3ee1c3a981bc0861477159ad21bf5c", "message": "Added instruction comment to run query96 using ZetaSQL", "committedDate": "2020-08-07T21:00:18Z", "type": "commit"}]}