{"pr_number": 12201, "pr_title": "CDAP-16709 batch spark auto-join implementation", "pr_createdAt": "2020-05-20T23:12:28Z", "pr_url": "https://github.com/cdapio/cdap/pull/12201", "timeline": [{"oid": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "url": "https://github.com/cdapio/cdap/commit/57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-21T21:47:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NDQ3MQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430674471", "bodyText": "So the submitterPlugin can be null after the following ifs checks? Is it being handled already? I don't think the logic can this value being null. It is probably better to have a else that throw if the joiner plugin is not of one of the supported classes.", "author": "chtyim", "createdAt": "2020-05-26T20:00:52Z", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java", "diffHunk": "@@ -89,7 +90,7 @@ public PipelinePhasePreparer(PluginContext pluginContext, Metrics metrics, Macro\n       boolean isConnectorSink =\n         Constants.Connector.PLUGIN_TYPE.equals(pluginType) && phase.getSinks().contains(stageName);\n \n-      SubmitterPlugin submitterPlugin;\n+      SubmitterPlugin submitterPlugin = null;", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java b/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java\nindex f247b557353..2be0770c0ab 100644\n--- a/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java\n+++ b/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java\n\n@@ -90,7 +90,7 @@ public abstract class PipelinePhasePreparer {\n       boolean isConnectorSink =\n         Constants.Connector.PLUGIN_TYPE.equals(pluginType) && phase.getSinks().contains(stageName);\n \n-      SubmitterPlugin submitterPlugin = null;\n+      SubmitterPlugin submitterPlugin;\n       if (BatchSource.PLUGIN_TYPE.equals(pluginType) || isConnectorSource) {\n         BatchConfigurable<BatchSourceContext> batchSource =\n           pluginInstantiator.newPluginInstance(stageName, macroEvaluator);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NjMwNA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430676304", "bodyText": "Use method reference .filter(JoinStage::isRequired)", "author": "chtyim", "createdAt": "2020-05-26T20:04:30Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3OTM5MA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430679390", "bodyText": "This list can be obtained by sorting the list directly.\nList<JoinStage> orderedStages = new ArrayList<>(joinDefinition.getStages())\norderedStages.sort((s1, s2) -> s1.isRequired() ? (s2.isRequired() ? 0 : -1) : s2.isRequired ? 1 : 0));", "author": "chtyim", "createdAt": "2020-05-26T20:10:41Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MDg5OA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430680898", "bodyText": "Use method reference instead of lambda toMap(JoinKey::getStageName, JoinKey::getFields)", "author": "chtyim", "createdAt": "2020-05-26T20:13:31Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);\n+\n+    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    JoinStage left = stageIter.next();\n+    SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n+    Schema leftSchema = left.getSchema();\n+\n+    JoinCondition condition = joinDefinition.getCondition();\n+    // currently this is the only operation, but check this for future changes\n+    if (condition.getOp() != JoinCondition.Op.KEY_EQUALITY) {\n+      throw new IllegalStateException(\"Unsupport join condition operation \" + condition.getOp());\n+    }\n+    JoinCondition.OnKeys onKeys = (JoinCondition.OnKeys) condition;\n+    Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n+      .collect(Collectors.toMap(j -> j.getStageName(), j -> j.getFields()));", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NjgzMw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430676833", "bodyText": "nit - extra line", "author": "yaojiefeng", "createdAt": "2020-05-26T20:05:39Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);\n+\n+    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    JoinStage left = stageIter.next();\n+    SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n+    Schema leftSchema = left.getSchema();\n+\n+    JoinCondition condition = joinDefinition.getCondition();\n+    // currently this is the only operation, but check this for future changes\n+    if (condition.getOp() != JoinCondition.Op.KEY_EQUALITY) {\n+      throw new IllegalStateException(\"Unsupport join condition operation \" + condition.getOp());\n+    }\n+    JoinCondition.OnKeys onKeys = (JoinCondition.OnKeys) condition;\n+    Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n+      .collect(Collectors.toMap(j -> j.getStageName(), j -> j.getFields()));\n+", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NzE3Mw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430677173", "bodyText": "This method and the handleJoin method is non-trivial logic, it is better to add some javadoc about the logic", "author": "yaojiefeng", "createdAt": "2020-05-26T20:06:18Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430681752", "bodyText": "Can you explain a bit why we want an ordered stages? From the logic below, we are basically going through the list one by one, and then determine the join type based on left is required or right is required. Why do we want to loop through required stages first?", "author": "yaojiefeng", "createdAt": "2020-05-26T20:15:06Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNTY5OA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430715698", "bodyText": "there's a comment above but I can expand on it. If there is an outer join and an inner join, it is more performant to do the inner join first because it will generate less intermediate data.", "author": "albertshau", "createdAt": "2020-05-26T21:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczODEyNQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430738125", "bodyText": "though now that I think more about it, it may be better not to do this, and just follow the order provided by the plugin. Can add this back in if it helps once we experiment a bit more on uneven joins.", "author": "albertshau", "createdAt": "2020-05-26T22:17:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 5ee912d54a8..f3277abc14f 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -356,22 +356,15 @@ public abstract class SparkPipelineRunner {\n     }\n   }\n \n+  /**\n+   * The purpose of this method is to collect various pieces of information together into a JoinRequest.\n+   * This amounts to gathering the SparkCollection, schema, join key, and join type for each stage involved in the join.\n+   */\n   private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n                                                  Map<String, SparkCollection<Object>> inputDataCollections) {\n     JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n \n-    // join required stages first to cut down the data as much as possible\n-    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n-      .filter(s -> s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n-      .filter(s -> !s.isRequired())\n-      .collect(Collectors.toList());\n-    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n-    orderedStages.addAll(requiredStages);\n-    orderedStages.addAll(optionalStages);\n-\n-    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    Iterator<JoinStage> stageIter = joinDefinition.getStages().iterator();\n     JoinStage left = stageIter.next();\n     SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n     Schema leftSchema = left.getSchema();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MjI4MA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430682280", "bodyText": "Is this used anywhere? The getter doesn't seem to get called. Also it is good to add some comment on what this means.", "author": "yaojiefeng", "createdAt": "2020-05-26T20:16:09Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.join;\n+\n+import io.cdap.cdap.api.data.schema.Schema;\n+import io.cdap.cdap.etl.spark.SparkCollection;\n+\n+import java.util.List;\n+\n+/**\n+ * Data to join.\n+ */\n+public class JoinCollection {\n+  private final String stage;\n+  private final String type;\n+  private final SparkCollection<?> data;\n+  private final Schema schema;\n+  private final List<String> key;\n+  private final boolean broadcast;", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTk5Mw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430741993", "bodyText": "not currently, was going to implement it later.", "author": "albertshau", "createdAt": "2020-05-26T22:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MjI4MA=="}], "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\nindex b0f5aeb83cd..f0157a3ff27 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\n\n@@ -26,19 +26,19 @@ import java.util.List;\n  */\n public class JoinCollection {\n   private final String stage;\n-  private final String type;\n   private final SparkCollection<?> data;\n   private final Schema schema;\n   private final List<String> key;\n+  private final boolean required;\n   private final boolean broadcast;\n \n-  public JoinCollection(String stage, String type, SparkCollection<?> data, Schema schema,\n-                        List<String> key, boolean broadcast) {\n+  public JoinCollection(String stage, SparkCollection<?> data, Schema schema,\n+                        List<String> key, boolean required, boolean broadcast) {\n     this.stage = stage;\n-    this.type = type;\n     this.data = data;\n     this.schema = schema;\n     this.key = key;\n+    this.required = required;\n     this.broadcast = broadcast;\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NTgxMQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430685811", "bodyText": "I feel it is good to add some comment on why we need different implementations for spark 1 and 2. Sometimes it takes some time for me to understand the difference. Some javadoc will help understand the compatibility or implementation difference between spark 1 and 2", "author": "yaojiefeng", "createdAt": "2020-05-26T20:22:54Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.batch;\n+\n+import io.cdap.cdap.api.data.DatasetContext;\n+import io.cdap.cdap.api.data.format.StructuredRecord;\n+import io.cdap.cdap.api.data.schema.Schema;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.api.spark.sql.DataFrames;\n+import io.cdap.cdap.etl.api.join.JoinField;\n+import io.cdap.cdap.etl.spark.SparkCollection;\n+import io.cdap.cdap.etl.spark.join.JoinCollection;\n+import io.cdap.cdap.etl.spark.join.JoinRequest;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.types.StructType;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Spark2 RDD collection.\n+ *\n+ * @param <T> type of object in the collection\n+ */\n+public class RDDCollection<T> extends BaseRDDCollection<T> {", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java b/cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java\nindex a495ea9fba3..0d7f168e936 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java\n\n@@ -67,6 +67,23 @@ public class RDDCollection<T> extends BaseRDDCollection<T> {\n       .map(left::col)\n       .collect(Collectors.toList());\n \n+    /*\n+        This flag keeps track of whether there is at least one required stage in the join.\n+        This is needed in case there is a join like:\n+\n+        A (optional), B (required), C (optional), D (required)\n+\n+        The correct thing to do here is:\n+\n+        1. A right outer join B as TMP1\n+        2. TMP1 left outer join C as TMP2\n+        3. TMP2 inner join D\n+\n+        Join #1 is a straightforward join between 2 sides.\n+        Join #2 is a left outer because TMP1 becomes 'required', since it uses required input B.\n+        Join #3 is an inner join because even though it contains 2 optional datasets, because 'B' is still required.\n+     */\n+    boolean seenRequired = joinRequest.isLeftRequired();\n     Dataset<Row> joined = left;\n     for (JoinCollection toJoin : joinRequest.getToJoin()) {\n       RDDCollection<StructuredRecord> data = (RDDCollection<StructuredRecord>) toJoin.getData();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NDk1NQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430754955", "bodyText": "is a just -> is just", "author": "yaojiefeng", "createdAt": "2020-05-26T23:05:49Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java", "diffHunk": "@@ -54,29 +54,35 @@\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.FlatMapFunction;\n import org.apache.spark.api.java.function.PairFlatMapFunction;\n+import org.apache.spark.sql.SQLContext;\n import org.apache.spark.storage.StorageLevel;\n import scala.Tuple2;\n \n import javax.annotation.Nullable;\n \n \n /**\n- * Implementation of {@link SparkCollection} that is backed by a JavaRDD.\n+ * Implementation of {@link SparkCollection} that is backed by a JavaRDD. Spark1 and Spark2 implementations need to be\n+ * separate because DataFrames are not compatible between Spark1 and Spark2. In Spark2, DataFrame is a just a", "originalCommit": "b7f16f4f3acbd1b1437b84e6e4841a902744da55", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java\nindex 0c917252ea8..1afda273a70 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java\n\n@@ -63,7 +63,7 @@ import javax.annotation.Nullable;\n \n /**\n  * Implementation of {@link SparkCollection} that is backed by a JavaRDD. Spark1 and Spark2 implementations need to be\n- * separate because DataFrames are not compatible between Spark1 and Spark2. In Spark2, DataFrame is a just a\n+ * separate because DataFrames are not compatible between Spark1 and Spark2. In Spark2, DataFrame is just a\n  * Dataset of Row. In Spark1, DataFrame is its own class. Spark1 and Spark2 also have different methods that they\n  * support on a DataFrame/Dataset.\n  *\n"}}, {"oid": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "url": "https://github.com/cdapio/cdap/commit/e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T00:46:41Z", "type": "forcePushed"}, {"oid": "d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "url": "https://github.com/cdapio/cdap/commit/d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T03:14:27Z", "type": "commit"}, {"oid": "d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "url": "https://github.com/cdapio/cdap/commit/d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T03:14:27Z", "type": "forcePushed"}]}