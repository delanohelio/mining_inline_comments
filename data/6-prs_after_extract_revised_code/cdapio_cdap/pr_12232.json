{"pr_number": 12232, "pr_title": "CDAP-16852 handle dynamic schemas in auto join", "pr_createdAt": "2020-05-29T23:47:25Z", "pr_url": "https://github.com/cdapio/cdap/pull/12232", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTk2Nw==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434285967", "bodyText": "Feel this should generate more meaningful error messages like in the comment to tell user what is the expected behavior to fix the problem", "author": "yaojiefeng", "createdAt": "2020-06-03T03:20:33Z", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java", "diffHunk": "@@ -48,8 +48,15 @@\n   private final Map<String, List<String>> joinKeys;\n   private final Map<String, List<JoinField>> stageFields;\n   private Schema keySchema;\n-\n-  public JoinerBridge(BatchAutoJoiner autoJoiner, JoinDefinition joinDefinition) {\n+  private Schema outputSchema;\n+\n+  public JoinerBridge(String stageName, BatchAutoJoiner autoJoiner, JoinDefinition joinDefinition) {\n+    // if this is not an inner join and the output schema is not set,\n+    // we have no way of determining what the output schema should be and need to error out.\n+    if (joinDefinition.getOutputSchema() == null &&\n+      joinDefinition.getStages().stream().anyMatch(s -> !s.isRequired())) {\n+      throw new IllegalArgumentException(\"An output schema could not be generated for joiner stage \" + stageName);", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java b/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java\nindex 76c8bbdcfe3..ab0f858aaf0 100644\n--- a/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java\n+++ b/cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/plugin/JoinerBridge.java\n\n@@ -55,7 +55,9 @@ public class JoinerBridge<INPUT_RECORD> extends BatchJoiner<StructuredRecord, IN\n     // we have no way of determining what the output schema should be and need to error out.\n     if (joinDefinition.getOutputSchema() == null &&\n       joinDefinition.getStages().stream().anyMatch(s -> !s.isRequired())) {\n-      throw new IllegalArgumentException(\"An output schema could not be generated for joiner stage \" + stageName);\n+      throw new IllegalArgumentException(\n+        String.format(\"An output schema could not be generated for joiner stage '%s'. \" +\n+                        \"Provide the expected output schema directly.\", stageName));\n     }\n     this.autoJoiner = autoJoiner;\n     this.joinDefinition = joinDefinition;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Njg4OQ==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434286889", "bodyText": "nit - unused import", "author": "yaojiefeng", "createdAt": "2020-06-03T03:24:49Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java", "diffHunk": "@@ -20,6 +20,7 @@\n import io.cdap.cdap.etl.spark.SparkCollection;\n \n import java.util.List;\n+import javax.annotation.Nullable;", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\nindex 45cb7684e37..f0157a3ff27 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java\n\n@@ -20,7 +20,6 @@ import io.cdap.cdap.api.data.schema.Schema;\n import io.cdap.cdap.etl.spark.SparkCollection;\n \n import java.util.List;\n-import javax.annotation.Nullable;\n \n /**\n  * Data to join.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkyMQ==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434286921", "bodyText": "nit - unused import", "author": "yaojiefeng", "createdAt": "2020-06-03T03:24:58Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java", "diffHunk": "@@ -20,6 +20,7 @@\n import io.cdap.cdap.etl.api.join.JoinField;\n \n import java.util.List;\n+import javax.annotation.Nullable;", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java\nindex 50fddff21ff..e01a7af2694 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinRequest.java\n\n@@ -20,7 +20,6 @@ import io.cdap.cdap.api.data.schema.Schema;\n import io.cdap.cdap.etl.api.join.JoinField;\n \n import java.util.List;\n-import javax.annotation.Nullable;\n \n /**\n  * Request to join some collection to another collection.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzMzNg==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434287336", "bodyText": "Can be replaced with outputSchema", "author": "yaojiefeng", "createdAt": "2020-06-03T03:26:45Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 95644afb4a8..9a0e4265e5c 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -424,7 +428,7 @@ public abstract class SparkPipelineRunner {\n     // but this approach will not work if the input RDD is empty.\n     // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n     // when we properly propagate schema at runtime, this condition should no longer happen\n-    if (joinDefinition.getOutputSchema() == null) {\n+    if (outputSchema == null) {\n       throw new IllegalArgumentException(\n         String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n                         \"one or more inputs have dynamic or unknown schema. \" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODI4NQ==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434288285", "bodyText": "It is better to put this logic under \n  \n    \n      cdap/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n    \n    \n         Line 403\n      in\n      fdf4d5c\n    \n    \n    \n    \n\n        \n          \n           Schema leftSchema = left.getSchema(); \n        \n    \n  \n\n, so that it is easier to know they are related", "author": "yaojiefeng", "createdAt": "2020-06-03T03:30:42Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 95644afb4a8..9a0e4265e5c 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -424,7 +428,7 @@ public abstract class SparkPipelineRunner {\n     // but this approach will not work if the input RDD is empty.\n     // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n     // when we properly propagate schema at runtime, this condition should no longer happen\n-    if (joinDefinition.getOutputSchema() == null) {\n+    if (outputSchema == null) {\n       throw new IllegalArgumentException(\n         String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n                         \"one or more inputs have dynamic or unknown schema. \" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODYwOA==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434288608", "bodyText": "Just curious - is there any benefit to use Table instead of Map<String, Map<String, JoinField>>?", "author": "yaojiefeng", "createdAt": "2020-06-03T03:32:08Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMTAxNw==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434721017", "bodyText": "it's just convenience, can do a single put instead of computeIfAbsent follow by a put, and can do a single get instead of two gets", "author": "albertshau", "createdAt": "2020-06-03T17:02:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODYwOA=="}], "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 95644afb4a8..9a0e4265e5c 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -424,7 +428,7 @@ public abstract class SparkPipelineRunner {\n     // but this approach will not work if the input RDD is empty.\n     // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n     // when we properly propagate schema at runtime, this condition should no longer happen\n-    if (joinDefinition.getOutputSchema() == null) {\n+    if (outputSchema == null) {\n       throw new IllegalArgumentException(\n         String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n                         \"one or more inputs have dynamic or unknown schema. \" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MDAyNg==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434290026", "bodyText": "should we log something here as a TRACE or DEBUG message to let the user know we mark it as nullable, since there is possibility that it is not nullable", "author": "yaojiefeng", "createdAt": "2020-06-03T03:38:32Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,\n+                                     joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+    }\n+\n     // JoinRequest contains the left side of the join, plus 1 or more other stages to join to.\n-    JoinRequest joinRequest = new JoinRequest(left.getStageName(), stageKeys.get(left.getStageName()), leftSchema,\n+    JoinRequest joinRequest = new JoinRequest(leftName, leftKey, leftSchema,\n                                               left.isRequired(), onKeys.isNullSafe(),\n                                               joinDefinition.getSelectedFields(),\n                                               joinDefinition.getOutputSchema(), toJoin);\n     return leftCollection.join(joinRequest);\n   }\n \n+  /**\n+   * Derive the key schema based on the provided output schema and the final list of selected fields.\n+   * This is not possible if the key is not present in the output schema, in which case the user will need to\n+   * add it to the output schema. However, this is an unlikely scenario as most joins will want to\n+   * preserve the key.\n+   */\n+  @VisibleForTesting\n+  static List<Schema> deriveKeySchema(String joinerStageName, Map<String, List<String>> keys,\n+                                      JoinDefinition joinDefinition) {\n+    int numKeyFields = keys.values().iterator().next().size();\n+\n+    // (stage, field) -> JoinField\n+    Table<String, String, JoinField> fieldTable = HashBasedTable.create();\n+    for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+      fieldTable.put(joinField.getStageName(), joinField.getFieldName(), joinField);\n+    }\n+\n+    /*\n+        Suppose the join keys are:\n+\n+        A, (x, y)\n+        B, (x, z))\n+\n+        and selected fields are:\n+\n+        A.x as id, A.y as name, B.z as item\n+     */\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    List<Schema> keySchema = new ArrayList<>(numKeyFields);\n+    for (int i = 0; i < numKeyFields; i++) {\n+      keySchema.add(null);\n+    }\n+    for (Map.Entry<String, List<String>> keyEntry : keys.entrySet()) {\n+      String keyStage = keyEntry.getKey();\n+      int keyFieldNum = 0;\n+      for (String keyField : keyEntry.getValue()) {\n+        // If key field is A.x, this will fetch (A.x as id)\n+        // the JoinField might not exist. For example, B.x will not find anything in the output\n+        JoinField selectedKeyField = fieldTable.get(keyStage, keyField);\n+        if (selectedKeyField == null) {\n+          continue;\n+        }\n+\n+        // if the key field is A.x, JoinField is (A.x as id), and outputField is the 'id' field in the output schema\n+        String outputFieldName = selectedKeyField.getAlias() == null ?\n+          selectedKeyField.getFieldName() : selectedKeyField.getAlias();\n+        Schema.Field outputField = outputSchema.getField(outputFieldName);\n+\n+        if (outputField == null) {\n+          // this is an invalid join definition\n+          throw new IllegalArgumentException(\n+            String.format(\"Joiner stage '%s' provided an invalid definition. \" +\n+                            \"The output schema does not contain a field for selected field '%s'.'%s'%s\",\n+                          joinerStageName, keyStage, selectedKeyField.getFieldName(),\n+                          selectedKeyField.getAlias() == null ? \"\" : \"as \" + selectedKeyField.getAlias()));\n+        }\n+\n+        // make the schema nullable because one stage might have it as non-nullable\n+        // while another stage has it as nullable.\n+        // for example, when joining on A.id = B.id,\n+        // A.id might not be nullable, but B.id could be.\n+        Schema keyFieldSchema = outputField.getSchema();\n+        if (!keyFieldSchema.isNullable()) {\n+          keyFieldSchema = Schema.nullableOf(keyFieldSchema);", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMTYwOQ==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434721609", "bodyText": "this won't matter in the end because this is just intermediate data, the output will still use whatever the user provided. I can add a comment about that.", "author": "albertshau", "createdAt": "2020-06-03T17:03:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MDAyNg=="}], "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 95644afb4a8..9a0e4265e5c 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -424,7 +428,7 @@ public abstract class SparkPipelineRunner {\n     // but this approach will not work if the input RDD is empty.\n     // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n     // when we properly propagate schema at runtime, this condition should no longer happen\n-    if (joinDefinition.getOutputSchema() == null) {\n+    if (outputSchema == null) {\n       throw new IllegalArgumentException(\n         String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n                         \"one or more inputs have dynamic or unknown schema. \" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MjM2Mw==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434292363", "bodyText": "notice there is no trim for the schema in this case, is there a reason for that?", "author": "yaojiefeng", "createdAt": "2020-06-03T03:48:59Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -411,26 +415,231 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     // C -> [z, k]\n     Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n       .collect(Collectors.toMap(JoinKey::getStageName, JoinKey::getFields));\n+\n+    Schema outputSchema = joinDefinition.getOutputSchema();\n+    // if the output schema is null it means it could not be generated because some of the input schemas are null.\n+    // there isn't a reliable way to get the schema from the data at this point, so we require the user to have\n+    // provided the output schema directly\n+    // it is technically possible to take a single StructuredRecord from each input to determine the schema,\n+    // but this approach will not work if the input RDD is empty.\n+    // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n+    // when we properly propagate schema at runtime, this condition should no longer happen\n+    if (joinDefinition.getOutputSchema() == null) {\n+      throw new IllegalArgumentException(\n+        String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n+                        \"one or more inputs have dynamic or unknown schema. \" +\n+                        \"An output schema must be directly provided.\", stageName));\n+    }\n     List<JoinCollection> toJoin = new ArrayList<>();\n+    List<Schema> keySchema = null;\n     while (stageIter.hasNext()) {\n       // in this loop, information for each stage to be joined is gathered together into a JoinCollection\n       JoinStage right = stageIter.next();\n       String rightName = right.getStageName();\n+      Schema rightSchema = right.getSchema();\n+      List<String> key = stageKeys.get(rightName);\n+      if (rightSchema == null) {\n+        if (keySchema == null) {\n+          keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+        }\n+        // if the schema is not known, generate it from the provided output schema and the selected fields\n+        rightSchema = deriveInputSchema(stageName, rightName, key, keySchema,\n+                                        joinDefinition.getSelectedFields(), joinDefinition.getOutputSchema());\n+      }\n+\n+      // drop fields that aren't included in the final output\n+      Set<String> requiredFields = new HashSet<>(key);\n+      for (JoinField joinField : joinDefinition.getSelectedFields()) {\n+        if (!joinField.getStageName().equals(rightName)) {\n+          continue;\n+        }\n+        requiredFields.add(joinField.getFieldName());\n+      }\n+      rightSchema = trimSchema(rightSchema, requiredFields);\n+\n       // JoinCollection contains the stage name,  SparkCollection, schema, joinkey,\n       // whether it's required, and whether to broadcast\n-      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName),\n-                                    right.getSchema(), stageKeys.get(rightName),\n+      toJoin.add(new JoinCollection(rightName, inputDataCollections.get(rightName), rightSchema, key,\n                                     right.isRequired(), right.isBroadcast()));\n     }\n \n+    String leftName = left.getStageName();\n+    List<String> leftKey = stageKeys.get(left.getStageName());\n+    if (leftSchema == null) {\n+      if (keySchema == null) {\n+        keySchema = deriveKeySchema(stageName, stageKeys, joinDefinition);\n+      }\n+      leftSchema = deriveInputSchema(stageName, leftName, leftKey, keySchema,", "originalCommit": "fdf4d5c6fc991e44b35de436c17d067a72318a1f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMjE1MQ==", "url": "https://github.com/cdapio/cdap/pull/12232#discussion_r434722151", "bodyText": "can add a comment, basically the derived schema is guaranteed to be trimmed because it is generated from the output schema and the join key, which means it will never have extra fields", "author": "albertshau", "createdAt": "2020-06-03T17:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI5MjM2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "chunk": "diff --git a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\nindex 95644afb4a8..9a0e4265e5c 100644\n--- a/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n+++ b/cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java\n\n@@ -424,7 +428,7 @@ public abstract class SparkPipelineRunner {\n     // but this approach will not work if the input RDD is empty.\n     // in addition, RDD.take(1) and RDD.isEmpty() both trigger a Spark job, which is not desirable.\n     // when we properly propagate schema at runtime, this condition should no longer happen\n-    if (joinDefinition.getOutputSchema() == null) {\n+    if (outputSchema == null) {\n       throw new IllegalArgumentException(\n         String.format(\"Joiner stage '%s' cannot calculate its output schema because \" +\n                         \"one or more inputs have dynamic or unknown schema. \" +\n"}}, {"oid": "3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "url": "https://github.com/cdapio/cdap/commit/3c6fd7f512025d5eea0cacbf33233ff3d3241c39", "message": "address comments and resolve conflicts", "committedDate": "2020-06-03T18:26:07Z", "type": "forcePushed"}, {"oid": "39141b2e41586e0dc1f06554059f92091f590b51", "url": "https://github.com/cdapio/cdap/commit/39141b2e41586e0dc1f06554059f92091f590b51", "message": "CDAP-16852 handle dynamic schemas in auto join\n\nProperly handle the case when the inputs into an auto-join stage\nhave null schemas due to the schema being unknown at deploy time,\nusually because of macros.\n\nIn order to support this type of use case, added a way for a\nplugin to specify the output schema in the JoinDefinition.\nThis should be used when the output schema cannot be derived\nat deployment time due to macros. This is what the existing\nJoiner currently requires the user to do for dynamic join\nuse cases.\n\nFor Spark, the implementation becomes significantly more difficult\ndue to the fact that the schema of each input stage needs to be\nknown in the Spark driver, in order to convert RDDs into\nDataFrames.\n\nAdded logic that derives the input schema using the output schema,\nselected fields, and join keys. It is possible to derive\na usable schema when all the join keys are present in the final\noutput schema.", "committedDate": "2020-06-03T21:30:15Z", "type": "commit"}, {"oid": "39141b2e41586e0dc1f06554059f92091f590b51", "url": "https://github.com/cdapio/cdap/commit/39141b2e41586e0dc1f06554059f92091f590b51", "message": "CDAP-16852 handle dynamic schemas in auto join\n\nProperly handle the case when the inputs into an auto-join stage\nhave null schemas due to the schema being unknown at deploy time,\nusually because of macros.\n\nIn order to support this type of use case, added a way for a\nplugin to specify the output schema in the JoinDefinition.\nThis should be used when the output schema cannot be derived\nat deployment time due to macros. This is what the existing\nJoiner currently requires the user to do for dynamic join\nuse cases.\n\nFor Spark, the implementation becomes significantly more difficult\ndue to the fact that the schema of each input stage needs to be\nknown in the Spark driver, in order to convert RDDs into\nDataFrames.\n\nAdded logic that derives the input schema using the output schema,\nselected fields, and join keys. It is possible to derive\na usable schema when all the join keys are present in the final\noutput schema.", "committedDate": "2020-06-03T21:30:15Z", "type": "forcePushed"}]}