{"pr_number": 5099, "pr_title": "GEODE-8029: Delete orphaned drf files (2nd try)", "pr_createdAt": "2020-05-12T16:16:32Z", "pr_url": "https://github.com/apache/geode/pull/5099", "timeline": [{"oid": "96fb570ebf00181e8a2dc287c0497f7fadde1192", "url": "https://github.com/apache/geode/commit/96fb570ebf00181e8a2dc287c0497f7fadde1192", "message": "GEODE-8029: Delete orphaned drf files (2nd try)\n\nThis reverts the revert of the original commit and also fixes the\noriginally introduced issues.\n\n----\n\nThe OpLog initialization now delete unused drf files to prevent the\nproliferation of unused records and files within the system, which\ncan could cause members to fail during startup while recovering\ndisk-stores (especially when they are isolated for gateway-senders).\n\n- Added distributed tests.\n- Delete orphaned drf files when deleting the corresponding\n  crf during recovery.", "committedDate": "2020-05-12T16:05:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyMTA2Mw==", "url": "https://github.com/apache/geode/pull/5099#discussion_r434821063", "bodyText": "This change comes into picture after the recovery is completed - \"initAfterRecovery\".\nThe real problem seems to be during recovery. If there is a periodic recovery this could be fine, but if there are large number of deleted records during the first recovery, the issue may still arise...We may need to have some checks during recovery (before reading the drfs) to avoid reading large deleted records. Please correct me if my understanding is wrong.\nAlso, looking at the other part of the code where \"setHasDeletes\" is called, it looks like it is called before the deleteDRF() - there could be a reason for doing this.\nAnd there is also call to \"getOplogSet().removeDrf(this);\" This may be needed here...", "author": "agingade", "createdAt": "2020-06-03T20:01:53Z", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "diffHunk": "@@ -937,14 +937,21 @@ void initAfterRecovery(boolean offline) {\n         // this.crf.raf.seek(this.crf.currSize);\n       } else if (!offline) {\n         // drf exists but crf has been deleted (because it was empty).\n-        // I don't think the drf needs to be opened. It is only used during\n-        // recovery.\n-        // At some point the compacter my identify that it can be deleted.\n         this.crf.RAFClosed = true;\n         deleteCRF();\n+\n+        // See GEODE-8029.", "originalCommit": "96fb570ebf00181e8a2dc287c0497f7fadde1192", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIzNzk4Mg==", "url": "https://github.com/apache/geode/pull/5099#discussion_r435237982", "bodyText": "This change comes into picture after the recovery is completed - \"initAfterRecovery\".\nThe real problem seems to be during recovery. If there is a periodic recovery this could be fine, but if there are large number of deleted records during the first recovery, the issue may still arise...We may need to have some checks during recovery (before reading the drfs) to avoid reading large deleted records. Please correct me if my understanding is wrong.\n\nYour understanding is correct, however, the change are to prevent the issue from happening in the first place. By deleting the unused drf files here and avoid storing them on disk when not needed, users won't hit the IllegalStateException at all.\nAs a side note, in order to hit the problem in the first place, the user would need to have more than 805306401 delete operations within the opLog files in one single run, and compaction should have not run at all, which is highly unlikely. Adding to that, when we record a delete operation within the drf file, we check whether the new drf size would be higher than the max configured size for the drf file, and roll to a new one afterwards. Considering that the drf file is 10% of the max-oplog-size, and that we use 10 bytes per DEL_ENTRY, to be able to hit the problem the max drf file should be higher than 8.5GB, meaning that the max-oplog-size should be configured, at least, to be 85GB (with no automatic compaction at all, which is something we don't recommend).\nIf you think we should add the count while reading the files instead and, somehow, expand the load factor when needed or something similar, let me know and I'll give it a try. I'm worried, though, about the performance impact this might have while recovering files from disk... the recovery time will suffer during every single startup and, considering that there's only a handful of scenarios on which the actual issue can happen, I believe making sure we delete unused files is a better approach.\n\nAlso, looking at the other part of the code where \"setHasDeletes\" is called, it looks like it is called before the deleteDRF() - there could be a reason for doing this.\n\nWill change this, thanks for catching it.\n\nAnd there is also call to \"getOplogSet().removeDrf(this);\" This may be needed here...\n\nThis is already done within the deleteDRF() method.\n\nPlease let me know what you think about point 1 (add an extra check while recovering to make sure we are below the load-factor threshold), so I can go ahead and make all the changes at once.", "author": "jujoramos", "createdAt": "2020-06-04T13:07:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyMTA2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4MTAwOA==", "url": "https://github.com/apache/geode/pull/5099#discussion_r435581008", "bodyText": "I am fine with the approach you have taken. Wanted to bring it up that there could be distinct possibility.\nLooks good to me.", "author": "agingade", "createdAt": "2020-06-04T22:11:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyMTA2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg3Mjg2Ng==", "url": "https://github.com/apache/geode/pull/5099#discussion_r435872866", "bodyText": "@agingade\nThanks, I've made the requested changes already and verified that no new regressions have been introduced due to the modifications, please go ahead and approve the PR (if there's nothing else you want me to change) so I can merge it \ud83d\udc4d.", "author": "jujoramos", "createdAt": "2020-06-05T11:57:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyMTA2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "558149b28a97b5e5c75aa8fc5a8ea5cbe7bdb66c", "chunk": "diff --git a/geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java b/geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java\nindex 3a62f751b8..d19547669b 100755\n--- a/geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java\n+++ b/geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java\n\n@@ -940,13 +940,13 @@ public class Oplog implements CompactableOplog, Flushable {\n         this.crf.RAFClosed = true;\n         deleteCRF();\n \n-        // See GEODE-8029.\n-        // The drf file needs to be deleted, especially when the disk-store is *only* used by\n-        // gateway-senders, otherwise there will be orphaned drfs that are never deleted by\n-        // compaction (unless there are pending events in the queue upon restart - crf exists - or\n-        // a manual compaction is executed).\n-        deleteDRF();\n+        // The drf file needs to be deleted (see GEODE-8029).\n+        // If compaction is not enabled, or if the compaction-threshold is never reached, there\n+        // will be orphaned drf files that are not automatically deleted (unless a manual\n+        // compaction is executed), in which case a later recovery might fail when the amount of\n+        // deleted records is too high (805306401).\n         setHasDeletes(false);\n+        deleteDRF();\n \n         this.closed = true;\n         this.deleted.set(true);\n"}}, {"oid": "558149b28a97b5e5c75aa8fc5a8ea5cbe7bdb66c", "url": "https://github.com/apache/geode/commit/558149b28a97b5e5c75aa8fc5a8ea5cbe7bdb66c", "message": "GEODE-8029: Changes requested by reviewers. Improved javadoc.", "committedDate": "2020-06-05T08:34:26Z", "type": "commit"}]}