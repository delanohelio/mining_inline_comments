{"pr_number": 60522, "pr_title": "Introduce index based snapshot blob cache for Searchable Snapshots", "pr_createdAt": "2020-07-31T15:17:16Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/60522", "timeline": [{"oid": "d0014198b691d898a85ddfccf1cb7262a849c568", "url": "https://github.com/elastic/elasticsearch/commit/d0014198b691d898a85ddfccf1cb7262a849c568", "message": "Introduce index based snapshot blob cache for Searchable Snapshots", "committedDate": "2020-07-31T14:43:16Z", "type": "commit"}, {"oid": "c4f251c97d6e23a1f358f1d5aac9f0624fd700b2", "url": "https://github.com/elastic/elasticsearch/commit/c4f251c97d6e23a1f358f1d5aac9f0624fd700b2", "message": "WIP adjust some logging and back out cache-size-times-two optimisation", "committedDate": "2020-08-04T15:04:26Z", "type": "commit"}, {"oid": "1f30c42b6f094b9f4bf4aae9c185cdfa38c680d7", "url": "https://github.com/elastic/elasticsearch/commit/1f30c42b6f094b9f4bf4aae9c185cdfa38c680d7", "message": "Avoid ActionListener#wrap", "committedDate": "2020-08-06T15:19:42Z", "type": "commit"}, {"oid": "02a8ed12cbd5c2503081c8062cb3a47c1c351d3d", "url": "https://github.com/elastic/elasticsearch/commit/02a8ed12cbd5c2503081c8062cb3a47c1c351d3d", "message": "Add TODO", "committedDate": "2020-08-06T15:24:03Z", "type": "commit"}, {"oid": "76b97ab793ce90a80a5af1a600eee4e11100f2da", "url": "https://github.com/elastic/elasticsearch/commit/76b97ab793ce90a80a5af1a600eee4e11100f2da", "message": "Move special openInputStream impl", "committedDate": "2020-08-06T15:55:41Z", "type": "commit"}, {"oid": "df7c0b5c81eef57c7150e620431fdbb166c69f58", "url": "https://github.com/elastic/elasticsearch/commit/df7c0b5c81eef57c7150e620431fdbb166c69f58", "message": "Add TODOs", "committedDate": "2020-08-06T16:01:35Z", "type": "commit"}, {"oid": "922c381a1de1860f53a7bb03daa458ee2bca7146", "url": "https://github.com/elastic/elasticsearch/commit/922c381a1de1860f53a7bb03daa458ee2bca7146", "message": "Add TODOs", "committedDate": "2020-08-06T16:09:04Z", "type": "commit"}, {"oid": "c24d78e6b6f92fdeb159a14ebde2be53f4bd0181", "url": "https://github.com/elastic/elasticsearch/commit/c24d78e6b6f92fdeb159a14ebde2be53f4bd0181", "message": "Rework implementation\n\nWith this commit we fill in any blob index cache misses from the cache\nfile, which may in turn be populated from the blob store.\n\nIt also renames/comments some methods and moves some methods down the\nclass hierarchy to their usage site.", "committedDate": "2020-08-07T13:03:02Z", "type": "commit"}, {"oid": "7e5bbc48c1fe0f82c08fdace56c2a5852f4b1c65", "url": "https://github.com/elastic/elasticsearch/commit/7e5bbc48c1fe0f82c08fdace56c2a5852f4b1c65", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-07T16:16:53Z", "type": "commit"}, {"oid": "1b459aed4a79035dc9366f4ff19118463f745eaf", "url": "https://github.com/elastic/elasticsearch/commit/1b459aed4a79035dc9366f4ff19118463f745eaf", "message": "One-shot reading", "committedDate": "2020-08-07T20:14:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMjgwMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467722803", "bodyText": "Pushed down to the cached index input only, the other implementation doesn't use this.", "author": "DaveCTurner", "createdAt": "2020-08-10T07:12:40Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/BaseSearchableSnapshotIndexInput.java", "diffHunk": "@@ -101,34 +102,6 @@ public final void close() throws IOException {\n \n     public abstract void innerClose() throws IOException;\n \n-    protected InputStream openInputStream(final long position, final long length) throws IOException {", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMjg1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467722857", "bodyText": "Pushed these methods down to the cached index input only, the other implementation doesn't use them.", "author": "DaveCTurner", "createdAt": "2020-08-10T07:12:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/BaseSearchableSnapshotIndexInput.java", "diffHunk": "@@ -149,29 +122,4 @@ protected final boolean assertCurrentThreadMayAccessBlobStore() {\n         return true;\n     }\n \n-    private long getPartNumberForPosition(long position) {", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyMzk4Mg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467723982", "bodyText": "In fact we (almost) never looped here, the inner read methods always satisfied the complete read. The only case where we did loop is if the read spanned two 32MB-aligned ranges, but in that case we may as well read both ranges in one go anyway so that's what we do now.", "author": "DaveCTurner", "createdAt": "2020-08-10T07:15:51Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNDI4MA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467724280", "bodyText": "This (and subsequent methods) moved here from the base class.", "author": "DaveCTurner", "createdAt": "2020-08-10T07:16:43Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -370,6 +558,76 @@ private void writeCacheFile(final FileChannel fc, final long start, final long e\n         }\n     }\n \n+    /**\n+     * Opens an {@link InputStream} for the given range of bytes which reads the data directly from the blob store. If the requested range\n+     * spans multiple blobs then this stream will request them in turn.\n+     *\n+     * @param position The start of the range of bytes to read, relative to the start of the corresponding Lucene file.\n+     * @param length The number of bytes to read\n+     */\n+    private InputStream openInputStreamFromBlobStore(final long position, final long length) throws IOException {", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b052d7e1b15a2405e004eb998d866c898755b5d1", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..fd09330a7c4 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -575,10 +614,20 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                 + \"] from [\"\n                 + fileInfo\n                 + \"]\";\n+            stats.addBlobStoreBytesRequested(length);\n             return blobContainer.readBlob(fileInfo.partName(0L), position, length);\n         } else {\n             final long startPart = getPartNumberForPosition(position);\n             final long endPart = getPartNumberForPosition(position + length - 1);\n+\n+            for (long currentPart = startPart; currentPart <= endPart; currentPart++) {\n+                final long startInPart = (currentPart == startPart) ? getRelativePositionInPart(position) : 0L;\n+                final long endInPart = (currentPart == endPart)\n+                    ? getRelativePositionInPart(position + length - 1) + 1\n+                    : getLengthOfPart(currentPart);\n+                stats.addBlobStoreBytesRequested(endInPart - startInPart);\n+            }\n+\n             return new SlicedInputStream(endPart - startPart + 1L) {\n                 @Override\n                 protected InputStream openSlice(long slice) throws IOException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzcyNDM3OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467724378", "bodyText": "Inlined this into another method.", "author": "DaveCTurner", "createdAt": "2020-08-10T07:16:59Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -431,43 +689,11 @@ public String toString() {\n             + getFilePointer()\n             + \", rangeSize=\"\n             + getDefaultRangeSize()\n+            + \", directory=\"\n+            + directory\n             + '}';\n     }\n \n-    private int readDirectly(long start, long end, ByteBuffer b) throws IOException {", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0ODUxNQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467748515", "bodyText": "Small preference for defining canBeFullyCached before isStartOfFile.\nAlso, I wonder whether the logic to cache up to 2 * DEFAULT_SIZE is truly needed (perhaps canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE; would work just as well in practice, and give us a simpler upper bound on the size of cached blobs?). Alternatively, we could extend this in the future, do it file-type or content-based instead?", "author": "ywelsch", "createdAt": "2020-08-10T08:13:39Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgwMjA4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467802081", "bodyText": "Ok, reordered defs in 974c095.\nI don't have a great way to decide on the heuristics for what should go in the cache, so I'm deferring to @tlrx's better judgement here. There's scope for all sorts of cleverness here, such as caching the header of slices within a .cfs file.\nNote that there's an assertion (on the put path) that cached blobs are no larger than the size of the copy buffer.", "author": "DaveCTurner", "createdAt": "2020-08-10T10:00:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0ODUxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1MTA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467751066", "bodyText": "Is this comment addressed now?", "author": "ywelsch", "createdAt": "2020-08-10T08:18:25Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));\n+            } else {\n+                indexCacheMisses = List.of(Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_SIZE));\n+            }\n+            logger.trace(\"recovery cache miss for [{}], falling through with regions [{}]\", this, indexCacheMisses);\n+        } else {\n+            indexCacheMisses = List.of();\n+        }\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Read all target ranges in one go, including any cache misses identified above.\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMisses.stream().mapToLong(Tuple::v1).max().orElse(Long.MAX_VALUE)),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMisses.stream().mapToLong(Tuple::v2).max().orElse(Long.MIN_VALUE))\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n                     }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                for (Tuple<Long, Long> indexCacheMiss : indexCacheMisses) {\n+                    cacheFile.populateAndRead(indexCacheMiss, indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc4Mzg1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467783852", "bodyText": "The comment is deliberate, to explain why we don't call readCacheFile here.", "author": "DaveCTurner", "createdAt": "2020-08-10T09:24:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1MTA2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1NjUzNA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467756534", "bodyText": "Why is indexCacheMisses a list if it only contains a single entry?", "author": "ywelsch", "createdAt": "2020-08-10T08:29:27Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgwMjE0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467802145", "bodyText": "Premature generalisation, changed to a nullable variable in c5b65bc.", "author": "DaveCTurner", "createdAt": "2020-08-10T10:00:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1NjUzNA=="}], "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1OTc4OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467759788", "bodyText": "let's make this dynamic: strict", "author": "ywelsch", "createdAt": "2020-08-10T08:36:10Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin().indices().prepareCreate(index).setSettings(settings()).setMapping(mappings()).execute(new ActionListener<>() {\n+                @Override\n+                public void onResponse(CreateIndexResponse createIndexResponse) {\n+                    assert createIndexResponse.index().equals(index);\n+                    listener.onResponse(createIndexResponse.index());\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof ResourceAlreadyExistsException\n+                        || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                        listener.onResponse(index);\n+                    } else {\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings settings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"false\");", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgwMzM5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467803391", "bodyText": "++ 2abf126.", "author": "DaveCTurner", "createdAt": "2020-08-10T10:03:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1OTc4OA=="}], "type": "inlineReview", "revised_code": {"commit": "2abf12671472b3a86002c85d797ccea52a6e7f09", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 45ac3b24fb4..39c49f61bc3 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -135,7 +135,7 @@ public class BlobStoreCacheService extends AbstractLifecycleComponent implements\n             builder.startObject();\n             {\n                 builder.startObject(SINGLE_MAPPING_NAME);\n-                builder.field(\"dynamic\", \"false\");\n+                builder.field(\"dynamic\", \"strict\");\n                 {\n                     builder.startObject(\"_meta\");\n                     builder.field(\"version\", Version.CURRENT);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2MTUxMg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467761512", "bodyText": "Where is this exception propagated to? Do we fail the blobstore read because we failed to  populate the index cache?", "author": "ywelsch", "createdAt": "2020-08-10T08:39:57Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                    return;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Cache miss may be that the cache is completely unavailable (no point in populating it) or that the blob is\n+                // definitely absent. TODO only bother populating the cache in the latter case.\n+            }\n+\n+            // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+            // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+            // {start, end} ranges where positions are relative to the whole file.\n+            if (canBeFullyCached) {\n+                // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                indexCacheMisses = List.of(Tuple.tuple(0L, fileInfo.length()));\n+            } else {\n+                indexCacheMisses = List.of(Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_SIZE));\n+            }\n+            logger.trace(\"recovery cache miss for [{}], falling through with regions [{}]\", this, indexCacheMisses);\n+        } else {\n+            indexCacheMisses = List.of();\n+        }\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Read all target ranges in one go, including any cache misses identified above.\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMisses.stream().mapToLong(Tuple::v1).max().orElse(Long.MAX_VALUE)),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMisses.stream().mapToLong(Tuple::v2).max().orElse(Long.MIN_VALUE))\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n                     }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                for (Tuple<Long, Long> indexCacheMiss : indexCacheMisses) {\n+                    cacheFile.populateAndRead(indexCacheMiss, indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n+                        return indexCacheMissLength;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n+                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, we don't retry, we simply fail to populate the index cache.\n+                        logger.debug(\n+                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            from,\n+                            to,\n+                            CachedBlobContainerIndexInput.this\n+                        );\n+                        throw new IOException(", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgwMjE3OQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467802179", "bodyText": "This doesn't happen if we failed to populate the index cache, it only happens if the larger read above already failed.\nHowever it is a good question to ask because I now realise that there may in principle be subsequent reads waiting on this range that we've claimed but then fail to fill. Let's just fill them, see f06e0ae.", "author": "DaveCTurner", "createdAt": "2020-08-10T10:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2MTUxMg=="}], "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2NTI3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467765277", "bodyText": "Probably good to address this in the PR here.", "author": "ywelsch", "createdAt": "2020-08-10T08:47:23Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,654 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.PluginsService;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.add(TrackingRepositoryPlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n+            // TODO randomized this", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3NjkwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469076909", "bodyText": "Yes this is now randomised.", "author": "DaveCTurner", "createdAt": "2020-08-12T08:00:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc2NTI3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 69eb78a789d..91c436ed006 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -107,11 +107,11 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n     protected Settings nodeSettings(int nodeOrdinal) {\n         return Settings.builder()\n             .put(super.nodeSettings(nodeOrdinal))\n-            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n-            // TODO randomized this\n             .put(\n                 CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n-                new ByteSizeValue(BlobStoreCacheService.DEFAULT_SIZE, ByteSizeUnit.BYTES)\n+                randomLongBetween(\n+                        new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(),\n+                        new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n             )\n             .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n             .build();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3MDkwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467770909", "bodyText": "???", "author": "ywelsch", "createdAt": "2020-08-10T08:58:36Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,654 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardsIterator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.blobstore.BlobContainer;\n+import org.elasticsearch.common.blobstore.BlobPath;\n+import org.elasticsearch.common.blobstore.BlobStore;\n+import org.elasticsearch.common.blobstore.support.FilterBlobContainer;\n+import org.elasticsearch.common.collect.Tuple;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.xcontent.NamedXContentRegistry;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.indices.recovery.RecoverySettings;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.plugins.PluginsService;\n+import org.elasticsearch.plugins.RepositoryPlugin;\n+import org.elasticsearch.repositories.Repository;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.FilterInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.function.BiConsumer;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.add(TrackingRepositoryPlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n+            // TODO randomized this\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                new ByteSizeValue(BlobStoreCacheService.DEFAULT_SIZE, ByteSizeUnit.BYTES)\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        final ForceMergeResponse forceMergeResponse = client().admin().indices().prepareForceMerge(indexName).setMaxNumSegments(1).get();\n+        assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+        assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(repositoryName));\n+        createRepository(\n+            repositoryName,\n+            TrackingRepositoryPlugin.TRACKING,\n+            Settings.builder().put(FsRepository.LOCATION_SETTING.getKey(), repositoryLocation).build(),\n+            false\n+        );\n+        assertBusy(this::ensureClusterStateConsistency);\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        final boolean usePrewarming = false; // TODO randomize this and adapt test\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), usePrewarming)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        refreshSystemIndex();\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+\n+        ensureBlobStoreRepositoriesWithActiveShards(\n+            restoredIndex,\n+            (nodeId, blobStore) -> assertThat(\n+                \"Blob read operations should have been executed on node [\" + nodeId + ']',\n+                blobStore.numberOfReads(),\n+                greaterThan(0L)\n+            )\n+        );\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+        resetTrackedFiles();\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), usePrewarming)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+\n+        logger.info(\"--> verifying blobs read from the repository\");\n+        assertBlobsReadFromRemoteRepository(restoredAgainIndex, blobsInSnapshot);\n+\n+        resetTrackedFiles();\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+\n+        logger.info(\"--> verifying blobs read from the repository after restart\");\n+        // Without the WaitForSnapshotBlobCacheShardsActivePlugin this would fail\n+        assertBlobsReadFromRemoteRepository(restoredAgainIndex, blobsInSnapshot);\n+\n+        // TODO would be great to test when the index is frozen\n+    }\n+\n+    /**\n+     * @return a {@link Client} that can be used to query the blob store cache system index\n+     */\n+    private Client systemClient() {\n+        return new OriginSettingClient(client(), ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN);\n+    }\n+\n+    private void refreshSystemIndex() {\n+        final RefreshResponse refreshResponse = systemClient().admin().indices().prepareRefresh(SNAPSHOT_BLOB_CACHE_INDEX).get();\n+        assertThat(refreshResponse.getSuccessfulShards(), greaterThan(0));\n+        assertThat(refreshResponse.getFailedShards(), equalTo(0));\n+    }\n+\n+    /**\n+     * Reads a repository location on disk and extracts the list of blobs for each shards\n+     */\n+    private Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot(Path repositoryLocation, String snapshotId) throws IOException {\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsPerShard = new HashMap<>();\n+        Files.walkFileTree(repositoryLocation.resolve(\"indices\"), new SimpleFileVisitor<>() {\n+            @Override\n+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n+                final String fileName = file.getFileName().toString();\n+                if (fileName.equals(\"snap-\" + snapshotId + \".dat\")) {\n+                    blobsPerShard.put(\n+                        String.join(\n+                            \"/\",\n+                            snapshotId,\n+                            file.getParent().getParent().getFileName().toString(),\n+                            file.getParent().getFileName().toString()\n+                        ),\n+                        INDEX_SHARD_SNAPSHOT_FORMAT.deserialize(fileName, xContentRegistry(), Streams.readFully(Files.newInputStream(file)))\n+                    );\n+                }\n+                return FileVisitResult.CONTINUE;\n+            }\n+        });\n+        return Map.copyOf(blobsPerShard);\n+    }\n+\n+    private void ensureExecutorsAreIdle() throws Exception {\n+        assertBusy(() -> {\n+            for (ThreadPool threadPool : internalCluster().getDataNodeInstances(ThreadPool.class)) {\n+                for (String threadPoolName : List.of(CACHE_FETCH_ASYNC_THREAD_POOL_NAME, CACHE_PREWARMING_THREAD_POOL_NAME)) {\n+                    final ThreadPoolExecutor executor = (ThreadPoolExecutor) threadPool.executor(threadPoolName);\n+                    assertThat(threadPoolName, executor.getQueue().size(), equalTo(0));\n+                    assertThat(threadPoolName, executor.getActiveCount(), equalTo(0));\n+                }\n+            }\n+        });\n+    }\n+\n+    private void assertCachedBlobsInSystemIndex(final String repositoryName, final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot)\n+        throws Exception {\n+        assertBusy(() -> {\n+            refreshSystemIndex();\n+\n+            long numberOfCachedBlobs = 0L;\n+            for (Map.Entry<String, BlobStoreIndexShardSnapshot> blob : blobsInSnapshot.entrySet()) {\n+                for (BlobStoreIndexShardSnapshot.FileInfo fileInfo : blob.getValue().indexFiles()) {\n+                    if (fileInfo.name().startsWith(\"__\") == false) {\n+                        continue;\n+                    }\n+\n+                    final String path = String.join(\"/\", repositoryName, blob.getKey(), fileInfo.physicalName());\n+                    if (fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2) {\n+                        // file has been fully cached\n+                        final GetResponse getResponse = systemClient().prepareGet(SNAPSHOT_BLOB_CACHE_INDEX, path + \"/@0\").get();\n+                        assertThat(\"not cached: [\" + path + \"/@0] for blob [\" + fileInfo + \"]\", getResponse.isExists(), is(true));\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(getResponse.getSourceAsMap());\n+                        assertThat(cachedBlob.from(), equalTo(0L));\n+                        assertThat(cachedBlob.to(), equalTo(fileInfo.length()));\n+                        assertThat((long) cachedBlob.length(), equalTo(fileInfo.length()));\n+                        numberOfCachedBlobs += 1;\n+\n+                    } else {\n+                        // first region of file has been cached\n+                        GetResponse getResponse = systemClient().prepareGet(SNAPSHOT_BLOB_CACHE_INDEX, path + \"/@0\").get();\n+                        assertThat(\n+                            \"not cached: [\" + path + \"/@0] for first region of blob [\" + fileInfo + \"]\",\n+                            getResponse.isExists(),\n+                            is(true)\n+                        );\n+\n+                        CachedBlob cachedBlob = CachedBlob.fromSource(getResponse.getSourceAsMap());\n+                        assertThat(cachedBlob.from(), equalTo(0L));\n+                        assertThat(cachedBlob.to(), equalTo((long) BlobStoreCacheService.DEFAULT_SIZE));\n+                        assertThat(cachedBlob.length(), equalTo(BlobStoreCacheService.DEFAULT_SIZE));\n+                        numberOfCachedBlobs += 1;\n+                    }\n+                }\n+            }\n+\n+            refreshSystemIndex();\n+            assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        });\n+    }\n+\n+    private void assertBlobsReadFromRemoteRepository(\n+        final String indexName,\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot\n+    ) {\n+        ensureBlobStoreRepositoriesWithActiveShards(indexName, (nodeId, blobStore) -> {\n+            for (Map.Entry<String, List<Tuple<Long, Long>>> blob : blobStore.blobs.entrySet()) {\n+                final String blobName = blob.getKey();\n+\n+                if (blobName.endsWith(\".dat\") || blobName.equals(\"index-0\")) {\n+                    // The snapshot metadata files are accessed when recovering from the snapshot during restore and do not benefit from\n+                    // the snapshot blob cache as the files are accessed outside of a searchable snapshot directory\n+                    assertThat(\n+                        blobName + \" should be fully read from the beginning\",\n+                        blob.getValue().stream().allMatch(read -> read.v1() == 0L),\n+                        is(true)\n+                    );\n+                    // TODO assert it is read til the end", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3NzA0OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469077048", "bodyText": "I've reworked these assertions now.", "author": "DaveCTurner", "createdAt": "2020-08-12T08:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3MDkwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 69eb78a789d..91c436ed006 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -107,11 +107,11 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n     protected Settings nodeSettings(int nodeOrdinal) {\n         return Settings.builder()\n             .put(super.nodeSettings(nodeOrdinal))\n-            // Use a cache range size setting aligned with BufferedIndexInput's buffer size and BlobStoreCacheService's default size\n-            // TODO randomized this\n             .put(\n                 CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n-                new ByteSizeValue(BlobStoreCacheService.DEFAULT_SIZE, ByteSizeUnit.BYTES)\n+                randomLongBetween(\n+                        new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(),\n+                        new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n             )\n             .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n             .build();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3NDY3NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467774674", "bodyText": "let's remove this field for now", "author": "ywelsch", "createdAt": "2020-08-10T09:06:01Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin().indices().prepareCreate(index).setSettings(settings()).setMapping(mappings()).execute(new ActionListener<>() {\n+                @Override\n+                public void onResponse(CreateIndexResponse createIndexResponse) {\n+                    assert createIndexResponse.index().equals(index);\n+                    listener.onResponse(createIndexResponse.index());\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof ResourceAlreadyExistsException\n+                        || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                        listener.onResponse(index);\n+                    } else {\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings settings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"false\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"accessed_time\");", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzgwMzMxMA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467803310", "bodyText": "++ 2abf126.", "author": "DaveCTurner", "createdAt": "2020-08-10T10:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3NDY3NA=="}], "type": "inlineReview", "revised_code": {"commit": "2abf12671472b3a86002c85d797ccea52a6e7f09", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 45ac3b24fb4..39c49f61bc3 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -135,7 +135,7 @@ public class BlobStoreCacheService extends AbstractLifecycleComponent implements\n             builder.startObject();\n             {\n                 builder.startObject(SINGLE_MAPPING_NAME);\n-                builder.field(\"dynamic\", \"false\");\n+                builder.field(\"dynamic\", \"strict\");\n                 {\n                     builder.startObject(\"_meta\");\n                     builder.field(\"version\", Version.CURRENT);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3ODU1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467778556", "bodyText": "Do we want to put the file during recovery into the node-local cache (at least if it's the full file)? Can be done in follow-up ofc.", "author": "ywelsch", "createdAt": "2020-08-10T09:13:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,221 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n-                        }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+        logger.trace(\"readInternal: read [{}-{}] from [{}]\", position, position + length, this);\n+\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        final List<Tuple<Long, Long>> indexCacheMisses;\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_SIZE);\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_SIZE * 2;\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+                if (cachedBlob != null) {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));", "originalCommit": "1b459aed4a79035dc9366f4ff19118463f745eaf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc5Nzk5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r467797997", "bodyText": "Mmm maybe, but it does introduce a good deal of troublesome interplay with cache eviction and other reads. We would have to acquire the cache file, possibly evicting other files, in order to put the data on disk. We may never need it again after the shard started up too.", "author": "DaveCTurner", "createdAt": "2020-08-10T09:52:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3ODU1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTA3MzIwMA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469073200", "bodyText": "Ok yes we need to do this, or else we start the shard without all this stuff in node-local cache and then have to fetch them on the first search since we ignore the index cache once recovery is finished.", "author": "DaveCTurner", "createdAt": "2020-08-12T07:53:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc3ODU1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 83007333fe9..2ea4f2a9ac7 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -252,28 +251,18 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                         directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content);\n                         return indexCacheMissLength;\n                     }, (channel, from, to, progressUpdater) -> {\n-                        // normally doesn't happen, we're already obtaining a range covering all cache misses above, but this\n-                        // can happen if the real populateAndRead call already failed to obtain this range of the file. In that\n-                        // case, we don't retry, we simply fail to populate the index cache.\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, try and fill just the cache miss from the blob store because there may be other reads waiting on this\n+                        // range.\n                         logger.debug(\n-                            \"failed to fill index cache miss [{}-{}] of {} due to earlier failure\",\n+                            \"directly filling index cache miss [{}-{}] of {} due to earlier failure\",\n                             from,\n                             to,\n                             CachedBlobContainerIndexInput.this\n                         );\n-                        throw new IOException(\n-                            \"failed to fill index cache miss [\"\n-                                + from\n-                                + \"-\"\n-                                + to\n-                                + \"] of [\"\n-                                + CachedBlobContainerIndexInput.this\n-                                + \"] due to earlier failure\"\n-                        );\n-                    },\n-                        EsExecutors.newDirectExecutorService() // if ranges are still missing, fail immediately, so no need to fork\n-                    );\n-\n+                        writeCacheFile(channel, from, to, progressUpdater);\n+                    }, directory.cacheFetchAsyncExecutor());\n                 }\n \n                 final int bytesRead = populateCacheFuture.get();\n"}}, {"oid": "f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "url": "https://github.com/elastic/elasticsearch/commit/f06e0ae49760a3fd12ca5ff271711df2cb12d0fb", "message": "Try to fill the cache miss rather than immediately failing", "committedDate": "2020-08-10T09:41:58Z", "type": "commit"}, {"oid": "c5b65bc4c58eea930382bc93a1fa9600fceeab1f", "url": "https://github.com/elastic/elasticsearch/commit/c5b65bc4c58eea930382bc93a1fa9600fceeab1f", "message": "No need for a list with <= 1 element", "committedDate": "2020-08-10T09:51:28Z", "type": "commit"}, {"oid": "974c09593f29b89546e751c42ad99f2983b1b7a4", "url": "https://github.com/elastic/elasticsearch/commit/974c09593f29b89546e751c42ad99f2983b1b7a4", "message": "Reorder defs", "committedDate": "2020-08-10T09:53:58Z", "type": "commit"}, {"oid": "2abf12671472b3a86002c85d797ccea52a6e7f09", "url": "https://github.com/elastic/elasticsearch/commit/2abf12671472b3a86002c85d797ccea52a6e7f09", "message": "Mapping tweaks", "committedDate": "2020-08-10T10:02:51Z", "type": "commit"}, {"oid": "dad98d8abad891f5945a9887c323382769ed1edc", "url": "https://github.com/elastic/elasticsearch/commit/dad98d8abad891f5945a9887c323382769ed1edc", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-11T08:41:22Z", "type": "commit"}, {"oid": "4c8ed359d350767d1123e095adfebe0fed28dad5", "url": "https://github.com/elastic/elasticsearch/commit/4c8ed359d350767d1123e095adfebe0fed28dad5", "message": "No need to index fields, we only get by ID", "committedDate": "2020-08-11T09:08:06Z", "type": "commit"}, {"oid": "4b6798a73a18c6488503d69a643ef661516632ee", "url": "https://github.com/elastic/elasticsearch/commit/4b6798a73a18c6488503d69a643ef661516632ee", "message": "Comment", "committedDate": "2020-08-11T09:29:06Z", "type": "commit"}, {"oid": "8a3b1742dda99cef89bc7ce2253a86b9b227f1e5", "url": "https://github.com/elastic/elasticsearch/commit/8a3b1742dda99cef89bc7ce2253a86b9b227f1e5", "message": "Rename", "committedDate": "2020-08-11T10:18:26Z", "type": "commit"}, {"oid": "80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "url": "https://github.com/elastic/elasticsearch/commit/80e42ffb0bcc09279332d3b0d5b06a9770dd7041", "message": "Assert no more indexing into cache after first startup", "committedDate": "2020-08-11T11:01:41Z", "type": "commit"}, {"oid": "b052d7e1b15a2405e004eb998d866c898755b5d1", "url": "https://github.com/elastic/elasticsearch/commit/b052d7e1b15a2405e004eb998d866c898755b5d1", "message": "Stronger assertions about what is read from the blob store when the cache is in play", "committedDate": "2020-08-11T16:04:45Z", "type": "commit"}, {"oid": "d4919fad9f2748e9e2c6e979baeaba0151e4c8bd", "url": "https://github.com/elastic/elasticsearch/commit/d4919fad9f2748e9e2c6e979baeaba0151e4c8bd", "message": "No need for a fake blob store when we track accesses in the index input stats instead", "committedDate": "2020-08-11T16:10:14Z", "type": "commit"}, {"oid": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "url": "https://github.com/elastic/elasticsearch/commit/d2632ee1a7c5fc973f9155ea127d86a0d214327e", "message": "Assert no indexing at all after restart", "committedDate": "2020-08-11T16:22:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTI0Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469101247", "bodyText": "A similar case could be made for setting doc_values : false. I wonder though if we should just leave these fields indexed for now, giving us maximum flexibility later to query them whichever way we want. The overhead of indexing them should be fairly small.", "author": "ywelsch", "createdAt": "2020-08-12T08:42:03Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.rest.RestStatus;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.field(\"index\", \"false\");", "originalCommit": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTUyNjQ0NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475526444", "bodyText": "Ok reverted in 4f9584c.", "author": "DaveCTurner", "createdAt": "2020-08-24T11:13:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMTI0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 3a794eafdd4..918f3312994 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -31,6 +31,7 @@ import org.elasticsearch.common.bytes.BytesReference;\n import org.elasticsearch.common.component.AbstractLifecycleComponent;\n import org.elasticsearch.common.settings.Settings;\n import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.TimeValue;\n import org.elasticsearch.common.xcontent.ToXContent;\n import org.elasticsearch.common.xcontent.XContentBuilder;\n import org.elasticsearch.index.IndexNotFoundException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMzkyNQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469103925", "bodyText": "this is no longer needed, right?", "author": "ywelsch", "createdAt": "2020-08-12T08:46:31Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,465 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations", "originalCommit": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTUyNjU0NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475526544", "bodyText": "Right. Removed in cd77a43.", "author": "DaveCTurner", "createdAt": "2020-08-24T11:14:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwMzkyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 406644c7148..915ac07c21c 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -158,6 +158,18 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n         ensureGreen(restoredIndex);\n         ensureExecutorsAreIdle();\n \n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                    SearchableSnapshotsStatsAction.INSTANCE,\n+                    new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n         for (final SearchableSnapshotShardStats shardStats : client().execute(\n             SearchableSnapshotsStatsAction.INSTANCE,\n             new SearchableSnapshotsStatsRequest()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTEwNTkwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469105901", "bodyText": "I wonder if this is a bit brittle perhaps, with these stats being tracked in-memory on the node that is hosting the shard. In particular, it does not survive restarts / relocations, and could also be different on a replica that recovered after the primary already had some docs. I wonder if we should instead query the number of docs in the SNAPSHOT_BLOB_CACHE_INDEX index.", "author": "ywelsch", "createdAt": "2020-08-12T08:49:50Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,465 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.fs.FsRepository;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        // register a new repository that can track blob read operations\n+        assertAcked(client().admin().cluster().prepareDeleteRepository(repositoryName));\n+        createRepository(\n+            repositoryName,\n+            \"fs\",\n+            Settings.builder().put(FsRepository.LOCATION_SETTING.getKey(), repositoryLocation).build(),\n+            false\n+        );\n+        assertBusy(this::ensureClusterStateConsistency);\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        refreshSystemIndex();\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()", "originalCommit": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 406644c7148..915ac07c21c 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -158,6 +158,18 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n         ensureGreen(restoredIndex);\n         ensureExecutorsAreIdle();\n \n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                    SearchableSnapshotsStatsAction.INSTANCE,\n+                    new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n         for (final SearchableSnapshotShardStats shardStats : client().execute(\n             SearchableSnapshotsStatsAction.INSTANCE,\n             new SearchableSnapshotsStatsRequest()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExMDczOA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469110738", "bodyText": "is this still an issue with the latest changes?", "author": "ywelsch", "createdAt": "2020-08-12T08:57:40Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java", "diffHunk": "@@ -793,11 +792,12 @@ private void assertSearchableSnapshotStats(String indexName, boolean cacheEnable\n                             equalTo(0L)\n                         );\n                     } else if (nodeIdsWithLargeEnoughCache.contains(stats.getShardRouting().currentNodeId())) {\n-                        assertThat(\n-                            \"Expected at least 1 cache read or write for \" + fileName + \" of shard \" + shardRouting,\n-                            Math.max(indexInputStats.getCachedBytesRead().getCount(), indexInputStats.getCachedBytesWritten().getCount()),\n-                            greaterThan(0L)\n-                        );\n+                        // not necessarily, it may have been entirely in blob cache TODO improve stats to handle this", "originalCommit": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTUyNjY1MA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475526650", "bodyText": "Stats look ok now I think", "author": "DaveCTurner", "createdAt": "2020-08-24T11:14:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExMDczOA=="}], "type": "inlineReview", "revised_code": {"commit": "4b01c9a01fad72570499103a843554bf2a2bf360", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java\nindex 9fad7da28c6..fe55601badf 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotsIntegTests.java\n\n@@ -792,12 +792,11 @@ public class SearchableSnapshotsIntegTests extends BaseSearchableSnapshotsIntegT\n                             equalTo(0L)\n                         );\n                     } else if (nodeIdsWithLargeEnoughCache.contains(stats.getShardRouting().currentNodeId())) {\n-                        // not necessarily, it may have been entirely in blob cache TODO improve stats to handle this\n-                        // assertThat(\n-                        // \"Expected at least 1 cache read or write for \" + fileName + \" of shard \" + shardRouting,\n-                        // Math.max(indexInputStats.getCachedBytesRead().getCount(), indexInputStats.getCachedBytesWritten().getCount()),\n-                        // greaterThan(0L)\n-                        // );\n+                        assertThat(\n+                            \"Expected no bytes requested from blob store for \" + fileName + \" of shard \" + shardRouting,\n+                            indexInputStats.getBlobStoreBytesRequested().getCount(),\n+                            equalTo(0L)\n+                        );\n                         assertThat(\n                             \"Expected no optimized read for \" + fileName + \" of shard \" + shardRouting,\n                             indexInputStats.getOptimizedBytesRead().getCount(),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExODU5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r469118593", "bodyText": "Do we still want to keep this check around? Given that we now always open a CacheFile, should we always go with that first, and then make use of this fallback instead?", "author": "ywelsch", "createdAt": "2020-08-12T09:11:26Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +146,260 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {", "originalCommit": "d2632ee1a7c5fc973f9155ea127d86a0d214327e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU2NjIxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475566211", "bodyText": "I've gone back and forth on this a few times, but now think this is a better idea, see b50d158. If we can serve the data from disk we do so; if we can't then we try the cache index, and finally fall back on the blob store, regardless of the phase of the shard's life.\nIn the case of no evictions it doesn't make much difference (we never have the data on disk during recovery) but I think this flow will work better for evicted data.", "author": "DaveCTurner", "createdAt": "2020-08-24T12:35:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTExODU5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex fd09330a7c4..f03c3a042ca 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -281,6 +282,7 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n                 }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n \n                 if (indexCacheMiss != null) {\n+                    final Releasable onCacheFillComplete = stats.addIndexCacheFill();\n                     cacheFile.populateAndRead(indexCacheMiss, indexCacheMiss, channel -> {\n                         final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n \n"}}, {"oid": "4df37d6ba01603841ff56bc31617b73e21bffa8d", "url": "https://github.com/elastic/elasticsearch/commit/4df37d6ba01603841ff56bc31617b73e21bffa8d", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-14T08:47:28Z", "type": "commit"}, {"oid": "24f646f8a0ba141ebebefac1cc719ffffdafb895", "url": "https://github.com/elastic/elasticsearch/commit/24f646f8a0ba141ebebefac1cc719ffffdafb895", "message": "Block test until cache fills are complete", "committedDate": "2020-08-14T11:08:59Z", "type": "commit"}, {"oid": "5e752dd926e663253929ce257faf9d64f90e100b", "url": "https://github.com/elastic/elasticsearch/commit/5e752dd926e663253929ce257faf9d64f90e100b", "message": "Add debugging", "committedDate": "2020-08-14T11:09:54Z", "type": "commit"}, {"oid": "5502545ca55441940e0d5961160d2d274deaaaaa", "url": "https://github.com/elastic/elasticsearch/commit/5502545ca55441940e0d5961160d2d274deaaaaa", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-18T11:29:49Z", "type": "commit"}, {"oid": "61701fef9ed4409cedb48f5911c461cd0d57f421", "url": "https://github.com/elastic/elasticsearch/commit/61701fef9ed4409cedb48f5911c461cd0d57f421", "message": "Revert", "committedDate": "2020-08-18T11:32:34Z", "type": "commit"}, {"oid": "f5948c4dfb43fb280b0897dab9f5d2c944f299a3", "url": "https://github.com/elastic/elasticsearch/commit/f5948c4dfb43fb280b0897dab9f5d2c944f299a3", "message": "Moar revert", "committedDate": "2020-08-18T11:39:22Z", "type": "commit"}, {"oid": "51a81ac2fe9a57dffc10eab1d801e8384d6ace47", "url": "https://github.com/elastic/elasticsearch/commit/51a81ac2fe9a57dffc10eab1d801e8384d6ace47", "message": "Revert", "committedDate": "2020-08-18T13:01:35Z", "type": "commit"}, {"oid": "7116f9ff990ae3a742cd6141063922c4eaf403df", "url": "https://github.com/elastic/elasticsearch/commit/7116f9ff990ae3a742cd6141063922c4eaf403df", "message": "Revert buffer size increase", "committedDate": "2020-08-18T15:47:44Z", "type": "commit"}, {"oid": "4b01c9a01fad72570499103a843554bf2a2bf360", "url": "https://github.com/elastic/elasticsearch/commit/4b01c9a01fad72570499103a843554bf2a2bf360", "message": "Better assertion", "committedDate": "2020-08-18T16:01:03Z", "type": "commit"}, {"oid": "d3d58cd731d97ec1326f643cb8aacd18ee1d4008", "url": "https://github.com/elastic/elasticsearch/commit/d3d58cd731d97ec1326f643cb8aacd18ee1d4008", "message": "Distinguish definite cache miss from cache-not-ready", "committedDate": "2020-08-18T16:12:44Z", "type": "commit"}, {"oid": "0bcabe126b44146670d7945fc586ef8bd0eb43f2", "url": "https://github.com/elastic/elasticsearch/commit/0bcabe126b44146670d7945fc586ef8bd0eb43f2", "message": "Precommit", "committedDate": "2020-08-18T16:14:57Z", "type": "commit"}, {"oid": "43f281ae86ec0d731480c5093eb8db079d06e084", "url": "https://github.com/elastic/elasticsearch/commit/43f281ae86ec0d731480c5093eb8db079d06e084", "message": "Test bug", "committedDate": "2020-08-18T16:24:36Z", "type": "commit"}, {"oid": "78680bebde572afcd0a77b976ef1489c773afc3d", "url": "https://github.com/elastic/elasticsearch/commit/78680bebde572afcd0a77b976ef1489c773afc3d", "message": "Track reads from index cache too", "committedDate": "2020-08-18T16:41:46Z", "type": "commit"}, {"oid": "c50dfa6dbc431bb169b3113d2ba41892d4daf832", "url": "https://github.com/elastic/elasticsearch/commit/c50dfa6dbc431bb169b3113d2ba41892d4daf832", "message": "Retry on INFE", "committedDate": "2020-08-19T07:03:12Z", "type": "commit"}, {"oid": "d6ef9cd76017046a60a2513d784733b866a681ef", "url": "https://github.com/elastic/elasticsearch/commit/d6ef9cd76017046a60a2513d784733b866a681ef", "message": "Always put a missing cache entry even if cache not ready", "committedDate": "2020-08-20T11:47:09Z", "type": "commit"}, {"oid": "03a575e7c92092925c10a3050ab248a3ab7dfb12", "url": "https://github.com/elastic/elasticsearch/commit/03a575e7c92092925c10a3050ab248a3ab7dfb12", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-20T11:51:25Z", "type": "commit"}, {"oid": "4f9584c07fcc840e4f66505f6173d092de4c2b23", "url": "https://github.com/elastic/elasticsearch/commit/4f9584c07fcc840e4f66505f6173d092de4c2b23", "message": "Let's keep indexing things for now", "committedDate": "2020-08-20T11:52:55Z", "type": "commit"}, {"oid": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "url": "https://github.com/elastic/elasticsearch/commit/cd77a43bce70607c961bbb9fd8e3ce6240da2304", "message": "Unnecessary repo", "committedDate": "2020-08-20T15:05:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0NDczMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474544733", "bodyText": "AFAICS, we allocate a 1KB array here, I wonder if we should use ByteArrayDataOutput instead since we know the exact result size? Alternatively limit the size of the allocation.\nMight not be important, at this time I am not sure of the frequency this would be called.", "author": "henningandersen", "createdAt": "2020-08-21T08:55:34Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java", "diffHunk": "@@ -131,14 +131,18 @@ private static void ensureReadOnceChecksumContext(IOContext context) {\n      * @throws IOException if something goes wrong when creating the {@link ChecksumBlobContainerIndexInput}\n      */\n     public static ChecksumBlobContainerIndexInput create(String name, long length, String checksum, IOContext context) throws IOException {\n+        return new ChecksumBlobContainerIndexInput(name, length, checksumToBytesArray(checksum), context);\n+    }\n+\n+    public static byte[] checksumToBytesArray(String checksum) throws IOException {\n         final ByteBuffersDataOutput out = new ByteBuffersDataOutput();", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU2NjUwMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475566503", "bodyText": "I don't think it's very important, but yes we can do better here (and it's shorter too), see 1c38aaa.", "author": "DaveCTurner", "createdAt": "2020-08-24T12:35:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0NDczMw=="}], "type": "inlineReview", "revised_code": {"commit": "1c38aaa0e052ca06d7aeda7e7e073ffd353be0b8", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java\nindex af9c6e69813..d0b7b1ebb2e 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/checksum/ChecksumBlobContainerIndexInput.java\n\n@@ -135,14 +133,13 @@ public class ChecksumBlobContainerIndexInput extends IndexInput {\n     }\n \n     public static byte[] checksumToBytesArray(String checksum) throws IOException {\n-        final ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n-        try (IndexOutput output = new ByteBuffersIndexOutput(out, \"footerChecksumToBytesArray\", \"tmp\")) {\n-            // reverse CodecUtil.writeFooter()\n-            output.writeInt(CodecUtil.FOOTER_MAGIC);\n-            output.writeInt(0);\n-            output.writeLong(Long.parseLong(checksum, Character.MAX_RADIX));\n-            output.close();\n-            return out.toArrayCopy();\n-        }\n+        final byte[] result = new byte[CodecUtil.footerLength()];\n+        final ByteArrayDataOutput output = new ByteArrayDataOutput(result);\n+        // reverse CodecUtil.writeFooter()\n+        output.writeInt(CodecUtil.FOOTER_MAGIC);\n+        output.writeInt(0);\n+        output.writeLong(Long.parseLong(checksum, Character.MAX_RADIX));\n+        assert output.getPosition() == result.length;\n+        return result;\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474550324", "bodyText": "I wonder if this introduces a risk of deadlock if all GET threads end here?", "author": "henningandersen", "createdAt": "2020-08-21T09:01:41Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzMjc1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475432752", "bodyText": "On further reflection, this is likely only an issue if system indices utilize searchable snapshots. Cannot think of a case where that makes sense now, so we could perhaps guard against system indices using searchable snapshots (if we do not do that already)?", "author": "henningandersen", "createdAt": "2020-08-24T08:42:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM2MTg4MA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r476361880", "bodyText": "We talked about this via another channel and we agreed that there could be bad situations in the case of system indices using searchable snapshots. Elasticsearch expects system indices to be writable and searchable snapshots indices are read-only by nature; this can lead to malfunctional features. Similarly system and hidden indices can be snapshotted but we want to prevent snapshot to be mounted as system indices like .security or .snapshot-blob-cache.\nI opened #61517 to forbid mounting system indices.\nWe also agreed on improving the timeouts around gets/puts in snapshot blob cache to allow recoveries to move forward even if the snapshot blob cache index is unavailable.", "author": "tlrx", "createdAt": "2020-08-25T11:01:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA4ODE1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477088152", "bodyText": "A possible alternative idea is to bypass the cache index for indices on which the GET would run on the SYSTEM_READ threadpool. This would let us support them as searchable snapshots, just without the recovery speed advantage of the cache; maybe we can recommend running them with replicas so as not to need the faster startup anyway.\nAdding a timeout is relatively easy, the only question remains how long should it be? What do you think about 5s?", "author": "DaveCTurner", "createdAt": "2020-08-26T07:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwOTMwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477109301", "bodyText": "A possible alternative idea is to bypass the cache index for indices on which the GET would run on the SYSTEM_READ threadpool.\n\nThat's a interesting idea and I did not know about this new thread pool. This would work I think, but I'm struggling finding a good reason to allow system indices being mounted as searchable snapshots whereas I can see reasons to prevent them being slowish and read-only. I think #61517 makes sense today (with comments made there) and this is something we can revisit later.\n\nWhat do you think about 5s?\n\nAnything below 10s looks good to me.", "author": "tlrx", "createdAt": "2020-08-26T08:00:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyMzE1OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477223158", "bodyText": "Bit of a wild goose chase checking this one. We could be on a GET thread and I suddenly worried that we also use a GET thread for reading from the cache index, which would still cause a deadlock:\n\n  \n    \n      elasticsearch/server/src/main/java/org/elasticsearch/action/get/TransportGetAction.java\n    \n    \n        Lines 53 to 54\n      in\n      8c51fc7\n    \n    \n    \n    \n\n        \n          \n           super(GetAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver, \n        \n\n        \n          \n                   GetRequest::new, ThreadPool.Names.GET); \n        \n    \n  \n\n\nBut then I saw that we have special handling for gets against system indices:\n\n  \n    \n      elasticsearch/server/src/main/java/org/elasticsearch/action/get/TransportGetAction.java\n    \n    \n        Lines 117 to 126\n      in\n      8c51fc7\n    \n    \n    \n    \n\n        \n          \n           protected String getExecutor(GetRequest request, ShardId shardId) { \n        \n\n        \n          \n               final ClusterState clusterState = clusterService.state(); \n        \n\n        \n          \n               if (clusterState.metadata().index(shardId.getIndex()).isSystem()) { \n        \n\n        \n          \n                   return ThreadPool.Names.SYSTEM_READ; \n        \n\n        \n          \n               } else if (indicesService.indexServiceSafe(shardId.getIndex()).getIndexSettings().isSearchThrottled()) { \n        \n\n        \n          \n                   return ThreadPool.Names.SEARCH_THROTTLED; \n        \n\n        \n          \n               } else { \n        \n\n        \n          \n                   return super.getExecutor(request, shardId); \n        \n\n        \n          \n               } \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nThe constructor parameter suggests that we might first dispatch to a GET thread, and then dispatch again to the result of getExecutor, but in fact we don't we handle the transport action on SAME and then dispatch once. So it's all ok.\nI added a 5-second timeout and some logging in eb1a08e.", "author": "DaveCTurner", "createdAt": "2020-08-26T11:17:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1MDMyNA=="}], "type": "inlineReview", "revised_code": {"commit": "0fbb71f14c73e5120420329519fe4eecfa814638", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..1518b65eb7f 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1NTkyNw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474555927", "bodyText": "I think we would see a remote wrapper exception here? Perhaps we can use TransportActions.isShardNotAvailableException?", "author": "henningandersen", "createdAt": "2020-08-21T09:07:45Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU2NjkwNg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475566906", "bodyText": "Yes, and we can also expect a ConnectTransportException and I think a NodeClosedException too. Extended this in 7bd0e47 and added an assertion that these are the only possibilities.", "author": "DaveCTurner", "createdAt": "2020-08-24T12:36:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1NTkyNw=="}], "type": "inlineReview", "revised_code": {"commit": "0fbb71f14c73e5120420329519fe4eecfa814638", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..1518b65eb7f 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1OTg0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474559841", "bodyText": "I am torn on this. If the cause is a bug, we would like to see it to surface it. But in any failure that is not a bug, I think we would like to assume the cache is unavailable?\nFor instance a network issue to the node holding the shard, I think that would fall out here? And in that case, I would prefer to warn and move on.", "author": "henningandersen", "createdAt": "2020-08-21T09:12:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU2NzUyMA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475567520", "bodyText": "See 7bd0e47, we now treat a network issue as something we expect, not even worthy of a warning, but we also now have an assert false to hopefully expose any bugs.", "author": "DaveCTurner", "createdAt": "2020-08-24T12:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU1OTg0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0fbb71f14c73e5120420329519fe4eecfa814638", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..1518b65eb7f 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4MTkyMA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474581920", "bodyText": "Should we also stat the time spent reading from the cache?", "author": "henningandersen", "createdAt": "2020-08-21T09:36:41Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n+                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n+                    // {start, end} ranges where positions are relative to the whole file.\n+                    if (canBeFullyCached) {\n+                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                    } else {\n+                        // the index input is too large to fully cache, so just cache the initial range\n+                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                    }\n+\n+                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                } else {\n+                    logger.trace(\n+                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n+                        length,\n+                        fileInfo.physicalName(),\n+                        position\n+                    );\n+                    stats.addIndexCacheBytesRead(cachedBlob.length());", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU4NzQ2NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475587464", "bodyText": "Sure; didn't need the timings for tests but it might be useful in future, see d53443e.", "author": "DaveCTurner", "createdAt": "2020-08-24T13:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4MTkyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjMxODY4OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r476318688", "bodyText": "I've reverted this change because it interferes with other assertions about how the stats are collected when we are concurrently fetching and reading data. It's probably fixable in a followup but since there's no immediate need for these timings I won't do it here.", "author": "DaveCTurner", "createdAt": "2020-08-25T09:41:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4MTkyMA=="}], "type": "inlineReview", "revised_code": {"commit": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 66a86ff3784..f5727e25f65 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -160,46 +161,77 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n \n         logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n \n-        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n-\n-        // We prefer to use the index cache if the recovery is not done yet\n-        if (directory.isRecoveryDone() == false) {\n-            // We try to use the snapshot blob cache if:\n-            // - the file is small enough to be fully cached in the blob cache\n-            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n-            // - we're reading the first N bytes of the file\n-            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-\n-            if (canBeFullyCached || isStartOfFile) {\n-                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n-\n-                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n-                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n-                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n-                    // {start, end} ranges where positions are relative to the whole file.\n-                    if (canBeFullyCached) {\n-                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n-                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                if (cacheFile.getAbsentRangeWithin(position, position + length) == null) {\n+                    final Tuple<Long, Long> targetRange = Tuple.tuple(position, position + length);\n+                    cacheFile.populateAndRead(targetRange, targetRange, channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        final String message = \"range [\"\n+                            + from\n+                            + \"-\"\n+                            + to\n+                            + \"] ([\"\n+                            + (to - from)\n+                            + \"] bytes) unexpectedly missing when reading [\"\n+                            + position\n+                            + \"-\"\n+                            + (position + length)\n+                            + \"] from \"\n+                            + CachedBlobContainerIndexInput.this;\n+                        assert false : message;\n+                        throw new IllegalStateException(message);\n+                    }, EsExecutors.newDirectExecutorService()); // never used\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                        } else {\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                        }\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n                     } else {\n-                        // the index input is too large to fully cache, so just cache the initial range\n-                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-                    }\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n \n-                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n-                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n-                } else {\n-                    logger.trace(\n-                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n-                        length,\n-                        fileInfo.physicalName(),\n-                        position\n-                    );\n-                    stats.addIndexCacheBytesRead(cachedBlob.length());\n-                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                        // TODO don't call toBytes here\n+                        b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n \n-                    try {\n-                        final CacheFile cacheFile = getCacheFileSafe();\n-                        try (Releasable ignored = cacheFile.fileLock()) {\n+                        try {\n                             final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n                             cacheFile.populateAndRead(\n                                 cachedRange,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYxNTk1MA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474615950", "bodyText": "I think we need to delegate this failure to the listener too to decrement the current index cache fills stats (and to keep the principle of listener always being notified).", "author": "henningandersen", "createdAt": "2020-08-21T10:33:05Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                    client.index(request, new ActionListener<>() {\n+                        @Override\n+                        public void onResponse(IndexResponse indexResponse) {\n+                            logger.trace(\"cache fill ({}): [{}]\", indexResponse.status(), request.id());\n+                            listener.onResponse(null);\n+                        }\n+\n+                        @Override\n+                        public void onFailure(Exception e) {\n+                            logger.debug(new ParameterizedMessage(\"failure in cache fill: [{}]\", request.id()), e);\n+                            listener.onFailure(e);\n+                        }\n+                    });\n+                } catch (IOException e) {\n+                    logger.warn(\n+                        new ParameterizedMessage(\"cache fill failure: [{}]\", CachedBlob.generateId(repository, name, path, offset)),\n+                        e\n+                    );\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.error(() -> new ParameterizedMessage(\"failed to create blob cache system index [{}]\", index), e);", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU4NzY4OQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475587689", "bodyText": "Ah yes, good catch. Fixed in 7e6eb53.", "author": "DaveCTurner", "createdAt": "2020-08-24T13:09:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYxNTk1MA=="}], "type": "inlineReview", "revised_code": {"commit": "0fbb71f14c73e5120420329519fe4eecfa814638", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..1518b65eb7f 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDUxNjg2Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474516867", "bodyText": "nit: inline in the previous assertion?", "author": "fcofdez", "createdAt": "2020-08-21T08:25:08Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/SearchableSnapshotDirectory.java", "diffHunk": "@@ -585,6 +614,17 @@ public static SearchableSnapshotDirectory unwrapDirectory(Directory dir) {\n         return null;\n     }\n \n+    public CachedBlob getCachedBlob(String name, long offset, int length) {\n+        final CachedBlob cachedBlob = blobStoreCacheService.get(repository, name, blobStoreCachePath, offset);\n+        assert cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY || cachedBlob.from() <= offset;\n+        assert cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY || offset + length <= cachedBlob.to();", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDUxOTM3Ng==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474519376", "bodyText": "I guess that it is unlikely CodecUtil.footerLength() will change in future versions, right?", "author": "fcofdez", "createdAt": "2020-08-21T08:28:03Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5NDUwMQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475594501", "bodyText": "It will certainly cause problems, however I added an assertion in 1c38aaa and another in 299443f which will catch this.", "author": "DaveCTurner", "createdAt": "2020-08-24T13:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDUxOTM3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 66a86ff3784..f5727e25f65 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -160,46 +161,77 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n \n         logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n \n-        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n-\n-        // We prefer to use the index cache if the recovery is not done yet\n-        if (directory.isRecoveryDone() == false) {\n-            // We try to use the snapshot blob cache if:\n-            // - the file is small enough to be fully cached in the blob cache\n-            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n-            // - we're reading the first N bytes of the file\n-            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-\n-            if (canBeFullyCached || isStartOfFile) {\n-                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n-\n-                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n-                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n-                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n-                    // {start, end} ranges where positions are relative to the whole file.\n-                    if (canBeFullyCached) {\n-                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n-                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                if (cacheFile.getAbsentRangeWithin(position, position + length) == null) {\n+                    final Tuple<Long, Long> targetRange = Tuple.tuple(position, position + length);\n+                    cacheFile.populateAndRead(targetRange, targetRange, channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        final String message = \"range [\"\n+                            + from\n+                            + \"-\"\n+                            + to\n+                            + \"] ([\"\n+                            + (to - from)\n+                            + \"] bytes) unexpectedly missing when reading [\"\n+                            + position\n+                            + \"-\"\n+                            + (position + length)\n+                            + \"] from \"\n+                            + CachedBlobContainerIndexInput.this;\n+                        assert false : message;\n+                        throw new IllegalStateException(message);\n+                    }, EsExecutors.newDirectExecutorService()); // never used\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                        } else {\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                        }\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n                     } else {\n-                        // the index input is too large to fully cache, so just cache the initial range\n-                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-                    }\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n \n-                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n-                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n-                } else {\n-                    logger.trace(\n-                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n-                        length,\n-                        fileInfo.physicalName(),\n-                        position\n-                    );\n-                    stats.addIndexCacheBytesRead(cachedBlob.length());\n-                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                        // TODO don't call toBytes here\n+                        b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n \n-                    try {\n-                        final CacheFile cacheFile = getCacheFileSafe();\n-                        try (Releasable ignored = cacheFile.fileLock()) {\n+                        try {\n                             final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n                             cacheFile.populateAndRead(\n                                 cachedRange,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MDc2OA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474590768", "bodyText": "I think we aren't handling the exceptions for this call, is that ok? maybe we should treat them as a CACHE_NOT_READY?", "author": "fcofdez", "createdAt": "2020-08-21T09:46:20Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {\n+            // We try to use the snapshot blob cache if:\n+            // - the file is small enough to be fully cached in the blob cache\n+            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+            // - we're reading the first N bytes of the file\n+            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+            if (canBeFullyCached || isStartOfFile) {\n+                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5NDgyMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475594823", "bodyText": "Some change since you last looked has meant that getCachedBlob doesn't return an error any more (at least not unless there's a bug)", "author": "DaveCTurner", "createdAt": "2020-08-24T13:21:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MDc2OA=="}], "type": "inlineReview", "revised_code": {"commit": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 66a86ff3784..f5727e25f65 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -160,46 +161,77 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n \n         logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n \n-        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n-\n-        // We prefer to use the index cache if the recovery is not done yet\n-        if (directory.isRecoveryDone() == false) {\n-            // We try to use the snapshot blob cache if:\n-            // - the file is small enough to be fully cached in the blob cache\n-            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n-            // - we're reading the first N bytes of the file\n-            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-\n-            if (canBeFullyCached || isStartOfFile) {\n-                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n-\n-                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n-                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n-                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n-                    // {start, end} ranges where positions are relative to the whole file.\n-                    if (canBeFullyCached) {\n-                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n-                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                if (cacheFile.getAbsentRangeWithin(position, position + length) == null) {\n+                    final Tuple<Long, Long> targetRange = Tuple.tuple(position, position + length);\n+                    cacheFile.populateAndRead(targetRange, targetRange, channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        final String message = \"range [\"\n+                            + from\n+                            + \"-\"\n+                            + to\n+                            + \"] ([\"\n+                            + (to - from)\n+                            + \"] bytes) unexpectedly missing when reading [\"\n+                            + position\n+                            + \"-\"\n+                            + (position + length)\n+                            + \"] from \"\n+                            + CachedBlobContainerIndexInput.this;\n+                        assert false : message;\n+                        throw new IllegalStateException(message);\n+                    }, EsExecutors.newDirectExecutorService()); // never used\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                        } else {\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                        }\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n                     } else {\n-                        // the index input is too large to fully cache, so just cache the initial range\n-                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-                    }\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n \n-                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n-                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n-                } else {\n-                    logger.trace(\n-                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n-                        length,\n-                        fileInfo.physicalName(),\n-                        position\n-                    );\n-                    stats.addIndexCacheBytesRead(cachedBlob.length());\n-                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                        // TODO don't call toBytes here\n+                        b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n \n-                    try {\n-                        final CacheFile cacheFile = getCacheFileSafe();\n-                        try (Releasable ignored = cacheFile.fileLock()) {\n+                        try {\n                             final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n                             cacheFile.populateAndRead(\n                                 cachedRange,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MTQ1Nw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474591457", "bodyText": "Currently If the pre-warm phase fails, recovery won't move to DONE, I guess that's ok?", "author": "fcofdez", "createdAt": "2020-08-21T09:47:09Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,276 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n-                        } else {\n-                            read = readCacheFile(channel, pos, b);\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+        // We prefer to use the index cache if the recovery is not done yet\n+        if (directory.isRecoveryDone() == false) {", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQwMzkxMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r476403913", "bodyText": "Ok, we no longer care about the recovery state as of b50d158 (also removed that machinery entirely in abdbf3c)", "author": "DaveCTurner", "createdAt": "2020-08-25T12:19:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MTQ1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\nindex 66a86ff3784..f5727e25f65 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java\n\n@@ -160,46 +161,77 @@ public class CachedBlobContainerIndexInput extends BaseSearchableSnapshotIndexIn\n \n         logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n \n-        final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n-\n-        // We prefer to use the index cache if the recovery is not done yet\n-        if (directory.isRecoveryDone() == false) {\n-            // We try to use the snapshot blob cache if:\n-            // - the file is small enough to be fully cached in the blob cache\n-            final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n-            // - we're reading the first N bytes of the file\n-            final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-\n-            if (canBeFullyCached || isStartOfFile) {\n-                final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n-\n-                if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n-                    // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested so\n-                    // we compute the regions of the file we would like to have the next time. The regions are expressed as tuples of\n-                    // {start, end} ranges where positions are relative to the whole file.\n-                    if (canBeFullyCached) {\n-                        // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n-                        indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                if (cacheFile.getAbsentRangeWithin(position, position + length) == null) {\n+                    final Tuple<Long, Long> targetRange = Tuple.tuple(position, position + length);\n+                    cacheFile.populateAndRead(targetRange, targetRange, channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }, (channel, from, to, progressUpdater) -> {\n+                        final String message = \"range [\"\n+                            + from\n+                            + \"-\"\n+                            + to\n+                            + \"] ([\"\n+                            + (to - from)\n+                            + \"] bytes) unexpectedly missing when reading [\"\n+                            + position\n+                            + \"-\"\n+                            + (position + length)\n+                            + \"] from \"\n+                            + CachedBlobContainerIndexInput.this;\n+                        assert false : message;\n+                        throw new IllegalStateException(message);\n+                    }, EsExecutors.newDirectExecutorService()); // never used\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n+                        } else {\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+                        }\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n                     } else {\n-                        // the index input is too large to fully cache, so just cache the initial range\n-                        indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n-                    }\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n \n-                    // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n-                    // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n-                } else {\n-                    logger.trace(\n-                        \"reading [{}] bytes of file [{}] at position [{}] using index cache\",\n-                        length,\n-                        fileInfo.physicalName(),\n-                        position\n-                    );\n-                    stats.addIndexCacheBytesRead(cachedBlob.length());\n-                    b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n+                        // TODO don't call toBytes here\n+                        b.put(BytesReference.toBytes(cachedBlob.bytes().slice(Math.toIntExact(position), length)));\n \n-                    try {\n-                        final CacheFile cacheFile = getCacheFileSafe();\n-                        try (Releasable ignored = cacheFile.fileLock()) {\n+                        try {\n                             final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n                             cacheFile.populateAndRead(\n                                 cachedRange,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzNzk1NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474637954", "bodyText": "This can be prone to races, as we add pre-warm tasks one by one. I've been hit by this in the past \ud83d\ude05 .", "author": "fcofdez", "createdAt": "2020-08-21T11:26:35Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@TestLogging(reason = \"debugging\", value = \"org.elasticsearch.index.store.cache.CachedBlobContainerIndexInput:TRACE,\"\n+    + \"org.elasticsearch.blobstore.cache.BlobStoreCacheService:TRACE\") // TODO remove this before merge\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(0L)\n+        );\n+\n+        // TODO also test when the index is frozen\n+        // TODO also test when prewarming is enabled\n+    }\n+\n+    /**\n+     * @return a {@link Client} that can be used to query the blob store cache system index\n+     */\n+    private Client systemClient() {\n+        return new OriginSettingClient(client(), ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN);\n+    }\n+\n+    private void refreshSystemIndex() {\n+        try {\n+            final RefreshResponse refreshResponse = systemClient().admin().indices().prepareRefresh(SNAPSHOT_BLOB_CACHE_INDEX).get();\n+            assertThat(refreshResponse.getSuccessfulShards(), greaterThan(0));\n+            assertThat(refreshResponse.getFailedShards(), equalTo(0));\n+        } catch (IndexNotFoundException indexNotFoundException) {\n+            throw new AssertionError(\"unexpected\", indexNotFoundException);\n+        }\n+    }\n+\n+    /**\n+     * Reads a repository location on disk and extracts the list of blobs for each shards\n+     */\n+    private Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot(Path repositoryLocation, String snapshotId) throws IOException {\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsPerShard = new HashMap<>();\n+        Files.walkFileTree(repositoryLocation.resolve(\"indices\"), new SimpleFileVisitor<>() {\n+            @Override\n+            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n+                final String fileName = file.getFileName().toString();\n+                if (fileName.equals(\"snap-\" + snapshotId + \".dat\")) {\n+                    blobsPerShard.put(\n+                        String.join(\n+                            \"/\",\n+                            snapshotId,\n+                            file.getParent().getParent().getFileName().toString(),\n+                            file.getParent().getFileName().toString()\n+                        ),\n+                        INDEX_SHARD_SNAPSHOT_FORMAT.deserialize(fileName, xContentRegistry(), Streams.readFully(Files.newInputStream(file)))\n+                    );\n+                }\n+                return FileVisitResult.CONTINUE;\n+            }\n+        });\n+        return Map.copyOf(blobsPerShard);\n+    }\n+\n+    private void ensureExecutorsAreIdle() throws Exception {\n+        assertBusy(() -> {\n+            for (ThreadPool threadPool : internalCluster().getDataNodeInstances(ThreadPool.class)) {\n+                for (String threadPoolName : List.of(CACHE_FETCH_ASYNC_THREAD_POOL_NAME, CACHE_PREWARMING_THREAD_POOL_NAME)) {\n+                    final ThreadPoolExecutor executor = (ThreadPoolExecutor) threadPool.executor(threadPoolName);\n+                    assertThat(threadPoolName, executor.getQueue().size(), equalTo(0));", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQxMDAxOA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r476410018", "bodyText": "Yes I think we don't need this any more (we aren't pre-warming) so I removed it in 38bf697. @tlrx does that look ok to you?", "author": "DaveCTurner", "createdAt": "2020-08-25T12:29:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzNzk1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQxMjYyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r476412629", "bodyText": "It does look OK to me, the main purpose was to wait for cache writes to complete and it now uses stats to do so.", "author": "tlrx", "createdAt": "2020-08-25T12:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzNzk1NA=="}], "type": "inlineReview", "revised_code": {"commit": "38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 8ff601b0c6c..775f089d2db 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -34,7 +34,6 @@ import org.elasticsearch.plugins.Plugin;\n import org.elasticsearch.snapshots.SnapshotId;\n import org.elasticsearch.test.InternalTestCluster;\n import org.elasticsearch.test.junit.annotations.TestLogging;\n-import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.core.ClientHelper;\n import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzOTYzMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474639633", "bodyText": "Can we add a test where BlobStoreCacheService#getCachedBlob fails?", "author": "fcofdez", "createdAt": "2020-08-21T11:30:32Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ThreadPoolExecutor;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_FETCH_ASYNC_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.CACHE_PREWARMING_THREAD_POOL_NAME;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@TestLogging(reason = \"debugging\", value = \"org.elasticsearch.index.store.cache.CachedBlobContainerIndexInput:TRACE,\"\n+    + \"org.elasticsearch.blobstore.cache.BlobStoreCacheService:TRACE\") // TODO remove this before merge\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+        ensureExecutorsAreIdle();\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+        ensureExecutorsAreIdle();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (after restart) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no cached blobs were indexed in system index [{}] after restart\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(0L)\n+        );\n+\n+        // TODO also test when the index is frozen\n+        // TODO also test when prewarming is enabled", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5NTAxMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475595013", "bodyText": "Some change since you last looked has meant that getCachedBlob doesn't return an error any more (at least not unless there's a bug)", "author": "DaveCTurner", "createdAt": "2020-08-24T13:21:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYzOTYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 8ff601b0c6c..775f089d2db 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -34,7 +34,6 @@ import org.elasticsearch.plugins.Plugin;\n import org.elasticsearch.snapshots.SnapshotId;\n import org.elasticsearch.test.InternalTestCluster;\n import org.elasticsearch.test.junit.annotations.TestLogging;\n-import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.core.ClientHelper;\n import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDY0MDkwNw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r474640907", "bodyText": "We're populating this field with Version.CURRENT I think I'm missing where is this useful? I think it would make more sense to populate with the version of the snapshot itself so we can determine if it's possible to read this or not?", "author": "fcofdez", "createdAt": "2020-08-21T11:33:37Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/CachedBlob.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.Base64;\n+import java.util.Map;\n+\n+public class CachedBlob implements ToXContent {\n+\n+    /**\n+     * Sentinel {@link CachedBlob} indicating that searching the cache index returned an error.\n+     */\n+    public static final CachedBlob CACHE_NOT_READY = new CachedBlob(null, null, null, \"CACHE_NOT_READY\", null, BytesArray.EMPTY, 0L, 0L);\n+\n+    /**\n+     * Sentinel {@link CachedBlob} indicating that the cache index definitely did not contain the requested data.\n+     */\n+    public static final CachedBlob CACHE_MISS = new CachedBlob(null, null, null, \"CACHE_MISS\", null, BytesArray.EMPTY, 0L, 0L);\n+\n+    private static final String TYPE = \"blob\";\n+\n+    private final Instant creationTime;\n+    private final Version version;\n+    private final String repository;\n+    private final String name;\n+    private final String path;\n+\n+    private final BytesReference bytes;\n+    private final long from;\n+    private final long to;\n+\n+    public CachedBlob(\n+        Instant creationTime,\n+        Version version,", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5NjYyNA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475596624", "bodyText": "The actual format of blobs in the index is not settled but is relatively easy to change. I can see value in recording the version of Elasticsearch that filled the cache since this should enable BWC should we need to change the format in future.", "author": "DaveCTurner", "createdAt": "2020-08-24T13:24:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDY0MDkwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYxOTcwNw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475619707", "bodyText": "this should enable BWC should we need to change the format in future.\n\nYes, that was the main reason I added this information. The version of the index is already stored in the index.version.created metadata which is set back during restore and never modified.", "author": "tlrx", "createdAt": "2020-08-24T13:48:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDY0MDkwNw=="}], "type": "inlineReview", "revised_code": null}, {"oid": "7aa25d0dc4a406460af0e6e5e03084a784295358", "url": "https://github.com/elastic/elasticsearch/commit/7aa25d0dc4a406460af0e6e5e03084a784295358", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-24T08:47:00Z", "type": "commit"}, {"oid": "1c38aaa0e052ca06d7aeda7e7e073ffd353be0b8", "url": "https://github.com/elastic/elasticsearch/commit/1c38aaa0e052ca06d7aeda7e7e073ffd353be0b8", "message": "Less allocation", "committedDate": "2020-08-24T08:52:29Z", "type": "commit"}, {"oid": "0fbb71f14c73e5120420329519fe4eecfa814638", "url": "https://github.com/elastic/elasticsearch/commit/0fbb71f14c73e5120420329519fe4eecfa814638", "message": "Use TransportActions.isShardNotAvailableException", "committedDate": "2020-08-24T09:22:39Z", "type": "commit"}, {"oid": "7e6eb5323617b9c19e3855a1f92e032d5cadcfad", "url": "https://github.com/elastic/elasticsearch/commit/7e6eb5323617b9c19e3855a1f92e032d5cadcfad", "message": "Always notify listener", "committedDate": "2020-08-24T09:23:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzODA3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475438073", "bodyText": "Should we trigger the listener on this exception too?", "author": "fcofdez", "createdAt": "2020-08-24T08:51:13Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        try {\n+            final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+            client.get(request, new ActionListener<>() {\n+                @Override\n+                public void onResponse(GetResponse response) {\n+                    if (response.isExists()) {\n+                        logger.debug(\"cache hit : [{}]\", request.id());\n+                        assert response.isSourceEmpty() == false;\n+\n+                        final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                        assert response.getId().equals(cachedBlob.generatedId());\n+                        listener.onResponse(cachedBlob);\n+                    } else {\n+                        logger.debug(\"cache miss: [{}]\", request.id());\n+                        listener.onResponse(CachedBlob.CACHE_MISS);\n+                    }\n+                }\n+\n+                @Override\n+                public void onFailure(Exception e) {\n+                    if (e instanceof IndexNotFoundException || e instanceof NoShardAvailableActionException) {\n+                        // In case the blob cache system index got unavailable, we pretend we didn't find a cache entry and we move on.\n+                        // Failing here might bubble up the exception and fail the searchable snapshot shard which is potentially\n+                        // recovering.\n+                        logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+                    } else {\n+                        logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                        listener.onFailure(e);\n+                    }\n+                }\n+            });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    final IndexRequest request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                    client.index(request, new ActionListener<>() {\n+                        @Override\n+                        public void onResponse(IndexResponse indexResponse) {\n+                            logger.trace(\"cache fill ({}): [{}]\", indexResponse.status(), request.id());\n+                            listener.onResponse(null);\n+                        }\n+\n+                        @Override\n+                        public void onFailure(Exception e) {\n+                            logger.debug(new ParameterizedMessage(\"failure in cache fill: [{}]\", request.id()), e);\n+                            listener.onFailure(e);\n+                        }\n+                    });\n+                } catch (IOException e) {\n+                    logger.warn(", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTUyMjMxMw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475522313", "bodyText": "Yes, I think this could be wrapped so that the listener is always called in case of exception\nedit: Henning saw it too earlier #60522 (comment)", "author": "tlrx", "createdAt": "2020-08-24T11:04:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzODA3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0fbb71f14c73e5120420329519fe4eecfa814638", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..1518b65eb7f 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -20,6 +20,7 @@ import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ3NjY3NA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475476674", "bodyText": "Why is this value 4kb? to match the page cache size?", "author": "fcofdez", "createdAt": "2020-08-24T09:43:49Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));", "originalCommit": "cd77a43bce70607c961bbb9fd8e3ce6240da2304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTU5MTY0Ng==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r475591646", "bodyText": "Nothing so scientific I think, we're trying to balance not putting too much data in the index against not needing to hit the blob store too often when restarting a shard.", "author": "DaveCTurner", "createdAt": "2020-08-24T13:16:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ3NjY3NA=="}], "type": "inlineReview", "revised_code": {"commit": "6391d8fa8b77a0746da33cb0e51993bfaf0ba9ed", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex 983a07c84e0..da5b715ecd4 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -13,13 +13,13 @@ import org.elasticsearch.ExceptionsHelper;\n import org.elasticsearch.ResourceAlreadyExistsException;\n import org.elasticsearch.Version;\n import org.elasticsearch.action.ActionListener;\n-import org.elasticsearch.action.NoShardAvailableActionException;\n import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n import org.elasticsearch.action.get.GetRequest;\n import org.elasticsearch.action.get.GetResponse;\n import org.elasticsearch.action.index.IndexRequest;\n import org.elasticsearch.action.index.IndexResponse;\n import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.client.OriginSettingClient;\n import org.elasticsearch.cluster.ClusterChangedEvent;\n"}}, {"oid": "6391d8fa8b77a0746da33cb0e51993bfaf0ba9ed", "url": "https://github.com/elastic/elasticsearch/commit/6391d8fa8b77a0746da33cb0e51993bfaf0ba9ed", "message": "Propagate direct exception directly to caller", "committedDate": "2020-08-24T10:40:53Z", "type": "commit"}, {"oid": "7bd0e47cd9e08d9d614cab9f8dda5e6230bfa29e", "url": "https://github.com/elastic/elasticsearch/commit/7bd0e47cd9e08d9d614cab9f8dda5e6230bfa29e", "message": "Also permit disconnected/not-connected/node-closed exceptions, and assert nothing else happens", "committedDate": "2020-08-24T10:47:09Z", "type": "commit"}, {"oid": "60ceabb49f9f59fb06ef36219e5ab490f17a55b9", "url": "https://github.com/elastic/elasticsearch/commit/60ceabb49f9f59fb06ef36219e5ab490f17a55b9", "message": "Precommit", "committedDate": "2020-08-24T10:56:32Z", "type": "commit"}, {"oid": "b50d1589f983176a3ad597dafb1f05d3c82f9825", "url": "https://github.com/elastic/elasticsearch/commit/b50d1589f983176a3ad597dafb1f05d3c82f9825", "message": "Check the cache index even after recovery complete", "committedDate": "2020-08-24T12:27:22Z", "type": "commit"}, {"oid": "d53443e753bc24bb787298fab97665874e89a384", "url": "https://github.com/elastic/elasticsearch/commit/d53443e753bc24bb787298fab97665874e89a384", "message": "Collect timings for cache index reads", "committedDate": "2020-08-24T13:08:53Z", "type": "commit"}, {"oid": "299443fe3e901c7fb5a10d15c48a04d4ab36865d", "url": "https://github.com/elastic/elasticsearch/commit/299443fe3e901c7fb5a10d15c48a04d4ab36865d", "message": "Assert enough space", "committedDate": "2020-08-24T13:20:45Z", "type": "commit"}, {"oid": "8b607b04ae0febff52635ae213f37a5ad7ee8d5c", "url": "https://github.com/elastic/elasticsearch/commit/8b607b04ae0febff52635ae213f37a5ad7ee8d5c", "message": "Fix bytes-read stats", "committedDate": "2020-08-25T09:31:21Z", "type": "commit"}, {"oid": "dbf6f2f2d327946d5f78f9e9e7095eeed4a4a06f", "url": "https://github.com/elastic/elasticsearch/commit/dbf6f2f2d327946d5f78f9e9e7095eeed4a4a06f", "message": "Revert back to untimed counter", "committedDate": "2020-08-25T09:31:34Z", "type": "commit"}, {"oid": "e8bc5616874e388fcc098e40b6ac5934c3901e38", "url": "https://github.com/elastic/elasticsearch/commit/e8bc5616874e388fcc098e40b6ac5934c3901e38", "message": "Another revert to counter", "committedDate": "2020-08-25T09:57:17Z", "type": "commit"}, {"oid": "1172e68e3b14291e4e56e73368106e390e2372a6", "url": "https://github.com/elastic/elasticsearch/commit/1172e68e3b14291e4e56e73368106e390e2372a6", "message": "Avoid allocation reading cached blob", "committedDate": "2020-08-25T10:40:40Z", "type": "commit"}, {"oid": "181b1f516029fa4cbe168cc55fe58139425a29d3", "url": "https://github.com/elastic/elasticsearch/commit/181b1f516029fa4cbe168cc55fe58139425a29d3", "message": "Allow waiting for a cache file range without committing to filling any gaps", "committedDate": "2020-08-25T11:44:20Z", "type": "commit"}, {"oid": "abdbf3cecdf4548732ef0653a65b105f84d71cab", "url": "https://github.com/elastic/elasticsearch/commit/abdbf3cecdf4548732ef0653a65b105f84d71cab", "message": "No longer need to track whether recovery is done", "committedDate": "2020-08-25T12:16:35Z", "type": "commit"}, {"oid": "38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "url": "https://github.com/elastic/elasticsearch/commit/38bf6976d402dfd7abaaf5dd35375c732c2f2d85", "message": "No need to wait for fetch threads to be idle", "committedDate": "2020-08-25T12:28:53Z", "type": "commit"}, {"oid": "9fbf8ce44185797e090fcd17e808891878ba4ca7", "url": "https://github.com/elastic/elasticsearch/commit/9fbf8ce44185797e090fcd17e808891878ba4ca7", "message": "Delete restored index first to make sure it doesn't re-create the cache index", "committedDate": "2020-08-25T17:43:35Z", "type": "commit"}, {"oid": "ec9cd242c602ce850a5bab8efac252283509ae44", "url": "https://github.com/elastic/elasticsearch/commit/ec9cd242c602ce850a5bab8efac252283509ae44", "message": "Remove test logging", "committedDate": "2020-08-25T17:44:23Z", "type": "commit"}, {"oid": "1535e58f69f6284f8d44ffc98795e10b1453e650", "url": "https://github.com/elastic/elasticsearch/commit/1535e58f69f6284f8d44ffc98795e10b1453e650", "message": "And another one", "committedDate": "2020-08-25T17:54:16Z", "type": "commit"}, {"oid": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "url": "https://github.com/elastic/elasticsearch/commit/790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "message": "Merge branch 'master' into poc-blob-cache", "committedDate": "2020-08-25T19:42:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA5NzczNg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477097736", "bodyText": "Can we update the stats.yml tests with the new fields?", "author": "tlrx", "createdAt": "2020-08-26T07:41:08Z", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/searchablesnapshots/SearchableSnapshotShardStats.java", "diffHunk": "@@ -263,6 +302,7 @@ public XContentBuilder toXContent(XContentBuilder builder, Params params) throws\n                 builder.field(\"contiguous_bytes_read\", getContiguousReads());\n                 builder.field(\"non_contiguous_bytes_read\", getNonContiguousReads());\n                 builder.field(\"cached_bytes_read\", getCachedBytesRead());\n+                builder.field(\"index_cache_bytes_read\", getIndexCacheBytesRead());", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI5MjMxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477292319", "bodyText": "++ see 2003ec0", "author": "DaveCTurner", "createdAt": "2020-08-26T13:17:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA5NzczNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzExODU0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477118545", "bodyText": "I think we should catch any exception on this and calls the listener appropriately (ActionListener.wrap() would do this for us)", "author": "tlrx", "createdAt": "2020-08-26T08:16:02Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.ResourceAlreadyExistsException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;\n+import org.elasticsearch.action.get.GetRequest;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.index.IndexResponse;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.ClusterChangedEvent;\n+import org.elasticsearch.cluster.ClusterStateListener;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.component.AbstractLifecycleComponent;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.xcontent.ToXContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.ConnectTransportException;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;\n+import static org.elasticsearch.index.mapper.MapperService.SINGLE_MAPPING_NAME;\n+import static org.elasticsearch.xpack.core.ClientHelper.SEARCHABLE_SNAPSHOTS_ORIGIN;\n+\n+public class BlobStoreCacheService extends AbstractLifecycleComponent implements ClusterStateListener {\n+\n+    private static final Logger logger = LogManager.getLogger(BlobStoreCacheService.class);\n+\n+    public static final int DEFAULT_CACHED_BLOB_SIZE = Math.toIntExact(ByteSizeUnit.KB.toBytes(4L));\n+\n+    private final ClusterService clusterService;\n+    private final ThreadPool threadPool;\n+    private final AtomicBoolean ready;\n+    private final Client client;\n+    private final String index;\n+\n+    public BlobStoreCacheService(ClusterService clusterService, ThreadPool threadPool, Client client, String index) {\n+        this.client = new OriginSettingClient(client, SEARCHABLE_SNAPSHOTS_ORIGIN);\n+        this.ready = new AtomicBoolean(false);\n+        this.clusterService = clusterService;\n+        this.threadPool = threadPool;\n+        this.index = index;\n+    }\n+\n+    @Override\n+    protected void doStart() {\n+        clusterService.addListener(this);\n+    }\n+\n+    @Override\n+    protected void doStop() {\n+        clusterService.removeListener(this);\n+    }\n+\n+    @Override\n+    protected void doClose() {}\n+\n+    @Override\n+    public void clusterChanged(ClusterChangedEvent event) {\n+        if (lifecycle.started() == false || event.routingTableChanged() == false) {\n+            return;\n+        }\n+        if (event.indexRoutingTableChanged(index)) {\n+            final IndexRoutingTable indexRoutingTable = event.state().routingTable().index(index);\n+            if (indexRoutingTable == null) {\n+                ready.set(false);\n+                return;\n+            }\n+            ready.set(indexRoutingTable.allPrimaryShardsActive());\n+        }\n+    }\n+\n+    private void createIndexIfNecessary(ActionListener<String> listener) {\n+        if (clusterService.state().routingTable().hasIndex(index)) {\n+            listener.onResponse(index);\n+            return;\n+        }\n+        try {\n+            client.admin()\n+                .indices()\n+                .prepareCreate(index)\n+                .setSettings(indexSettings())\n+                .setMapping(mappings())\n+                .execute(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(CreateIndexResponse createIndexResponse) {\n+                        assert createIndexResponse.index().equals(index);\n+                        listener.onResponse(createIndexResponse.index());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        if (e instanceof ResourceAlreadyExistsException\n+                            || ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException) {\n+                            listener.onResponse(index);\n+                        } else {\n+                            listener.onFailure(e);\n+                        }\n+                    }\n+                });\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private static Settings indexSettings() {\n+        return Settings.builder()\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexMetadata.SETTING_AUTO_EXPAND_REPLICAS, \"0-1\")\n+            .put(IndexMetadata.SETTING_PRIORITY, \"900\")\n+            .build();\n+    }\n+\n+    private static XContentBuilder mappings() throws IOException {\n+        final XContentBuilder builder = jsonBuilder();\n+        {\n+            builder.startObject();\n+            {\n+                builder.startObject(SINGLE_MAPPING_NAME);\n+                builder.field(\"dynamic\", \"strict\");\n+                {\n+                    builder.startObject(\"_meta\");\n+                    builder.field(\"version\", Version.CURRENT);\n+                    builder.endObject();\n+                }\n+                {\n+                    builder.startObject(\"properties\");\n+                    {\n+                        builder.startObject(\"type\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"creation_time\");\n+                        builder.field(\"type\", \"date\");\n+                        builder.field(\"format\", \"epoch_millis\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"version\");\n+                        builder.field(\"type\", \"integer\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"repository\");\n+                        builder.field(\"type\", \"keyword\");\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"blob\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"name\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                                builder.startObject(\"path\");\n+                                builder.field(\"type\", \"keyword\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    {\n+                        builder.startObject(\"data\");\n+                        builder.field(\"type\", \"object\");\n+                        {\n+                            builder.startObject(\"properties\");\n+                            {\n+                                builder.startObject(\"content\");\n+                                builder.field(\"type\", \"binary\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"length\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"from\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            {\n+                                builder.startObject(\"to\");\n+                                builder.field(\"type\", \"long\");\n+                                builder.endObject();\n+                            }\n+                            builder.endObject();\n+                        }\n+                        builder.endObject();\n+                    }\n+                    builder.endObject();\n+                }\n+                builder.endObject();\n+            }\n+            builder.endObject();\n+        }\n+        return builder;\n+    }\n+\n+    public CachedBlob get(String repository, String name, String path, long offset) {\n+        final PlainActionFuture<CachedBlob> future = PlainActionFuture.newFuture();\n+        getAsync(repository, name, path, offset, future);\n+        return future.actionGet();\n+    }\n+\n+    protected void getAsync(String repository, String name, String path, long offset, ActionListener<CachedBlob> listener) {\n+        if ((lifecycle.started() && ready.get()) == false) {\n+            // TODO TBD can we just execute the GET request and let it fail if the index isn't ready yet?\n+            // We might get lucky and hit a started shard anyway.\n+            logger.debug(\"not ready : [{}]\", CachedBlob.generateId(repository, name, path, offset));\n+            listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            return;\n+        }\n+        final GetRequest request = new GetRequest(index).id(CachedBlob.generateId(repository, name, path, offset));\n+        client.get(request, new ActionListener<>() {\n+            @Override\n+            public void onResponse(GetResponse response) {\n+                if (response.isExists()) {\n+                    logger.debug(\"cache hit : [{}]\", request.id());\n+                    assert response.isSourceEmpty() == false;\n+\n+                    final CachedBlob cachedBlob = CachedBlob.fromSource(response.getSource());\n+                    assert response.getId().equals(cachedBlob.generatedId());\n+                    listener.onResponse(cachedBlob);\n+                } else {\n+                    logger.debug(\"cache miss: [{}]\", request.id());\n+                    listener.onResponse(CachedBlob.CACHE_MISS);\n+                }\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                // In case the blob cache system index is unavailable, we indicate it's not ready and move on. We do not fail the request:\n+                // a failure here is not fatal since the data exists in the blob store, so we can simply indicate the cache is not ready.\n+                if (isExpectedCacheGetException(e)) {\n+                    logger.debug(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                } else {\n+                    logger.warn(() -> new ParameterizedMessage(\"failed to retrieve cached blob from system index [{}]\", index), e);\n+                    assert false : e;\n+                }\n+                listener.onResponse(CachedBlob.CACHE_NOT_READY);\n+            }\n+        });\n+    }\n+\n+    private static boolean isExpectedCacheGetException(Exception e) {\n+        return TransportActions.isShardNotAvailableException(e)\n+            || e instanceof ConnectTransportException\n+            || ExceptionsHelper.unwrapCause(e) instanceof NodeClosedException;\n+    }\n+\n+    public void putAsync(String repository, String name, String path, long offset, BytesReference content, ActionListener<Void> listener) {\n+        createIndexIfNecessary(new ActionListener<>() {\n+            @Override\n+            public void onResponse(String s) {\n+                final IndexRequest request;\n+                try {\n+                    final CachedBlob cachedBlob = new CachedBlob(\n+                        Instant.ofEpochMilli(threadPool.absoluteTimeInMillis()),\n+                        Version.CURRENT,\n+                        repository,\n+                        name,\n+                        path,\n+                        content,\n+                        offset\n+                    );\n+                    request = new IndexRequest(index).id(cachedBlob.generatedId());\n+                    try (XContentBuilder builder = jsonBuilder()) {\n+                        request.source(cachedBlob.toXContent(builder, ToXContent.EMPTY_PARAMS));\n+                    }\n+                } catch (IOException e) {\n+                    logger.warn(\n+                        new ParameterizedMessage(\"cache fill failure: [{}]\", CachedBlob.generateId(repository, name, path, offset)),\n+                        e\n+                    );\n+                    listener.onFailure(e);\n+                    return;\n+                }\n+\n+                client.index(request, new ActionListener<>() {", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI5MjA4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477292081", "bodyText": "++ see 62fe090.", "author": "DaveCTurner", "createdAt": "2020-08-26T13:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzExODU0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "eb1a08ec99bde821f3a131ef6e684cd1140b15e7", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\nindex ea6ff6d22ba..a8cc669137a 100644\n--- a/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n+++ b/x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/blobstore/cache/BlobStoreCacheService.java\n\n@@ -9,6 +9,7 @@ package org.elasticsearch.blobstore.cache;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.elasticsearch.ElasticsearchTimeoutException;\n import org.elasticsearch.ExceptionsHelper;\n import org.elasticsearch.ResourceAlreadyExistsException;\n import org.elasticsearch.Version;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MDc1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477150753", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n          \n          \n            \n                                        @Override\n          \n          \n            \n                                        public void onResponse(Void response) {\n          \n          \n            \n                                            onCacheFillComplete.close();\n          \n          \n            \n                                        }\n          \n          \n            \n            \n          \n          \n            \n                                        @Override\n          \n          \n            \n                                        public void onFailure(Exception e1) {\n          \n          \n            \n                                            onCacheFillComplete.close();\n          \n          \n            \n                                        }\n          \n          \n            \n                                    });\n          \n          \n            \n                                    directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, ActionListener.wrap(onCacheFillComplete::close));", "author": "tlrx", "createdAt": "2020-08-26T09:07:28Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,296 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                final CompletableFuture<Integer> waitingForRead = cacheFile.readIfAvailableOrPending(\n+                    Tuple.tuple(position, position + length),\n+                    channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }\n+                );\n+\n+                if (waitingForRead != null) {\n+                    final Integer read = waitingForRead.get();\n+                    assert read == length;\n+                    readComplete(position, length);\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n                         } else {\n-                            read = readCacheFile(channel, pos, b);\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n                         }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                    } else {\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n+\n+                        final BytesRefIterator cachedBytesIterator = cachedBlob.bytes().slice(Math.toIntExact(position), length).iterator();\n+                        BytesRef bytesRef;\n+                        while ((bytesRef = cachedBytesIterator.next()) != null) {\n+                            b.put(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n+                        }\n+                        assert b.position() == length : \"copied \" + b.position() + \" but expected \" + length;\n+\n+                        try {\n+                            final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n+                            cacheFile.populateAndRead(\n+                                cachedRange,\n+                                cachedRange,\n+                                channel -> cachedBlob.length(),\n+                                (channel, from, to, progressUpdater) -> {\n+                                    final long startTimeNanos = stats.currentTimeNanos();\n+                                    final BytesRefIterator iterator = cachedBlob.bytes()\n+                                        .slice(Math.toIntExact(from - cachedBlob.from()), Math.toIntExact(to - from))\n+                                        .iterator();\n+                                    long writePosition = from;\n+                                    BytesRef current;\n+                                    while ((current = iterator.next()) != null) {\n+                                        final ByteBuffer byteBuffer = ByteBuffer.wrap(current.bytes, current.offset, current.length);\n+                                        while (byteBuffer.remaining() > 0) {\n+                                            writePosition += positionalWrite(channel, writePosition, byteBuffer);\n+                                            progressUpdater.accept(writePosition);\n+                                        }\n+                                    }\n+                                    assert writePosition == to : writePosition + \" vs \" + to;\n+                                    final long endTimeNanos = stats.currentTimeNanos();\n+                                    stats.addCachedBytesWritten(to - from, endTimeNanos - startTimeNanos);\n+                                    logger.trace(\"copied bytes [{}-{}] of file [{}] from cache index to disk\", from, to, fileInfo);\n+                                },\n+                                directory.cacheFetchAsyncExecutor()\n+                            );\n+                        } catch (Exception e) {\n+                            logger.debug(\n+                                new ParameterizedMessage(\n+                                    \"failed to store bytes [{}-{}] of file [{}] obtained from index cache\",\n+                                    cachedBlob.from(),\n+                                    cachedBlob.to(),\n+                                    fileInfo\n+                                ),\n+                                e\n+                            );\n+                            // oh well, no big deal, at least we can return them to the caller.\n+                        }\n+\n+                        readComplete(position, length);\n+\n+                        return;\n+                    }\n+                } else {\n+                    // requested range is not eligible for caching\n+                    indexCacheMiss = null;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Requested data is also not in the cache index, so we must visit the blob store to satisfy both the target range and any\n+                // miss in the cache index.\n+\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMiss == null ? Long.MAX_VALUE : indexCacheMiss.v1()),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMiss == null ? Long.MIN_VALUE : indexCacheMiss.v2())\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n+                    }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                if (indexCacheMiss != null) {\n+                    final Releasable onCacheFillComplete = stats.addIndexCacheFill();\n+                    final CompletableFuture<Integer> readFuture = cacheFile.readIfAvailableOrPending(indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n+                            @Override\n+                            public void onResponse(Void response) {\n+                                onCacheFillComplete.close();\n+                            }\n+\n+                            @Override\n+                            public void onFailure(Exception e1) {\n+                                onCacheFillComplete.close();\n+                            }\n+                        });", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI0MzY3MA==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477243670", "bodyText": "ActionListener#wrap is trappy, if onResponse throws then it calls onFailure which is effectively a double-notification. Doesn't really matter here, perhaps, but I'd rather not add another usage that needs thought when it comes time to remove it.", "author": "DaveCTurner", "createdAt": "2020-08-26T11:57:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MDc1Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MjA5MQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477152091", "bodyText": "Would it be possible to check that readFuture is effectively done in case of indexCacheMiss not null?", "author": "tlrx", "createdAt": "2020-08-26T09:09:50Z", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -136,53 +148,296 @@ protected void readInternal(ByteBuffer b) throws IOException {\n         final long position = getFilePointer() + this.offset;\n         final int length = b.remaining();\n \n-        int totalBytesRead = 0;\n-        while (totalBytesRead < length) {\n-            final long pos = position + totalBytesRead;\n-            final int len = length - totalBytesRead;\n-            int bytesRead = 0;\n-            try {\n-                final CacheFile cacheFile = getCacheFileSafe();\n-                try (Releasable ignored = cacheFile.fileLock()) {\n-                    final Tuple<Long, Long> rangeToWrite = computeRange(pos);\n-                    final Tuple<Long, Long> rangeToRead = Tuple.tuple(pos, Math.min(pos + len, rangeToWrite.v2()));\n-\n-                    bytesRead = cacheFile.fetchAsync(rangeToWrite, rangeToRead, (channel) -> {\n-                        final int read;\n-                        if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n-                            final ByteBuffer duplicate = b.duplicate();\n-                            duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n-                            read = readCacheFile(channel, pos, duplicate);\n-                            assert duplicate.position() <= b.limit();\n-                            b.position(duplicate.position());\n+        // We can detect that we're going to read the last 16 bytes (that contains the footer checksum) of the file. Such reads are often\n+        // executed when opening a Directory and since we have the checksum in the snapshot metadata we can use it to fill the ByteBuffer.\n+        if (length == CodecUtil.footerLength() && isClone == false && position == fileInfo.length() - length) {\n+            if (readChecksumFromFileInfo(b)) {\n+                logger.trace(\"read footer of file [{}] at position [{}], bypassing all caches\", fileInfo.physicalName(), position);\n+                return;\n+            }\n+            assert b.remaining() == length;\n+        }\n+\n+        logger.trace(\"readInternal: read [{}-{}] ([{}] bytes) from [{}]\", position, position + length, length, this);\n+\n+        try {\n+            final CacheFile cacheFile = getCacheFileSafe();\n+            try (Releasable ignored = cacheFile.fileLock()) {\n+\n+                // Can we serve the read directly from disk? If so, do so and don't worry about anything else.\n+\n+                final CompletableFuture<Integer> waitingForRead = cacheFile.readIfAvailableOrPending(\n+                    Tuple.tuple(position, position + length),\n+                    channel -> {\n+                        final int read = readCacheFile(channel, position, b);\n+                        assert read == length : read + \" vs \" + length;\n+                        return read;\n+                    }\n+                );\n+\n+                if (waitingForRead != null) {\n+                    final Integer read = waitingForRead.get();\n+                    assert read == length;\n+                    readComplete(position, length);\n+                    return;\n+                }\n+\n+                // Requested data is not on disk, so try the cache index next.\n+\n+                final Tuple<Long, Long> indexCacheMiss; // null if not a miss\n+\n+                // We try to use the cache index if:\n+                // - the file is small enough to be fully cached\n+                final boolean canBeFullyCached = fileInfo.length() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2;\n+                // - we're reading the first N bytes of the file\n+                final boolean isStartOfFile = (position + length <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n+\n+                if (canBeFullyCached || isStartOfFile) {\n+                    final CachedBlob cachedBlob = directory.getCachedBlob(fileInfo.physicalName(), 0L, length);\n+\n+                    if (cachedBlob == CachedBlob.CACHE_MISS || cachedBlob == CachedBlob.CACHE_NOT_READY) {\n+                        // We would have liked to find a cached entry but we did not find anything: the cache on the disk will be requested\n+                        // so we compute the region of the file we would like to have the next time. The region is expressed as a tuple of\n+                        // {start, end} where positions are relative to the whole file.\n+\n+                        if (canBeFullyCached) {\n+                            // if the index input is smaller than twice the size of the blob cache, it will be fully indexed\n+                            indexCacheMiss = Tuple.tuple(0L, fileInfo.length());\n                         } else {\n-                            read = readCacheFile(channel, pos, b);\n+                            // the index input is too large to fully cache, so just cache the initial range\n+                            indexCacheMiss = Tuple.tuple(0L, (long) BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE);\n                         }\n-                        return read;\n-                    }, this::writeCacheFile, directory.cacheFetchAsyncExecutor()).get();\n+\n+                        // We must fill in a cache miss even if CACHE_NOT_READY since the cache index is only created on the first put.\n+                        // TODO TBD use a different trigger for creating the cache index and avoid a put in the CACHE_NOT_READY case.\n+                    } else {\n+                        logger.trace(\n+                            \"reading [{}] bytes of file [{}] at position [{}] using cache index\",\n+                            length,\n+                            fileInfo.physicalName(),\n+                            position\n+                        );\n+                        stats.addIndexCacheBytesRead(cachedBlob.length());\n+\n+                        final BytesRefIterator cachedBytesIterator = cachedBlob.bytes().slice(Math.toIntExact(position), length).iterator();\n+                        BytesRef bytesRef;\n+                        while ((bytesRef = cachedBytesIterator.next()) != null) {\n+                            b.put(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n+                        }\n+                        assert b.position() == length : \"copied \" + b.position() + \" but expected \" + length;\n+\n+                        try {\n+                            final Tuple<Long, Long> cachedRange = Tuple.tuple(cachedBlob.from(), cachedBlob.to());\n+                            cacheFile.populateAndRead(\n+                                cachedRange,\n+                                cachedRange,\n+                                channel -> cachedBlob.length(),\n+                                (channel, from, to, progressUpdater) -> {\n+                                    final long startTimeNanos = stats.currentTimeNanos();\n+                                    final BytesRefIterator iterator = cachedBlob.bytes()\n+                                        .slice(Math.toIntExact(from - cachedBlob.from()), Math.toIntExact(to - from))\n+                                        .iterator();\n+                                    long writePosition = from;\n+                                    BytesRef current;\n+                                    while ((current = iterator.next()) != null) {\n+                                        final ByteBuffer byteBuffer = ByteBuffer.wrap(current.bytes, current.offset, current.length);\n+                                        while (byteBuffer.remaining() > 0) {\n+                                            writePosition += positionalWrite(channel, writePosition, byteBuffer);\n+                                            progressUpdater.accept(writePosition);\n+                                        }\n+                                    }\n+                                    assert writePosition == to : writePosition + \" vs \" + to;\n+                                    final long endTimeNanos = stats.currentTimeNanos();\n+                                    stats.addCachedBytesWritten(to - from, endTimeNanos - startTimeNanos);\n+                                    logger.trace(\"copied bytes [{}-{}] of file [{}] from cache index to disk\", from, to, fileInfo);\n+                                },\n+                                directory.cacheFetchAsyncExecutor()\n+                            );\n+                        } catch (Exception e) {\n+                            logger.debug(\n+                                new ParameterizedMessage(\n+                                    \"failed to store bytes [{}-{}] of file [{}] obtained from index cache\",\n+                                    cachedBlob.from(),\n+                                    cachedBlob.to(),\n+                                    fileInfo\n+                                ),\n+                                e\n+                            );\n+                            // oh well, no big deal, at least we can return them to the caller.\n+                        }\n+\n+                        readComplete(position, length);\n+\n+                        return;\n+                    }\n+                } else {\n+                    // requested range is not eligible for caching\n+                    indexCacheMiss = null;\n                 }\n-            } catch (final Exception e) {\n-                if (e instanceof AlreadyClosedException || (e.getCause() != null && e.getCause() instanceof AlreadyClosedException)) {\n-                    try {\n-                        // cache file was evicted during the range fetching, read bytes directly from source\n-                        bytesRead = readDirectly(pos, pos + len, b);\n-                        continue;\n-                    } catch (Exception inner) {\n-                        e.addSuppressed(inner);\n+\n+                // Requested data is also not in the cache index, so we must visit the blob store to satisfy both the target range and any\n+                // miss in the cache index.\n+\n+                final Tuple<Long, Long> startRangeToWrite = computeRange(position);\n+                final Tuple<Long, Long> endRangeToWrite = computeRange(position + length - 1);\n+                assert startRangeToWrite.v2() <= endRangeToWrite.v2() : startRangeToWrite + \" vs \" + endRangeToWrite;\n+                final Tuple<Long, Long> rangeToWrite = Tuple.tuple(\n+                    Math.min(startRangeToWrite.v1(), indexCacheMiss == null ? Long.MAX_VALUE : indexCacheMiss.v1()),\n+                    Math.max(endRangeToWrite.v2(), indexCacheMiss == null ? Long.MIN_VALUE : indexCacheMiss.v2())\n+                );\n+\n+                assert rangeToWrite.v1() <= position && position + length <= rangeToWrite.v2() : \"[\"\n+                    + position\n+                    + \"-\"\n+                    + (position + length)\n+                    + \"] vs \"\n+                    + rangeToWrite;\n+                final Tuple<Long, Long> rangeToRead = Tuple.tuple(position, position + length);\n+\n+                final CompletableFuture<Integer> populateCacheFuture = cacheFile.populateAndRead(rangeToWrite, rangeToRead, channel -> {\n+                    final int read;\n+                    if ((rangeToRead.v2() - rangeToRead.v1()) < b.remaining()) {\n+                        final ByteBuffer duplicate = b.duplicate();\n+                        duplicate.limit(duplicate.position() + Math.toIntExact(rangeToRead.v2() - rangeToRead.v1()));\n+                        read = readCacheFile(channel, position, duplicate);\n+                        assert duplicate.position() <= b.limit();\n+                        b.position(duplicate.position());\n+                    } else {\n+                        read = readCacheFile(channel, position, b);\n+                    }\n+                    return read;\n+                }, this::writeCacheFile, directory.cacheFetchAsyncExecutor());\n+\n+                if (indexCacheMiss != null) {\n+                    final Releasable onCacheFillComplete = stats.addIndexCacheFill();\n+                    final CompletableFuture<Integer> readFuture = cacheFile.readIfAvailableOrPending(indexCacheMiss, channel -> {\n+                        final int indexCacheMissLength = Math.toIntExact(indexCacheMiss.v2() - indexCacheMiss.v1());\n+\n+                        // We assume that we only cache small portions of blobs so that we do not need to:\n+                        // - use a BigArrays for allocation\n+                        // - use an intermediate copy buffer to read the file in sensibly-sized chunks\n+                        // - release the buffer once the indexing operation is complete\n+                        assert indexCacheMissLength <= COPY_BUFFER_SIZE : indexCacheMiss;\n+\n+                        final ByteBuffer byteBuffer = ByteBuffer.allocate(indexCacheMissLength);\n+                        Channels.readFromFileChannelWithEofException(channel, indexCacheMiss.v1(), byteBuffer);\n+                        // NB use Channels.readFromFileChannelWithEofException not readCacheFile() to avoid counting this in the stats\n+                        byteBuffer.flip();\n+                        final BytesReference content = BytesReference.fromByteBuffer(byteBuffer);\n+                        directory.putCachedBlob(fileInfo.physicalName(), indexCacheMiss.v1(), content, new ActionListener<>() {\n+                            @Override\n+                            public void onResponse(Void response) {\n+                                onCacheFillComplete.close();\n+                            }\n+\n+                            @Override\n+                            public void onFailure(Exception e1) {\n+                                onCacheFillComplete.close();\n+                            }\n+                        });\n+                        return indexCacheMissLength;\n+                    });\n+\n+                    if (readFuture == null) {\n+                        // Normally doesn't happen, we're already obtaining a range covering all cache misses above, but theoretically\n+                        // possible in the case that the real populateAndRead call already failed to obtain this range of the file. In that\n+                        // case, simply move on.\n+                        onCacheFillComplete.close();\n                     }\n                 }\n-                throw new IOException(\"Fail to read data from cache\", e);\n \n-            } finally {\n-                totalBytesRead += bytesRead;\n+                final int bytesRead = populateCacheFuture.get();", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI0NTYxMg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477245612", "bodyText": "It is possible but I don't think it's needed. It gets completed by whoever filled in the last piece of the range needed for the cache index, so is still protected from eviction, but it's completed before the call to directory.putCachedBlob returns. If it's completed exceptionally then we don't really care.", "author": "DaveCTurner", "createdAt": "2020-08-26T12:00:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE1MjA5MQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477177945", "bodyText": "\ud83d\udc4d", "author": "tlrx", "createdAt": "2020-08-26T09:49:35Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 55a34197248..2b22fef67e3 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -273,7 +273,6 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n \n         logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n         assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n-        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n         assertHitCount(\n             client().prepareSearch(restoredAgainIndex)\n                 .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4MDI0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/60522#discussion_r477180242", "bodyText": "nit: duplicate", "author": "tlrx", "createdAt": "2020-08-26T09:53:27Z", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java", "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.blobstore.cache;\n+\n+import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeResponse;\n+import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;\n+import org.elasticsearch.action.get.GetResponse;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.client.OriginSettingClient;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexRoutingTable;\n+import org.elasticsearch.cluster.routing.RoutingNode;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;\n+import org.elasticsearch.cluster.routing.allocation.decider.Decision;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.ClusterSettings;\n+import org.elasticsearch.common.settings.Setting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.index.IndexNotFoundException;\n+import org.elasticsearch.index.query.QueryBuilders;\n+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot;\n+import org.elasticsearch.plugins.ClusterPlugin;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.xpack.core.ClientHelper;\n+import org.elasticsearch.xpack.core.searchablesnapshots.SearchableSnapshotShardStats;\n+import org.elasticsearch.xpack.searchablesnapshots.BaseSearchableSnapshotsIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots;\n+import org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsAction;\n+import org.elasticsearch.xpack.searchablesnapshots.action.SearchableSnapshotsStatsRequest;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.io.IOException;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.SimpleFileVisitor;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.INDEX_SHARD_SNAPSHOT_FORMAT;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;\n+import static org.elasticsearch.xpack.searchablesnapshots.SearchableSnapshotsConstants.SNAPSHOT_BLOB_CACHE_INDEX;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        final List<Class<? extends Plugin>> plugins = new ArrayList<>();\n+        plugins.add(WaitForSnapshotBlobCacheShardsActivePlugin.class);\n+        plugins.addAll(super.nodePlugins());\n+        return List.copyOf(plugins);\n+    }\n+\n+    @Override\n+    protected int numberOfReplicas() {\n+        return 0;\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            .put(\n+                CacheService.SNAPSHOT_CACHE_RANGE_SIZE_SETTING.getKey(),\n+                randomLongBetween(new ByteSizeValue(4, ByteSizeUnit.KB).getBytes(), new ByteSizeValue(20, ByteSizeUnit.KB).getBytes()) + \"b\"\n+            )\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(Long.MAX_VALUE, ByteSizeUnit.BYTES))\n+            .build();\n+    }\n+\n+    public void testBlobStoreCache() throws Exception {\n+        final String indexName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        createIndex(indexName);\n+\n+        final List<IndexRequestBuilder> indexRequestBuilders = new ArrayList<>();\n+        for (int i = scaledRandomIntBetween(0, 10_000); i >= 0; i--) {\n+            indexRequestBuilders.add(client().prepareIndex(indexName).setSource(\"text\", randomUnicodeOfLength(10), \"num\", i));\n+        }\n+        indexRandom(true, false, true, indexRequestBuilders);\n+        final long numberOfDocs = indexRequestBuilders.size();\n+        final NumShards numberOfShards = getNumShards(indexName);\n+\n+        if (randomBoolean()) {\n+            logger.info(\"--> force-merging index before snapshotting\");\n+            final ForceMergeResponse forceMergeResponse = client().admin()\n+                .indices()\n+                .prepareForceMerge(indexName)\n+                .setMaxNumSegments(1)\n+                .get();\n+            assertThat(forceMergeResponse.getSuccessfulShards(), equalTo(numberOfShards.totalNumShards));\n+            assertThat(forceMergeResponse.getFailedShards(), equalTo(0));\n+        }\n+\n+        final String repositoryName = randomAlphaOfLength(10).toLowerCase(Locale.ROOT);\n+        final Path repositoryLocation = randomRepoPath();\n+        createFsRepository(repositoryName, repositoryLocation);\n+\n+        final SnapshotId snapshot = createSnapshot(repositoryName, List.of(indexName));\n+        assertAcked(client().admin().indices().prepareDelete(indexName));\n+\n+        // extract the list of blobs per shard from the snapshot directory on disk\n+        final Map<String, BlobStoreIndexShardSnapshot> blobsInSnapshot = blobsInSnapshot(repositoryLocation, snapshot.getUUID());\n+        assertThat(\"Failed to load all shard snapshot metadata files\", blobsInSnapshot.size(), equalTo(numberOfShards.numPrimaries));\n+\n+        expectThrows(\n+            IndexNotFoundException.class,\n+            \".snapshot-blob-cache system index should not be created yet\",\n+            () -> systemClient().admin().indices().prepareGetIndex().addIndices(SNAPSHOT_BLOB_CACHE_INDEX).get()\n+        );\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the first time\", snapshot);\n+        final String restoredIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredIndex);\n+\n+        // wait for all async cache fills to complete\n+        assertBusy(() -> {\n+            for (final SearchableSnapshotShardStats shardStats : client().execute(\n+                SearchableSnapshotsStatsAction.INSTANCE,\n+                new SearchableSnapshotsStatsRequest()\n+            ).actionGet().getStats()) {\n+                for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getCurrentIndexCacheFills(), equalTo(0L));\n+                }\n+            }\n+        });\n+\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), greaterThan(0L));\n+            }\n+        }\n+\n+        logger.info(\"--> verifying cached documents in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        final long numberOfCachedBlobs = systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).get().getHits().getTotalHits().value;\n+        final long numberOfCacheWrites = systemClient().admin()\n+            .indices()\n+            .prepareStats(SNAPSHOT_BLOB_CACHE_INDEX)\n+            .clear()\n+            .setIndexing(true)\n+            .get()\n+            .getTotal().indexing.getTotal().getIndexCount();\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredIndex);\n+        assertHitCount(client().prepareSearch(restoredIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        assertAcked(client().admin().indices().prepareDelete(restoredIndex));\n+\n+        logger.info(\"--> mount snapshot [{}] as an index for the second time\", snapshot);\n+        final String restoredAgainIndex = mountSnapshot(\n+            repositoryName,\n+            snapshot.getName(),\n+            indexName,\n+            Settings.builder()\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_ENABLED_SETTING.getKey(), true)\n+                .put(SearchableSnapshots.SNAPSHOT_CACHE_PREWARM_ENABLED_SETTING.getKey(), false)\n+                .build()\n+        );\n+        ensureGreen(restoredAgainIndex);\n+\n+        logger.info(\"--> verifying shards of [{}] were started without using the blob store more than necessary\", restoredAgainIndex);\n+        for (final SearchableSnapshotShardStats shardStats : client().execute(\n+            SearchableSnapshotsStatsAction.INSTANCE,\n+            new SearchableSnapshotsStatsRequest()\n+        ).actionGet().getStats()) {\n+            for (final SearchableSnapshotShardStats.CacheIndexInputStats indexInputStats : shardStats.getStats()) {\n+                final boolean mayReadMoreThanHeader\n+                // we read the header of each file contained within the .cfs file, which could be anywhere\n+                    = indexInputStats.getFileName().endsWith(\".cfs\")\n+                        // we read a couple of longs at the end of the .fdt file (see https://issues.apache.org/jira/browse/LUCENE-9456)\n+                        // TODO revisit this when this issue is addressed in Lucene\n+                        || indexInputStats.getFileName().endsWith(\".fdt\");\n+                if (indexInputStats.getFileLength() <= BlobStoreCacheService.DEFAULT_CACHED_BLOB_SIZE * 2\n+                    || mayReadMoreThanHeader == false) {\n+                    assertThat(Strings.toString(indexInputStats), indexInputStats.getBlobStoreBytesRequested().getCount(), equalTo(0L));\n+                }\n+            }\n+        }\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            numberOfDocs\n+        );\n+        assertHitCount(\n+            client().prepareSearch(restoredAgainIndex)\n+                .setQuery(QueryBuilders.rangeQuery(\"num\").gt(numberOfDocs + 1))\n+                .setSize(0)\n+                .setTrackTotalHits(true)\n+                .get(),\n+            0L\n+        );\n+\n+        logger.info(\"--> verifying cached documents (again) in system index [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        assertCachedBlobsInSystemIndex(repositoryName, blobsInSnapshot);\n+\n+        logger.info(\"--> verifying that no extra cached blobs were indexed [{}]\", SNAPSHOT_BLOB_CACHE_INDEX);\n+        refreshSystemIndex();\n+        assertHitCount(systemClient().prepareSearch(SNAPSHOT_BLOB_CACHE_INDEX).setSize(0).get(), numberOfCachedBlobs);\n+        assertThat(\n+            systemClient().admin().indices().prepareStats(SNAPSHOT_BLOB_CACHE_INDEX).clear().setIndexing(true).get().getTotal().indexing\n+                .getTotal()\n+                .getIndexCount(),\n+            equalTo(numberOfCacheWrites)\n+        );\n+\n+        logger.info(\"--> restarting cluster\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                return Settings.builder()\n+                    .put(super.onNodeStopped(nodeName))\n+                    .put(WaitForSnapshotBlobCacheShardsActivePlugin.ENABLED.getKey(), true)\n+                    .build();\n+            }\n+        });\n+        ensureGreen(restoredAgainIndex);\n+\n+        logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n+        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);", "originalCommit": "790bdc10bc4090c4bf41510057cda0e9b6fffc9d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "chunk": "diff --git a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\nindex 55a34197248..2b22fef67e3 100644\n--- a/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n+++ b/x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/blobstore/cache/SearchableSnapshotsBlobStoreCacheIntegTests.java\n\n@@ -273,7 +273,6 @@ public class SearchableSnapshotsBlobStoreCacheIntegTests extends BaseSearchableS\n \n         logger.info(\"--> verifying documents in index [{}]\", restoredAgainIndex);\n         assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n-        assertHitCount(client().prepareSearch(restoredAgainIndex).setSize(0).setTrackTotalHits(true).get(), numberOfDocs);\n         assertHitCount(\n             client().prepareSearch(restoredAgainIndex)\n                 .setQuery(QueryBuilders.rangeQuery(\"num\").lte(numberOfDocs))\n"}}, {"oid": "eb1a08ec99bde821f3a131ef6e684cd1140b15e7", "url": "https://github.com/elastic/elasticsearch/commit/eb1a08ec99bde821f3a131ef6e684cd1140b15e7", "message": "Time out cache get after 5s", "committedDate": "2020-08-26T11:09:52Z", "type": "commit"}, {"oid": "deda3054dbb45980c1d8e332ba54a16e352afe4c", "url": "https://github.com/elastic/elasticsearch/commit/deda3054dbb45980c1d8e332ba54a16e352afe4c", "message": "Assert not blocking a SYSTEM_READ thread", "committedDate": "2020-08-26T11:20:45Z", "type": "commit"}, {"oid": "dad5dfd471dbeb82bcb54fcf45ce719cd64ff7f6", "url": "https://github.com/elastic/elasticsearch/commit/dad5dfd471dbeb82bcb54fcf45ce719cd64ff7f6", "message": "Spotless", "committedDate": "2020-08-26T11:22:14Z", "type": "commit"}, {"oid": "99045e06e7052a892a8faa88ad6c63c19fed0ae4", "url": "https://github.com/elastic/elasticsearch/commit/99045e06e7052a892a8faa88ad6c63c19fed0ae4", "message": "Delimiters", "committedDate": "2020-08-26T11:34:03Z", "type": "commit"}, {"oid": "2003ec07b8a22409ba94675ce942530eb7faad95", "url": "https://github.com/elastic/elasticsearch/commit/2003ec07b8a22409ba94675ce942530eb7faad95", "message": "Add stats to tests", "committedDate": "2020-08-26T11:48:27Z", "type": "commit"}, {"oid": "62fe090c99be6cc15b3f0def26763c10bf3beabe", "url": "https://github.com/elastic/elasticsearch/commit/62fe090c99be6cc15b3f0def26763c10bf3beabe", "message": "Catch exception during indexing", "committedDate": "2020-08-26T11:51:55Z", "type": "commit"}, {"oid": "2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "url": "https://github.com/elastic/elasticsearch/commit/2c2a7c0c6d7174a0818a1baa15c30d58daacbdca", "message": "Remove duplicate assertion", "committedDate": "2020-08-26T11:54:58Z", "type": "commit"}, {"oid": "bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a", "url": "https://github.com/elastic/elasticsearch/commit/bb6d4abd97cdfa1db803683d1cf30324e5f7fd6a", "message": "Another test needing cleanup", "committedDate": "2020-08-26T12:02:12Z", "type": "commit"}, {"oid": "9e228c15a5075a562243bca9b5518b5ae32170cc", "url": "https://github.com/elastic/elasticsearch/commit/9e228c15a5075a562243bca9b5518b5ae32170cc", "message": "Account for 'any kind' reads from index cache", "committedDate": "2020-08-26T14:49:57Z", "type": "commit"}]}