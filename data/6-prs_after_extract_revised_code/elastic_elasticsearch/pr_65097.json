{"pr_number": 65097, "pr_title": "Skip range optimization if it'd be slower", "pr_createdAt": "2020-11-16T17:23:53Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/65097", "timeline": [{"oid": "57d796c19de6de5c0b54f633267a0377244534bd", "url": "https://github.com/elastic/elasticsearch/commit/57d796c19de6de5c0b54f633267a0377244534bd", "message": "WIP", "committedDate": "2020-11-12T18:10:37Z", "type": "commit"}, {"oid": "bdf6bde304a1085e1e4a179068869ce26eae438e", "url": "https://github.com/elastic/elasticsearch/commit/bdf6bde304a1085e1e4a179068869ce26eae438e", "message": "Merge branch 'master' into skip_optimization_if_pricey", "committedDate": "2020-11-12T18:18:11Z", "type": "commit"}, {"oid": "e8b0f61428adebcde7352d4570fb29efe5ba65fb", "url": "https://github.com/elastic/elasticsearch/commit/e8b0f61428adebcde7352d4570fb29efe5ba65fb", "message": "Emite estimated cost", "committedDate": "2020-11-12T22:37:42Z", "type": "commit"}, {"oid": "302d3165647592a4bf2aa634c5da16f77316e0aa", "url": "https://github.com/elastic/elasticsearch/commit/302d3165647592a4bf2aa634c5da16f77316e0aa", "message": "Cache scorers", "committedDate": "2020-11-16T13:51:24Z", "type": "commit"}, {"oid": "1a9e915a2251e88142b5645f70ef1b8b842dd749", "url": "https://github.com/elastic/elasticsearch/commit/1a9e915a2251e88142b5645f70ef1b8b842dd749", "message": "TEst", "committedDate": "2020-11-16T17:09:08Z", "type": "commit"}, {"oid": "9fe8d5204cc38bbeea6f3d661ba54895aa88c9ce", "url": "https://github.com/elastic/elasticsearch/commit/9fe8d5204cc38bbeea6f3d661ba54895aa88c9ce", "message": "Guard debug capture", "committedDate": "2020-11-16T17:18:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk5ODE4NA==", "url": "https://github.com/elastic/elasticsearch/pull/65097#discussion_r526998184", "bodyText": "Any reason not to just early terminate the loop if we've hit the limit?", "author": "not-napoleon", "createdAt": "2020-11-19T15:55:14Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -286,6 +299,57 @@ public InternalAggregation buildEmptyAggregation() {\n         ) throws IOException {\n             super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n             this.filters = filters;\n+            this.profiling = context.getProfilers() != null;\n+        }\n+\n+        /**\n+         * Estimate the number of documents that this aggregation must visit. We'll\n+         * stop counting once we've passed {@code maxEstimatedCost} if we aren't profiling.\n+         */\n+        public long estimateCost(long maxCost) throws IOException {\n+            this.maxCost = maxCost;\n+            if (estimatedCost != -1) {\n+                return estimatedCost;\n+            }\n+            long limit = profiling ? Long.MAX_VALUE : maxCost;\n+            long start = profiling ? System.nanoTime() : 0;\n+            estimatedCost = 0;\n+            weights = buildWeights(topLevelQuery(), filters);\n+            List<LeafReaderContext> leaves = searcher().getIndexReader().leaves();\n+            /*\n+             * Its important that we save a copy of the BulkScorer because for\n+             * queries like PointInRangeQuery building the scorer can be a big\n+             * chunk of the run time.\n+             */\n+            scorers = new BulkScorer[leaves.size()][];\n+            for (LeafReaderContext ctx : leaves) {\n+                scorers[ctx.ord] = new BulkScorer[filters.length];\n+                for (int f = 0; f < filters.length; f++) {\n+                    scorers[ctx.ord][f] = weights[f].bulkScorer(ctx);\n+                    if (scorers[ctx.ord][f] == null) {\n+                        // Doesn't find anything in this leaf\n+                        continue;\n+                    }\n+                    if (estimatedCost >= 0 && estimatedCost <= limit) {\n+                        // If we've overflowed or are past the limit skip the cost\n+                        estimatedCost += scorers[ctx.ord][f].cost();\n+                    }", "originalCommit": "9fe8d5204cc38bbeea6f3d661ba54895aa88c9ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1ODQ1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/65097#discussion_r527058455", "bodyText": "Yeah! If build the scorers array we want to complete it so we don't NPE later. But, now that I think about it, I could avoid that and probably should because its expensive to build the scorer if we ain't gonna use it. I'll merge this as is and open up a follow up with that change.", "author": "nik9000", "createdAt": "2020-11-19T17:14:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk5ODE4NA=="}], "type": "inlineReview", "revised_code": null}]}