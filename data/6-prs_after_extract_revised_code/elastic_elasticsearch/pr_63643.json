{"pr_number": 63643, "pr_title": "Speed up date_histogram without children", "pr_createdAt": "2020-10-13T20:37:20Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/63643", "timeline": [{"oid": "3eb94bcf3a9285cade4bd8a423e7bafb76328377", "url": "https://github.com/elastic/elasticsearch/commit/3eb94bcf3a9285cade4bd8a423e7bafb76328377", "message": "Execute date_histo agg as date_range agg\n\nWIP", "committedDate": "2020-10-12T19:34:46Z", "type": "commit"}, {"oid": "3f99f0431ed868959b4941f64189c4d87f7a8ef1", "url": "https://github.com/elastic/elasticsearch/commit/3f99f0431ed868959b4941f64189c4d87f7a8ef1", "message": "factor out collector", "committedDate": "2020-10-12T19:34:46Z", "type": "commit"}, {"oid": "6c8cb0bcbd9742d7aa3c2423f71ccde190069478", "url": "https://github.com/elastic/elasticsearch/commit/6c8cb0bcbd9742d7aa3c2423f71ccde190069478", "message": "ordered", "committedDate": "2020-10-12T19:34:46Z", "type": "commit"}, {"oid": "995ca2428be12b7c3d6bb9b6570a4ccb9ed97d08", "url": "https://github.com/elastic/elasticsearch/commit/995ca2428be12b7c3d6bb9b6570a4ccb9ed97d08", "message": "refactor", "committedDate": "2020-10-12T19:34:46Z", "type": "commit"}, {"oid": "d7becd802ac13a937b90d87489001c4b20bfecd8", "url": "https://github.com/elastic/elasticsearch/commit/d7becd802ac13a937b90d87489001c4b20bfecd8", "message": "Fixup", "committedDate": "2020-10-12T19:44:36Z", "type": "commit"}, {"oid": "0a1987d99b5ff07cdbbe5100cf291cb45cfefd08", "url": "https://github.com/elastic/elasticsearch/commit/0a1987d99b5ff07cdbbe5100cf291cb45cfefd08", "message": "Better name", "committedDate": "2020-10-12T20:17:42Z", "type": "commit"}, {"oid": "1258ad4d88b441a88f315fdcf5984b9349473e38", "url": "https://github.com/elastic/elasticsearch/commit/1258ad4d88b441a88f315fdcf5984b9349473e38", "message": "Experiment", "committedDate": "2020-10-12T21:36:44Z", "type": "commit"}, {"oid": "da02047b005365a94f687994e8426cd921a8c521", "url": "https://github.com/elastic/elasticsearch/commit/da02047b005365a94f687994e8426cd921a8c521", "message": "Rework", "committedDate": "2020-10-12T22:01:26Z", "type": "commit"}, {"oid": "9157f00d32ed4293b6fd9325675d62d72af23c82", "url": "https://github.com/elastic/elasticsearch/commit/9157f00d32ed4293b6fd9325675d62d72af23c82", "message": "Use query", "committedDate": "2020-10-13T12:21:57Z", "type": "commit"}, {"oid": "20047dc92fc4bf190ede12ecf9759c6cfe0210f8", "url": "https://github.com/elastic/elasticsearch/commit/20047dc92fc4bf190ede12ecf9759c6cfe0210f8", "message": "Super hack", "committedDate": "2020-10-13T13:41:08Z", "type": "commit"}, {"oid": "e69628e7e081daea268a860fb329f8a1b984f44e", "url": "https://github.com/elastic/elasticsearch/commit/e69628e7e081daea268a860fb329f8a1b984f44e", "message": "Shuffle", "committedDate": "2020-10-13T17:01:09Z", "type": "commit"}, {"oid": "607ae5f7c245c05a2414e0ceb50e417ad7c0e67e", "url": "https://github.com/elastic/elasticsearch/commit/607ae5f7c245c05a2414e0ceb50e417ad7c0e67e", "message": "Tests", "committedDate": "2020-10-13T17:38:51Z", "type": "commit"}, {"oid": "0e68cad023bb5594110e176f53865b889c39be22", "url": "https://github.com/elastic/elasticsearch/commit/0e68cad023bb5594110e176f53865b889c39be22", "message": "look", "committedDate": "2020-10-13T18:21:28Z", "type": "commit"}, {"oid": "9754fb0303803ae7fec06351ca3cbdebd0ee4391", "url": "https://github.com/elastic/elasticsearch/commit/9754fb0303803ae7fec06351ca3cbdebd0ee4391", "message": "no looking", "committedDate": "2020-10-13T18:34:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504244714", "bodyText": "This method and everything in it is kind of shameful but it gives a 2x speed improvement.", "author": "nik9000", "createdAt": "2020-10-13T20:39:23Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +235,234 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    private static class FilterOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        public FilterOrderAggregator(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return true;\n+        }\n     }\n \n+    private static class StandardOrderAggregator extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+\n+        private final int totalNumKeys;\n+\n+        public StandardOrderAggregator(\n+            String name,\n+            AggregatorFactories factories,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            String otherBucketKey,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, factories, keys, keyed, otherBucketKey, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+            if (otherBucketKey == null) {\n+                this.totalNumKeys = keys.length;\n+            } else {\n+                this.totalNumKeys = keys.length + 1;\n+            }\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(new MatchAllDocsQuery(), filters);\n+            }\n+            final Bits[] bits = new Bits[filters.length];\n+            for (int i = 0; i < filters.length; ++i) {\n+                bits[i] = Lucene.asSequentialAccessBits(ctx.reader().maxDoc(), filterWeights[i].scorerSupplier(ctx));\n+            }\n+            return new LeafBucketCollectorBase(sub, null) {\n+                @Override\n+                public void collect(int doc, long bucket) throws IOException {\n+                    boolean matched = false;\n+                    for (int i = 0; i < bits.length; i++) {\n+                        if (bits[i].get(doc)) {\n+                            collectBucket(sub, doc, bucketOrd(bucket, i));\n+                            matched = true;\n+                        }\n+                    }\n+                    if (otherBucketKey != null && false == matched) {\n+                        collectBucket(sub, doc, bucketOrd(bucket, bits.length));\n+                    }\n+                }\n+            };\n+        }\n+\n+        final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n+            return owningBucketOrdinal * totalNumKeys + filterOrd;\n+        }\n+\n+        @Override\n+        public boolean collectsInFilterOrder() {\n+            return false;\n+        }\n+    }\n+\n+    protected Weight[] buildWeights(Query topLevelQuery, Query filters[]) throws IOException{\n+        Weight[] weights = new Weight[filters.length];\n+        for (int i = 0; i < filters.length; ++i) {\n+            Query filter = filterMatchingBoth(topLevelQuery, filters[i]);\n+            weights[i] = context.searcher().createWeight(context.searcher().rewrite(filter), ScoreMode.COMPLETE_NO_SCORES, 1);\n+        }\n+        return weights;\n+    }\n+\n+    private Query filterMatchingBoth(Query lhs, Query rhs) {", "originalCommit": "9754fb0303803ae7fec06351ca3cbdebd0ee4391", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwMzA0NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505703045", "bodyText": "so, this merges two filter queries so they can be performed in one pass?  I know it's a private method, but I still think a bit of documentation for what it does and why that's important would be good.", "author": "not-napoleon", "createdAt": "2020-10-15T17:06:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTMwNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339304", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-10-22T17:33:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0NDcxNA=="}], "type": "inlineReview", "revised_code": {"commit": "0750aa8bbe9c8a6bee2d1fa5c44307fa2369fe4b", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\nindex 1987a2f4fe1..2f697d53b6c 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n\n@@ -249,7 +249,7 @@ public abstract class FiltersAggregator extends BucketsAggregator {\n         private final Query[] filters;\n         private Weight[] filterWeights;\n \n-        public FilterOrderAggregator(\n+        FilterOrderAggregator(\n             String name,\n             String[] keys,\n             Query[] filters,\n"}}, {"oid": "0750aa8bbe9c8a6bee2d1fa5c44307fa2369fe4b", "url": "https://github.com/elastic/elasticsearch/commit/0750aa8bbe9c8a6bee2d1fa5c44307fa2369fe4b", "message": "tests", "committedDate": "2020-10-13T20:42:17Z", "type": "commit"}, {"oid": "c57c98a787bc9fab00b042de89a04dd06a937b6d", "url": "https://github.com/elastic/elasticsearch/commit/c57c98a787bc9fab00b042de89a04dd06a937b6d", "message": "Handle unbounded ranges", "committedDate": "2020-10-13T21:51:33Z", "type": "commit"}, {"oid": "402132781b8530f616ad6474c0b7652a588404c7", "url": "https://github.com/elastic/elasticsearch/commit/402132781b8530f616ad6474c0b7652a588404c7", "message": "Test for max and min", "committedDate": "2020-10-13T21:53:25Z", "type": "commit"}, {"oid": "7a44c21e1c1a24f66aefccbe107e648662665866", "url": "https://github.com/elastic/elasticsearch/commit/7a44c21e1c1a24f66aefccbe107e648662665866", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-14T16:07:02Z", "type": "commit"}, {"oid": "73aaa455a23286c8275510d09766e96346e5afbb", "url": "https://github.com/elastic/elasticsearch/commit/73aaa455a23286c8275510d09766e96346e5afbb", "message": "Fixup profiler", "committedDate": "2020-10-14T17:13:01Z", "type": "commit"}, {"oid": "b73df70e183e4940051134dedb6b0fbe7d589f9d", "url": "https://github.com/elastic/elasticsearch/commit/b73df70e183e4940051134dedb6b0fbe7d589f9d", "message": "Rate agg\n\nThis is weird", "committedDate": "2020-10-14T19:57:33Z", "type": "commit"}, {"oid": "b7e8dccef147a4916c6f514ee7928218ebde15d3", "url": "https://github.com/elastic/elasticsearch/commit/b7e8dccef147a4916c6f514ee7928218ebde15d3", "message": "WIP", "committedDate": "2020-10-14T20:03:57Z", "type": "commit"}, {"oid": "7c18141d23dc3e7f4dcd5883068c88424147af2f", "url": "https://github.com/elastic/elasticsearch/commit/7c18141d23dc3e7f4dcd5883068c88424147af2f", "message": "Fixup weird formats", "committedDate": "2020-10-14T20:36:19Z", "type": "commit"}, {"oid": "2d04e358e33aeaf528221b900d41854118b37125", "url": "https://github.com/elastic/elasticsearch/commit/2d04e358e33aeaf528221b900d41854118b37125", "message": "Feh", "committedDate": "2020-10-14T20:43:06Z", "type": "commit"}, {"oid": "102e5263357ef58f73b24333f46dddc5dfb6e52d", "url": "https://github.com/elastic/elasticsearch/commit/102e5263357ef58f73b24333f46dddc5dfb6e52d", "message": "Shift", "committedDate": "2020-10-14T20:45:37Z", "type": "commit"}, {"oid": "e333cbb179ef4d3392e2439aa26ae8702a6c6287", "url": "https://github.com/elastic/elasticsearch/commit/e333cbb179ef4d3392e2439aa26ae8702a6c6287", "message": "Forbidden", "committedDate": "2020-10-14T21:12:45Z", "type": "commit"}, {"oid": "e2fd164096dc653417c16587a43805c5a02e2a90", "url": "https://github.com/elastic/elasticsearch/commit/e2fd164096dc653417c16587a43805c5a02e2a90", "message": "Moar tests", "committedDate": "2020-10-15T12:59:54Z", "type": "commit"}, {"oid": "46a10159401aac957f10196e619f3f08b64958d9", "url": "https://github.com/elastic/elasticsearch/commit/46a10159401aac957f10196e619f3f08b64958d9", "message": "test", "committedDate": "2020-10-15T13:53:22Z", "type": "commit"}, {"oid": "deb56842f63ef0827f354f52582c7e830ca5b4ea", "url": "https://github.com/elastic/elasticsearch/commit/deb56842f63ef0827f354f52582c7e830ca5b4ea", "message": "Fixup tests", "committedDate": "2020-10-15T14:14:28Z", "type": "commit"}, {"oid": "de96179d12412bf51045607ab1b27e87e6a43d4e", "url": "https://github.com/elastic/elasticsearch/commit/de96179d12412bf51045607ab1b27e87e6a43d4e", "message": "precommit", "committedDate": "2020-10-15T15:37:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504682380", "bodyText": "On my first read, I thought we might round down to null.  How about:\nIf this rounding mechanism precalculates rounding points, this array stores dates such that each\ndate between each entry will be rounded down to that entry.  If the rounding mechanism does not\nsupport the optimization, this array is {@code null}.", "author": "not-napoleon", "createdAt": "2020-10-14T13:36:32Z", "path": "server/src/main/java/org/elasticsearch/common/Rounding.java", "diffHunk": "@@ -291,6 +291,12 @@ public void writeTo(StreamOutput out) throws IOException {\n          * next rounded value in specified units if possible.\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n+        /**\n+         * An array of dates such that each date between each entry is will\n+         * be rounded down to that entry or {@code null} if this rounding\n+         * mechanism doesn't or can't precalculate these points.\n+         */", "originalCommit": "402132781b8530f616ad6474c0b7652a588404c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyOTk5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510329993", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-10-22T17:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4MjM4MA=="}], "type": "inlineReview", "revised_code": {"commit": "9da87c50f3b30f192b3830c4803c3f77002032bd", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/common/Rounding.java b/server/src/main/java/org/elasticsearch/common/Rounding.java\nindex 16ec3552d9c..081a08a2f4a 100644\n--- a/server/src/main/java/org/elasticsearch/common/Rounding.java\n+++ b/server/src/main/java/org/elasticsearch/common/Rounding.java\n\n@@ -292,9 +292,10 @@ public abstract class Rounding implements Writeable {\n          */\n         double roundingSize(long utcMillis, DateTimeUnit timeUnit);\n         /**\n-         * An array of dates such that each date between each entry is will\n-         * be rounded down to that entry or {@code null} if this rounding\n-         * mechanism doesn't or can't precalculate these points.\n+         * If this rounding mechanism precalculates rounding points then\n+         * this array stores dates such that each date between each entry.\n+         * if the rounding mechanism doesn't precalculate points then this\n+         * is {@code null}.\n          */\n         long[] fixedRoundingPoints();\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r504747042", "bodyText": "I'm not entirely convinced that the way we're shoehorning GeoDistance into the range framework is a good idea.  The fact that we need this supports that viewpoint, I think.", "author": "not-napoleon", "createdAt": "2020-10-14T14:56:50Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/GeoDistanceRangeAggregatorFactory.java", "diffHunk": "@@ -66,7 +66,7 @@ public static void registerAggregators(ValuesSourceRegistry.Builder builder) {\n                 cardinality,\n                 metadata) -> {\n                 DistanceSource distanceSource = new DistanceSource((ValuesSource.GeoPoint) valuesSource, distanceType, origin, units);\n-                return new RangeAggregator(\n+                return RangeAggregator.buildWithoutAttemptedToAdaptToFilters(", "originalCommit": "402132781b8530f616ad6474c0b7652a588404c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyNjg0MA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510326840", "bodyText": "I wonder if it'd make sense to shoe horn it via adapting the data source on the way in and the result on the way out. Maybe. I'm not sure. A thing for later, I think.", "author": "nik9000", "createdAt": "2020-10-22T17:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc0NzA0Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505653109", "bodyText": "This could use some javadoc.  It's not clear from the context that it's intended to be used with the AddaptingAggregator (or, really, wrapping aggs in general, although I don't think we have any others)", "author": "not-napoleon", "createdAt": "2020-10-15T15:51:17Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -227,6 +227,18 @@ public int countAggregators() {\n         return factories.length;\n     }\n \n+    public AggregatorFactories fixParent(Aggregator fixedParent) {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTUwNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331504", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-10-22T17:21:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1MzEwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "9da87c50f3b30f192b3830c4803c3f77002032bd", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java b/server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java\nindex 112a183e586..479ea05e0f8 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java\n\n@@ -227,6 +227,15 @@ public class AggregatorFactories {\n         return factories.length;\n     }\n \n+    /**\n+     * This returns a copy of {@link AggregatorFactories} modified so that\n+     * calls to {@link #createSubAggregators} will ignore the provided parent\n+     * aggregator and always use {@code fixedParent} provided in to this\n+     * method.\n+     * <p>\n+     * {@link AdaptingAggregator} uses this to make sure that sub-aggregators\n+     * get the {@link AdaptingAggregator} aggregator itself as the parent.\n+     */\n     public AggregatorFactories fixParent(Aggregator fixedParent) {\n         AggregatorFactories previous = this;\n         return new AggregatorFactories(factories) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1OTUyOQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505659529", "bodyText": "Bit of javadoc would be nice here.  It looks like the intention is for this to only have the implementations already built in this file, would be good to document that.", "author": "not-napoleon", "createdAt": "2020-10-15T15:59:57Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -45,9 +56,11 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n-import java.util.function.Supplier;\n+import java.util.function.BiConsumer;\n \n-public class FiltersAggregator extends BucketsAggregator {\n+import static java.util.Arrays.compareUnsigned;\n+\n+public abstract class FiltersAggregator extends BucketsAggregator {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTg0OA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339848", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-10-22T17:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY1OTUyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\nindex c122623125a..cadcf566988 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n\n@@ -60,6 +60,13 @@ import java.util.function.BiConsumer;\n \n import static java.util.Arrays.compareUnsigned;\n \n+/**\n+ * Aggregator for {@code filters}. There are two known subclasses,\n+ * {@link FilterByFilter} which is fast but only works in some cases and\n+ * {@link Compatible} which works in all cases.\n+ * {@link FiltersAggregator#build} will build the fastest version that\n+ * works with the configuration.\n+ */\n public abstract class FiltersAggregator extends BucketsAggregator {\n \n     public static final ParseField FILTERS_FIELD = new ParseField(\"filters\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5NTk1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505695952", "bodyText": "Javadoc here too please.  This operates differently from what this method usually does (e.g. doesn't return a LeafBucketCollector), and it would be good to make a note of why that's correct.", "author": "not-napoleon", "createdAt": "2020-10-15T16:55:15Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "97c35cdb46ee01c6fa4db505107d47b305f88791", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\nindex c122623125a..5e748fde888 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n\n@@ -251,8 +258,6 @@ public abstract class FiltersAggregator extends BucketsAggregator {\n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    public abstract boolean collectsInFilterOrder();\n-\n     /**\n      * Collects results by running each filter against the searcher and doesn't\n      * build any {@link LeafBucketCollector}s which is generally faster than\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5OTAzNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505699034", "bodyText": "Why do we throw here?", "author": "not-napoleon", "createdAt": "2020-10-15T17:00:01Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java", "diffHunk": "@@ -185,16 +243,254 @@ public InternalAggregation buildEmptyAggregation() {\n             buckets.add(bucket);\n         }\n \n-        if (showOtherBucket) {\n+        if (otherBucketKey != null) {\n             InternalFilters.InternalBucket bucket = new InternalFilters.InternalBucket(otherBucketKey, 0, subAggs, keyed);\n             buckets.add(bucket);\n         }\n \n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    final long bucketOrd(long owningBucketOrdinal, int filterOrd) {\n-        return owningBucketOrdinal * totalNumKeys + filterOrd;\n+    public abstract boolean collectsInFilterOrder();\n+\n+    /**\n+     * Collects results by running each filter against the searcher and doesn't\n+     * build any {@link LeafBucketCollector}s which is generally faster than\n+     * {@link Compatible} but doesn't support when there is a parent aggregator\n+     * or any child aggregators.\n+     */\n+    private static class FilterByFilter extends FiltersAggregator {\n+        private final Query[] filters;\n+        private Weight[] filterWeights;\n+        private int segmentsWithDeletedDocs;\n+\n+        FilterByFilter(\n+            String name,\n+            String[] keys,\n+            Query[] filters,\n+            boolean keyed,\n+            SearchContext context,\n+            Aggregator parent,\n+            CardinalityUpperBound cardinality,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+            super(name, AggregatorFactories.EMPTY, keys, keyed, null, context, parent, cardinality, metadata);\n+            this.filters = filters;\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+            if (filterWeights == null) {\n+                filterWeights = buildWeights(context.query(), filters);\n+            }\n+            Bits live = ctx.reader().getLiveDocs();\n+            for (int filterOrd = 0; filterOrd < filters.length; filterOrd++) {\n+                Scorer scorer = filterWeights[filterOrd].scorer(ctx);\n+                if (scorer == null) {\n+                    // the filter doesn't match any docs\n+                    continue;\n+                }\n+                DocIdSetIterator itr = scorer.iterator();\n+                if (live == null) {\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        collectBucket(sub, itr.docID(), filterOrd);\n+                    }\n+                } else {\n+                    segmentsWithDeletedDocs++;\n+                    while (itr.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n+                        if (live.get(itr.docID())) {\n+                            collectBucket(sub, itr.docID(), filterOrd);\n+                        }\n+                    }\n+                }\n+            }\n+            throw new CollectionTerminatedException();", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM0MzI4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510343285", "bodyText": "Throwing this exception is how we communicate to the collection mechanism that we don't need the segment.. I'll add docs.", "author": "nik9000", "createdAt": "2020-10-22T17:40:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY5OTAzNA=="}], "type": "inlineReview", "revised_code": {"commit": "97c35cdb46ee01c6fa4db505107d47b305f88791", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\nindex c122623125a..5e748fde888 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FiltersAggregator.java\n\n@@ -251,8 +258,6 @@ public abstract class FiltersAggregator extends BucketsAggregator {\n         return new InternalFilters(name, buckets, keyed, metadata());\n     }\n \n-    public abstract boolean collectsInFilterOrder();\n-\n     /**\n      * Collects results by running each filter against the searcher and doesn't\n      * build any {@link LeafBucketCollector}s which is generally faster than\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505706035", "bodyText": "I think you lost a verb: \"because it doesn't need to the rounding points\"?  Compute, maybe?", "author": "not-napoleon", "createdAt": "2020-10-15T17:10:47Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzMTk2NA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510331964", "bodyText": "\"to round points\" is what I meant.", "author": "nik9000", "createdAt": "2020-10-22T17:22:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwNjAzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\nindex 22345b5c9af..5a8281825d6 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n\n@@ -56,10 +56,13 @@ import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTE1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709159", "bodyText": "This is probably redundant, FYI.  In theory, if hasValues() == false, then we went into the createUnmapped path from the builder.  Harmless to check though.", "author": "not-napoleon", "createdAt": "2020-10-15T17:16:03Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMjQxNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510232414", "bodyText": "I can drop it.", "author": "nik9000", "createdAt": "2020-10-22T14:58:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTE1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\nindex 22345b5c9af..5a8281825d6 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n\n@@ -56,10 +56,13 @@ import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTY4NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505709685", "bodyText": "fine for now, but I'm not sure why hard bounds can't be translated into the range aggregation.", "author": "not-napoleon", "createdAt": "2020-10-15T17:16:59Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -51,6 +62,143 @@\n  * @see Rounding\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMjAyNA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510232024", "bodyText": "Mostly because I don't know what they are or how to translate them. Maybe I can poke @imotov.", "author": "nik9000", "createdAt": "2020-10-22T14:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcwOTY4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\nindex 22345b5c9af..5a8281825d6 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n\n@@ -56,10 +56,13 @@ import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505718210", "bodyText": "I'm trying to move us away from null values sources.  By default, ValuesSourceConfig will return an empty values source for unmapped fields now.  Some aggregators still override that to be null internally, because of how we handle createUnmapped now.  But in general, we shouldn't be expecting null values sources.  Check ValuesSourceConfig#hasValues() instead.\nAlso, what ValuesSource?", "author": "not-napoleon", "createdAt": "2020-10-15T17:31:45Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -195,4 +344,95 @@ public double bucketSize(long bucket, Rounding.DateTimeUnit unitSize) {\n             return 1.0;\n         }\n     }\n+\n+    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+        private final DocValueFormat format;\n+        private final Rounding rounding;\n+        private final Rounding.Prepared preparedRounding;\n+        private final BucketOrder order;\n+        private final long minDocCount;\n+        private final LongBounds extendedBounds;\n+        private final boolean keyed;\n+        private final long[] fixedRoundingPoints;\n+\n+        FromDateRange(\n+            Aggregator parent,\n+            AggregatorFactories subAggregators,\n+            CheckedFunction<AggregatorFactories, Aggregator, IOException> delegate,\n+            DocValueFormat format,\n+            Rounding rounding,\n+            Rounding.Prepared preparedRounding,\n+            BucketOrder order,\n+            long minDocCount,\n+            LongBounds extendedBounds,\n+            boolean keyed,\n+            long[] fixedRoundingPoints\n+        ) throws IOException {\n+            super(parent, subAggregators, delegate);\n+            this.format = format;\n+            this.rounding = rounding;\n+            this.preparedRounding = preparedRounding;\n+            this.order = order;\n+            order.validate(this);\n+            this.minDocCount = minDocCount;\n+            this.extendedBounds = extendedBounds;\n+            this.keyed = keyed;\n+            this.fixedRoundingPoints = fixedRoundingPoints;\n+        }\n+\n+        @Override\n+        protected InternalAggregation adapt(InternalAggregation delegateResult) {\n+            InternalDateRange range = (InternalDateRange) delegateResult;\n+            List<InternalDateHistogram.Bucket> buckets = new ArrayList<>(range.getBuckets().size());\n+            for (InternalDateRange.Bucket rangeBucket : range.getBuckets()) {\n+                if (rangeBucket.getDocCount() > 0) {\n+                    buckets.add(\n+                        new InternalDateHistogram.Bucket(\n+                            rangeBucket.getFrom().toInstant().toEpochMilli(),\n+                            rangeBucket.getDocCount(),\n+                            keyed,\n+                            format,\n+                            rangeBucket.getAggregations()\n+                        )\n+                    );\n+                }\n+            }\n+            CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+            // value source will be null for unmapped fields", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzE3OA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510233178", "bodyText": "I'll have another look at this one! I missed this comment 7 days ago and am just seeing it now.", "author": "nik9000", "createdAt": "2020-10-22T14:59:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMyODg1Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510328852", "bodyText": "I copied this from above but I don't think it is accurate either place. I've zapped it.", "author": "nik9000", "createdAt": "2020-10-22T17:16:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcxODIxMA=="}], "type": "inlineReview", "revised_code": {"commit": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\nindex 22345b5c9af..1f87979a641 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n\n@@ -345,7 +344,7 @@ class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAg\n         }\n     }\n \n-    private static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n+    static class FromDateRange extends AdaptingAggregator implements SizedBucketAggregator {\n         private final DocValueFormat format;\n         private final Rounding rounding;\n         private final Rounding.Prepared preparedRounding;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMDU3Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505720573", "bodyText": "\ud83d\udc4d", "author": "not-napoleon", "createdAt": "2020-10-15T17:35:15Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeAggregatorFactory.java", "diffHunk": "@@ -92,8 +91,7 @@ protected Aggregator doCreateInternal(\n             .build(\n                 name,\n                 factories,\n-                (Numeric) config.getValuesSource(),\n-                config.format(),", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMTY2MA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505721660", "bodyText": "Like with filters, I think a note explaining when to subclass this would be helpful.  Especially since there's already a lot of weird reuse patterns in the Range family.", "author": "not-napoleon", "createdAt": "2020-10-15T17:37:11Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -52,7 +63,7 @@\n \n import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n \n-public class RangeAggregator extends BucketsAggregator {\n+public abstract class RangeAggregator extends BucketsAggregator {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTkyMA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739920", "bodyText": "++", "author": "nik9000", "createdAt": "2020-10-15T18:07:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyMTY2MA=="}], "type": "inlineReview", "revised_code": {"commit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\nindex 0a82ff2c9df..686d11d7ed4 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n\n@@ -63,6 +63,15 @@ import java.util.Objects;\n \n import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n \n+/**\n+ * Aggregator for {@code range}. There are two known subclasses,\n+ * {@link NoOverlap} which is fast but only compatible with ranges that\n+ * don't have overlaps and {@link Overlap} which handles overlapping\n+ * ranges. There is also {@link FromFilters} which isn't a subclass\n+ * but is also a functional aggregator for {@code range}.\n+ * {@link RangeAggregator#build} will build the fastest of the three\n+ * that is compatible with the requested configuration.\n+ */\n public abstract class RangeAggregator extends BucketsAggregator {\n \n     public static final ParseField RANGES_FIELD = new ParseField(\"ranges\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505728720", "bodyText": "ValuesSourceConfig#getPointReaderOrNull() uses nearly the same set of checks for nearly the same reason - would it make sense to wrap all of these into one predicate on VSConfig?", "author": "not-napoleon", "createdAt": "2020-10-15T17:49:05Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTc5NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739795", "bodyText": "Yes.", "author": "nik9000", "createdAt": "2020-10-15T18:07:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MDA1Ng==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510260056", "bodyText": "Do you plan to do that in this PR, or open a ticket for it?  Could be a good candidate for good first issue, since it's mostly just moving code around.", "author": "not-napoleon", "createdAt": "2020-10-22T15:35:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI4MzI0OA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510283248", "bodyText": "Actually, I see now that you added the method on VSConfig, but aren't calling it from here.  Oversight?", "author": "not-napoleon", "createdAt": "2020-10-22T16:06:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTcyODcyMA=="}], "type": "inlineReview", "revised_code": {"commit": "6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\nindex 0a82ff2c9df..d634fb172b5 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n\n@@ -318,6 +318,7 @@ public abstract class RangeAggregator extends BucketsAggregator {\n              * we can just cast to a long here and it'll be taken as millis.\n              */\n             DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            // TODO correct the loss of precision from the range somehow.....?\n             filters[i] = valuesSourceConfig.fieldType()\n                 .rangeQuery(\n                     ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMjgyOA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505732828", "bodyText": "Just to say it out loud: we don't care that we just built and then threw away an aggregator here, because aggregator creation time is not highly sensitive to performance hits, correct?", "author": "not-napoleon", "createdAt": "2020-10-15T17:55:39Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +226,186 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n \n-    final double[] maxTo;\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            keys[i] = Integer.toString(i);\n+            /*\n+             * Use the native format on the field rather than the one provided\n+             * on the valuesSourceConfig because the format on the field is what\n+             * we parse. With https://github.com/elastic/elasticsearch/pull/63692\n+             * we can just cast to a long here and it'll be taken as millis.\n+             */\n+            DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            filters[i] = valuesSourceConfig.fieldType()\n+                .rangeQuery(\n+                    ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n+                    ranges[i].to == Double.POSITIVE_INFINITY ? null : format.format(ranges[i].to),\n+                    true,\n+                    false,\n+                    ShapeRelation.CONTAINS,\n+                    null,\n+                    null,\n+                    context.getQueryShardContext()\n+                );\n+        }\n+        RangeAggregator.FromFilters<?> fromFilters = new RangeAggregator.FromFilters<>(\n+            parent,\n+            factories,\n+            subAggregators -> FiltersAggregator.build(\n+                name,\n+                subAggregators,\n+                keys,\n+                filters,\n+                false,\n+                null,\n+                context,\n+                parent,\n+                cardinality,\n+                metadata\n+            ),\n+            valuesSourceConfig.format(),\n+            ranges,\n+            keyed,\n+            rangeFactory\n+        );\n+        if (false == ((FiltersAggregator) fromFilters.delegate()).collectsInFilterOrder()) {", "originalCommit": "de96179d12412bf51045607ab1b27e87e6a43d4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczOTc1OA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r505739758", "bodyText": "That is how believe that is so. It might be kind of me to make a method that returns null if it can't collect in order though.", "author": "nik9000", "createdAt": "2020-10-15T18:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTczMjgyOA=="}], "type": "inlineReview", "revised_code": {"commit": "6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\nindex 0a82ff2c9df..d634fb172b5 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n\n@@ -318,6 +318,7 @@ public abstract class RangeAggregator extends BucketsAggregator {\n              * we can just cast to a long here and it'll be taken as millis.\n              */\n             DocValueFormat format = valuesSourceConfig.fieldType().docValueFormat(null, null);\n+            // TODO correct the loss of precision from the range somehow.....?\n             filters[i] = valuesSourceConfig.fieldType()\n                 .rangeQuery(\n                     ranges[i].from == Double.NEGATIVE_INFINITY ? null : format.format(ranges[i].from),\n"}}, {"oid": "4e9c9180286a9de2c358e5ba2adef8f0ab17c36c", "url": "https://github.com/elastic/elasticsearch/commit/4e9c9180286a9de2c358e5ba2adef8f0ab17c36c", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-16T16:24:33Z", "type": "commit"}, {"oid": "f13a8ae89d99e7a8bc448ca734b2ed7c03fde224", "url": "https://github.com/elastic/elasticsearch/commit/f13a8ae89d99e7a8bc448ca734b2ed7c03fde224", "message": "Drop old extra test", "committedDate": "2020-10-16T16:25:11Z", "type": "commit"}, {"oid": "6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "url": "https://github.com/elastic/elasticsearch/commit/6b3324713d5ba6934c3cd72cab56dd3f1eb5d438", "message": "TODO", "committedDate": "2020-10-19T13:21:41Z", "type": "commit"}, {"oid": "14b49626d5c0f96c6062b3c524069953ad51cc72", "url": "https://github.com/elastic/elasticsearch/commit/14b49626d5c0f96c6062b3c524069953ad51cc72", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-20T13:39:47Z", "type": "commit"}, {"oid": "2fd7e53762cbfc9ded7fae5b79796fe28941d442", "url": "https://github.com/elastic/elasticsearch/commit/2fd7e53762cbfc9ded7fae5b79796fe28941d442", "message": "Don't attempt the optimization if rounding would break it", "committedDate": "2020-10-20T14:42:09Z", "type": "commit"}, {"oid": "97c35cdb46ee01c6fa4db505107d47b305f88791", "url": "https://github.com/elastic/elasticsearch/commit/97c35cdb46ee01c6fa4db505107d47b305f88791", "message": "move building\n\nThis way we don't have to worry about BigArrays and releasing and stuff.", "committedDate": "2020-10-20T16:00:20Z", "type": "commit"}, {"oid": "6c7eaa1ef54adcf49f091459f22885b4e1ed88ec", "url": "https://github.com/elastic/elasticsearch/commit/6c7eaa1ef54adcf49f091459f22885b4e1ed88ec", "message": "Computers are hard", "committedDate": "2020-10-20T16:06:25Z", "type": "commit"}, {"oid": "5b5ac7be4ec09dee13f1d9d211d924098eba48dc", "url": "https://github.com/elastic/elasticsearch/commit/5b5ac7be4ec09dee13f1d9d211d924098eba48dc", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-20T17:18:15Z", "type": "commit"}, {"oid": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "url": "https://github.com/elastic/elasticsearch/commit/117eb77edc372d3d707dca96dbcdf161e3c33faf", "message": "Moar javadoc", "committedDate": "2020-10-20T17:56:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510233823", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if ((double) max != max) {\n          \n          \n            \n                    if ((long) ((double) max) != max) {\n          \n      \n    \n    \n  \n\nLet's be explicit about the casting here.  I don't want to have to wonder if some optimization decided to cast the RHS to a double instead of the LHS to a long.  Ditto for other instances of this check.", "author": "not-napoleon", "createdAt": "2020-10-22T15:00:26Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java", "diffHunk": "@@ -22,35 +22,186 @@\n import org.apache.lucene.index.SortedNumericDocValues;\n import org.apache.lucene.search.ScoreMode;\n import org.apache.lucene.util.CollectionUtil;\n+import org.elasticsearch.common.CheckedFunction;\n import org.elasticsearch.common.Nullable;\n import org.elasticsearch.common.Rounding;\n+import org.elasticsearch.common.Rounding.DateTimeUnit;\n import org.elasticsearch.common.lease.Releasables;\n import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.AdaptingAggregator;\n import org.elasticsearch.search.aggregations.Aggregator;\n import org.elasticsearch.search.aggregations.AggregatorFactories;\n import org.elasticsearch.search.aggregations.BucketOrder;\n import org.elasticsearch.search.aggregations.CardinalityUpperBound;\n import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n import org.elasticsearch.search.aggregations.LeafBucketCollector;\n import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;\n+import org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.InternalDateRange;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;\n+import org.elasticsearch.search.aggregations.bucket.range.RangeAggregatorSupplier;\n import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.function.BiConsumer;\n \n /**\n- * An aggregator for date values. Every date is rounded down using a configured\n- * {@link Rounding}.\n- *\n- * @see Rounding\n+ * Aggregator for {@code date_histogram} that rounds values using\n+ * {@link Rounding}. See {@link FromDateRange} which also aggregates for\n+ * {@code date_histogram} but does so by running a {@code range} aggregation\n+ * over the date and transforming the results. In general\n+ * {@link FromDateRange} is faster than {@link DateHistogramAggregator}\n+ * but {@linkplain DateHistogramAggregator} works when we can't precalculate\n+ * all of the {@link Rounding.Prepared#fixedRoundingPoints() fixed rounding points}.\n  */\n class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAggregator {\n+    /**\n+     * Build an {@link Aggregator} for a {@code date_histogram} aggregation.\n+     * If we can determine the bucket boundaries from\n+     * {@link Rounding.Prepared#fixedRoundingPoints()} we use\n+     * {@link RangeAggregator} to do the actual collecting, otherwise we use\n+     * an specialized {@link DateHistogramAggregator Aggregator} specifically\n+     * for the {@code date_histogram}s. We prefer to delegate to the\n+     * {@linkplain RangeAggregator} because it can sometimes be further\n+     * optimized into a {@link FiltersAggregator}. Even when it can't be\n+     * optimized, it is going to be marginally faster and consume less memory\n+     * than the {@linkplain DateHistogramAggregator} because it doesn't need\n+     * to the rounding points and because it can pass precise cardinality\n+     * estimates to its child aggregations.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Rounding.Prepared preparedRounding = valuesSourceConfig.roundingPreparer().apply(rounding);\n+        Aggregator asRange = adaptIntoRangeOrNull(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (asRange != null) {\n+            return asRange;\n+        }\n+        return new DateHistogramAggregator(\n+            name,\n+            factories,\n+            rounding,\n+            preparedRounding,\n+            order,\n+            keyed,\n+            minDocCount,\n+            extendedBounds,\n+            hardBounds,\n+            valuesSourceConfig,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    private static FromDateRange adaptIntoRangeOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        Rounding rounding,\n+        Rounding.Prepared preparedRounding,\n+        BucketOrder order,\n+        boolean keyed,\n+        long minDocCount,\n+        @Nullable LongBounds extendedBounds,\n+        @Nullable LongBounds hardBounds,\n+        ValuesSourceConfig valuesSourceConfig,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (hardBounds != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.hasValues() == false) {\n+            return null;\n+        }\n+        long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n+        if (fixedRoundingPoints == null) {\n+            return null;\n+        }\n+        // Range aggs use a double to aggregate and we don't want to lose precision.\n+        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n+        if ((double) max != max) {", "originalCommit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMwMzQxOA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510303418", "bodyText": "I'm going to replace this with a hard comparison which is easier to reason about. max > 1L << 53.", "author": "nik9000", "createdAt": "2020-10-22T16:35:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDIzMzgyMw=="}], "type": "inlineReview", "revised_code": {"commit": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\nindex 5a8281825d6..1f87979a641 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java\n\n@@ -151,19 +151,15 @@ class DateHistogramAggregator extends BucketsAggregator implements SizedBucketAg\n         if (hardBounds != null) {\n             return null;\n         }\n-        if (valuesSourceConfig.hasValues() == false) {\n-            return null;\n-        }\n         long[] fixedRoundingPoints = preparedRounding.fixedRoundingPoints();\n         if (fixedRoundingPoints == null) {\n             return null;\n         }\n         // Range aggs use a double to aggregate and we don't want to lose precision.\n-        long max = fixedRoundingPoints[fixedRoundingPoints.length - 1];\n-        if ((double) max != max) {\n+        if (fixedRoundingPoints[0] > 1L << 53) {\n             return null;\n         }\n-        if ((double) fixedRoundingPoints[0] != fixedRoundingPoints[0]) {\n+        if (fixedRoundingPoints[fixedRoundingPoints.length - 1] > 1L << 53) {\n             return null;\n         }\n         RangeAggregatorSupplier rangeSupplier = context.getQueryShardContext()\n"}}, {"oid": "08fee6c3882cb82080ff21baf69bdd314e228df4", "url": "https://github.com/elastic/elasticsearch/commit/08fee6c3882cb82080ff21baf69bdd314e228df4", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-22T15:16:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MzQ3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510263477", "bodyText": "My instinct is that this should be delegated to the ValuesSourceType in some way, but I'm not sure how right now.  Something to think about.", "author": "not-napoleon", "createdAt": "2020-10-22T15:39:43Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {", "originalCommit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDMzOTUzMA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510339530", "bodyText": "Yeah.", "author": "nik9000", "createdAt": "2020-10-22T17:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2MzQ3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\nindex 686d11d7ed4..06b2372f94e 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n\n@@ -298,16 +298,7 @@ public abstract class RangeAggregator extends BucketsAggregator {\n         CardinalityUpperBound cardinality,\n         Map<String, Object> metadata\n     ) throws IOException {\n-        if (valuesSourceConfig.fieldType() == null) {\n-            return null;\n-        }\n-        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n-            return null;\n-        }\n-        if (valuesSourceConfig.missing() != null) {\n-            return null;\n-        }\n-        if (valuesSourceConfig.script() != null) {\n+        if (false == valuesSourceConfig.alignesWithSearchIndex()) {\n             return null;\n         }\n         // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2Nzc5Mg==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510267792", "bodyText": "Because we've been talking about it all week, I know what 1 << 53 is doing here, but someone coming fresh to this code without the IEEE floating point spec on their mind probably won't know why that number is magic.  I'd suggest making a constant on this class LARGEST_PRECISE_DOUBLE or something like that", "author": "not-napoleon", "createdAt": "2020-10-22T15:45:33Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java", "diffHunk": "@@ -215,15 +235,207 @@ public boolean equals(Object obj) {\n         }\n     }\n \n-    final ValuesSource.Numeric valuesSource;\n-    final DocValueFormat format;\n-    final Range[] ranges;\n-    final boolean keyed;\n-    final InternalRange.Factory rangeFactory;\n+    /**\n+     * Build an {@link Aggregator} for a {@code range} aggregation. If the\n+     * {@code ranges} can be converted into filters then it builds a\n+     * {@link FiltersAggregator} and uses that to collect the results\n+     * <strong>if</strong> that aggregator can run in \"filter by filter\"\n+     * collection mode. If it can't then we'll collect the ranges using\n+     * a native {@link RangeAggregator} which is significantly faster\n+     * than the \"compatible\" collection mechanism for the filters agg.\n+     */\n+    public static Aggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        Aggregator adapted = adaptIntoFiltersOrNull(\n+            name,\n+            factories,\n+            valuesSourceConfig,\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+        if (adapted != null) {\n+            return adapted;\n+        }\n+        return buildWithoutAttemptedToAdaptToFilters(\n+            name,\n+            factories,\n+            (ValuesSource.Numeric) valuesSourceConfig.getValuesSource(),\n+            valuesSourceConfig.format(),\n+            rangeFactory,\n+            ranges,\n+            keyed,\n+            context,\n+            parent,\n+            cardinality,\n+            metadata\n+        );\n+    }\n+\n+    public static Aggregator adaptIntoFiltersOrNull(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSourceConfig valuesSourceConfig,\n+        InternalRange.Factory<?, ?> rangeFactory,\n+        Range[] ranges,\n+        boolean keyed,\n+        SearchContext context,\n+        Aggregator parent,\n+        CardinalityUpperBound cardinality,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        if (valuesSourceConfig.fieldType() == null) {\n+            return null;\n+        }\n+        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.missing() != null) {\n+            return null;\n+        }\n+        if (valuesSourceConfig.script() != null) {\n+            return null;\n+        }\n+        // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n+        if (valuesSourceConfig.fieldType() instanceof DateFieldType\n+            && ((DateFieldType) valuesSourceConfig.fieldType()).resolution() == Resolution.NANOSECONDS) {\n+            // We don't generate sensible Queries for nanoseconds.\n+            return null;\n+        }\n+        boolean wholeNumbersOnly = false == ((ValuesSource.Numeric) valuesSourceConfig.getValuesSource()).isFloatingPoint();\n+        String[] keys = new String[ranges.length];\n+        Query[] filters = new Query[ranges.length];\n+        for (int i = 0; i < ranges.length; i++) {\n+            /*\n+             * If the bounds on the ranges are too high then the `double`s\n+             * that we work with will round differently in the native range\n+             * aggregator than in the filters aggregator. So we can't use\n+             * the filters. That is, if the input data type is a `long` in\n+             * the first place. If it isn't then \n+             */\n+            if (wholeNumbersOnly && ranges[i].from != Double.NEGATIVE_INFINITY && Math.abs(ranges[i].from) > 1L << 53) {", "originalCommit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\nindex 686d11d7ed4..06b2372f94e 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java\n\n@@ -298,16 +298,7 @@ public abstract class RangeAggregator extends BucketsAggregator {\n         CardinalityUpperBound cardinality,\n         Map<String, Object> metadata\n     ) throws IOException {\n-        if (valuesSourceConfig.fieldType() == null) {\n-            return null;\n-        }\n-        if (false == valuesSourceConfig.fieldType().isSearchable()) {\n-            return null;\n-        }\n-        if (valuesSourceConfig.missing() != null) {\n-            return null;\n-        }\n-        if (valuesSourceConfig.script() != null) {\n+        if (false == valuesSourceConfig.alignesWithSearchIndex()) {\n             return null;\n         }\n         // TODO bail here for runtime fields. They'll be slower this way. Maybe we can somehow look at the Query?\n"}}, {"oid": "ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "url": "https://github.com/elastic/elasticsearch/commit/ce640e3543934de558ad5f3e21b9a26fcfa53f4a", "message": "Test cases we can't do it", "committedDate": "2020-10-22T16:38:35Z", "type": "commit"}, {"oid": "523d420a065bac8298969997a8638a8846bb0870", "url": "https://github.com/elastic/elasticsearch/commit/523d420a065bac8298969997a8638a8846bb0870", "message": "Fix broken test", "committedDate": "2020-10-22T17:12:42Z", "type": "commit"}, {"oid": "f1ae9801b38dacebf8913ce06340cfb22f32e31b", "url": "https://github.com/elastic/elasticsearch/commit/f1ae9801b38dacebf8913ce06340cfb22f32e31b", "message": "Zap", "committedDate": "2020-10-22T17:19:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI1NDA1Mw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r510254053", "bodyText": "Looks like I forgot to use this in RangeAggregator!", "author": "nik9000", "createdAt": "2020-10-22T15:27:10Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java", "diffHunk": "@@ -385,11 +385,17 @@ public boolean hasGlobalOrdinals() {\n      */\n     @Nullable\n     public Function<byte[], Number> getPointReaderOrNull() {\n-        MappedFieldType fieldType = fieldType();\n-        if (fieldType != null && script() == null && missing() == null) {\n-            return fieldType.pointReaderIfPossible();\n-        }\n-        return null;\n+        return alignesWithSearchIndex() ? fieldType().pointReaderIfPossible() : null;\n+    }\n+\n+    /**\n+     * Do {@link ValuesSource}s built by this config line up with the search\n+     * index of the underlying field? This'll only return true if the fields\n+     * is searchable and there aren't missing values or a script to confuse\n+     * the ordering.\n+     */\n+    public boolean alignesWithSearchIndex() {", "originalCommit": "117eb77edc372d3d707dca96dbcdf161e3c33faf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "9da87c50f3b30f192b3830c4803c3f77002032bd", "url": "https://github.com/elastic/elasticsearch/commit/9da87c50f3b30f192b3830c4803c3f77002032bd", "message": "Words", "committedDate": "2020-10-22T17:40:30Z", "type": "commit"}, {"oid": "3d3ea198857f7dd269e1b9e08f9356b1a69243e3", "url": "https://github.com/elastic/elasticsearch/commit/3d3ea198857f7dd269e1b9e08f9356b1a69243e3", "message": "Coment", "committedDate": "2020-10-22T17:42:40Z", "type": "commit"}, {"oid": "4291b701930cc85774df23be425225101e7b90c0", "url": "https://github.com/elastic/elasticsearch/commit/4291b701930cc85774df23be425225101e7b90c0", "message": "Add fancy query", "committedDate": "2020-10-22T20:27:26Z", "type": "commit"}, {"oid": "ff41cdbf5a7d6f40c198cad1831a616d07e3bf29", "url": "https://github.com/elastic/elasticsearch/commit/ff41cdbf5a7d6f40c198cad1831a616d07e3bf29", "message": "NOCOMMIT", "committedDate": "2020-10-26T13:34:51Z", "type": "commit"}, {"oid": "a5f00099ad9368fda09bc8284dba1f68544488ad", "url": "https://github.com/elastic/elasticsearch/commit/a5f00099ad9368fda09bc8284dba1f68544488ad", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-28T13:43:58Z", "type": "commit"}, {"oid": "ec5bf41875bd04c13bab742828d0d4062f193834", "url": "https://github.com/elastic/elasticsearch/commit/ec5bf41875bd04c13bab742828d0d4062f193834", "message": "tests\n\nnot enough yet", "committedDate": "2020-10-28T19:15:44Z", "type": "commit"}, {"oid": "6b5c09ad14bd9cce098418dea6b8be0b46b33086", "url": "https://github.com/elastic/elasticsearch/commit/6b5c09ad14bd9cce098418dea6b8be0b46b33086", "message": "missing tests", "committedDate": "2020-10-28T21:33:47Z", "type": "commit"}, {"oid": "9c26fd879a34d00274bd729f0199737d1b4cd2bd", "url": "https://github.com/elastic/elasticsearch/commit/9c26fd879a34d00274bd729f0199737d1b4cd2bd", "message": "remove!", "committedDate": "2020-10-28T21:39:51Z", "type": "commit"}, {"oid": "c5b04647e1539a481d1db7584bddfd5c74f399bb", "url": "https://github.com/elastic/elasticsearch/commit/c5b04647e1539a481d1db7584bddfd5c74f399bb", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-10-29T14:01:19Z", "type": "commit"}, {"oid": "66684f2983799e2430770b8a8daa95349f20ea7e", "url": "https://github.com/elastic/elasticsearch/commit/66684f2983799e2430770b8a8daa95349f20ea7e", "message": "I think this is more normal", "committedDate": "2020-10-29T14:27:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjI1MQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966251", "bodyText": "I think we don't need to create this weight here but do it lazy in the same way we are doing for singleValue.", "author": "iverase", "createdAt": "2020-11-02T13:20:54Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);", "originalCommit": "66684f2983799e2430770b8a8daa95349f20ea7e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODg0MQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328841", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-11-05T19:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjI1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "997e9b9948631e6463009961308790e3d414abf1", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex 36a9a278a2d..a51dd56a026 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -105,13 +105,13 @@ public class MergedPointRangeQuery extends Query {\n \n     @Override\n     public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n-        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n         return new Weight(this) {\n-            Weight mostCompactWeight;\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n \n             @Override\n             public boolean isCacheable(LeafReaderContext ctx) {\n-                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+                return true;\n             }\n \n             @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjUxOA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966518", "bodyText": "point queries always return true so we can probably just return true here?", "author": "iverase", "createdAt": "2020-11-02T13:21:24Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);", "originalCommit": "66684f2983799e2430770b8a8daa95349f20ea7e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODc5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328799", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-11-05T19:58:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NjUxOA=="}], "type": "inlineReview", "revised_code": {"commit": "997e9b9948631e6463009961308790e3d414abf1", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex 36a9a278a2d..a51dd56a026 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -105,13 +105,13 @@ public class MergedPointRangeQuery extends Query {\n \n     @Override\n     public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n-        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n         return new Weight(this) {\n-            Weight mostCompactWeight;\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n \n             @Override\n             public boolean isCacheable(LeafReaderContext ctx) {\n-                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+                return true;\n             }\n \n             @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2Njc0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515966749", "bodyText": "I think this can be safely a NOOP", "author": "iverase", "createdAt": "2020-11-02T13:21:50Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);", "originalCommit": "66684f2983799e2430770b8a8daa95349f20ea7e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyODc4MA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518328780", "bodyText": "\ud83d\udc4d", "author": "nik9000", "createdAt": "2020-11-05T19:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2Njc0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "997e9b9948631e6463009961308790e3d414abf1", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex 36a9a278a2d..a51dd56a026 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -105,13 +105,13 @@ public class MergedPointRangeQuery extends Query {\n \n     @Override\n     public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n-        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n         return new Weight(this) {\n-            Weight mostCompactWeight;\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n \n             @Override\n             public boolean isCacheable(LeafReaderContext ctx) {\n-                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+                return true;\n             }\n \n             @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r515967211", "bodyText": "We can safely delegate this method to the default implementation?", "author": "iverase", "createdAt": "2020-11-02T13:22:38Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.Explanation;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+        return new Weight(this) {\n+            Weight mostCompactWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    if (mostCompactWeight == null) {\n+                        mostCompactWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                    }\n+                    return mostCompactWeight.scorerSupplier(context);\n+                }\n+                return delegateForMultiValuedSegmentsWeight.scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                return super.bulkScorer(context);\n+            }\n+\n+            @Override\n+            @Deprecated\n+            public void extractTerms(Set<Term> terms) {\n+                delegateForMultiValuedSegmentsWeight.extractTerms(terms);\n+            }\n+\n+            @Override\n+            public Explanation explain(LeafReaderContext context, int doc) throws IOException {\n+                return delegateForMultiValuedSegmentsWeight.explain(context, doc);\n+            }", "originalCommit": "66684f2983799e2430770b8a8daa95349f20ea7e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyNzE1OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518327159", "bodyText": "Sorry, I don't understand which default implementation you mean.", "author": "nik9000", "createdAt": "2020-11-05T19:55:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1Mjk2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518752961", "bodyText": "I meant that the parent class has already an implementation and we do not need to override it. In any case I think this method is only use when called explain and in this case, it will never be used?", "author": "iverase", "createdAt": "2020-11-06T13:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc2ODY0NA==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r518768644", "bodyText": "I talked to @iverase and he thought I was using ConstantScoreWeight. I probably should have been using it. So I am now.", "author": "nik9000", "createdAt": "2020-11-06T14:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTk2NzIxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "997e9b9948631e6463009961308790e3d414abf1", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex 36a9a278a2d..a51dd56a026 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -105,13 +105,13 @@ public class MergedPointRangeQuery extends Query {\n \n     @Override\n     public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n-        Weight delegateForMultiValuedSegmentsWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n         return new Weight(this) {\n-            Weight mostCompactWeight;\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n \n             @Override\n             public boolean isCacheable(LeafReaderContext ctx) {\n-                return delegateForMultiValuedSegmentsWeight.isCacheable(ctx);\n+                return true;\n             }\n \n             @Override\n"}}, {"oid": "b1c79b24153fc796779dd0e2c0c92963f5ec7843", "url": "https://github.com/elastic/elasticsearch/commit/b1c79b24153fc796779dd0e2c0c92963f5ec7843", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-11-05T19:50:44Z", "type": "commit"}, {"oid": "997e9b9948631e6463009961308790e3d414abf1", "url": "https://github.com/elastic/elasticsearch/commit/997e9b9948631e6463009961308790e3d414abf1", "message": "Feedback", "committedDate": "2020-11-05T20:01:43Z", "type": "commit"}, {"oid": "9494447f523c1785230423819e8ccec08349fb28", "url": "https://github.com/elastic/elasticsearch/commit/9494447f523c1785230423819e8ccec08349fb28", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-11-05T20:43:20Z", "type": "commit"}, {"oid": "bfb10bd614bcfbafb77ff4d5306632a8ce67c00f", "url": "https://github.com/elastic/elasticsearch/commit/bfb10bd614bcfbafb77ff4d5306632a8ce67c00f", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-11-06T13:45:07Z", "type": "commit"}, {"oid": "1e1b1dd99159fdfca5688210ac62afa2de90ddfb", "url": "https://github.com/elastic/elasticsearch/commit/1e1b1dd99159fdfca5688210ac62afa2de90ddfb", "message": "ConstantScoreWeight", "committedDate": "2020-11-06T13:52:09Z", "type": "commit"}, {"oid": "ca09463ad6846a4579d477713a9d26e54f27bf07", "url": "https://github.com/elastic/elasticsearch/commit/ca09463ad6846a4579d477713a9d26e54f27bf07", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-11-06T15:15:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYyOTY1NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519629655", "bodyText": "I think this is incomplete, we should check delegateForSingleValuedSegments as well?", "author": "iverase", "createdAt": "2020-11-09T08:30:49Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().bulkScorer(context);\n+                }\n+                return multiValuedSegmentWeight().bulkScorer(context);\n+            }\n+\n+            private Weight singleValuedSegmentWeight() throws IOException {\n+                if (singleValuedSegmentWeight == null) {\n+                    singleValuedSegmentWeight = delegateForSingleValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return singleValuedSegmentWeight;\n+            }\n+\n+            private Weight multiValuedSegmentWeight() throws IOException {\n+                if (multiValuedSegmentWeight == null) {\n+                    multiValuedSegmentWeight = delegateForMultiValuedSegments.createWeight(searcher, scoreMode, boost);\n+                }\n+                return multiValuedSegmentWeight;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * The query used when we have single valued segments.\n+     */\n+    Query delegateForSingleValuedSegments() {\n+        return delegateForSingleValuedSegments;\n+    }\n+\n+    @Override\n+    public String toString(String field) {\n+        return \"MergedPointRange[\" + delegateForMultiValuedSegments.toString(field) + \"]\";\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (obj == null || obj.getClass() != getClass()) {\n+            return false;\n+        }\n+        MergedPointRangeQuery other = (MergedPointRangeQuery) obj;\n+        return delegateForMultiValuedSegments.equals(other.delegateForMultiValuedSegments);", "originalCommit": "ca09463ad6846a4579d477713a9d26e54f27bf07", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg5MjE5OQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519892199", "bodyText": "They depend on one another so I don't think we technically have to, but I agree, we may as well.", "author": "nik9000", "createdAt": "2020-11-09T15:19:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYyOTY1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex a1221ae4ea1..b21cf396ede 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -35,6 +35,7 @@ import org.apache.lucene.search.ScorerSupplier;\n import org.apache.lucene.search.Weight;\n \n import java.io.IOException;\n+import java.util.Objects;\n \n import static java.util.Arrays.compareUnsigned;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYzMDE3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519630175", "bodyText": "My feeling is that we do not need to implement this method. Note that queries are used in a filter context so we do not need to worry about scores?", "author": "iverase", "createdAt": "2020-11-09T08:31:46Z", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.search.aggregations.bucket.filter;\n+\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.BulkScorer;\n+import org.apache.lucene.search.ConstantScoreWeight;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.search.MatchNoDocsQuery;\n+import org.apache.lucene.search.PointRangeQuery;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.search.Scorer;\n+import org.apache.lucene.search.ScorerSupplier;\n+import org.apache.lucene.search.Weight;\n+\n+import java.io.IOException;\n+\n+import static java.util.Arrays.compareUnsigned;\n+\n+/**\n+ * Query merging two point in range queries.\n+ */\n+public class MergedPointRangeQuery extends Query {\n+    /**\n+     * Merge two {@linkplain PointRangeQuery}s into a {@linkplain MergedPointRangeQuery}\n+     * that matches points that match both filters.\n+     */\n+    public static Query merge(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        if (lhs.equals(rhs)) {\n+            // Lucky case! The queries were the same so their UNION is just the query itself.\n+            return lhs;\n+        }\n+        if (lhs.getField() != rhs.getField() || lhs.getNumDims() != rhs.getNumDims() || lhs.getBytesPerDim() != rhs.getBytesPerDim()) {\n+            return null;\n+        }\n+        return new MergedPointRangeQuery(lhs, rhs);\n+    }\n+\n+    private final String field;\n+    private final Query delegateForMultiValuedSegments;\n+    private final Query delegateForSingleValuedSegments;\n+\n+    private MergedPointRangeQuery(PointRangeQuery lhs, PointRangeQuery rhs) {\n+        field = lhs.getField();\n+        delegateForMultiValuedSegments = new BooleanQuery.Builder().add(lhs, Occur.MUST).add(rhs, Occur.MUST).build();\n+        int numDims = lhs.getNumDims();\n+        int bytesPerDim = lhs.getBytesPerDim();\n+        this.delegateForSingleValuedSegments = pickDelegateForSingleValuedSegments(\n+            mergeBound(lhs.getLowerPoint(), rhs.getLowerPoint(), numDims, bytesPerDim, true),\n+            mergeBound(lhs.getUpperPoint(), rhs.getUpperPoint(), numDims, bytesPerDim, false),\n+            numDims,\n+            bytesPerDim\n+        );\n+    }\n+\n+    private Query pickDelegateForSingleValuedSegments(byte[] lower, byte[] upper, int numDims, int bytesPerDim) {\n+        // If we ended up with disjoint ranges in any dimension then on single valued segments we can't match any docs.\n+        for (int dim = 0; dim < numDims; dim++) {\n+            int offset = dim * bytesPerDim;\n+            if (compareUnsigned(lower, offset, offset + bytesPerDim, upper, offset, offset + bytesPerDim) > 0) {\n+                return new MatchNoDocsQuery(\"disjoint ranges\");\n+            }\n+        }\n+        // Otherwise on single valued segments we can only match docs the match the UNION of the two ranges.\n+        return new PointRangeQuery(field, lower, upper, numDims) {\n+            @Override\n+            protected String toString(int dimension, byte[] value) {\n+                // Stolen from Lucene's Binary range query. It'd be best to delegate, but the method isn't visible.\n+                StringBuilder sb = new StringBuilder();\n+                sb.append(\"(\");\n+                for (int i = 0; i < value.length; i++) {\n+                    if (i > 0) {\n+                        sb.append(' ');\n+                    }\n+                    sb.append(Integer.toHexString(value[i] & 0xFF));\n+                }\n+                sb.append(')');\n+                return sb.toString();\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\n+        return new ConstantScoreWeight(this, boost) {\n+            Weight multiValuedSegmentWeight;\n+            Weight singleValuedSegmentWeight;\n+\n+            @Override\n+            public boolean isCacheable(LeafReaderContext ctx) {\n+                return true;\n+            }\n+\n+            @Override\n+            public Scorer scorer(LeafReaderContext context) throws IOException {\n+                ScorerSupplier scorerSupplier = scorerSupplier(context);\n+                if (scorerSupplier == null) {\n+                    return null;\n+                }\n+                return scorerSupplier.get(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public ScorerSupplier scorerSupplier(LeafReaderContext context) throws IOException {\n+                /*\n+                 * If we're sure docs only have a single value for the field\n+                 * we can pick the most compact bounds. If there are multiple values\n+                 * for the field we have to run the boolean query.\n+                 */\n+                PointValues points = context.reader().getPointValues(field);\n+                if (points == null) {\n+                    return null;\n+                }\n+                if (points.size() == points.getDocCount()) {\n+                    // Each doc that has points has exactly one point.\n+                    return singleValuedSegmentWeight().scorerSupplier(context);\n+                }\n+                return multiValuedSegmentWeight().scorerSupplier(context);\n+            }\n+\n+            @Override\n+            public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n+                PointValues points = context.reader().getPointValues(field);", "originalCommit": "ca09463ad6846a4579d477713a9d26e54f27bf07", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg4NTUxMw==", "url": "https://github.com/elastic/elasticsearch/pull/63643#discussion_r519885513", "bodyText": "Spoke with Nik offline and even though at the moment there is probably no benefit in implementing this method, in a future it can be improved on the Lucene side, so this implementation will take advance of it.", "author": "iverase", "createdAt": "2020-11-09T15:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYzMDE3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\nindex a1221ae4ea1..b21cf396ede 100644\n--- a/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n+++ b/server/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/MergedPointRangeQuery.java\n\n@@ -35,6 +35,7 @@ import org.apache.lucene.search.ScorerSupplier;\n import org.apache.lucene.search.Weight;\n \n import java.io.IOException;\n+import java.util.Objects;\n \n import static java.util.Arrays.compareUnsigned;\n \n"}}, {"oid": "5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "url": "https://github.com/elastic/elasticsearch/commit/5b5d6ccf659b2cf24b5a39ef1fbc6b6f0d99a6be", "message": "Iter", "committedDate": "2020-11-09T16:35:31Z", "type": "commit"}, {"oid": "fc1444d81576cd0451f2c76b3a6b9ccc58ad750b", "url": "https://github.com/elastic/elasticsearch/commit/fc1444d81576cd0451f2c76b3a6b9ccc58ad750b", "message": "Merge branch 'master' into date_histo_as_range", "committedDate": "2020-11-09T17:58:06Z", "type": "commit"}]}