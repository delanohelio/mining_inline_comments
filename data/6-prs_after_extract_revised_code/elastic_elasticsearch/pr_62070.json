{"pr_number": 62070, "pr_title": "Optimize Snapshot Shard Status Update Handling", "pr_createdAt": "2020-09-07T17:35:14Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62070", "timeline": [{"oid": "f8c866566e7012db59e18c9298c46effeca13de6", "url": "https://github.com/elastic/elasticsearch/commit/f8c866566e7012db59e18c9298c46effeca13de6", "message": "Optimize Snapshot Shard Status Update Handling\n\nAvoiding a number of noop updates that were observed to cause trouble (as in needless CS updates) in production.", "committedDate": "2020-09-07T17:31:04Z", "type": "commit"}, {"oid": "0c560fff6a67ee5d596566cef0c0816daa000b23", "url": "https://github.com/elastic/elasticsearch/commit/0c560fff6a67ee5d596566cef0c0816daa000b23", "message": "faster", "committedDate": "2020-09-07T17:42:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484697990", "bodyText": "changedCount++;?", "author": "ywelsch", "createdAt": "2020-09-08T07:10:47Z", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1887,55 +1887,67 @@ public boolean assertAllListenersResolved() {\n                         execute(ClusterState currentState, List<UpdateIndexShardSnapshotStatusRequest> tasks) {\n             int changedCount = 0;\n             final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n-            final Map<String, Set<ShardId>> reusedShardIdsByRepo = new HashMap<>();\n+            final List<UpdateIndexShardSnapshotStatusRequest> unconsumedTasks = new ArrayList<>(tasks);\n             for (SnapshotsInProgress.Entry entry : currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n-                ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();\n-                boolean updated = false;\n-\n-                for (UpdateIndexShardSnapshotStatusRequest updateSnapshotState : tasks) {\n+                if (entry.state().completed()) {\n+                    entries.add(entry);\n+                    continue;\n+                }\n+                ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = null;\n+                for (Iterator<UpdateIndexShardSnapshotStatusRequest> iterator = unconsumedTasks.iterator(); iterator.hasNext(); ) {\n+                    final UpdateIndexShardSnapshotStatusRequest updateSnapshotState = iterator.next();\n+                    final Snapshot updatedSnapshot = updateSnapshotState.snapshot();\n+                    final String updatedRepository = updatedSnapshot.getRepository();\n+                    if (entry.repository().equals(updatedRepository) == false) {\n+                        continue;\n+                    }\n                     final ShardId finishedShardId = updateSnapshotState.shardId();\n-                    if (entry.snapshot().equals(updateSnapshotState.snapshot())) {\n-                        logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updateSnapshotState.snapshot(),\n+                    if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n+                        final ShardSnapshotStatus existing = entry.shards().get(finishedShardId);\n+                        if (existing == null) {\n+                            logger.warn(\"Received shard snapshot status update [{}] but this shard is not tracked in [{}]\",\n+                                    updateSnapshotState, entry);\n+                            assert false : \"This should never happen, data nodes should only send updates for expected shards\";\n+                            continue;\n+                        }\n+                        if (existing.state().completed()) {\n+                            // No point in doing noop updates that might happen if data nodes resend shard status after a disconnect\n+                            continue;\n+                        }\n+                        logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updatedSnapshot,\n                                 finishedShardId, updateSnapshotState.status().state());\n-                        if (updated == false) {\n-                            shards.putAll(entry.shards());\n-                            updated = true;\n+                        if (shards == null) {\n+                            shards = ImmutableOpenMap.builder(entry.shards());\n                         }\n                         shards.put(finishedShardId, updateSnapshotState.status());\n                         changedCount++;\n                     } else {\n-                        final String updatedRepository = updateSnapshotState.snapshot().getRepository();\n-                        final Set<ShardId> reusedShardIds = reusedShardIdsByRepo.computeIfAbsent(updatedRepository, k -> new HashSet<>());\n-                        if (entry.state().completed() == false && entry.repository().equals(updatedRepository)\n-                                && reusedShardIds.contains(finishedShardId) == false) {\n-                            final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId);\n-                            if (existingStatus == null || existingStatus.state() != ShardState.QUEUED) {\n-                                continue;\n-                            }\n-                            if (updated == false) {\n-                                shards.putAll(entry.shards());\n-                                updated = true;\n-                            }\n-                            final ShardSnapshotStatus finishedStatus = updateSnapshotState.status();\n-                            logger.trace(\"Starting [{}] on [{}] with generation [{}]\", finishedShardId,\n-                                    finishedStatus.nodeId(), finishedStatus.generation());\n-                            shards.put(finishedShardId, new ShardSnapshotStatus(finishedStatus.nodeId(), finishedStatus.generation()));\n-                            reusedShardIds.add(finishedShardId);\n+                        final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId);\n+                        if (existingStatus == null || existingStatus.state() != ShardState.QUEUED) {\n+                            continue;\n+                        }\n+                        if (shards == null) {\n+                            shards = ImmutableOpenMap.builder(entry.shards());\n                         }\n+                        final ShardSnapshotStatus finishedStatus = updateSnapshotState.status();\n+                        logger.trace(\"Starting [{}] on [{}] with generation [{}]\", finishedShardId,\n+                                finishedStatus.nodeId(), finishedStatus.generation());\n+                        shards.put(finishedShardId, new ShardSnapshotStatus(finishedStatus.nodeId(), finishedStatus.generation()));\n+                        iterator.remove();", "originalCommit": "0c560fff6a67ee5d596566cef0c0816daa000b23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDczNjA2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484736066", "bodyText": "We only counted the updates to existing running snapshots so far but not the updates that made new shard snapshots start. I don't have much of an opinion here but I think I like the current approach marginally better than counting up here as well. It at least allows you to judge how well state update batching is working if you turn on trace logging (if you count an update either once or twice it's more confusing).", "author": "original-brownbear", "createdAt": "2020-09-08T08:17:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc1MjM3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484752371", "bodyText": "Note that we later on down use if (changedCount) to determine if a CS update is needed.", "author": "ywelsch", "createdAt": "2020-09-08T08:43:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc1MjgyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484752821", "bodyText": "Perhaps we can count both separately (and  expose in log message)", "author": "ywelsch", "createdAt": "2020-09-08T08:44:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc2MzkyNw==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484763927", "bodyText": "Note that we later on down use if (changedCount) to determine if a CS update is needed.\n\nIt is actually very important that we do that. Otherwise a dangling data node that re-sends its shard updates long after the snapshot has finished might result in two queued up snapshots having the same shard start snapshotting.\nI just realized that even with this check that is still a possibility though and we have a (very unlikely) bug here in fact in situations where data nodes resend their updates after a snapshot has finished.\nReproducing this requires a pretty tricky test and a fix needs additional changes to this loop. Maybe we can take the straight forward speedups that don't change behavior in here here separately now as is and I'll refactor this in a follow-up that also fixes the above bug?", "author": "original-brownbear", "createdAt": "2020-09-08T09:01:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc2NzQzOA==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r484767438", "bodyText": "Nevermind actually, the fix fits in here just fine. On it.", "author": "original-brownbear", "createdAt": "2020-09-08T09:07:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE0Mjk2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/62070#discussion_r485142962", "bodyText": "Alright I pushed 5c05dc1 and eb14918 to keep two counts for the log message.\nAlso, I made a change there that fixes the bug I was talking about. Currently, if a dangling data node were to send a shard update for a snapshot that is long gone from the CS it might trigger a queued up shard snapshot to start and another snapshot for that shard is already running. This is only a possibility if it gets batched up with a shard update for an existing in-progress snapshot due to the changedCount condition so it's incredibly unlikely in practice and I'm having a very hard timing coding up a test for it (even in SnapshotsResiliencyTests I can't seem to find a scenario where this kind of CS update batching occurs at any non-trivial frequency so far). I still think this is a valid fix, plus it actually improves performance to check if a task has even been reused as well to a trivial degree in some corner cases I suppose.", "author": "original-brownbear", "createdAt": "2020-09-08T19:19:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDY5Nzk5MA=="}], "type": "inlineReview", "revised_code": {"commit": "5c05dc11c8a626f625d7833a58ea5194368aa12b", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java\nindex 4c86647b676..eea90880bfc 100644\n--- a/server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java\n+++ b/server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java\n\n@@ -1886,8 +1886,10 @@ public class SnapshotsService extends AbstractLifecycleComponent implements Clus\n         public ClusterTasksResult<UpdateIndexShardSnapshotStatusRequest>\n                         execute(ClusterState currentState, List<UpdateIndexShardSnapshotStatusRequest> tasks) {\n             int changedCount = 0;\n+            int startedCount = 0;\n             final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n             final List<UpdateIndexShardSnapshotStatusRequest> unconsumedTasks = new ArrayList<>(tasks);\n+            final Set<UpdateIndexShardSnapshotStatusRequest> executedTasks = new HashSet<>();\n             for (SnapshotsInProgress.Entry entry : currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n                 if (entry.state().completed()) {\n                     entries.add(entry);\n"}}, {"oid": "bc4239a8b7d51b90bb848ba0ae39f2275a3e2c66", "url": "https://github.com/elastic/elasticsearch/commit/bc4239a8b7d51b90bb848ba0ae39f2275a3e2c66", "message": "Merge remote-tracking branch 'elastic/master' into further-optimize-snapshot-shard-update", "committedDate": "2020-09-08T08:11:12Z", "type": "commit"}, {"oid": "9d4918256ed5132d28f5654ce36fe008e7b00d14", "url": "https://github.com/elastic/elasticsearch/commit/9d4918256ed5132d28f5654ce36fe008e7b00d14", "message": "Merge remote-tracking branch 'elastic/master' into further-optimize-snapshot-shard-update", "committedDate": "2020-09-08T09:04:11Z", "type": "commit"}, {"oid": "5c05dc11c8a626f625d7833a58ea5194368aa12b", "url": "https://github.com/elastic/elasticsearch/commit/5c05dc11c8a626f625d7833a58ea5194368aa12b", "message": "reused state updates counter", "committedDate": "2020-09-08T19:01:33Z", "type": "commit"}, {"oid": "eb14918fc193edaefb8e302915fdcb6c65fd7bea", "url": "https://github.com/elastic/elasticsearch/commit/eb14918fc193edaefb8e302915fdcb6c65fd7bea", "message": "comments", "committedDate": "2020-09-08T19:13:09Z", "type": "commit"}, {"oid": "fdcc74153810282319afa1dde23b8310a63e61c1", "url": "https://github.com/elastic/elasticsearch/commit/fdcc74153810282319afa1dde23b8310a63e61c1", "message": "Merge remote-tracking branch 'elastic/master' into further-optimize-snapshot-shard-update", "committedDate": "2020-09-08T19:37:59Z", "type": "commit"}]}