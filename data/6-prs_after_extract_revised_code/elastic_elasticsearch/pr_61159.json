{"pr_number": 61159, "pr_title": "Reduce allocations when persisting cluster state", "pr_createdAt": "2020-08-14T15:53:34Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/61159", "timeline": [{"oid": "1be5be761ee5ec8c91d6aca03de15beb2e48678f", "url": "https://github.com/elastic/elasticsearch/commit/1be5be761ee5ec8c91d6aca03de15beb2e48678f", "message": "Reduce allocations when persisting cluster state\n\nToday we allocate a new `byte[]` for each document written to the\ncluster state. Some of these documents may be quite large. We need a\nbuffer that's at least as large as the largest document, but there's no\nneed to use a fresh buffer for each document.\n\nWith this commit we re-use the same `byte[]` much more, only allocating\nit afresh if we need a larger one, and using the buffer needed for one\nround of persistence as a hint for the size needed for the next one.", "committedDate": "2020-08-14T15:48:06Z", "type": "commit"}, {"oid": "933c393475bae85f9e665b0924d0b180ad4a85af", "url": "https://github.com/elastic/elasticsearch/commit/933c393475bae85f9e665b0924d0b180ad4a85af", "message": "variable", "committedDate": "2020-08-14T15:54:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDcxNDE2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r470714165", "bodyText": "I debated keeping the whole buffer around for the next time we need to write the cluster state, maybe shrinking it based on some heuristics (e.g. it was <50% used this time), but decided against it since we don't write the cluster state out that much so it might just sit there consuming 10s of MBs of heap (or more).", "author": "DaveCTurner", "createdAt": "2020-08-14T16:00:24Z", "path": "server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java", "diffHunk": "@@ -547,6 +523,10 @@ public void close() throws IOException {\n         boolean fullStateWritten = false;\n         private final AtomicBoolean closed = new AtomicBoolean();\n \n+        // The size of the document buffer that was used for the last write operation, used as a hint for allocating the buffer for the\n+        // next one.\n+        private int documentBufferUsed;", "originalCommit": "933c393475bae85f9e665b0924d0b180ad4a85af", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "46c949cdb34ecbe7077c79f64fa8dd2fc1fbfec8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\nindex e5eeeac0bd0b..1b5bb0ac8c0d 100644\n--- a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n+++ b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n\n@@ -523,10 +547,6 @@ public class PersistedClusterStateService {\n         boolean fullStateWritten = false;\n         private final AtomicBoolean closed = new AtomicBoolean();\n \n-        // The size of the document buffer that was used for the last write operation, used as a hint for allocating the buffer for the\n-        // next one.\n-        private int documentBufferUsed;\n-\n         private Writer(List<MetadataIndexWriter> metadataIndexWriters, String nodeId, BigArrays bigArrays,\n                        LongSupplier relativeTimeMillisSupplier, Supplier<TimeValue> slowWriteLoggingThresholdSupplier) {\n             this.metadataIndexWriters = metadataIndexWriters;\n"}}, {"oid": "2a105edfc63cce31a1ed8f8a24227aea032f241e", "url": "https://github.com/elastic/elasticsearch/commit/2a105edfc63cce31a1ed8f8a24227aea032f241e", "message": "Oops must preserve on resize", "committedDate": "2020-08-14T16:16:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2NDE5Nw==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471264197", "bodyText": "NIT: IOUtils.close(overflow)", "author": "original-brownbear", "createdAt": "2020-08-17T06:51:03Z", "path": "server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.common.bytes;\n+\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.BytesRefIterator;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.io.stream.BytesStream;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.util.BigArrays;\n+import org.elasticsearch.common.util.ByteArray;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+/**\n+ * An in-memory {@link StreamOutput} which first fills the given {@code byte[]} and then allocates more space from the given\n+ * {@link BigArrays} if needed. The idea is that you can use this for passing data to an API that requires a single {@code byte[]} (or a\n+ * {@link org.apache.lucene.util.BytesRef}) which you'd prefer to re-use if possible, avoiding excessive allocations, but which may not\n+ * always be large enough.\n+ */\n+public class RecyclingBytesStreamOutput extends BytesStream {\n+\n+    private final byte[] buffer;\n+    private final BigArrays bigArrays;\n+\n+    private int position;\n+\n+    @Nullable // if buffer is large enough\n+    private ByteArray overflow;\n+\n+    public RecyclingBytesStreamOutput(byte[] buffer, BigArrays bigArrays) {\n+        this.buffer = Objects.requireNonNull(buffer);\n+        this.bigArrays = Objects.requireNonNull(bigArrays);\n+    }\n+\n+    @Override\n+    public void writeByte(byte b) {\n+        if (position < buffer.length) {\n+            buffer[position++] = b;\n+        } else {\n+            ensureCapacity(position + 1);\n+            overflow.set(position++ - buffer.length, b);\n+        }\n+    }\n+\n+    private void ensureCapacity(int size) {\n+        final int overflowSize = size - buffer.length;\n+        assert overflowSize > 0 : \"no need to ensureCapacity(\" + size + \") with buffer of size [\" + buffer.length + \"]\";\n+        assert position >= buffer.length\n+                : \"no need to ensureCapacity(\" + size + \") with buffer of size [\" + buffer.length + \"] at position [\" + position + \"]\";\n+        if (overflow == null) {\n+            overflow = bigArrays.newByteArray(overflowSize, false);\n+        } else if (overflowSize > overflow.size()) {\n+            overflow = bigArrays.resize(overflow, overflowSize);\n+        }\n+        assert overflow.size() >= overflowSize;\n+    }\n+\n+    @Override\n+    public void writeBytes(byte[] b, int offset, int length) {\n+        if (position < buffer.length) {\n+            final int lengthForBuffer = Math.min(length, buffer.length - position);\n+            System.arraycopy(b, offset, buffer, position, lengthForBuffer);\n+            position += lengthForBuffer;\n+            offset += lengthForBuffer;\n+            length -= lengthForBuffer;\n+        }\n+\n+        if (length > 0) {\n+            ensureCapacity(position + length);\n+            overflow.set(position - buffer.length, b, offset, length);\n+            position += length;\n+        }\n+    }\n+\n+    @Override\n+    public void flush() {\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (overflow != null) {", "originalCommit": "2a105edfc63cce31a1ed8f8a24227aea032f241e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0MjgzMQ==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471342831", "bodyText": "Thanks, done in dbb6ef1.", "author": "DaveCTurner", "createdAt": "2020-08-17T09:05:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI2NDE5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "dbb6ef114948d74b470fddc1ed711a313bb5bf8f", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java b/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\nindex 057b0fb10f2a..38c83e9b1f9c 100644\n--- a/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\n+++ b/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\n\n@@ -26,6 +26,7 @@ import org.elasticsearch.common.io.stream.BytesStream;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.common.util.ByteArray;\n+import org.elasticsearch.core.internal.io.IOUtils;\n \n import java.io.IOException;\n import java.util.Objects;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI4MTI0Mg==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471281242", "bodyText": "I wonder if we need this as long as buffer is smaller than the page size used by BigArrays? Maybe see my suggested change here: https://github.com/elastic/elasticsearch/pull/61183/files#diff-80cb0d35f54671354739d96c0d8fc65aR61\nMaybe we could do an analogous thing here and just start with a ByteArray of capacity PageCacheRecycler.PAGE_SIZE_IN_BYTES that we pull from the pool and increase it's size as needed and get rid of having to allocate this array as long as it's under 16k, only allocating buffer if we have to materialize anything larger than 16k in toBytesRef and otherwise just unwrapping the ByteArray?\nThen we might only have to allocate something for buffer once we exceed 16k for the size of a piece of metadata but also avoid the expensive case where we allocate size >> 3 redundantly for every write for a large value of size when someone's dealing with large metadata?", "author": "original-brownbear", "createdAt": "2020-08-17T07:09:31Z", "path": "server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.common.bytes;\n+\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.BytesRefIterator;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.io.stream.BytesStream;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.util.BigArrays;\n+import org.elasticsearch.common.util.ByteArray;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+/**\n+ * An in-memory {@link StreamOutput} which first fills the given {@code byte[]} and then allocates more space from the given\n+ * {@link BigArrays} if needed. The idea is that you can use this for passing data to an API that requires a single {@code byte[]} (or a\n+ * {@link org.apache.lucene.util.BytesRef}) which you'd prefer to re-use if possible, avoiding excessive allocations, but which may not\n+ * always be large enough.\n+ */\n+public class RecyclingBytesStreamOutput extends BytesStream {\n+\n+    private final byte[] buffer;", "originalCommit": "2a105edfc63cce31a1ed8f8a24227aea032f241e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0MjYxOQ==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471342619", "bodyText": "Yeah ok that's not too bad actually, see ffa7726.", "author": "DaveCTurner", "createdAt": "2020-08-17T09:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI4MTI0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "dbb6ef114948d74b470fddc1ed711a313bb5bf8f", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java b/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\nindex 057b0fb10f2a..38c83e9b1f9c 100644\n--- a/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\n+++ b/server/src/main/java/org/elasticsearch/common/bytes/RecyclingBytesStreamOutput.java\n\n@@ -26,6 +26,7 @@ import org.elasticsearch.common.io.stream.BytesStream;\n import org.elasticsearch.common.io.stream.StreamOutput;\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.common.util.ByteArray;\n+import org.elasticsearch.core.internal.io.IOUtils;\n \n import java.io.IOException;\n import java.util.Objects;\n"}}, {"oid": "ffa7726020bf1f89302aa7eeaaaf163f76497fdc", "url": "https://github.com/elastic/elasticsearch/commit/ffa7726020bf1f89302aa7eeaaaf163f76497fdc", "message": "Allocate initial buffer from recycler too if possible", "committedDate": "2020-08-17T08:46:09Z", "type": "commit"}, {"oid": "ce97b40489cd2d0ce923c3c1b3bfd3fed30508bd", "url": "https://github.com/elastic/elasticsearch/commit/ce97b40489cd2d0ce923c3c1b3bfd3fed30508bd", "message": "Use MockBigArrays more", "committedDate": "2020-08-17T08:46:34Z", "type": "commit"}, {"oid": "dbb6ef114948d74b470fddc1ed711a313bb5bf8f", "url": "https://github.com/elastic/elasticsearch/commit/dbb6ef114948d74b470fddc1ed711a313bb5bf8f", "message": "IOUtils", "committedDate": "2020-08-17T09:01:16Z", "type": "commit"}, {"oid": "923425e3e7673bb09e66a58f25bd955721ec69a2", "url": "https://github.com/elastic/elasticsearch/commit/923425e3e7673bb09e66a58f25bd955721ec69a2", "message": "Assert exclusive access", "committedDate": "2020-08-17T09:01:36Z", "type": "commit"}, {"oid": "1718530b9ba72d51e07f1b36debba24982e9f42c", "url": "https://github.com/elastic/elasticsearch/commit/1718530b9ba72d51e07f1b36debba24982e9f42c", "message": "Precommit", "committedDate": "2020-08-17T09:04:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0NDI0OQ==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471344249", "bodyText": "We can simplify all this once https://github.com/elastic/elasticsearch/pull/61183/files#diff-80cb0d35f54671354739d96c0d8fc65a is merged", "author": "DaveCTurner", "createdAt": "2020-08-17T09:08:05Z", "path": "server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java", "diffHunk": "@@ -802,59 +803,97 @@ public void close() throws IOException {\n             }\n         }\n \n-        private ReleasableDocument makeIndexMetadataDocument(IndexMetadata indexMetadata) throws IOException {\n-            final ReleasableDocument indexMetadataDocument = makeDocument(INDEX_TYPE_NAME, indexMetadata);\n-            boolean success = false;\n-            try {\n-                final String indexUUID = indexMetadata.getIndexUUID();\n-                assert indexUUID.equals(IndexMetadata.INDEX_UUID_NA_VALUE) == false;\n-                indexMetadataDocument.getDocument().add(new StringField(INDEX_UUID_FIELD_NAME, indexUUID, Field.Store.NO));\n-                success = true;\n-                return indexMetadataDocument;\n-            } finally {\n-                if (success == false) {\n-                    IOUtils.closeWhileHandlingException(indexMetadataDocument);\n-                }\n-            }\n+        private Document makeIndexMetadataDocument(IndexMetadata indexMetadata, DocumentBuffer documentBuffer) throws IOException {\n+            final Document indexMetadataDocument = makeDocument(INDEX_TYPE_NAME, indexMetadata, documentBuffer);\n+            final String indexUUID = indexMetadata.getIndexUUID();\n+            assert indexUUID.equals(IndexMetadata.INDEX_UUID_NA_VALUE) == false;\n+            indexMetadataDocument.add(new StringField(INDEX_UUID_FIELD_NAME, indexUUID, Field.Store.NO));\n+            return indexMetadataDocument;\n         }\n \n-        private ReleasableDocument makeGlobalMetadataDocument(Metadata metadata) throws IOException {\n-            return makeDocument(GLOBAL_TYPE_NAME, metadata);\n+        private Document makeGlobalMetadataDocument(Metadata metadata, DocumentBuffer documentBuffer) throws IOException {\n+            return makeDocument(GLOBAL_TYPE_NAME, metadata, documentBuffer);\n         }\n \n-        private ReleasableDocument makeDocument(String typeName, ToXContent metadata) throws IOException {\n+        private Document makeDocument(String typeName, ToXContent metadata, DocumentBuffer documentBuffer) throws IOException {\n             final Document document = new Document();\n             document.add(new StringField(TYPE_FIELD_NAME, typeName, Field.Store.NO));\n \n-            boolean success = false;\n-            final ReleasableBytesStreamOutput releasableBytesStreamOutput = new ReleasableBytesStreamOutput(bigArrays);\n-            try {\n-                final FilterOutputStream outputStream = new FilterOutputStream(releasableBytesStreamOutput) {\n-\n-                    @Override\n-                    public void write(byte[] b, int off, int len) throws IOException {\n-                        out.write(b, off, len);\n-                    }\n-\n-                    @Override\n-                    public void close() {\n-                        // closing the XContentBuilder should not release the bytes yet\n-                    }\n-                };\n-                try (XContentBuilder xContentBuilder = XContentFactory.contentBuilder(XContentType.SMILE, outputStream)) {\n+            try (RecyclingBytesStreamOutput streamOutput = documentBuffer.streamOutput()) {\n+                try (XContentBuilder xContentBuilder = XContentFactory.contentBuilder(XContentType.SMILE,\n+                        Streams.flushOnCloseStream(streamOutput))) {\n                     xContentBuilder.startObject();\n                     metadata.toXContent(xContentBuilder, FORMAT_PARAMS);\n                     xContentBuilder.endObject();\n                 }\n-                document.add(new StoredField(DATA_FIELD_NAME, releasableBytesStreamOutput.bytes().toBytesRef()));\n-                final ReleasableDocument releasableDocument = new ReleasableDocument(document, releasableBytesStreamOutput);\n-                success = true;\n-                return releasableDocument;\n-            } finally {\n-                if (success == false) {\n-                    IOUtils.closeWhileHandlingException(releasableBytesStreamOutput);\n+                document.add(new StoredField(DATA_FIELD_NAME, streamOutput.toBytesRef()));\n+            }\n+\n+            return document;\n+        }\n+    }\n+\n+    /**\n+     * Holds the current buffer, keeping track of new allocations as it grows.\n+     */\n+    private static class DocumentBuffer implements Releasable {\n+        private final BigArrays bigArrays;\n+\n+        @Nullable // if the initial page doesn't need releasing\n+        private final Releasable releasable;\n+        private byte[] buffer;\n+        private int maxUsed;\n+\n+        DocumentBuffer(int size, BigArrays bigArrays) {\n+            if (size <= PageCacheRecycler.PAGE_SIZE_IN_BYTES) {\n+                final ByteArray byteArray = bigArrays.newByteArray(PageCacheRecycler.PAGE_SIZE_IN_BYTES);\n+                final BytesRefIterator iterator = new PagedBytesReference(byteArray, Math.toIntExact(byteArray.size())).iterator();", "originalCommit": "1718530b9ba72d51e07f1b36debba24982e9f42c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "46c949cdb34ecbe7077c79f64fa8dd2fc1fbfec8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\nindex ef733223762b..1b5bb0ac8c0d 100644\n--- a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n+++ b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n\n@@ -803,97 +802,59 @@ public class PersistedClusterStateService {\n             }\n         }\n \n-        private Document makeIndexMetadataDocument(IndexMetadata indexMetadata, DocumentBuffer documentBuffer) throws IOException {\n-            final Document indexMetadataDocument = makeDocument(INDEX_TYPE_NAME, indexMetadata, documentBuffer);\n-            final String indexUUID = indexMetadata.getIndexUUID();\n-            assert indexUUID.equals(IndexMetadata.INDEX_UUID_NA_VALUE) == false;\n-            indexMetadataDocument.add(new StringField(INDEX_UUID_FIELD_NAME, indexUUID, Field.Store.NO));\n-            return indexMetadataDocument;\n+        private ReleasableDocument makeIndexMetadataDocument(IndexMetadata indexMetadata) throws IOException {\n+            final ReleasableDocument indexMetadataDocument = makeDocument(INDEX_TYPE_NAME, indexMetadata);\n+            boolean success = false;\n+            try {\n+                final String indexUUID = indexMetadata.getIndexUUID();\n+                assert indexUUID.equals(IndexMetadata.INDEX_UUID_NA_VALUE) == false;\n+                indexMetadataDocument.getDocument().add(new StringField(INDEX_UUID_FIELD_NAME, indexUUID, Field.Store.NO));\n+                success = true;\n+                return indexMetadataDocument;\n+            } finally {\n+                if (success == false) {\n+                    IOUtils.closeWhileHandlingException(indexMetadataDocument);\n+                }\n+            }\n         }\n \n-        private Document makeGlobalMetadataDocument(Metadata metadata, DocumentBuffer documentBuffer) throws IOException {\n-            return makeDocument(GLOBAL_TYPE_NAME, metadata, documentBuffer);\n+        private ReleasableDocument makeGlobalMetadataDocument(Metadata metadata) throws IOException {\n+            return makeDocument(GLOBAL_TYPE_NAME, metadata);\n         }\n \n-        private Document makeDocument(String typeName, ToXContent metadata, DocumentBuffer documentBuffer) throws IOException {\n+        private ReleasableDocument makeDocument(String typeName, ToXContent metadata) throws IOException {\n             final Document document = new Document();\n             document.add(new StringField(TYPE_FIELD_NAME, typeName, Field.Store.NO));\n \n-            try (RecyclingBytesStreamOutput streamOutput = documentBuffer.streamOutput()) {\n-                try (XContentBuilder xContentBuilder = XContentFactory.contentBuilder(XContentType.SMILE,\n-                        Streams.flushOnCloseStream(streamOutput))) {\n+            boolean success = false;\n+            final ReleasableBytesStreamOutput releasableBytesStreamOutput = new ReleasableBytesStreamOutput(bigArrays);\n+            try {\n+                final FilterOutputStream outputStream = new FilterOutputStream(releasableBytesStreamOutput) {\n+\n+                    @Override\n+                    public void write(byte[] b, int off, int len) throws IOException {\n+                        out.write(b, off, len);\n+                    }\n+\n+                    @Override\n+                    public void close() {\n+                        // closing the XContentBuilder should not release the bytes yet\n+                    }\n+                };\n+                try (XContentBuilder xContentBuilder = XContentFactory.contentBuilder(XContentType.SMILE, outputStream)) {\n                     xContentBuilder.startObject();\n                     metadata.toXContent(xContentBuilder, FORMAT_PARAMS);\n                     xContentBuilder.endObject();\n                 }\n-                document.add(new StoredField(DATA_FIELD_NAME, streamOutput.toBytesRef()));\n-            }\n-\n-            return document;\n-        }\n-    }\n-\n-    /**\n-     * Holds the current buffer, keeping track of new allocations as it grows.\n-     */\n-    private static class DocumentBuffer implements Releasable {\n-        private final BigArrays bigArrays;\n-\n-        @Nullable // if the initial page doesn't need releasing\n-        private final Releasable releasable;\n-        private byte[] buffer;\n-        private int maxUsed;\n-\n-        DocumentBuffer(int size, BigArrays bigArrays) {\n-            if (size <= PageCacheRecycler.PAGE_SIZE_IN_BYTES) {\n-                final ByteArray byteArray = bigArrays.newByteArray(PageCacheRecycler.PAGE_SIZE_IN_BYTES);\n-                final BytesRefIterator iterator = new PagedBytesReference(byteArray, Math.toIntExact(byteArray.size())).iterator();\n-                final BytesRef firstPage;\n-                try {\n-                    firstPage = iterator.next();\n-                    assert iterator.next() == null : \"should be one page\";\n-                } catch (IOException e) {\n-                    throw new AssertionError(\"impossible\", e);\n+                document.add(new StoredField(DATA_FIELD_NAME, releasableBytesStreamOutput.bytes().toBytesRef()));\n+                final ReleasableDocument releasableDocument = new ReleasableDocument(document, releasableBytesStreamOutput);\n+                success = true;\n+                return releasableDocument;\n+            } finally {\n+                if (success == false) {\n+                    IOUtils.closeWhileHandlingException(releasableBytesStreamOutput);\n                 }\n-\n-                // we require that we have the whole page to ourselves\n-                assert firstPage.offset == 0 : firstPage.offset;\n-                assert firstPage.bytes.length == PageCacheRecycler.PAGE_SIZE_IN_BYTES : firstPage.bytes.length;\n-                buffer = firstPage.bytes;\n-                releasable = byteArray;\n-            } else {\n-                buffer = new byte[size];\n-                releasable = null;\n             }\n-            this.bigArrays = bigArrays;\n-            maxUsed = 0;\n-        }\n-\n-        RecyclingBytesStreamOutput streamOutput() {\n-            return new RecyclingBytesStreamOutput(buffer, bigArrays) {\n-                @Override\n-                public BytesRef toBytesRef() {\n-                    final BytesRef bytesRef = super.toBytesRef();\n-                    maxUsed = Math.max(maxUsed, bytesRef.length);\n-                    if (buffer != bytesRef.bytes) {\n-                        assert bytesRef.length > buffer.length;\n-                        logger.trace(\"growing document buffer from [{}] to [{}]\", buffer.length, maxUsed);\n-                        buffer = bytesRef.bytes;\n-                    }\n-                    assert maxUsed <= buffer.length;\n-                    return bytesRef;\n-                }\n-            };\n-        }\n-\n-        int getMaxUsed() {\n-            return maxUsed;\n-        }\n-\n-        @Override\n-        public void close() {\n-            Releasables.close(releasable);\n         }\n     }\n }\n-\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0ODU3NQ==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r471348575", "bodyText": "This is something of an educated guess TBH. An extra 16kB on top of whatever we needed last time doesn't seem so bad, we could also use something like min(16kB, documentBufferUsed/8) to make it a bit more proportional without blowing up on enormous states but whatever we do here is almost certainly good enough.", "author": "DaveCTurner", "createdAt": "2020-08-17T09:16:24Z", "path": "server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java", "diffHunk": "@@ -716,28 +706,39 @@ private WriterStats overwriteMetadata(Metadata metadata) throws IOException {\n          * Add documents for the metadata of the given cluster state, assuming that there are currently no documents.\n          */\n         private WriterStats addMetadata(Metadata metadata) throws IOException {\n-            try (ReleasableDocument globalMetadataDocument = makeGlobalMetadataDocument(metadata)) {\n+            try (DocumentBuffer documentBuffer = allocateBuffer()) {\n+\n+                final Document globalMetadataDocument = makeGlobalMetadataDocument(metadata, documentBuffer);\n                 for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                    metadataIndexWriter.updateGlobalMetadata(globalMetadataDocument.getDocument());\n+                    metadataIndexWriter.updateGlobalMetadata(globalMetadataDocument);\n                 }\n-            }\n \n-            for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n-                final IndexMetadata indexMetadata = cursor.value;\n-                try (ReleasableDocument indexMetadataDocument = makeIndexMetadataDocument(indexMetadata)) {\n+                for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n+                    final IndexMetadata indexMetadata = cursor.value;\n+                    final Document indexMetadataDocument = makeIndexMetadataDocument(indexMetadata, documentBuffer);\n                     for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                        metadataIndexWriter.updateIndexMetadataDocument(indexMetadataDocument.getDocument(), indexMetadata.getIndex());\n+                        metadataIndexWriter.updateIndexMetadataDocument(indexMetadataDocument, indexMetadata.getIndex());\n                     }\n                 }\n-            }\n \n-            // Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more\n-            // gracefully than one that occurs during the commit process.\n-            for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                metadataIndexWriter.flush();\n+                documentBufferUsed = documentBuffer.getMaxUsed();\n+\n+                // Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more\n+                // gracefully than one that occurs during the commit process.\n+                for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n+                    metadataIndexWriter.flush();\n+                }\n+\n+                return new WriterStats(true, metadata.indices().size(), 0);\n             }\n+        }\n \n-            return new WriterStats(true, metadata.indices().size(), 0);\n+        private DocumentBuffer allocateBuffer() {\n+            // heuristics for picking the initial buffer size based on the buffer we needed last time: try and fit within a single page,\n+            // but if we needed more than a single page last time then allow a bit more space to try and avoid needing to grow the buffer\n+            // later on.\n+            final int extraSpace = documentBufferUsed <= PageCacheRecycler.PAGE_SIZE_IN_BYTES ? 0 : PageCacheRecycler.PAGE_SIZE_IN_BYTES;", "originalCommit": "1718530b9ba72d51e07f1b36debba24982e9f42c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "46c949cdb34ecbe7077c79f64fa8dd2fc1fbfec8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\nindex ef733223762b..1b5bb0ac8c0d 100644\n--- a/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n+++ b/server/src/main/java/org/elasticsearch/gateway/PersistedClusterStateService.java\n\n@@ -706,39 +716,28 @@ public class PersistedClusterStateService {\n          * Add documents for the metadata of the given cluster state, assuming that there are currently no documents.\n          */\n         private WriterStats addMetadata(Metadata metadata) throws IOException {\n-            try (DocumentBuffer documentBuffer = allocateBuffer()) {\n-\n-                final Document globalMetadataDocument = makeGlobalMetadataDocument(metadata, documentBuffer);\n+            try (ReleasableDocument globalMetadataDocument = makeGlobalMetadataDocument(metadata)) {\n                 for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                    metadataIndexWriter.updateGlobalMetadata(globalMetadataDocument);\n+                    metadataIndexWriter.updateGlobalMetadata(globalMetadataDocument.getDocument());\n                 }\n+            }\n \n-                for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n-                    final IndexMetadata indexMetadata = cursor.value;\n-                    final Document indexMetadataDocument = makeIndexMetadataDocument(indexMetadata, documentBuffer);\n+            for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n+                final IndexMetadata indexMetadata = cursor.value;\n+                try (ReleasableDocument indexMetadataDocument = makeIndexMetadataDocument(indexMetadata)) {\n                     for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                        metadataIndexWriter.updateIndexMetadataDocument(indexMetadataDocument, indexMetadata.getIndex());\n+                        metadataIndexWriter.updateIndexMetadataDocument(indexMetadataDocument.getDocument(), indexMetadata.getIndex());\n                     }\n                 }\n+            }\n \n-                documentBufferUsed = documentBuffer.getMaxUsed();\n-\n-                // Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more\n-                // gracefully than one that occurs during the commit process.\n-                for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n-                    metadataIndexWriter.flush();\n-                }\n-\n-                return new WriterStats(true, metadata.indices().size(), 0);\n+            // Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more\n+            // gracefully than one that occurs during the commit process.\n+            for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) {\n+                metadataIndexWriter.flush();\n             }\n-        }\n \n-        private DocumentBuffer allocateBuffer() {\n-            // heuristics for picking the initial buffer size based on the buffer we needed last time: try and fit within a single page,\n-            // but if we needed more than a single page last time then allow a bit more space to try and avoid needing to grow the buffer\n-            // later on.\n-            final int extraSpace = documentBufferUsed <= PageCacheRecycler.PAGE_SIZE_IN_BYTES ? 0 : PageCacheRecycler.PAGE_SIZE_IN_BYTES;\n-            return new DocumentBuffer(documentBufferUsed + extraSpace, bigArrays);\n+            return new WriterStats(true, metadata.indices().size(), 0);\n         }\n \n         public void writeIncrementalTermUpdateAndCommit(long currentTerm, long lastAcceptedVersion) throws IOException {\n"}}, {"oid": "46c949cdb34ecbe7077c79f64fa8dd2fc1fbfec8", "url": "https://github.com/elastic/elasticsearch/commit/46c949cdb34ecbe7077c79f64fa8dd2fc1fbfec8", "message": "Log persistently-failing join attempts at `WARN`", "committedDate": "2020-08-17T09:25:10Z", "type": "commit"}, {"oid": "4d5f315c29e137774430858dce871def2734782e", "url": "https://github.com/elastic/elasticsearch/commit/4d5f315c29e137774430858dce871def2734782e", "message": "Merge branch 'master' into 2020-08-14-less-allocation-when-persisting-cluster-state", "committedDate": "2020-08-17T09:41:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAzMTkxNw==", "url": "https://github.com/elastic/elasticsearch/pull/61159#discussion_r472031917", "bodyText": "D'oh this wasn't supposed to be here, it leaked from another branch. It's a good change IMO but completely unrelated. Just raising awareness in case there's any objections.", "author": "DaveCTurner", "createdAt": "2020-08-18T09:08:09Z", "path": "server/src/main/java/org/elasticsearch/cluster/coordination/JoinHelper.java", "diffHunk": "@@ -205,7 +205,7 @@ static Level getLogLevel(TransportException e) {\n         }\n \n         void logWarnWithTimestamp() {\n-            logger.info(() -> new ParameterizedMessage(\"last failed join attempt was {} ago, failed to join {} with {}\",\n+            logger.warn(() -> new ParameterizedMessage(\"last failed join attempt was {} ago, failed to join {} with {}\",", "originalCommit": "4d5f315c29e137774430858dce871def2734782e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}