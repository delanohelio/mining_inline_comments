{"pr_number": 66769, "pr_title": "Fix azure repo stream exhaust check for multipart uploads", "pr_createdAt": "2020-12-23T07:28:33Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/66769", "timeline": [{"oid": "b5846a7d83670467d707957abec6b90a08dd1f92", "url": "https://github.com/elastic/elasticsearch/commit/b5846a7d83670467d707957abec6b90a08dd1f92", "message": "Fix", "committedDate": "2020-12-23T07:12:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE4NTg4OA==", "url": "https://github.com/elastic/elasticsearch/pull/66769#discussion_r550185888", "bodyText": "I wonder if we should just drop this check altogether instead? It seems kind of weird to check that we drained the stream fully when we also give the size we want to write (and the check relies on the fact that the stream reports available() as well which is not guaranteed).\nMaybe we should just drop this check and only check that currentTotalLength == blobSize at the end? I think I'd prefer that option and it simplifies things?", "author": "original-brownbear", "createdAt": "2020-12-30T12:56:11Z", "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -438,8 +449,21 @@ private void executeMultipartUpload(String blobName, InputStream inputStream, lo\n             final List<String> blockIds = new ArrayList<>(nbParts);\n             for (int i = 0; i < nbParts; i++) {\n                 final long length = i < nbParts - 1 ? partSize : lastPartSize;\n-                final Flux<ByteBuffer> byteBufferFlux =\n-                    convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+                Flux<ByteBuffer> byteBufferFlux = convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n+                if (i == nbParts - 1) {\n+                    byteBufferFlux.doOnComplete(() -> {\n+                        try {\n+                            if (inputStream.available() > 0) {", "originalCommit": "b5846a7d83670467d707957abec6b90a08dd1f92", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQxOTI2NA==", "url": "https://github.com/elastic/elasticsearch/pull/66769#discussion_r550419264", "bodyText": "I'll take your suggestion to remove the check, although I'm torn between the complexity it adds in the code vs the aid in troubleshooting, but maybe the existing check about \"currentTotalLength > blobSize length\" is sufficient.", "author": "albertzaharovits", "createdAt": "2020-12-31T07:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE4NTg4OA=="}], "type": "inlineReview", "revised_code": {"commit": "491d348e5d3b8c15c782e2b42bfea6daff0feba9", "chunk": "diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java b/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java\nindex 2a25b9b74cd..e411552d6d1 100644\n--- a/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java\n+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java\n\n@@ -450,20 +438,6 @@ public class AzureBlobStore implements BlobStore {\n             for (int i = 0; i < nbParts; i++) {\n                 final long length = i < nbParts - 1 ? partSize : lastPartSize;\n                 Flux<ByteBuffer> byteBufferFlux = convertStreamToByteBuffer(inputStream, length, DEFAULT_UPLOAD_BUFFERS_SIZE);\n-                if (i == nbParts - 1) {\n-                    byteBufferFlux.doOnComplete(() -> {\n-                        try {\n-                            if (inputStream.available() > 0) {\n-                                long totalLength = blobSize + inputStream.available();\n-                                throw new IllegalStateException(\n-                                        \"InputStream provided \" + totalLength + \" bytes, more than the expected \" + totalLength + \" bytes\"\n-                                );\n-                            }\n-                        } catch (IOException e) {\n-                            throw new RuntimeException(e);\n-                        }\n-                    });\n-                }\n \n                 final String blockId = UUIDs.base64UUID();\n                 blockBlobAsyncClient.stageBlock(blockId, byteBufferFlux, length).block();\n"}}, {"oid": "cd6048967e703cbd97de19123126fbfee297d333", "url": "https://github.com/elastic/elasticsearch/commit/cd6048967e703cbd97de19123126fbfee297d333", "message": "Merge branch 'master' into fix-azure-repo-flux-ensure-exhausted-stream", "committedDate": "2020-12-31T07:13:24Z", "type": "commit"}, {"oid": "491d348e5d3b8c15c782e2b42bfea6daff0feba9", "url": "https://github.com/elastic/elasticsearch/commit/491d348e5d3b8c15c782e2b42bfea6daff0feba9", "message": "Remove doOnComplete that stream is exhausted", "committedDate": "2020-12-31T07:36:57Z", "type": "commit"}, {"oid": "491d348e5d3b8c15c782e2b42bfea6daff0feba9", "url": "https://github.com/elastic/elasticsearch/commit/491d348e5d3b8c15c782e2b42bfea6daff0feba9", "message": "Remove doOnComplete that stream is exhausted", "committedDate": "2020-12-31T07:36:57Z", "type": "forcePushed"}, {"oid": "17a735e1be60e87fa1eb0000c32ec1471ee74296", "url": "https://github.com/elastic/elasticsearch/commit/17a735e1be60e87fa1eb0000c32ec1471ee74296", "message": "Merge branch 'master' into fix-azure-repo-flux-ensure-exhausted-stream", "committedDate": "2021-01-04T14:04:52Z", "type": "commit"}]}