{"pr_number": 55260, "pr_title": "[ML] partitions model definitions into chunks", "pr_createdAt": "2020-04-15T19:10:14Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/55260", "timeline": [{"oid": "364b0eddc89717bfad484932e3f133f13f4ff1ea", "url": "https://github.com/elastic/elasticsearch/commit/364b0eddc89717bfad484932e3f133f13f4ff1ea", "message": "[ML] partitions model definitions into chunks", "committedDate": "2020-04-15T19:08:54Z", "type": "commit"}, {"oid": "532c236f76562c06e8eb44c7ced176936f34ee4a", "url": "https://github.com/elastic/elasticsearch/commit/532c236f76562c06e8eb44c7ced176936f34ee4a", "message": "add warning about bwc", "committedDate": "2020-04-16T11:52:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMyNjkwNw==", "url": "https://github.com/elastic/elasticsearch/pull/55260#discussion_r411326907", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            \"Creating a new model that all nodes are at least version [{}]\",\n          \n          \n            \n                            \"Creating a new model requires that all nodes are at least version [{}]\",", "author": "droberts195", "createdAt": "2020-04-20T12:12:10Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportPutTrainedModelAction.java", "diffHunk": "@@ -82,6 +82,15 @@ protected void masterOperation(Task task,\n                                    PutTrainedModelAction.Request request,\n                                    ClusterState state,\n                                    ActionListener<Response> listener) {\n+        // 7.8.0 introduced splitting the model definition across multiple documents.\n+        // This means that new models will not be usable on nodes that cannot handle multiple definition documents\n+        if (state.nodes().getMinNodeVersion().before(Version.V_7_8_0)) {\n+            listener.onFailure(ExceptionsHelper.badRequestException(\n+                \"Creating a new model that all nodes are at least version [{}]\",", "originalCommit": "532c236f76562c06e8eb44c7ced176936f34ee4a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "562e0a45f456c66c3b970e96e64346c27d0b74e7", "chunk": "diff --git a/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportPutTrainedModelAction.java b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportPutTrainedModelAction.java\nindex ae7620addb6..8b8499391bb 100644\n--- a/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportPutTrainedModelAction.java\n+++ b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/action/TransportPutTrainedModelAction.java\n\n@@ -86,7 +86,7 @@ public class TransportPutTrainedModelAction extends TransportMasterNodeAction<Re\n         // This means that new models will not be usable on nodes that cannot handle multiple definition documents\n         if (state.nodes().getMinNodeVersion().before(Version.V_7_8_0)) {\n             listener.onFailure(ExceptionsHelper.badRequestException(\n-                \"Creating a new model that all nodes are at least version [{}]\",\n+                \"Creating a new model requires that all nodes are at least version [{}]\",\n                 request.getTrainedModelConfig().getModelId(),\n                 Version.V_7_8_0.toString()));\n             return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMyODAzNQ==", "url": "https://github.com/elastic/elasticsearch/pull/55260#discussion_r411328035", "bodyText": "It should return here.", "author": "droberts195", "createdAt": "2020-04-20T12:14:07Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -138,30 +143,40 @@ public void storeTrainedModel(TrainedModelConfig trainedModelConfig,\n     private void storeTrainedModelAndDefinition(TrainedModelConfig trainedModelConfig,\n                                                 ActionListener<Boolean> listener) {\n \n-        TrainedModelDefinitionDoc trainedModelDefinitionDoc;\n+        List<TrainedModelDefinitionDoc> trainedModelDefinitionDocs = new ArrayList<>();\n         try {\n-            // TODO should we check length against allowed stream size???\n             String compressedString = trainedModelConfig.getCompressedDefinition();\n-            trainedModelDefinitionDoc = new TrainedModelDefinitionDoc.Builder()\n-                .setDocNum(0)\n-                .setModelId(trainedModelConfig.getModelId())\n-                .setCompressedString(compressedString)\n-                .setCompressionVersion(TrainedModelConfig.CURRENT_DEFINITION_COMPRESSION_VERSION)\n-                .setDefinitionLength(compressedString.length())\n-                .setTotalDefinitionLength(compressedString.length())\n-                .build();\n+            if (compressedString.length() > MAX_COMPRESSED_STRING_SIZE) {\n+                listener.onFailure(\n+                    ExceptionsHelper.badRequestException(\n+                        \"Unable to store model as compressed definition has length [{}] the limit is [{}]\",\n+                        compressedString.length(),\n+                        MAX_COMPRESSED_STRING_SIZE));", "originalCommit": "532c236f76562c06e8eb44c7ced176936f34ee4a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "562e0a45f456c66c3b970e96e64346c27d0b74e7", "chunk": "diff --git a/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java\nindex 116f85a600a..cd5b65ee729 100644\n--- a/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java\n+++ b/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java\n\n@@ -152,6 +152,7 @@ public class TrainedModelProvider {\n                         \"Unable to store model as compressed definition has length [{}] the limit is [{}]\",\n                         compressedString.length(),\n                         MAX_COMPRESSED_STRING_SIZE));\n+                return;\n             }\n             List<String> chunkedStrings = chunkStringWithSize(compressedString, COMPRESSED_STRING_CHUNK_SIZE);\n             for(int i = 0; i < chunkedStrings.size(); ++i) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM1MTgwOA==", "url": "https://github.com/elastic/elasticsearch/pull/55260#discussion_r411351808", "bodyText": "I am happy to merge this PR more-or-less as-is (see the two other minor comments I made).\nHowever, in the long term if we're expecting to deal with models whose compressed definition is of the order of a gigabyte then I think more optimisations will be needed.  For example:\n\nSuppose this method is dealing with a str of length 1000000000\nThat requires 2000000000 bytes to store as a UTF-16 String\nThen at the point this method returns every character has been copied into new strings but the original still exists so the memory usage at the point of return is over 4000000000 bytes\n\nSo we've gone from ~1GB of JSON model to temporarily using 4GB memory to manipulate it.\nSince the compressed model is Base64, if it was stored as UTF-8 then that would be 1 byte per character.  And if this was wrapped in a BytesReference instead of a String then the returned List<String> could be a List<BytesReference> where the raw data was not duplicated but just referenced in slices by the list elements.\nBasically, as we move from dealing with a few kilobytes of data to gigabytes we need to think much more about avoiding unnecessary duplication, most efficient character sets, etc.  But not in this PR, it can be done in a future one.", "author": "droberts195", "createdAt": "2020-04-20T12:52:34Z", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -677,13 +709,36 @@ private static QueryBuilder buildQueryIdExpressionQuery(String[] tokens, String\n     private static <T> T handleSearchItem(MultiSearchResponse.Item item,\n                                           String resourceId,\n                                           CheckedBiFunction<BytesReference, String, T, Exception> parseLeniently) throws Exception {\n+        return handleSearchItems(item, resourceId, parseLeniently).get(0);\n+    }\n+\n+    // NOTE: This ignores any results that are in a different index than the first one seen in the search response.\n+    private static <T> List<T> handleSearchItems(MultiSearchResponse.Item item,\n+                                                 String resourceId,\n+                                                 CheckedBiFunction<BytesReference, String, T, Exception> parseLeniently) throws Exception {\n         if (item.isFailure()) {\n             throw item.getFailure();\n         }\n         if (item.getResponse().getHits().getHits().length == 0) {\n             throw new ResourceNotFoundException(resourceId);\n         }\n-        return parseLeniently.apply(item.getResponse().getHits().getHits()[0].getSourceRef(), resourceId);\n+        List<T> results = new ArrayList<>(item.getResponse().getHits().getHits().length);\n+        String initialIndex = item.getResponse().getHits().getHits()[0].getIndex();\n+        for (SearchHit hit : item.getResponse().getHits().getHits()) {\n+            // We don't want to spread across multiple backing indices\n+            if (hit.getIndex().equals(initialIndex)) {\n+                results.add(parseLeniently.apply(hit.getSourceRef(), resourceId));\n+            }\n+        }\n+        return results;\n+    }\n+\n+    static List<String> chunkStringWithSize(String str, int chunkSize) {\n+        List<String> subStrings = new ArrayList<>((int)Math.ceil(str.length()/(double)chunkSize));\n+        for (int i = 0; i < str.length();i += chunkSize) {\n+            subStrings.add(str.substring(i, Math.min(i + chunkSize, str.length())));\n+        }\n+        return subStrings;", "originalCommit": "532c236f76562c06e8eb44c7ced176936f34ee4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQxOTA5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/55260#discussion_r411419093", "bodyText": "@droberts195 yes, I realize this is an issue. My goal with this PR is to lay the groundwork, so that at least at the data layer (how things are stored/read in/from the index) we truly support more than one doc.", "author": "benwtrent", "createdAt": "2020-04-20T14:22:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM1MTgwOA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "ac8a3fde0ffa55209cccb23cf5fb1c37d2516acf", "url": "https://github.com/elastic/elasticsearch/commit/ac8a3fde0ffa55209cccb23cf5fb1c37d2516acf", "message": "Merge branch 'master' into feature/ml-inference-split-inference-docs", "committedDate": "2020-04-20T17:00:10Z", "type": "commit"}, {"oid": "562e0a45f456c66c3b970e96e64346c27d0b74e7", "url": "https://github.com/elastic/elasticsearch/commit/562e0a45f456c66c3b970e96e64346c27d0b74e7", "message": "addressing PR comments", "committedDate": "2020-04-20T17:01:45Z", "type": "commit"}]}