{"pr_number": 62509, "pr_title": "Faster sequential access for stored fields", "pr_createdAt": "2020-09-16T22:33:40Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/62509", "timeline": [{"oid": "7ce055ba46ee082580bc7efa49ffbea315e080e8", "url": "https://github.com/elastic/elasticsearch/commit/7ce055ba46ee082580bc7efa49ffbea315e080e8", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T00:04:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA0NTI2MQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490045261", "bodyText": "No need to clone, it's a thread-local instance already.", "author": "jpountz", "createdAt": "2020-09-17T07:56:49Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -114,7 +118,17 @@ public void execute(SearchContext context) {\n                 int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n                 if (currentReaderIndex != readerIndex) {\n                     currentReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n+                    storedFieldsReader = context.searcher().getCodecReader(readerIndex).getFieldsReader();\n                     currentReaderIndex = readerIndex;\n+                    if (hasSequentialDocs && docs.length >= 10) {\n+                        // All the docs to fetch are adjacent but Lucene stored fields are optimized\n+                        // for random access and don't optimize for sequential access - except for merging.\n+                        // So we do a little hack here and pretend we're going to do merges in order to\n+                        // get better sequential access.\n+                        storedFieldsReader = storedFieldsReader.getMergeInstance();\n+                    } else {\n+                        storedFieldsReader = storedFieldsReader.clone();", "originalCommit": "3377e71c3aade39debdfc194125ddc067d2357e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI1OTU5Mw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490259593", "bodyText": "got it, thanks", "author": "jimczi", "createdAt": "2020-09-17T13:47:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDA0NTI2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "6ed45fae4d36290930cf56b76473433738ac84e9", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex 39dcd9404b7..af7a09c1220 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -118,16 +129,17 @@ public class FetchPhase {\n                 int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n                 if (currentReaderIndex != readerIndex) {\n                     currentReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n-                    storedFieldsReader = context.searcher().getCodecReader(readerIndex).getFieldsReader();\n                     currentReaderIndex = readerIndex;\n-                    if (hasSequentialDocs && docs.length >= 10) {\n+                    if (currentReaderContext.reader() instanceof SequentialStoredFieldsLeafReader\n+                            && hasSequentialDocs && docs.length >= 10) {\n                         // All the docs to fetch are adjacent but Lucene stored fields are optimized\n                         // for random access and don't optimize for sequential access - except for merging.\n                         // So we do a little hack here and pretend we're going to do merges in order to\n                         // get better sequential access.\n-                        storedFieldsReader = storedFieldsReader.getMergeInstance();\n+                        SequentialStoredFieldsLeafReader lf = (SequentialStoredFieldsLeafReader) currentReaderContext.reader();\n+                        fieldReader = lf.getSequentialStoredFieldsReader()::visitDocument;\n                     } else {\n-                        storedFieldsReader = storedFieldsReader.clone();\n+                        fieldReader = currentReaderContext.reader()::document;\n                     }\n                     for (FetchSubPhaseProcessor processor : processors) {\n                         processor.setNextReader(currentReaderContext);\n"}}, {"oid": "6ed45fae4d36290930cf56b76473433738ac84e9", "url": "https://github.com/elastic/elasticsearch/commit/6ed45fae4d36290930cf56b76473433738ac84e9", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T13:45:51Z", "type": "commit"}, {"oid": "6ed45fae4d36290930cf56b76473433738ac84e9", "url": "https://github.com/elastic/elasticsearch/commit/6ed45fae4d36290930cf56b76473433738ac84e9", "message": "Faster sequential access for stored fields\n\nSpinoff of #61806\nToday retrieving stored fields at search time is optimized for random access.\nSo we make no effort to keep state in order to not decompress the same data\nmultiple times because two documents might be in the same compressed block.\nThis strategy is acceptable when retrieving a top N sorted by score since\nthere is no guarantee that documents will be on the same block.\nHowever, we have some use cases where the document to retrieve might be\ncompletely sequential:\n* Scrolls or normal search sorted by document id.\n* Queries on Runtime fields that extract from _source.\n\nThis commit allows to expose all the custom readers that we use at search time\nas codec readers in order to be able to leverage the merge instances of\nstored fields readers that are optimized for sequential access.\nThis change focuses on the fetch phase for now and leverages the merge instances\nfor stored fields only if all documents to retrieve are adjacent.\nApplying the same logic in the source lookup of runtime fields should\nbe trivial but will be done in a follow up.\n\nThe speedup on queries sorted by doc id is significant.\nI played with the scroll task of the [http_logs rally track](https://elasticsearch-benchmarks.elastic.co/#tracks/http-logs/nightly/default/30d)\non my laptop and had the following result:\n```\n|                                                        Metric |   Task |    Baseline |   Contender |     Diff |    Unit |\n|--------------------------------------------------------------:|-------:|------------:|------------:|---------:|--------:|\n|                                            Total Young Gen GC |        |       0.199 |       0.231 |    0.032 |       s |\n|                                              Total Old Gen GC |        |           0 |           0 |        0 |       s |\n|                                                    Store size |        |     17.9704 |     17.9704 |        0 |      GB |\n|                                                 Translog size |        | 2.04891e-06 | 2.04891e-06 |        0 |      GB |\n|                                        Heap used for segments |        |    0.820332 |    0.820332 |        0 |      MB |\n|                                      Heap used for doc values |        |    0.113979 |    0.113979 |        0 |      MB |\n|                                           Heap used for terms |        |     0.37973 |     0.37973 |        0 |      MB |\n|                                           Heap used for norms |        |     0.03302 |     0.03302 |        0 |      MB |\n|                                          Heap used for points |        |           0 |           0 |        0 |      MB |\n|                                   Heap used for stored fields |        |    0.293602 |    0.293602 |        0 |      MB |\n|                                                 Segment count |        |         541 |         541 |        0 |         |\n|                                                Min Throughput | scroll |     12.7872 |     12.8747 |  0.08758 | pages/s |\n|                                             Median Throughput | scroll |     12.9679 |     13.0556 |  0.08776 | pages/s |\n|                                                Max Throughput | scroll |     13.4001 |     13.5705 |  0.17046 | pages/s |\n|                                       50th percentile latency | scroll |     524.966 |     251.396 |  -273.57 |      ms |\n|                                       90th percentile latency | scroll |     577.593 |     271.066 | -306.527 |      ms |\n|                                      100th percentile latency | scroll |      664.73 |     272.734 | -391.997 |      ms |\n|                                  50th percentile service time | scroll |     522.387 |     248.776 | -273.612 |      ms |\n|                                  90th percentile service time | scroll |     573.118 |      267.79 | -305.328 |      ms |\n|                                 100th percentile service time | scroll |     660.642 |     268.963 | -391.678 |      ms |\n|                                                    error rate | scroll |           0 |           0 |        0 |       % |\n```\n\nCloses #62024", "committedDate": "2020-09-17T13:45:51Z", "type": "forcePushed"}, {"oid": "c06020d37bf3214997c257d766526c18f5ce6241", "url": "https://github.com/elastic/elasticsearch/commit/c06020d37bf3214997c257d766526c18f5ce6241", "message": "fix SeqIdGeneratingFilterReader", "committedDate": "2020-09-17T14:11:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3MzcwNQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490273705", "bodyText": "Can you add javadocs?", "author": "jpountz", "createdAt": "2020-09-17T14:05:08Z", "path": "server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.common.lucene.index;\n+\n+import org.apache.lucene.codecs.StoredFieldsReader;\n+import org.apache.lucene.index.CodecReader;\n+import org.apache.lucene.index.FilterLeafReader;\n+import org.apache.lucene.index.LeafReader;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A {@link FilterLeafReader} that exposes a {@link StoredFieldsReader}\n+ * optimized for sequential access. This class should be used by custom\n+ * {@link FilterLeafReader} that are used at search time in order to\n+ * leverage sequential access when retrieving stored fields in queries,\n+ * aggregations or during the fetch phase.\n+ */\n+public abstract class SequentialStoredFieldsLeafReader extends FilterLeafReader {\n+    /**\n+     * <p>Construct a StoredFieldsFilterLeafReader based on the specified base reader.\n+     * <p>Note that base reader is closed if this FilterLeafReader is closed.</p>\n+     *\n+     * @param in specified base reader.\n+     */\n+    public SequentialStoredFieldsLeafReader(LeafReader in) {\n+        super(in);\n+    }\n+\n+    protected abstract StoredFieldsReader doGetSequentialStoredFieldsReader(StoredFieldsReader reader);", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNjg4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490326886", "bodyText": "I pushed 9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:05:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3MzcwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "c06020d37bf3214997c257d766526c18f5ce6241", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java b/server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java\nindex 740b1847c75..76c0d70449f 100644\n--- a/server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java\n+++ b/server/src/main/java/org/elasticsearch/common/lucene/index/SequentialStoredFieldsLeafReader.java\n\n@@ -57,7 +57,7 @@ public abstract class SequentialStoredFieldsLeafReader extends FilterLeafReader\n             SequentialStoredFieldsLeafReader reader = (SequentialStoredFieldsLeafReader) in;\n             return doGetSequentialStoredFieldsReader(reader.getSequentialStoredFieldsReader());\n         } else {\n-            throw new IOException(\"requires a CodecReader or a StoredFieldsFilterLeafReader, got \" + in.getClass());\n+            throw new IOException(\"requires a CodecReader or a SequentialStoredFieldsLeafReader, got \" + in.getClass());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NTk3Nw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490275977", "bodyText": "nit: since doc ids are sorted, we should be able to do something like return docs.length > 0 && docs[docs.length-1].docId - docs[0].docId == docs.length - 1;", "author": "jpountz", "createdAt": "2020-09-17T14:08:03Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -524,4 +550,18 @@ private static void fillDocAndMetaFields(SearchContext context, FieldsVisitor fi\n             }\n         }\n     }\n+\n+    /**\n+     * Returns <code>true</code> if the provided <code>docs</code> are\n+     * stored sequentially (Dn = Dn-1 + 1).\n+     */\n+    static boolean hasSequentialDocs(DocIdToIndex[] docs) {\n+        for (int i = 1; i < docs.length; i++) {\n+            assert docs[i].docId >= docs[i-1].docId : \"doc ids out of order\";\n+            if (docs[i].docId - docs[i-1].docId > 1) {\n+                return false;\n+            }\n+        }\n+        return true;", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzc2Ng==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327766", "bodyText": "++, I pushed 9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NTk3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex af7a09c1220..aea1a38414c 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -556,12 +556,6 @@ public class FetchPhase {\n      * stored sequentially (Dn = Dn-1 + 1).\n      */\n     static boolean hasSequentialDocs(DocIdToIndex[] docs) {\n-        for (int i = 1; i < docs.length; i++) {\n-            assert docs[i].docId >= docs[i-1].docId : \"doc ids out of order\";\n-            if (docs[i].docId - docs[i-1].docId > 1) {\n-                return false;\n-            }\n-        }\n-        return true;\n+        return docs.length > 0 && docs[docs.length-1].docId - docs[0].docId == docs.length - 1;\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NjQyMw==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490276423", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Make sure that the engine produces a CodecReader.\n          \n          \n            \n                    // Make sure that the engine produces a SequentialStoredFieldsLeafReader.", "author": "jpountz", "createdAt": "2020-09-17T14:08:37Z", "path": "server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java", "diffHunk": "@@ -5940,4 +5941,25 @@ public void testNotWarmUpSearcherInEngineCtor() throws Exception {\n             }\n         }\n     }\n+\n+    public void testProducesStoredFieldsReader() throws Exception {\n+        // Make sure that the engine produces a CodecReader.", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzg2OQ==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327869", "bodyText": "9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3NjQyMw=="}], "type": "inlineReview", "revised_code": {"commit": "9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java\nindex edd640cb156..9b1eb756211 100644\n--- a/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java\n+++ b/server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java\n\n@@ -5943,7 +5943,7 @@ public class InternalEngineTests extends EngineTestCase {\n     }\n \n     public void testProducesStoredFieldsReader() throws Exception {\n-        // Make sure that the engine produces a CodecReader.\n+        // Make sure that the engine produces a SequentialStoredFieldsLeafReader.\n         // This is required for optimizations on SourceLookup to work, which is in-turn useful for runtime fields.\n         ParsedDocument doc = testParsedDocument(\"1\", null, testDocumentWithTextField(\"test\"),\n             new BytesArray(\"{}\".getBytes(Charset.defaultCharset())), null);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3OTk3MA==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490279970", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testProducesCodecReader() throws Exception {\n          \n          \n            \n                public void testProducesSequentialStoredFieldsLeafReader() throws Exception {", "author": "jpountz", "createdAt": "2020-09-17T14:13:00Z", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java", "diffHunk": "@@ -215,6 +218,41 @@ public void testCoreCacheKey() throws Exception {\n         IOUtils.close(ir, ir2, iw, dir);\n     }\n \n+    public void testProducesCodecReader() throws Exception {", "originalCommit": "6ed45fae4d36290930cf56b76473433738ac84e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDMyNzk1MA==", "url": "https://github.com/elastic/elasticsearch/pull/62509#discussion_r490327950", "bodyText": "9d3f33f", "author": "jimczi", "createdAt": "2020-09-17T15:07:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDI3OTk3MA=="}], "type": "inlineReview", "revised_code": {"commit": "9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "chunk": "diff --git a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java\nindex aabc5a17c24..9f5c25e8898 100644\n--- a/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java\n+++ b/x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/security/authz/accesscontrol/DocumentSubsetReaderTests.java\n\n@@ -218,7 +218,7 @@ public class DocumentSubsetReaderTests extends ESTestCase {\n         IOUtils.close(ir, ir2, iw, dir);\n     }\n \n-    public void testProducesCodecReader() throws Exception {\n+    public void testProducesStoredFieldsReader() throws Exception {\n         Directory dir = newDirectory();\n         IndexWriterConfig iwc = new IndexWriterConfig(null);\n         iwc.setMaxBufferedDocs(100);\n"}}, {"oid": "9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "url": "https://github.com/elastic/elasticsearch/commit/9d3f33fa1ad4877d55bfc1a13acd73111cfb6d46", "message": "feedback", "committedDate": "2020-09-17T15:04:48Z", "type": "commit"}, {"oid": "2bee5fc87014a881cc46f7fc1d79bb33be886818", "url": "https://github.com/elastic/elasticsearch/commit/2bee5fc87014a881cc46f7fc1d79bb33be886818", "message": "unused import", "committedDate": "2020-09-17T15:56:53Z", "type": "commit"}]}