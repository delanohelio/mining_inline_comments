{"pr_number": 57273, "pr_title": "Group docIds by segment in FetchPhase to better use LRU cache", "pr_createdAt": "2020-05-28T10:27:36Z", "pr_url": "https://github.com/elastic/elasticsearch/pull/57273", "timeline": [{"oid": "65132dfd9bf1f71a17cc2be2033836bb0b9f3bfc", "url": "https://github.com/elastic/elasticsearch/commit/65132dfd9bf1f71a17cc2be2033836bb0b9f3bfc", "message": "Group docIds by segment in FetchPhase to better use LRU cache", "committedDate": "2020-05-28T07:43:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc3MzczNw==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r431773737", "bodyText": "We could sort the doc ids to load once and move the LeafReaderContext while iterating ? In fact that's what we do already in FetchDocValuesPhase and other fetch sub-phases. Sorting the doc ids would remove the need to sort hits on every sub-phase.", "author": "jimczi", "createdAt": "2020-05-28T11:43:23Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -140,31 +142,47 @@ public void execute(SearchContext context) {\n         }\n \n         try {\n-            SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()];\n-            FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();\n+            // group docIds by segment in order to better use LRU cache\n+            Map<Integer, List<Integer>> segmentTasks = new HashMap<>();\n+            Map<Integer, Integer> docIdToIndex = new HashMap<>();\n             for (int index = 0; index < context.docIdsToLoadSize(); index++) {", "originalCommit": "65132dfd9bf1f71a17cc2be2033836bb0b9f3bfc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI4NTg4MQ==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r432285881", "bodyText": "We also need to preserve the original order of hits in the response so sorting the doc ids is only internal (for fetching stored values and executing sub-phases). The original order must be restored when setting the hits in the response.", "author": "jimczi", "createdAt": "2020-05-29T06:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc3MzczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI4NzY3MQ==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r432287671", "bodyText": "Thanks for you comment @jimczi. Sort the doc ids to load seems better. But if we change the order of doc ids, the order of the search hits will relatedly be changed, it would cause a lot of test cases to fail, such as testInsideTerms.", "author": "boicehuang", "createdAt": "2020-05-29T06:50:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc3MzczNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMwMzcyNg==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r432303726", "bodyText": "But if we change the order of doc ids, the order of the search hits will relatedly be changed, it would cause a lot of test cases to fail, such as testInsideTerms.\n\nSee my previous comment.\nWe can change the order in the fetch phase but we have to preserve the original order in the response. The final hits must be re-sorted based on their original order in the request (context.docIdsToLoad).", "author": "jimczi", "createdAt": "2020-05-29T07:28:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc3MzczNw=="}], "type": "inlineReview", "revised_code": {"commit": "0545448210e5a8912b5250d6e5da33f99f617ce1", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex 139e9dd2fb8..874bc06ad12 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -142,47 +141,43 @@ public class FetchPhase implements SearchPhase {\n         }\n \n         try {\n-            // group docIds by segment in order to better use LRU cache\n-            Map<Integer, List<Integer>> segmentTasks = new HashMap<>();\n+            int[] sortedDocIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n+            Arrays.sort(sortedDocIds);\n+\n+            // preserve the original order of hits in inverted index\n             Map<Integer, Integer> docIdToIndex = new HashMap<>();\n             for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n                 int docId = context.docIdsToLoad()[context.docIdsToLoadFrom() + index];\n-                int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n                 docIdToIndex.put(docId, index);\n-                segmentTasks.putIfAbsent(readerIndex, new ArrayList<>());\n-                segmentTasks.get(readerIndex).add(docId);\n             }\n \n             SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()];\n+            SearchHit[] sortedHits = new SearchHit[context.docIdsToLoadSize()];\n             FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();\n-            Iterator<Map.Entry<Integer, List<Integer>>> readerIndexIterator = segmentTasks.entrySet().iterator();\n-            while (readerIndexIterator.hasNext()) {\n-                Map.Entry<Integer, List<Integer>> entry = readerIndexIterator.next();\n-                int readerIndex = entry.getKey();\n-                Iterator<Integer> docIdIterator = entry.getValue().iterator();\n-                while (docIdIterator.hasNext()) {\n-                    if (context.isCancelled()) {\n-                        throw new TaskCancelledException(\"cancelled\");\n-                    }\n-                    int docId = docIdIterator.next();\n-                    LeafReaderContext subReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n-                    int subDocId = docId - subReaderContext.docBase;\n-\n-                    final SearchHit searchHit;\n-                    int rootDocId = findRootDocumentIfNested(context, subReaderContext, subDocId);\n-                    if (rootDocId != -1) {\n-                        searchHit = createNestedSearchHit(context, docId, subDocId, rootDocId,\n-                            storedToRequestedFields, subReaderContext);\n-                    } else {\n-                        searchHit = createSearchHit(context, fieldsVisitor, docId, subDocId,\n-                            storedToRequestedFields, subReaderContext);\n-                    }\n+            for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n+                if (context.isCancelled()) {\n+                    throw new TaskCancelledException(\"cancelled\");\n+                }\n+                int docId = sortedDocIds[index];\n+                int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n+                LeafReaderContext subReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n+                int subDocId = docId - subReaderContext.docBase;\n+\n+                final SearchHit searchHit;\n+                int rootDocId = findRootDocumentIfNested(context, subReaderContext, subDocId);\n+                if (rootDocId != -1) {\n+                    searchHit = createNestedSearchHit(context, docId, subDocId, rootDocId,\n+                        storedToRequestedFields, subReaderContext);\n+                } else {\n+                    searchHit = createSearchHit(context, fieldsVisitor, docId, subDocId,\n+                        storedToRequestedFields, subReaderContext);\n+                }\n \n-                    hits[docIdToIndex.get(docId)] = searchHit;\n-                    hitContext.reset(searchHit, subReaderContext, subDocId, context.searcher());\n-                    for (FetchSubPhase fetchSubPhase : fetchSubPhases) {\n-                        fetchSubPhase.hitExecute(context, hitContext);\n-                    }\n+                sortedHits[index] = searchHit;\n+                hits[docIdToIndex.get(docId)] = searchHit;\n+                hitContext.reset(searchHit, subReaderContext, subDocId, context.searcher());\n+                for (FetchSubPhase fetchSubPhase : fetchSubPhases) {\n+                    fetchSubPhase.hitExecute(context, hitContext);\n                 }\n             }\n             if (context.isCancelled()) {\n"}}, {"oid": "0545448210e5a8912b5250d6e5da33f99f617ce1", "url": "https://github.com/elastic/elasticsearch/commit/0545448210e5a8912b5250d6e5da33f99f617ce1", "message": "Sort docIdsToLoad once instead of in each sub-phase.", "committedDate": "2020-05-29T15:15:26Z", "type": "commit"}, {"oid": "c0a615fb1d4b16599555bc89614760f2733cc299", "url": "https://github.com/elastic/elasticsearch/commit/c0a615fb1d4b16599555bc89614760f2733cc299", "message": "Merge remote-tracking branch 'upstream/master' into fetch_optimize", "committedDate": "2020-06-01T04:05:54Z", "type": "commit"}, {"oid": "8bb36b112b0c86174d85a0bd58714d6389dc2315", "url": "https://github.com/elastic/elasticsearch/commit/8bb36b112b0c86174d85a0bd58714d6389dc2315", "message": "Merge remote-tracking branch 'upstream/master' into fetch_optimize", "committedDate": "2020-06-08T08:37:08Z", "type": "commit"}, {"oid": "84856d52cd21fd0c53188c391d8af9878872cb06", "url": "https://github.com/elastic/elasticsearch/commit/84856d52cd21fd0c53188c391d8af9878872cb06", "message": "Add a comment in FetchSubPhase#hitsExecute", "committedDate": "2020-06-08T09:15:03Z", "type": "commit"}, {"oid": "212df2afeaf340787290c821ee9e71a3457fa53d", "url": "https://github.com/elastic/elasticsearch/commit/212df2afeaf340787290c821ee9e71a3457fa53d", "message": "Use an ArrayList to store indexes of duplicate ids", "committedDate": "2020-06-14T07:48:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA1NDQ0Mw==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r440054443", "bodyText": "You can retrieve the array list once ?", "author": "jimczi", "createdAt": "2020-06-15T09:41:33Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -161,7 +177,10 @@ public void execute(SearchContext context) {\n                         storedToRequestedFields, subReaderContext);\n                 }\n \n-                hits[index] = searchHit;\n+                sortedHits[index] = searchHit;\n+                for (int i = 0; i < docIdToIndex.get(docId).size(); i++) {\n+                    hits[docIdToIndex.get(docId).get(i)] = searchHit;", "originalCommit": "212df2afeaf340787290c821ee9e71a3457fa53d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b1e512d7bbb4932467ced88e9fc08edf3ff48ef8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex f11dde80579..59ab278efe4 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -177,10 +167,7 @@ public class FetchPhase implements SearchPhase {\n                         storedToRequestedFields, subReaderContext);\n                 }\n \n-                sortedHits[index] = searchHit;\n-                for (int i = 0; i < docIdToIndex.get(docId).size(); i++) {\n-                    hits[docIdToIndex.get(docId).get(i)] = searchHit;\n-                }\n+                hits[index] = searchHit;\n                 hitContext.reset(searchHit, subReaderContext, subDocId, context.searcher());\n                 for (FetchSubPhase fetchSubPhase : fetchSubPhases) {\n                     fetchSubPhase.hitExecute(context, hitContext);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA1NTYwNw==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r440055607", "bodyText": "I wonder if it'd be better to use a static inner class and a custom comparator ?", "author": "jimczi", "createdAt": "2020-06-15T09:43:30Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -140,13 +142,27 @@ public void execute(SearchContext context) {\n         }\n \n         try {\n+            int[] sortedDocIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n+            Arrays.sort(sortedDocIds);\n+\n+            // preserve the original order of hits in inverted index\n+            Map<Integer, ArrayList<Integer>> docIdToIndex = new HashMap<>();", "originalCommit": "212df2afeaf340787290c821ee9e71a3457fa53d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4ODY4Mw==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r441388683", "bodyText": "It seems better to preserve the original order in a custom comparator than using a hash map. I am going to update it.", "author": "boicehuang", "createdAt": "2020-06-17T08:53:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA1NTYwNw=="}], "type": "inlineReview", "revised_code": {"commit": "b1e512d7bbb4932467ced88e9fc08edf3ff48ef8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex f11dde80579..59ab278efe4 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -142,21 +142,11 @@ public class FetchPhase implements SearchPhase {\n         }\n \n         try {\n-            int[] sortedDocIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n+            int[] docIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n+            int[] sortedDocIds = docIds.clone();\n             Arrays.sort(sortedDocIds);\n \n-            // preserve the original order of hits in inverted index\n-            Map<Integer, ArrayList<Integer>> docIdToIndex = new HashMap<>();\n-            for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n-                int docId = context.docIdsToLoad()[context.docIdsToLoadFrom() + index];\n-                if (docIdToIndex.get(docId) == null) {\n-                    docIdToIndex.put(docId, new ArrayList<>());\n-                }\n-                docIdToIndex.get(docId).add(index);\n-            }\n-\n             SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()];\n-            SearchHit[] sortedHits = new SearchHit[context.docIdsToLoadSize()];\n             FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();\n             for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n                 if (context.isCancelled()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzEyNw==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r440493127", "bodyText": "It would be nice if these fetch sub phases, which implement hitsExecute (plural), could also benefit from locality. Currently they each loop through the hits array separately, so any cached data from processing a hit may be lost by the time the next fetch phase is run.\nI think this is a distinct idea from this PR though, I filed the separate issue #58155.", "author": "jtibshirani", "createdAt": "2020-06-15T23:02:31Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -172,7 +191,7 @@ public void execute(SearchContext context) {\n             }\n \n             for (FetchSubPhase fetchSubPhase : fetchSubPhases) {\n-                fetchSubPhase.hitsExecute(context, hits);\n+                fetchSubPhase.hitsExecute(context, sortedHits);", "originalCommit": "212df2afeaf340787290c821ee9e71a3457fa53d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b1e512d7bbb4932467ced88e9fc08edf3ff48ef8", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex f11dde80579..59ab278efe4 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -191,12 +178,17 @@ public class FetchPhase implements SearchPhase {\n             }\n \n             for (FetchSubPhase fetchSubPhase : fetchSubPhases) {\n-                fetchSubPhase.hitsExecute(context, sortedHits);\n+                fetchSubPhase.hitsExecute(context, hits);\n                 if (context.isCancelled()) {\n                     throw new TaskCancelledException(\"cancelled\");\n                 }\n             }\n \n+            // re-sort final hits to original order\n+            List originalOrder = Arrays.asList(docIds);\n+            Comparator<SearchHit> hitsComparator = Comparator.comparing(o -> originalOrder.indexOf(o.docId()));\n+            Arrays.sort(hits, hitsComparator);\n+\n             TotalHits totalHits = context.queryResult().getTotalHits();\n             context.fetchResult().hits(new SearchHits(hits, totalHits, context.queryResult().getMaxScore()));\n         } catch (IOException e) {\n"}}, {"oid": "7afbd2f04aae19970f97101298a1b538466d0b1d", "url": "https://github.com/elastic/elasticsearch/commit/7afbd2f04aae19970f97101298a1b538466d0b1d", "message": "Removed unused imports", "committedDate": "2020-06-17T02:24:13Z", "type": "commit"}, {"oid": "50d0c6ee752f0a0767f384b905b7af845aa7452e", "url": "https://github.com/elastic/elasticsearch/commit/50d0c6ee752f0a0767f384b905b7af845aa7452e", "message": "Merge remote-tracking branch 'upstream/master' into fetch_optimize", "committedDate": "2020-06-17T03:12:05Z", "type": "commit"}, {"oid": "b1e512d7bbb4932467ced88e9fc08edf3ff48ef8", "url": "https://github.com/elastic/elasticsearch/commit/b1e512d7bbb4932467ced88e9fc08edf3ff48ef8", "message": "Preserve the original order of hits in comparator", "committedDate": "2020-06-17T13:16:09Z", "type": "commit"}, {"oid": "f61067ddcc30e893c0d829f99af50b9ffe2cdad6", "url": "https://github.com/elastic/elasticsearch/commit/f61067ddcc30e893c0d829f99af50b9ffe2cdad6", "message": "Merge remote-tracking branch 'upstream/master' into fetch_optimize", "committedDate": "2020-06-17T15:20:21Z", "type": "commit"}, {"oid": "17b892b26148f590723f5b14da8c456c989062bc", "url": "https://github.com/elastic/elasticsearch/commit/17b892b26148f590723f5b14da8c456c989062bc", "message": "fix preservation of the original order of hits", "committedDate": "2020-06-17T16:45:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjcyMQ==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r441712721", "bodyText": "Sorry I wasn't clear. I was more thinking of something like this:\nstatic class DocIdAndIndex implements Comparable<DocIdAndIndex> {\n        final int docId;\n        final int index;\n\n        DocIdAndIndex(int docId, int index) {\n            this.docId = docId;\n            this.index = index;\n        }\n\n\n        @Override\n        public int compareTo(DocIdAndIndex o) {\n            return Integer.compare(docId, o.docId);\n        }\n}\n....\n  DocIdAndIndex[] docs = new DocIdAndIndex[context.docIdsToLoadSize()]; \n  for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n    docs[index] = new DocIdAndIndex(context.docIdsToLoad()[context.docIdsToLoadFrom() + index], index);\n  }\n  Arrays.sort(docs)\n\nYou can then use docs to retrieve the original index and you don't have the array twice ?", "author": "jimczi", "createdAt": "2020-06-17T17:33:11Z", "path": "server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java", "diffHunk": "@@ -140,13 +143,17 @@ public void execute(SearchContext context) {\n         }\n \n         try {\n+            int[] docIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n+            int[] sortedDocIds = docIds.clone();\n+            Arrays.sort(sortedDocIds);\n+", "originalCommit": "17b892b26148f590723f5b14da8c456c989062bc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkwNjM2NQ==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r441906365", "bodyText": "Thanks @jimczi . I have one question here. If we use a custom comparator, the average performance of timSort or quicksort is O(nlogn).but the complexity of constructing a hashmap is O(n). Maybe we can have a better performance if we use the latter?", "author": "boicehuang", "createdAt": "2020-06-18T00:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjAzNjU1NA==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r442036554", "bodyText": "I don't see how you'd avoid the initial sort by doc ids ? The proposed solution requires to sort the array once and avoids building an hashmap, isn't it better ?", "author": "jimczi", "createdAt": "2020-06-18T07:53:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEyMTg2Mg==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r442121862", "bodyText": "In the first commit, I built the hashmap with the array without sorting it. It only took O(n) to iterate the array once. I think using a hashmap may be better? Do we have to do the sorting? Does using a hashmap have a different impact on every sub-phase?", "author": "boicehuang", "createdAt": "2020-06-18T10:17:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjcyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEzOTI4Ng==", "url": "https://github.com/elastic/elasticsearch/pull/57273#discussion_r442139286", "bodyText": "We still need to provide the array of SearchHit sorted by doc ids to the sub fetch phase (hitsExecute) so I don't see how the hashmap would be enough. The current change allows to sort the array once before executing the sub fetch phase so it's an enhancement. Also note that the array is limited to 10k by default since we have a soft limit for the number of hits that can retrieved.", "author": "jimczi", "createdAt": "2020-06-18T10:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcxMjcyMQ=="}], "type": "inlineReview", "revised_code": {"commit": "d0e3f35a094846617ded75c19cbc1512e8d49d45", "chunk": "diff --git a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\nindex 466e7b6b5d1..c170bec93de 100644\n--- a/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n+++ b/server/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java\n\n@@ -143,17 +143,20 @@ public class FetchPhase implements SearchPhase {\n         }\n \n         try {\n-            int[] docIds = Arrays.copyOfRange(context.docIdsToLoad(), context.docIdsToLoadFrom(), context.docIdsToLoadSize());\n-            int[] sortedDocIds = docIds.clone();\n-            Arrays.sort(sortedDocIds);\n+            DocIdToIndex[] docs = new DocIdToIndex[context.docIdsToLoadSize()];\n+            for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n+                docs[index] = new DocIdToIndex(context.docIdsToLoad()[context.docIdsToLoadFrom() + index], index);\n+            }\n+            Arrays.sort(docs);\n \n             SearchHit[] hits = new SearchHit[context.docIdsToLoadSize()];\n+            SearchHit[] sortedHits = new SearchHit[context.docIdsToLoadSize()];\n             FetchSubPhase.HitContext hitContext = new FetchSubPhase.HitContext();\n             for (int index = 0; index < context.docIdsToLoadSize(); index++) {\n                 if (context.isCancelled()) {\n                     throw new TaskCancelledException(\"cancelled\");\n                 }\n-                int docId = sortedDocIds[index];\n+                int docId = docs[index].docId;\n                 int readerIndex = ReaderUtil.subIndex(docId, context.searcher().getIndexReader().leaves());\n                 LeafReaderContext subReaderContext = context.searcher().getIndexReader().leaves().get(readerIndex);\n                 int subDocId = docId - subReaderContext.docBase;\n"}}, {"oid": "d0e3f35a094846617ded75c19cbc1512e8d49d45", "url": "https://github.com/elastic/elasticsearch/commit/d0e3f35a094846617ded75c19cbc1512e8d49d45", "message": "Use a custom comparator to retrieve the original index", "committedDate": "2020-06-27T14:22:12Z", "type": "commit"}, {"oid": "c6f8a8e5a06c46a02692cb2e5cb830770db59c91", "url": "https://github.com/elastic/elasticsearch/commit/c6f8a8e5a06c46a02692cb2e5cb830770db59c91", "message": "removed unused imports", "committedDate": "2020-06-28T02:10:00Z", "type": "commit"}, {"oid": "79ccfee63d0135022c6912bd967e718fc92a5ad1", "url": "https://github.com/elastic/elasticsearch/commit/79ccfee63d0135022c6912bd967e718fc92a5ad1", "message": "Merge remote-tracking branch 'upstream/master' into fetch_optimize", "committedDate": "2020-06-28T03:13:35Z", "type": "commit"}]}