{"pr_number": 1510, "pr_title": "Kafka support", "pr_createdAt": "2020-03-13T12:09:39Z", "pr_url": "https://github.com/oracle/helidon/pull/1510", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392898324", "bodyText": "Helidon has a flat package structure - each module can only use one package (and spi if needed).", "author": "tomas-langer", "createdAt": "2020-03-16T09:56:40Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzE5Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457197", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-18T15:57:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392916808", "bodyText": "This method should be private (or package local)", "author": "tomas-langer", "createdAt": "2020-03-16T10:30:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MDU3Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397780576", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392918072", "bodyText": "public method with no javadoc.\nAlso Helidon uses getters without get verb, so the method should be called consumers.\nI am not sure it should be public at all. Please minimize number of public methods that are not implementing interface methods.", "author": "tomas-langer", "createdAt": "2020-03-16T10:31:56Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTAxMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781011", "bodyText": "It is for testing purposes, to check that all resources are closed. Not it has package visibility and other name without get.", "author": "jbescos", "createdAt": "2020-03-25T11:23:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392919773", "bodyText": "You are creating a new thread pool for each publisher. That is probably not intended.", "author": "tomas-langer", "createdAt": "2020-03-16T10:33:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTM3Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781377", "bodyText": "Right, now it reuses a scheduler thread pool", "author": "jbescos", "createdAt": "2020-03-25T11:23:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392920553", "bodyText": "In this case you do not close the producer.", "author": "tomas-langer", "createdAt": "2020-03-16T10:34:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                simpleKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {\n+                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTU0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781549", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:24:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921003", "bodyText": "No backpressure support may cause issues in reactive environment.", "author": "tomas-langer", "createdAt": "2020-03-16T10:35:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781784", "bodyText": "It is now configured a configured parameter.", "author": "jbescos", "createdAt": "2020-03-25T11:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\ndeleted file mode 100644\nindex c0954ac0b..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java\n+++ /dev/null\n\n@@ -1,111 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka.connector;\n-\n-import java.util.Collection;\n-import java.util.Queue;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.context.BeforeDestroyed;\n-import javax.enterprise.event.Observes;\n-\n-import io.helidon.common.configurable.ThreadPoolSupplier;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n-import io.helidon.messaging.kafka.SimpleKafkaProducer;\n-\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n-import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n-import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n- */\n-@ApplicationScoped\n-@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n-public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n-\n-    /**\n-     * Microprofile messaging Kafka connector name.\n-     */\n-    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n-\n-    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n-    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n-\n-    /**\n-     * Called when container is terminated.\n-     *\n-     * @param event termination event\n-     */\n-    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n-        SimpleKafkaConsumer<Object, Object> consumer;\n-        while ((consumer = consumers.poll()) != null) {\n-            consumer.close();\n-        }\n-    }\n-\n-    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n-        return consumers;\n-    }\n-\n-    @Override\n-    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n-        consumers.add(simpleKafkaConsumer);\n-        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n-    }\n-\n-    @Override\n-    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n-        Config helidonConfig = (Config) config;\n-        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n-        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n-\n-            @Override\n-            public void onSubscribe(Subscription s) {\n-                s.request(Long.MAX_VALUE);\n-            }\n-\n-            @Override\n-            public void onNext(Message<?> message) {\n-                simpleKafkaProducer.produce(message.getPayload());\n-                message.ack();\n-            }\n-\n-            @Override\n-            public void onError(Throwable t) {\n-                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n-            }\n-\n-            @Override\n-            public void onComplete() {\n-                simpleKafkaProducer.close();\n-            }\n-        });\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921592", "bodyText": "This class should not be public maybe?", "author": "tomas-langer", "createdAt": "2020-03-16T10:36:01Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjEyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782120", "bodyText": "It has packaged visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n\n@@ -14,10 +14,11 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionStage;\n+import java.util.logging.Logger;\n \n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.eclipse.microprofile.reactive.messaging.Message;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392930784", "bodyText": "Getter should not use get, also I guess this should not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:47:09Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjI3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782270", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n\n@@ -14,10 +14,11 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionStage;\n+import java.util.logging.Logger;\n \n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.eclipse.microprofile.reactive.messaging.Message;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931269", "bodyText": "Constructors may never be public in Helidon, unless required by CDI or JAX-RS.\nWe use factory methods (if need to be public).", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:03Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MzAyMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397783021", "bodyText": "It has package visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n\n@@ -14,10 +14,11 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionStage;\n+import java.util.logging.Logger;\n \n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.eclipse.microprofile.reactive.messaging.Message;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931585", "bodyText": "Is there no support for ack in Kafka itself? This basically makes the ack method a no-op.", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:36Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {\n+        return ackFuture;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ackFuture.complete(null);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTU4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791588", "bodyText": "I spoke with @danielkec about this. The thing is that polling from Kafka is a blocking operation, and in reactive streams we cannot have threads blocked. So we need some way to make it in non-blocking way.\nThe workaround to deal with this is the BackPressureLayer. There we are buffering events coming from polling. That KafkaMessage.ack() is communication with BackPressureLayer, instead of doing it with Kafka.", "author": "jbescos", "createdAt": "2020-03-25T11:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nsimilarity index 84%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 254c63e56..3d3598ad8 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n\n@@ -14,10 +14,11 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.CompletionStage;\n+import java.util.logging.Logger;\n \n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.eclipse.microprofile.reactive.messaging.Message;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392932046", "bodyText": "The name is not good.", "author": "tomas-langer", "createdAt": "2020-03-16T10:49:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934566", "bodyText": "Also do you expect users to use this class? If not, it must not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTg3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791871", "bodyText": "It is not public and it is renamed.", "author": "jbescos", "createdAt": "2020-03-25T11:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nsimilarity index 81%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nindex a4f89b8d0..1b0d30245 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\n\n@@ -14,11 +14,10 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.function.Consumer;\n \n-import org.reactivestreams.Publisher;\n import org.reactivestreams.Subscriber;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934828", "bodyText": "Public constructor cannot be used. Also if class is not to be public, this method will not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:33Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n+\n+    private Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher;\n+\n+    /**\n+     * Create new Reactive Streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+     *\n+     * @param publisher {@link java.util.function.Consumer}\n+     */\n+    public SimplePublisher(Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MjEzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397792139", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nsimilarity index 81%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\nindex a4f89b8d0..1b0d30245 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicPublisher.java\n\n@@ -14,11 +14,10 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka.connector;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.function.Consumer;\n \n-import org.reactivestreams.Publisher;\n import org.reactivestreams.Subscriber;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936058", "bodyText": "I am not sure why this class implements Properties - is this used to send to Kafka itself?\nIf not, this class should not implement Properties.\nI am not sure if the properties are \"free\" - if so, use an internal Map<String, String> to store them.\nIf not free, use explicit fields to store such configuration.", "author": "tomas-langer", "createdAt": "2020-03-16T10:56:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjQyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936422", "bodyText": "Do not use get verb in getters.", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzM1MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937350", "bodyText": "Helidon classes (except for very specific cases) must be immutable. Properties is not immutable.", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTMwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809301", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T12:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n\n@@ -14,9 +14,10 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936328", "bodyText": "This should happen at the time this instance is created and stored in a field.", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:30Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTc1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809754", "bodyText": "This class is not instanced anymore. It contains some utility static methods.", "author": "jbescos", "createdAt": "2020-03-25T12:17:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n\n@@ -14,9 +14,10 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937012", "bodyText": "If you want to get everything into a map, just use config.detach().asMap().ifPresent(map -> ...)", "author": "tomas-langer", "createdAt": "2020-03-16T10:58:46Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTczOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875739", "bodyText": "Great, thanks", "author": "jbescos", "createdAt": "2020-03-25T14:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 67%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex a4df16a30..8d33fee53 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n\n@@ -14,9 +14,10 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937487", "bodyText": "Why is this class public?", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:35Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+public class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTg0Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875842", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\nsimilarity index 89%\nrename from messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\nindex 51a8a4488..e9b36146e 100644\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/PartitionsAssignedLatch.java\n\n@@ -14,7 +14,7 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.messaging.kafka;\n+package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Collection;\n import java.util.concurrent.CountDownLatch;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392938671", "bodyText": "If I understand correctly, we want an implementation for Microprofile reactive messaging. In such a case, this class should not be public.\nAlso the name is not very good.", "author": "tomas-langer", "createdAt": "2020-03-16T11:01:42Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzU1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457551", "bodyText": "I have changed the name and visibility", "author": "jbescos", "createdAt": "2020-03-18T15:57:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n\n@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939334", "bodyText": "Never use public constructors in Helidon.\nIf this class should be public (in which case the name should change), use a Builder pattern as in other Helidon classes.\nYou may have a static factory method, such as create(String, Config), but should be limited to one such method.", "author": "tomas-langer", "createdAt": "2020-03-16T11:02:53Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTE2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461168", "bodyText": "I think that class was designed with 2 purposes:\n\nTo be integrated with cdi\nTo be used from any other code to consume from Kafka.\n\nI have simplified this class for the first point. There is only one constructor.", "author": "jbescos", "createdAt": "2020-03-18T16:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n\n@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939603", "bodyText": "Creating a new executor service for each request is wrong use of executors.", "author": "tomas-langer", "createdAt": "2020-03-16T11:03:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTg1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461854", "bodyText": "This part is out.", "author": "jbescos", "createdAt": "2020-03-18T16:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n\n@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392940357", "bodyText": "Info messages should be only printed if we know it will happen only once per runtime of Helidon. I am not sure this is the case.", "author": "tomas-langer", "createdAt": "2020-03-16T11:04:49Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n+    }\n+\n+    /**\n+     * Execute supplied consumer by provided executor service for each received record.\n+     *\n+     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n+     * @param customTopics    Can be null, list of topics appended to the list from configuration\n+     * @param function        Consumer method executed in new thread for each received record\n+     * @return The Future's get method will return null when consumer is closed\n+     */\n+    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n+                                  Consumer<ConsumerRecord<K, V>> function) {\n+        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MjEwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394462103", "bodyText": "This is removed too.", "author": "jbescos", "createdAt": "2020-03-18T16:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n\n@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392942335", "bodyText": "This seems to be mutable (and wrongly).\nIf the consumer is only for a single use, make sure you construct the instance with all the configuration, then have a method to start listening. This will simplify a lot of checks.\nAlso not sure why there is executorService and externalExecutorService", "author": "tomas-langer", "createdAt": "2020-03-16T11:08:32Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2ODY4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394468687", "bodyText": "Now it is simpler.\nRegarding the executors, I followed your suggestion about usage of one unique ScheduledExecutorService that is shared between all the kafka consumers.", "author": "jbescos", "createdAt": "2020-03-18T16:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java b/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\ndeleted file mode 100644\nindex d40646a45..000000000\n--- a/messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java\n+++ /dev/null\n\n@@ -1,322 +0,0 @@\n-/*\n- * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package io.helidon.messaging.kafka;\n-\n-import java.io.Closeable;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Optional;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.function.Consumer;\n-import java.util.logging.Level;\n-import java.util.logging.Logger;\n-\n-import io.helidon.common.context.Context;\n-import io.helidon.common.context.Contexts;\n-import io.helidon.config.Config;\n-import io.helidon.messaging.kafka.connector.KafkaMessage;\n-import io.helidon.messaging.kafka.connector.SimplePublisher;\n-\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n-import org.apache.kafka.common.errors.WakeupException;\n-import org.eclipse.microprofile.reactive.messaging.Message;\n-import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n-import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n-import org.reactivestreams.Subscriber;\n-import org.reactivestreams.Subscription;\n-\n-/**\n- * Simple Kafka consumer covering basic use-cases.\n- * Configurable by Helidon {@link io.helidon.config.Config Config},\n- * For more info about configuration see {@link KafkaConfigProperties}\n- * <p>\n- * Usage:\n- * <pre>{@code\n- *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n- *         c.consumeAsync(r -> System.out.println(r.value()));\n- *   }\n- * }</pre>\n- *\n- * @param <K> Key type\n- * @param <V> Value type\n- * @see KafkaConfigProperties\n- * @see io.helidon.config.Config\n- */\n-public class SimpleKafkaConsumer<K, V> implements Closeable {\n-\n-    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n-    private final KafkaConfigProperties properties;\n-\n-    private final AtomicBoolean closed = new AtomicBoolean(false);\n-    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n-    private final String consumerId;\n-    private ExecutorService executorService;\n-    private ExecutorService externalExecutorService;\n-    private final List<String> topicNameList;\n-    private final KafkaConsumer<K, V> consumer;\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName key in configuration\n-     * @param config      Helidon {@link io.helidon.config.Config config}\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config) {\n-        this(channelName, config, null);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param channelName     key in configuration\n-     * @param config          Helidon {@link io.helidon.config.Config config}\n-     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n-     * @see KafkaConfigProperties\n-     * @see io.helidon.config.Config\n-     */\n-    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n-        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = channelName;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Kafka consumer created from {@link io.helidon.config.Config config}\n-     * see configuration {@link KafkaConfigProperties example}.\n-     *\n-     * @param config Helidon {@link io.helidon.config.Config config}\n-     */\n-    public SimpleKafkaConsumer(Config config) {\n-        this.properties = new KafkaConfigProperties(config);\n-        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n-        this.topicNameList = properties.getTopicNameList();\n-        this.consumerId = null;\n-        this.consumer = new KafkaConsumer<>(properties);\n-    }\n-\n-    /**\n-     * Execute supplied consumer for each received record.\n-     *\n-     * @param function to be executed for each received record\n-     * @return {@link java.util.concurrent.Future}\n-     */\n-    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n-        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n-    }\n-\n-    /**\n-     * Execute supplied consumer by provided executor service for each received record.\n-     *\n-     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n-     * @param customTopics    Can be null, list of topics appended to the list from configuration\n-     * @param function        Consumer method executed in new thread for each received record\n-     * @return The Future's get method will return null when consumer is closed\n-     */\n-    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n-                                  Consumer<ConsumerRecord<K, V>> function) {\n-        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",\n-                consumerId, topicNameList, properties.getProperty(KafkaConfigProperties.GROUP_ID)));\n-\n-        List<String> mergedTopics = new ArrayList<>();\n-        mergedTopics.addAll(properties.getTopicNameList());\n-        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n-\n-        if (mergedTopics.isEmpty()) {\n-            throw new InvalidKafkaConsumerState(\"No topic names provided in configuration or by parameter.\");\n-        }\n-\n-        validateConsumer();\n-        this.executorService = executorService;\n-        return executorService.submit(() -> {\n-            consumer.subscribe(mergedTopics, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    ConsumerRecords<K, V> consumerRecords = consumer.poll(Duration.ofSeconds(5));\n-                    consumerRecords.forEach(cr -> executorService.execute(() -> function.accept(cr)));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        });\n-    }\n-\n-    /**\n-     * Create publisher builder.\n-     *\n-     * @param executorService {@link java.util.concurrent.ExecutorService}\n-     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n-     */\n-    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder(ExecutorService executorService) {\n-        validateConsumer();\n-        this.externalExecutorService = executorService;\n-        return ReactiveStreams.fromPublisher(new SimplePublisher<K, V>(subscriber -> {\n-            subscriber.onSubscribe(new Subscription() {\n-                @Override\n-                public void request(long n) {\n-                    LOGGER.log(Level.FINE, \"Pushing Kafka consumer doesn't support requests.\");\n-                }\n-\n-                @Override\n-                public void cancel() {\n-                    SimpleKafkaConsumer.this.close();\n-                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n-                }\n-            });\n-            externalExecutorService.submit(new BackPressureLayer(subscriber));\n-        }));\n-    }\n-\n-    private void validateConsumer() {\n-        if (this.closed.get()) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already closed\");\n-        }\n-        if (this.executorService != null) {\n-            throw new InvalidKafkaConsumerState(\"Invalid consumer state, already consuming\");\n-        }\n-    }\n-\n-    /**\n-     * Blocks current thread until partitions are assigned,\n-     * since when is consumer effectively ready to receive.\n-     *\n-     * @param timeout the maximum time to wait\n-     * @param unit    the time unit of the timeout argument\n-     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n-     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n-     */\n-    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n-        if (!partitionsAssignedLatch.await(timeout, unit)) {\n-            throw new TimeoutException(\"Timeout for subscription reached\");\n-        }\n-    }\n-\n-    /**\n-     * Close consumer gracefully. Stops polling loop,\n-     * wakes possible blocked poll and shuts down executor service.\n-     */\n-    @Override\n-    public void close() {\n-        this.closed.set(true);\n-        this.consumer.wakeup();\n-        Optional.ofNullable(this.executorService).ifPresent(ExecutorService::shutdown);\n-        LOGGER.log(Level.FINE, \"SimpleKafkaConsumer is closed.\");\n-    }\n-\n-    /**\n-     * Use supplied customGroupId if not null\n-     * or take it from configuration if exist\n-     * or generate random in this order.\n-     *\n-     * @param customGroupId custom group.id, overrides group.id from configuration\n-     * @return returns or generate new groupId\n-     */\n-    protected String getOrGenerateGroupId(String customGroupId) {\n-        return Optional.ofNullable(customGroupId)\n-                .orElse(Optional.ofNullable(properties.getProperty(KafkaConfigProperties.GROUP_ID))\n-                        .orElse(UUID.randomUUID().toString()));\n-    }\n-\n-    //Move to messaging incoming connector\n-    private void runInNewContext(Runnable runnable) {\n-        Context parentContext = Context.create();\n-        Context context = Context\n-                .builder()\n-                .parent(parentContext)\n-                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n-                .build();\n-        Contexts.runInContext(context, runnable);\n-    }\n-\n-    private final class BackPressureLayer implements Runnable {\n-\n-        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n-        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n-        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n-\n-        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n-            this.subscriber = subscriber;\n-        }\n-\n-        @Override\n-        public void run() {\n-            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n-            try {\n-                while (!closed.get()) {\n-                    waitForAcksAndPoll();\n-                    if (backPressureBuffer.isEmpty()) continue;\n-                    ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n-                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n-                    ackFutures.add(kafkaMessage.getAckFuture());\n-                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n-                }\n-            } catch (WakeupException ex) {\n-                if (!closed.get()) {\n-                    throw ex;\n-                }\n-            } finally {\n-                LOGGER.info(\"Closing consumer\" + consumerId);\n-                consumer.close();\n-            }\n-        }\n-\n-        /**\n-         * Naive impl of back pressure wise lazy poll.\n-         * Wait for the last batch of records to be acknowledged before commit and another poll.\n-         */\n-        private void waitForAcksAndPoll() {\n-            if (backPressureBuffer.isEmpty()) {\n-                try {\n-                    if (!ackFutures.isEmpty()) {\n-                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n-                        ackFutures.clear();\n-                        consumer.commitSync();\n-                    }\n-                    consumer.poll(Duration.ofSeconds(1)).forEach(backPressureBuffer::add);\n-                } catch (InterruptedException | ExecutionException e) {\n-                    LOGGER.log(Level.SEVERE, \"Error when waiting for all polled records acknowledgements.\", e);\n-                }\n-\n-            }\n-        }\n-\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394455046", "bodyText": "I am not sure if this while is acceptable because of CPU usage. However I put it there because the time frame must be very small (few milliseconds) or zero.\nLet me know if you prefer other way, with count down latch for example.", "author": "jbescos", "createdAt": "2020-03-18T15:54:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber, \n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0, \n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        while (running.get()) {", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0NTQ2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394845460", "bodyText": "I will do this with a synchronize.", "author": "jbescos", "createdAt": "2020-03-19T07:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0Njc1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394846756", "bodyText": "In this case I think it is much better to use a lock.", "author": "jbescos", "createdAt": "2020-03-19T08:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 3300bb42a..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,13 +20,15 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394479326", "bodyText": "Uf this was my super bad idea, we have to do something about this, it basically ignores backpressure. Something like this would be much better:\nhttps://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java\nBut that depends on protected RS with SequentialSubscriber,\nI expect David to remove SequentialSubscriber from RS implemetation in #1511 so it gets little more complicated then.", "author": "danielkec", "createdAt": "2020-03-18T16:27:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjQwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876405", "bodyText": "I integrated the EmittingSubscriber", "author": "jbescos", "createdAt": "2020-03-25T14:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 3300bb42a..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,13 +20,15 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989380", "bodyText": "This field is never used", "author": "tomas-langer", "createdAt": "2020-03-19T12:29:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjUwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876504", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989805", "bodyText": "Please do not extends Properties in KafkaConfigProperties.\nAdd a method toProperties to that class that would return the properties required by KafkaConsumer", "author": "tomas-langer", "createdAt": "2020-03-19T12:30:02Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NzY5MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397877691", "bodyText": "I implemented it differently", "author": "jbescos", "createdAt": "2020-03-25T14:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990527", "bodyText": "config is an immutable snapshot - read configuration options when creating this instance.", "author": "tomas-langer", "createdAt": "2020-03-19T12:31:20Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDk2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990969", "bodyText": "Not sure - is this about \"polling\" or \"pooling\"?\nIf this is how often we poll Kafka for changes, then the correct key should be poll-timeout and constant POLL_TIMEOUT.", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODgzMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878833", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991351", "bodyText": "If you return a constant from Optional.orElseGet, then use Optional.orElse", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:53Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODA4Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878082", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991673", "bodyText": "Method should be package private", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:27Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODkyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878922", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991780", "bodyText": "Method should be package private", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:39Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTE0MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879140", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394992404", "bodyText": "If you unlock a lock, another thread may lock it before you close the consumer.\nThe taskLock should be unlocked in a finally block after consumer.close()", "author": "tomas-langer", "createdAt": "2020-03-19T12:34:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAyNDgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395024831", "bodyText": "In this case there is no other thread that could run later because the scheduler was stopped before forever. Still I'm thinking there is a very rare scenario that:\n\nScheduler started a task, and it doesn't reach the lock.\nShutdown is executed.\nclose() is executed and blocks. So task of point 1 is waiting.\nTask run and fails with unexpected error because the kafka connection is closed.\n\nSo a part of doing what you said, I will modify BackPressureLayer to check !scheduler.isShutdown()", "author": "jbescos", "createdAt": "2020-03-19T13:30:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394994736", "bodyText": "Please replace with:\nContext.Builder contextBuilder = Context.builder()\n                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n\nContexts.context().ifPresent(contextBuilder::parent);\n        \nContexts.runInContext(contextBuilder.build(), runnable);", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:03Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTI2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879260", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995234", "bodyText": "Definitely pollTimeout not poolTimeout", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);\n+    }\n+\n+    private final class BackPressureLayer implements Runnable {\n+\n+        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n+        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n+        private final long poolTimeout;\n+\n+        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n+            this.subscriber = subscriber;\n+            this.poolTimeout = poolTimeout;\n+        }\n+\n+        @Override\n+        public void run() {\n+            try {\n+                taskLock.lock();\n+                waitForAcksAndPoll();\n+                ConsumerRecord<K, V> cr;\n+                while ((cr = backPressureBuffer.poll()) != null) {\n+                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                    ackFutures.add(kafkaMessage.getAckFuture());\n+                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                }\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }\n+\n+        /**\n+         * Naive impl of back pressure wise lazy poll.\n+         * Wait for the last batch of records to be acknowledged before commit and another poll.\n+         */\n+        private void waitForAcksAndPoll() {\n+            if (backPressureBuffer.isEmpty()) {\n+                try {\n+                    if (!ackFutures.isEmpty()) {\n+                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n+                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n+                        ackFutures.clear();\n+                        consumer.commitSync();\n+                    }\n+                    consumer.poll(Duration.ofMillis(poolTimeout)).forEach(backPressureBuffer::add);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTM4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879386", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\nindex 18d71ad5e..7018434cb 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java\n\n@@ -20,6 +20,7 @@ import java.io.Closeable;\n import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Properties;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995494", "bodyText": "method should be package local", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:22Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTc5OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995798", "bodyText": "(and all other public methods in this class)", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTUzNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879536", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n\n@@ -18,10 +18,8 @@ package io.helidon.microprofile.connectors.kafka;\n \n import java.io.Closeable;\n import java.util.ArrayList;\n-import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n+import java.util.Properties;\n import java.util.concurrent.Future;\n import java.util.logging.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996367", "bodyText": "Using null is not encouraged in Helidon. Would be better to send correct defaults rather than nulls - this is very error prone", "author": "tomas-langer", "createdAt": "2020-03-19T12:41:54Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTY4OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879689", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n\n@@ -18,10 +18,8 @@ package io.helidon.microprofile.connectors.kafka;\n \n import java.io.Closeable;\n import java.util.ArrayList;\n-import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n+import java.util.Properties;\n import java.util.concurrent.Future;\n import java.util.logging.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996949", "bodyText": "This is an error - throw an exception", "author": "tomas-langer", "createdAt": "2020-03-19T12:42:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);\n+        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n+\n+        for (Future<RecordMetadata> future : futureRecords) {\n+            try {\n+                metadataList.add(future.get());\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new RuntimeException(\"Failed to send topic\", e);\n+            }\n+        }\n+        return metadataList;\n+    }\n+\n+    /**\n+     * Produce asynchronously.\n+     *\n+     * @param value value to be produced\n+     * @return list of futures\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(V value) {\n+        return produceAsync(null, null, null, null, value, null);\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param customTopics Can be null, list of topics appended to the list from configuration,\n+     *                     record will be sent to all topics iteratively\n+     * @param partition    Can be null, if key is also null topic is sent to random partition\n+     * @param timestamp    Can be null System.currentTimeMillis() is used then\n+     * @param key          Can be null, if not, topics are grouped to partitions by key\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @param headers      Can be null, custom headers for additional meta information if needed\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n+                                                     Integer partition,\n+                                                     Long timestamp,\n+                                                     K key,\n+                                                     V value,\n+                                                     Iterable<Header> headers) {\n+\n+        List<String> mergedTopics = new ArrayList<>();\n+        mergedTopics.addAll(properties.getTopicNameList());\n+        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n+\n+        if (mergedTopics.isEmpty()) {\n+            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDg4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880887", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\nindex 56bc53b15..16ccf02fe 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java\n\n@@ -18,10 +18,8 @@ package io.helidon.microprofile.connectors.kafka;\n \n import java.io.Closeable;\n import java.util.ArrayList;\n-import java.util.Collections;\n import java.util.List;\n-import java.util.Optional;\n-import java.util.concurrent.ExecutionException;\n+import java.util.Properties;\n import java.util.concurrent.Future;\n import java.util.logging.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394997949", "bodyText": "This is never used.", "author": "tomas-langer", "createdAt": "2020-03-19T12:44:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDk4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880988", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:07:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 69%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex 8c3b1bee4..8d33fee53 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n\n@@ -17,6 +17,7 @@\n package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394998973", "bodyText": "This is not the correct way to do this (I mentioned this in my previous review.", "author": "tomas-langer", "createdAt": "2020-03-19T12:46:33Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)\n+                .split(\",\"))\n+                .map(String::trim)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private void addProperty(Config c) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MjgwMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882800", "bodyText": "Sorry I didn't notice github was hiding some comments.", "author": "jbescos", "createdAt": "2020-03-25T14:09:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nsimilarity index 69%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\nrename to microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\nindex 8c3b1bee4..8d33fee53 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/HelidonToKafkaConfigParser.java\n\n@@ -17,6 +17,7 @@\n package io.helidon.microprofile.connectors.kafka;\n \n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999609", "bodyText": "Please use Helidon ScheduledThreadPoolSupplier, that is already fully configurable using Helidon Config", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTY4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999686", "bodyText": "Also supports context propagation.", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDM4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000387", "bodyText": "scheduler = ScheduledThreadPoolSupplier.builder()\n                .threadNamePrefix(\"kafka-\")\n                .config(config)\n                .build()\n                .get();", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MTQ3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397881471", "bodyText": "Thanks for the information, I have modified it to that", "author": "jbescos", "createdAt": "2020-03-25T14:07:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n\n@@ -16,10 +16,10 @@\n \n package io.helidon.microprofile.connectors.kafka;\n \n-import java.util.Collection;\n+import java.io.Closeable;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Queue;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000643", "bodyText": "This method should be private or package local.", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4Mjk0OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882948", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n\n@@ -16,10 +16,10 @@\n \n package io.helidon.microprofile.connectors.kafka;\n \n-import java.util.Collection;\n+import java.io.Closeable;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Queue;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTEwMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001102", "bodyText": "Do not use info log level so much. If you want to log an info message, just log a single one in this method.", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:35Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n\n@@ -16,10 +16,10 @@\n \n package io.helidon.microprofile.connectors.kafka;\n \n-import java.util.Collection;\n+import java.io.Closeable;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Queue;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001252", "bodyText": "Method should be package local.", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzM1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883353", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n\n@@ -16,10 +16,10 @@\n \n package io.helidon.microprofile.connectors.kafka;\n \n-import java.util.Collection;\n+import java.io.Closeable;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Queue;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001708", "bodyText": "You should close the producer in onError as well, as onComplete may never be called.", "author": "tomas-langer", "createdAt": "2020-03-19T12:51:42Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n+        consumers.add(basicKafkaConsumer);\n+        return basicKafkaConsumer.createPushPublisherBuilder();\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n+        producers.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                LOGGER.fine(\"On next received \" + message.getPayload());\n+                basicKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883566", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA=="}], "type": "inlineReview", "revised_code": {"commit": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\nindex 84a50d525..e1e683ab2 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java\n\n@@ -16,10 +16,10 @@\n \n package io.helidon.microprofile.connectors.kafka;\n \n-import java.util.Collection;\n+import java.io.Closeable;\n import java.util.LinkedList;\n+import java.util.List;\n import java.util.Queue;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"oid": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "url": "https://github.com/oracle/helidon/commit/e260c32d99f3f0078ee691ebf7063b02e7aba188", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T11:18:43Z", "type": "forcePushed"}, {"oid": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "url": "https://github.com/oracle/helidon/commit/c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T12:07:54Z", "type": "forcePushed"}, {"oid": "07a11a58331466f830337cf71cd033aec1022418", "url": "https://github.com/oracle/helidon/commit/07a11a58331466f830337cf71cd033aec1022418", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T13:59:28Z", "type": "forcePushed"}, {"oid": "c63e74cbc47b8f480e92e3d0c804576d1b144061", "url": "https://github.com/oracle/helidon/commit/c63e74cbc47b8f480e92e3d0c804576d1b144061", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-26T11:51:26Z", "type": "forcePushed"}, {"oid": "10612d66d9b6094133053f2f2778682db3616ef3", "url": "https://github.com/oracle/helidon/commit/10612d66d9b6094133053f2f2778682db3616ef3", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-27T23:33:51Z", "type": "forcePushed"}, {"oid": "aad54c4aefca8c3c7235adc20db2515630a88681", "url": "https://github.com/oracle/helidon/commit/aad54c4aefca8c3c7235adc20db2515630a88681", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T06:42:11Z", "type": "forcePushed"}, {"oid": "ede74fc9dedaffdf9b79684633b7b33e341ff5db", "url": "https://github.com/oracle/helidon/commit/ede74fc9dedaffdf9b79684633b7b33e341ff5db", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T07:05:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMjY2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r400112666", "bodyText": "NPE rethrow was intentional, as its reserved as a signal for upstream (\u00a72.13) and this is not the place we are able to solve kafka client errors. On the other hand I am more and more convinced we should remove abstraction layer between BasicKafkaConsumer, BasicKafkaPublisher,  EmittingPublisher and create one specialized KafkaConsumingPublisher, which can be used in both Helidon MP and SE", "author": "danielkec", "createdAt": "2020-03-30T11:17:04Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Optional<Callback<Long>> requestsCallback;\n+\n+    protected EmittingPublisher(Optional<Callback<Long>> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.ifPresent(callback -> callback.nofity(n));\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {\n+                    return false;\n+                }\n+                try {\n+                    publisher.subscriber.onNext(item);\n+                    return true;\n+                } catch (Throwable t) {\n+                    publisher.fail(t);\n+                    return false;\n+                }", "originalCommit": "dcf791bbc62232b546d3e492a4e9bfb127a7a7a3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4f234782d06edb4522b570f6f3c42e6d90e41866", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex 223745ddb..d3f796647 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n\n@@ -41,9 +41,9 @@ class EmittingPublisher<T> implements Publisher<T> {\n     private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n     private final AtomicLong requested = new AtomicLong();\n     private final AtomicBoolean terminated = new AtomicBoolean();\n-    private final Optional<Callback<Long>> requestsCallback;\n+    private final Callback<Long> requestsCallback;\n \n-    protected EmittingPublisher(Optional<Callback<Long>> requestsCallback) {\n+    protected EmittingPublisher(Callback<Long> requestsCallback) {\n         this.requestsCallback = requestsCallback;\n     }\n \n"}}, {"oid": "4f234782d06edb4522b570f6f3c42e6d90e41866", "url": "https://github.com/oracle/helidon/commit/4f234782d06edb4522b570f6f3c42e6d90e41866", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:43:40Z", "type": "forcePushed"}, {"oid": "1a55afff628b5ccfb12aac8637b96b8550f77569", "url": "https://github.com/oracle/helidon/commit/1a55afff628b5ccfb12aac8637b96b8550f77569", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:53:47Z", "type": "forcePushed"}, {"oid": "47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "url": "https://github.com/oracle/helidon/commit/47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T18:45:28Z", "type": "forcePushed"}, {"oid": "434d47cdac68eedd818d8e4e74d60b553343d6f2", "url": "https://github.com/oracle/helidon/commit/434d47cdac68eedd818d8e4e74d60b553343d6f2", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T06:24:43Z", "type": "forcePushed"}, {"oid": "7fc5933aa86e4078b94479a1f6138996f560a714", "url": "https://github.com/oracle/helidon/commit/7fc5933aa86e4078b94479a1f6138996f560a714", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T07:16:57Z", "type": "forcePushed"}, {"oid": "e594ef5c5ada615d6aa957e7109f10fdab5caf09", "url": "https://github.com/oracle/helidon/commit/e594ef5c5ada615d6aa957e7109f10fdab5caf09", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T10:39:01Z", "type": "forcePushed"}, {"oid": "f2e4290553f1092f9033a01b928e13843519e282", "url": "https://github.com/oracle/helidon/commit/f2e4290553f1092f9033a01b928e13843519e282", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T06:23:31Z", "type": "forcePushed"}, {"oid": "82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "url": "https://github.com/oracle/helidon/commit/82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "message": "Refactoring\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T07:28:03Z", "type": "forcePushed"}, {"oid": "716fd4729baca6548038a27ada9f6dbe6bd9acac", "url": "https://github.com/oracle/helidon/commit/716fd4729baca6548038a27ada9f6dbe6bd9acac", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T08:14:58Z", "type": "forcePushed"}, {"oid": "89106e8b66fc27f890e8515fc484f72082c1a23b", "url": "https://github.com/oracle/helidon/commit/89106e8b66fc27f890e8515fc484f72082c1a23b", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:44:17Z", "type": "forcePushed"}, {"oid": "3aac54995e0518abe8d0da8c50bc26fb69ebc591", "url": "https://github.com/oracle/helidon/commit/3aac54995e0518abe8d0da8c50bc26fb69ebc591", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:45:30Z", "type": "forcePushed"}, {"oid": "cd91f520f05970477ea967c68ef7119789ccf874", "url": "https://github.com/oracle/helidon/commit/cd91f520f05970477ea967c68ef7119789ccf874", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T09:46:39Z", "type": "forcePushed"}, {"oid": "fd9112ad67622206ce02eaeec03751c1199671e9", "url": "https://github.com/oracle/helidon/commit/fd9112ad67622206ce02eaeec03751c1199671e9", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T11:04:04Z", "type": "forcePushed"}, {"oid": "22dc4e2f129a748097971fa3e1cf8999ba083ce4", "url": "https://github.com/oracle/helidon/commit/22dc4e2f129a748097971fa3e1cf8999ba083ce4", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T12:33:05Z", "type": "forcePushed"}, {"oid": "e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "url": "https://github.com/oracle/helidon/commit/e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:38:27Z", "type": "forcePushed"}, {"oid": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "url": "https://github.com/oracle/helidon/commit/f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:40:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MjYyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406472620", "bodyText": "Spotbugs is \"picky\", maxEvents doesn't have to be volatile at all. Kafka publisher should be unbounded in runtime so maxEvents shoudn't be needed. It can be tested like this:\npublic Publisher<KafkaMessage<String, Long>> createPublisher(long elements) {\n...\n        return ReactiveStreams.fromPublisher(\n                KafkaPublisher.build(Executors.newScheduledThreadPool(2), \n                        kafkaConsumer, \n                        Arrays.asList(TEST_TOPIC_1), \n                        1L, \n                        POLL_TIMEOUT, \n                        true))\n                .limit(elements);", "author": "danielkec", "createdAt": "2020-04-09T20:56:46Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "800055dd4a6dc3ded303ab27234f22af832ded5b", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..becd9df4e 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n\n@@ -83,7 +83,7 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n     private final boolean autoCommit;\n     private final long ackTimeout;\n     private final int limitNoAck;\n-    private volatile long maxEvents;\n+    private long maxEvents;\n \n     private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n             List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406483079", "bodyText": "Wakeup should normally get you from polling loop, but in our case its inside the scheduled runnable, It can be better to stop scheduler before wakeup and in the catch block jump out:\n} catch (WakeupException e) {\n  LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n  return;\n}", "author": "danielkec", "createdAt": "2020-04-09T21:18:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5MTQ3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r407991472", "bodyText": "You already stopped it in connector, sorry", "author": "danielkec", "createdAt": "2020-04-14T09:22:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "800055dd4a6dc3ded303ab27234f22af832ded5b", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..becd9df4e 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n\n@@ -83,7 +83,7 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n     private final boolean autoCommit;\n     private final long ackTimeout;\n     private final int limitNoAck;\n-    private volatile long maxEvents;\n+    private long maxEvents;\n \n     private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n             List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4NDE1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406484151", "bodyText": "Can be swallowed by used code, would be great to log it", "author": "danielkec", "createdAt": "2020-04-09T21:20:31Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            processACK();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream).forEach(message -> message.wakeUp());\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "800055dd4a6dc3ded303ab27234f22af832ded5b", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\nindex 87e655961..becd9df4e 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java\n\n@@ -83,7 +83,7 @@ class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n     private final boolean autoCommit;\n     private final long ackTimeout;\n     private final int limitNoAck;\n-    private volatile long maxEvents;\n+    private long maxEvents;\n \n     private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n             List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjc4MjE0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406782143", "bodyText": "ack is going to be called frequently, can we avoid spinning up ForkJoinPool and reuse emit loop we already have? I know it introduces another queue", "author": "danielkec", "createdAt": "2020-04-10T14:27:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaMessage.class.getName());\n+    private final ConsumerRecord<K, V> consumerRecord;\n+    private final AtomicBoolean ack = new AtomicBoolean(false);\n+    private final CountDownLatch waitForCommit;\n+    private final long millisWaitingTimeout;\n+    private final AtomicReference<Exception> ackException = new AtomicReference<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     * @param autoCommit when false it will ack will wait till it is really commited in Kafka,\n+     *        otherwise there is no waiting time because it was committed already.\n+     * @param millisWaitingTimeout this is the time in milliseconds that the ack will be waiting\n+     *        the commit in Kafka. Applies only if autoCommit is false.\n+     */\n+    KafkaMessage(ConsumerRecord<K, V> consumerRecord, boolean autoCommit, long millisWaitingTimeout) {\n+        this.consumerRecord = consumerRecord;\n+        this.waitForCommit = new CountDownLatch(autoCommit ? 0 : 1);\n+        this.millisWaitingTimeout = millisWaitingTimeout;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ack.set(true);\n+        return CompletableFuture.runAsync(() -> {", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "800055dd4a6dc3ded303ab27234f22af832ded5b", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\nindex 8486d63ff..c4c7d4d97 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java\n\n@@ -22,7 +22,6 @@ import java.util.concurrent.CompletionStage;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"oid": "800055dd4a6dc3ded303ab27234f22af832ded5b", "url": "https://github.com/oracle/helidon/commit/800055dd4a6dc3ded303ab27234f22af832ded5b", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T05:19:31Z", "type": "forcePushed"}, {"oid": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "url": "https://github.com/oracle/helidon/commit/9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:36:01Z", "type": "forcePushed"}, {"oid": "bb915d2406926c4a51ac9237bf797680511beb7a", "url": "https://github.com/oracle/helidon/commit/bb915d2406926c4a51ac9237bf797680511beb7a", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:39:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODExODAzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408118039", "bodyText": "This is my bug, it should have had a bottom bound\nif (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {", "author": "danielkec", "createdAt": "2020-04-14T13:02:50Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    protected EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.nofity(n);\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+                EmittingPublisher.this.subscriber = null;\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    boolean isTerminated() {\n+        return terminated.get();\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b0c66b2fff95eb07754c409c655227e897c886c5", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\nindex f1595a402..0bf164f73 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n+++ b/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java\n\n@@ -42,7 +42,7 @@ class EmittingPublisher<T> implements Publisher<T> {\n     private final AtomicBoolean terminated = new AtomicBoolean();\n     private final Callback<Long> requestsCallback;\n \n-    protected EmittingPublisher(Callback<Long> requestsCallback) {\n+    EmittingPublisher(Callback<Long> requestsCallback) {\n         this.requestsCallback = requestsCallback;\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMjc4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408132788", "bodyText": "Needs to be public to be usable in Helidon SE", "author": "danielkec", "createdAt": "2020-04-14T13:24:24Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nsimilarity index 91%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 8f5e862cb..05948b418 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n\n@@ -14,7 +14,7 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.microprofile.connectors.kafka;\n+package io.helidon.messaging.connectors.kafka;\n \n import java.io.Closeable;\n import java.util.LinkedList;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMzE4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408133180", "bodyText": "Needs to be public to be usable in Helidon SE", "author": "danielkec", "createdAt": "2020-04-14T13:24:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "chunk": "diff --git a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nsimilarity index 91%\nrename from microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\nrename to messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 8f5e862cb..05948b418 100644\n--- a/microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n\n@@ -14,7 +14,7 @@\n  * limitations under the License.\n  */\n \n-package io.helidon.microprofile.connectors.kafka;\n+package io.helidon.messaging.connectors.kafka;\n \n import java.io.Closeable;\n import java.util.LinkedList;\n"}}, {"oid": "b0c66b2fff95eb07754c409c655227e897c886c5", "url": "https://github.com/oracle/helidon/commit/b0c66b2fff95eb07754c409c655227e897c886c5", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T13:36:36Z", "type": "forcePushed"}, {"oid": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "url": "https://github.com/oracle/helidon/commit/29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T14:27:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408194418", "bodyText": "Backpressure is not driven by anything here, use .request(Long.MAX) instead pls. no need for request counting", "author": "danielkec", "createdAt": "2020-04-14T14:45:38Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNTM0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408315349", "bodyText": "Backpressure has its place\n\nwhen reading messages from Kafka and delivering them to consumers, we should not deliver more than requested\nwhen writing message to Kafka and reading them from producers, we should not request more than we can send at that time\n\nIn both cases, requesting Long.MAX may result in memory issues, as the messages need to be buffered, or in thread issues, as you would block threads.", "author": "tomas-langer", "createdAt": "2020-04-14T17:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408339334", "bodyText": "No doubt about that, but without taking result of producer.produceAsync(message.getPayload()); in to the account its just unbounded stream with more bureaucracy", "author": "danielkec", "createdAt": "2020-04-14T18:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0MzYwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408343601", "bodyText": "Jorge already came with nice idea of combining callbacks", "author": "danielkec", "createdAt": "2020-04-14T18:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}], "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n\n@@ -16,9 +16,11 @@\n \n package io.helidon.messaging.connectors.kafka;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI5MDY3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408290670", "bodyText": "When you do less than warning levels of logging, please use the lambda approach:\nLogger.fine(() -> String.format(\"...\", n)) - otherwise the string formatting is evaluated for every single request.", "author": "tomas-langer", "createdAt": "2020-04-14T16:56:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\nindex 872b32b38..c2eb941f7 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java\n\n@@ -56,7 +56,7 @@ class EmittingPublisher<T> implements Publisher<T> {\n                 if (n < 1) {\n                     fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n                 }\n-                LOGGER.fine(String.format(\"Request %s events\", n));\n+                LOGGER.fine(() -> String.format(\"Request %s events\", n));\n                 requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n                 state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n                 requestsCallback.nofity(n);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408311905", "bodyText": "If this class should be part of Helidon SE, then the public constructor is an issue.\nAlso you need to add a Builder for any configurable options - in Helidon SE, most of things that can be done using Config should be doable using Builder and vice-versa", "author": "tomas-langer", "createdAt": "2020-04-14T17:30:06Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMjk2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408312966", "bodyText": "For correct SE/MP split, I think you should:\n\nRemove public constructor from this class\nCreate a builder that also supports config through config(Config) method as other builders in Helidon\nAdd a CDI extension for Kafka connector to another module, that would create the connector instance for MP messaging correctly injecting values using CDI", "author": "tomas-langer", "createdAt": "2020-04-14T17:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 05948b418..37b99647a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n\n@@ -116,7 +116,7 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        KafkaSubscriber<Object, Object> subscriber = KafkaSubscriber.build(helidonConfig);\n         return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzIwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313201", "bodyText": "@Observes methods should not be public", "author": "tomas-langer", "createdAt": "2020-04-14T17:32:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n+     * to terminate the messaging and release Kafka connections.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\nindex 05948b418..37b99647a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java\n\n@@ -116,7 +116,7 @@ public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnect\n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n         Config helidonConfig = (Config) config;\n-        KafkaSubscriber<Object> subscriber = KafkaSubscriber.build(helidonConfig);\n+        KafkaSubscriber<Object, Object> subscriber = KafkaSubscriber.build(helidonConfig);\n         return ReactiveStreams.fromSubscriber(subscriber);\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzk2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313968", "bodyText": "This is a use case for a builder.", "author": "tomas-langer", "createdAt": "2020-04-14T17:33:30Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,312 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                } else {\n+                                    kafkaCommit.complete(null);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * Those events that are committed will make the {@link KafkaMessage#ack()}\n+     * to complete.\n+     */\n+    private void processACK() {\n+        if (!autoCommit) {\n+            Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+            List<KafkaMessage<K, V>> messagesToCommit = new LinkedList<>();\n+            // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+            for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+                // No need to sort it, offsets are consumed in order\n+                List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+                Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+                KafkaMessage<K, V> highest = null;\n+                while (iterator.hasNext()) {\n+                    KafkaMessage<K, V> element = iterator.next();\n+                    if (element.isAck()) {\n+                        messagesToCommit.add(element);\n+                        highest = element;\n+                        iterator.remove();\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                if (highest != null) {\n+                    OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                    LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                    offsets.put(entry.getKey(), offset);\n+                }\n+            }\n+            if (!messagesToCommit.isEmpty()) {\n+                Optional<RuntimeException> exception = commitInKafka(offsets);\n+                messagesToCommit.stream().forEach(message -> {\n+                    exception.ifPresentOrElse(\n+                            ex -> message.kafkaCommit().completeExceptionally(ex),\n+                            () -> message.kafkaCommit().complete(null));\n+                });\n+            }\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n+        LOGGER.fine(() -> \"Closed\");\n+    }\n+\n+    //Move to messaging incoming connector\n+    protected void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     *\n+     * Note: after creating a KafkaPublisher you must always\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\nindex 3e7e4d74a..3bb4a673a 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java\n\n@@ -44,7 +44,6 @@ import io.helidon.config.Config;\n \n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.KafkaConsumer;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.common.errors.WakeupException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzU1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317556", "bodyText": "The only allows factory method is create unless there is a good reason not to use it. I do not see a good reason, so please rename to create(Config) (even though this is package local only)", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:28Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n\n@@ -16,9 +16,11 @@\n \n package io.helidon.messaging.connectors.kafka;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317831", "bodyText": "Please do not do this.\nUse a builder if you want to specify details and fill them either manually or from config.", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:56Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, new KafkaProducer<>(kafkaConfig)), backpressure);\n+    }\n+\n+    // For tests", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\nindex 8d2f48e92..23abff5e0 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java\n\n@@ -16,9 +16,11 @@\n \n package io.helidon.messaging.connectors.kafka;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408318293", "bodyText": "This seems not to do anything useful. Isn't the contract of ConsumerRebalancerListener a bit more complicated?", "author": "tomas-langer", "createdAt": "2020-04-14T17:40:43Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NDQ5NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408594495", "bodyText": "From the point of view of reading new events, we don't care about the partitions because we don't specify them.\nBut this could affects pending commits. This is the current strategy when this happens:\n\nEvent is coming from partition A.\nUser is processing it.\nKafka revoke that partition.\nUser ack, commit is sent.\nKafka should throw CommitFailedException, because that partition doesn't exist.\nThe exception is sent to the ack, so user can decide. Note that publisher doesn't fail in this scenario, but the user can cancel the subscription if he wants to.\n\nOnce partition is up again, Kafka should send the message again (because it was not successfully committed).", "author": "jbescos", "createdAt": "2020-04-15T05:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "907340606f71273dc743136f182a860bdb84d348", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java\nindex cf664f63c..2956ffb3c 100644\n--- a/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java\n+++ b/messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java\n\n@@ -36,13 +36,13 @@ class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanc\n \n     @Override\n     public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n-        LOGGER.fine(\"Partitions revoked: \" + partitions);\n+        LOGGER.fine(() -> \"Partitions revoked: \" + partitions);\n         // Do nothing\n     }\n \n     @Override\n     public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n-        LOGGER.fine(\"Partitions assigned: \" + partitions);\n+        LOGGER.fine(() -> \"Partitions assigned: \" + partitions);\n         this.countDown();\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTA2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319069", "bodyText": "Helidon MP implementations must not be required in an SE module.\nIf you want to split into SE/MP, then SE modules can only depend on other SE modules.\nSE modules can depend on MP APIs and SPIs of the specifications.", "author": "tomas-langer", "createdAt": "2020-04-14T17:41:55Z", "path": "messaging/connectors/kafka/src/main/java/module-info.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+module io.helidon.microprofile.connectors.kafka {\n+    requires java.logging;\n+\n+    requires static cdi.api;\n+    requires static javax.inject;\n+    requires static java.activation;\n+    requires static kafka.clients;\n+    requires io.helidon.microprofile.config;", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "chunk": "diff --git a/messaging/connectors/kafka/src/main/java/module-info.java b/messaging/connectors/kafka/src/main/java/module-info.java\nindex 27cb3ee3a..d8f63acd2 100644\n--- a/messaging/connectors/kafka/src/main/java/module-info.java\n+++ b/messaging/connectors/kafka/src/main/java/module-info.java\n\n@@ -14,21 +14,20 @@\n  * limitations under the License.\n  */\n \n-module io.helidon.microprofile.connectors.kafka {\n+module io.helidon.messaging.connectors.kafka {\n     requires java.logging;\n \n     requires static cdi.api;\n     requires static javax.inject;\n-    requires static java.activation;\n     requires static kafka.clients;\n-    requires io.helidon.microprofile.config;\n-    requires io.helidon.microprofile.server;\n     requires io.helidon.microprofile.reactive;\n     requires org.reactivestreams;\n     requires transitive io.helidon.config;\n+    // FIXME Remove microprofile\n     requires transitive microprofile.reactive.messaging.api;\n     requires transitive microprofile.reactive.streams.operators.api;\n-    requires io.helidon.microprofile.messaging;\n+    requires io.helidon.common.context;\n+    requires io.helidon.common.configurable;\n \n     exports io.helidon.messaging.connectors.kafka;\n }\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTMzNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319337", "bodyText": "This is a CDI bean, should not be part of SE impementation at all.", "author": "tomas-langer", "createdAt": "2020-04-14T17:42:21Z", "path": "messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import io.helidon.messaging.connectors.kafka.KafkaMessage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.Outgoing;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * This class contains the outputs of the tests. In order to avoid that one test mess up in the results\n+ * of other tests (this could happen when some data is produced in one test and it is not committed),\n+ * there are many subclasses of AbstractSampleBean.\n+ */\n+abstract class AbstractSampleBean {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "chunk": "diff --git a/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java b/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\nindex 83f6a5257..d48153103 100644\n--- a/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\n+++ b/messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java\n\n@@ -218,7 +218,7 @@ abstract class AbstractSampleBean {\n             // Certain messages are not ACK. We can check later that they will be sent again.\n             if (!NO_ACK.equals(msg.getPayload().value())) {\n                 LOGGER.fine(\"ACK sent\");\n-                msg.ack();\n+                msg.ack().whenComplete((a, b) -> countDown(\"channel8()\"));\n             } else {\n                 partitionIdOfNoAck.set(kafkaMessage.getPayload().partition());\n                 LOGGER.fine(\"ACK is not sent\");\n"}}, {"oid": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "url": "https://github.com/oracle/helidon/commit/a5ee8f4ea70246d599e74c8967656108e723fb6e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T18:42:50Z", "type": "forcePushed"}, {"oid": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "url": "https://github.com/oracle/helidon/commit/b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T06:44:07Z", "type": "forcePushed"}, {"oid": "724b9f8a6ee57542b101efb18ee500be8f96aa7b", "url": "https://github.com/oracle/helidon/commit/724b9f8a6ee57542b101efb18ee500be8f96aa7b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T09:17:11Z", "type": "forcePushed"}, {"oid": "819cb9c1a418302727f9f2d2dfffb59d350c951c", "url": "https://github.com/oracle/helidon/commit/819cb9c1a418302727f9f2d2dfffb59d350c951c", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T10:15:53Z", "type": "forcePushed"}, {"oid": "443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "url": "https://github.com/oracle/helidon/commit/443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T16:30:56Z", "type": "forcePushed"}, {"oid": "127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "url": "https://github.com/oracle/helidon/commit/127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T17:10:53Z", "type": "forcePushed"}, {"oid": "5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "url": "https://github.com/oracle/helidon/commit/5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T05:43:59Z", "type": "forcePushed"}, {"oid": "1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "url": "https://github.com/oracle/helidon/commit/1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T06:57:02Z", "type": "forcePushed"}, {"oid": "ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "url": "https://github.com/oracle/helidon/commit/ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T08:22:05Z", "type": "forcePushed"}, {"oid": "040e496c48dfb131b358d5bc47394fdeeb0ab990", "url": "https://github.com/oracle/helidon/commit/040e496c48dfb131b358d5bc47394fdeeb0ab990", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T09:24:46Z", "type": "forcePushed"}, {"oid": "5164ac2f69b9548de389795d65fc47e06603fee5", "url": "https://github.com/oracle/helidon/commit/5164ac2f69b9548de389795d65fc47e06603fee5", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T10:11:13Z", "type": "forcePushed"}, {"oid": "f92986265ac9529f58a0d10e4e256624f4f6fe1b", "url": "https://github.com/oracle/helidon/commit/f92986265ac9529f58a0d10e4e256624f4f6fe1b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:27:35Z", "type": "forcePushed"}, {"oid": "6ce484aeb96ee5891ce29899845a44cc5d67dc80", "url": "https://github.com/oracle/helidon/commit/6ce484aeb96ee5891ce29899845a44cc5d67dc80", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:31:34Z", "type": "forcePushed"}, {"oid": "5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "url": "https://github.com/oracle/helidon/commit/5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T12:11:27Z", "type": "forcePushed"}, {"oid": "85dd94f6a91b16c06439440205939007d81c4011", "url": "https://github.com/oracle/helidon/commit/85dd94f6a91b16c06439440205939007d81c4011", "message": "Merge branch 'master' into kafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T05:38:12Z", "type": "forcePushed"}, {"oid": "3d22f6ce368e806ceea0047da247afc6460b84f3", "url": "https://github.com/oracle/helidon/commit/3d22f6ce368e806ceea0047da247afc6460b84f3", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:29:55Z", "type": "forcePushed"}, {"oid": "9f76d6cd40937d5622e165bf013edb08cfcf75ef", "url": "https://github.com/oracle/helidon/commit/9f76d6cd40937d5622e165bf013edb08cfcf75ef", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:44:22Z", "type": "forcePushed"}, {"oid": "907340606f71273dc743136f182a860bdb84d348", "url": "https://github.com/oracle/helidon/commit/907340606f71273dc743136f182a860bdb84d348", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:02:46Z", "type": "forcePushed"}, {"oid": "6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "url": "https://github.com/oracle/helidon/commit/6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:09:09Z", "type": "forcePushed"}, {"oid": "c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "url": "https://github.com/oracle/helidon/commit/c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:24:44Z", "type": "forcePushed"}, {"oid": "48eae481f9df3b5c50d586874f8f5610e2af0a3d", "url": "https://github.com/oracle/helidon/commit/48eae481f9df3b5c50d586874f8f5610e2af0a3d", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:01:08Z", "type": "forcePushed"}, {"oid": "c556cc8a85175caf5a60eb795a41cd425acb40b7", "url": "https://github.com/oracle/helidon/commit/c556cc8a85175caf5a60eb795a41cd425acb40b7", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:42:19Z", "type": "forcePushed"}, {"oid": "e17c7ab5db7ddb6861e52dd90796cf77806556ec", "url": "https://github.com/oracle/helidon/commit/e17c7ab5db7ddb6861e52dd90796cf77806556ec", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-21T10:11:04Z", "type": "forcePushed"}, {"oid": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "url": "https://github.com/oracle/helidon/commit/3a896fa19cbc3a64ea69121d1bec080ce30389f8", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:37Z", "type": "commit"}, {"oid": "b5e59d0868995c86edabe0588e078bc4c47e8660", "url": "https://github.com/oracle/helidon/commit/b5e59d0868995c86edabe0588e078bc4c47e8660", "message": "A few small changes.\n\nSigned-off-by: Tomas Langer <tomas.langer@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "0bae61f51a5d949b3b26503945948f198cfa2534", "url": "https://github.com/oracle/helidon/commit/0bae61f51a5d949b3b26503945948f198cfa2534", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "32be94abc930b1516e6eca952fd4e379619563c1", "url": "https://github.com/oracle/helidon/commit/32be94abc930b1516e6eca952fd4e379619563c1", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "url": "https://github.com/oracle/helidon/commit/150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "message": "Move tck tests inside kafka module\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "c4558980f68943d3afb03efc198fa9ba94bf30cb", "url": "https://github.com/oracle/helidon/commit/c4558980f68943d3afb03efc198fa9ba94bf30cb", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "url": "https://github.com/oracle/helidon/commit/430995f595845e9f88bdb7296f3ee72bf8a750c0", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7d68ce582ca13d11dedf930423920617870c62fd", "url": "https://github.com/oracle/helidon/commit/7d68ce582ca13d11dedf930423920617870c62fd", "message": "Improving the tests\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "url": "https://github.com/oracle/helidon/commit/8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7081c593461d66f7eea754455b70d825a6a7cd96", "url": "https://github.com/oracle/helidon/commit/7081c593461d66f7eea754455b70d825a6a7cd96", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:24:13Z", "type": "forcePushed"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "commit"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "forcePushed"}, {"oid": "620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "url": "https://github.com/oracle/helidon/commit/620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "message": "Copyright fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-23T09:07:42Z", "type": "commit"}]}