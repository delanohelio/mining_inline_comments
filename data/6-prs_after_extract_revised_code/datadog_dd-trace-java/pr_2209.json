{"pr_number": 2209, "pr_title": "Try reducing allocation in stream processing", "pr_createdAt": "2020-12-16T08:10:52Z", "pr_url": "https://github.com/DataDog/dd-trace-java/pull/2209", "timeline": [{"oid": "3b029a29998693d85a61cc910498b463cf399159", "url": "https://github.com/DataDog/dd-trace-java/commit/3b029a29998693d85a61cc910498b463cf399159", "message": "Try reducing allocation in stream processing", "committedDate": "2021-03-12T18:13:38Z", "type": "commit"}, {"oid": "3b029a29998693d85a61cc910498b463cf399159", "url": "https://github.com/DataDog/dd-trace-java/commit/3b029a29998693d85a61cc910498b463cf399159", "message": "Try reducing allocation in stream processing", "committedDate": "2021-03-12T18:13:38Z", "type": "forcePushed"}, {"oid": "a79ba0c8fdf348cd627aa462d7337e44074b446d", "url": "https://github.com/DataDog/dd-trace-java/commit/a79ba0c8fdf348cd627aa462d7337e44074b446d", "message": "Docs cleanup", "committedDate": "2021-03-15T11:50:46Z", "type": "commit"}, {"oid": "f6ee868ae29772ce4e5f3eb4e068609b8b6239a2", "url": "https://github.com/DataDog/dd-trace-java/commit/f6ee868ae29772ce4e5f3eb4e068609b8b6239a2", "message": "Track and log the read/written bytes", "committedDate": "2021-03-15T14:56:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDQ3ODMxMg==", "url": "https://github.com/DataDog/dd-trace-java/pull/2209#discussion_r594478312", "bodyText": "try-with-resources?", "author": "jpbempel", "createdAt": "2021-03-15T16:14:12Z", "path": "dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package com.datadog.profiling.uploader;\n+\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.zip.GZIPOutputStream;\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import net.jpountz.lz4.LZ4FrameOutputStream;\n+import okhttp3.MediaType;\n+import okhttp3.RequestBody;\n+import okio.BufferedSink;\n+import okio.Okio;\n+import okio.Source;\n+import org.openjdk.jmc.common.io.IOToolkit;\n+\n+/**\n+ * A specialized {@linkplain RequestBody} subclass performing on-the fly compression of the uploaded\n+ * data.\n+ */\n+final class CompressingRequestBody extends RequestBody {\n+  /** A simple functional supplier throwing an {@linkplain IOException} */\n+  @FunctionalInterface\n+  interface InputStreamSupplier {\n+    InputStream get() throws IOException;\n+  }\n+\n+  /** A simple functional mapper allowing to throw {@linkplain IOException} */\n+  @FunctionalInterface\n+  interface OutputStreamMappingFunction {\n+    OutputStream apply(OutputStream param) throws IOException;\n+  }\n+\n+  /**\n+   * A data upload retry policy. By using this policy it is possible to customize how many times the\n+   * data upload will be reattempted if the input data stream is unavailable.\n+   */\n+  @FunctionalInterface\n+  interface RetryPolicy {\n+    /**\n+     * @param ordinal number of data upload attempts so far\n+     * @return {@literal true} if the data upload should be retried\n+     */\n+    boolean shouldRetry(int ordinal);\n+  }\n+\n+  /**\n+   * A data upload retry backoff policy. This policy will be used to obtain the delay before the\n+   * next retry.\n+   */\n+  @FunctionalInterface\n+  interface RetryBackoff {\n+    /**\n+     * @param ordinal number of data upload attempts so far\n+     * @return the required delay in milliscenods before next retry\n+     */\n+    int backoff(int ordinal);\n+  }\n+\n+  static final MediaType OCTET_STREAM = MediaType.parse(\"application/octet-stream\");\n+\n+  // https://github.com/lz4/lz4/blob/dev/doc/lz4_Frame_format.md#general-structure-of-lz4-frame-format\n+  private static final int[] LZ4_MAGIC = new int[] {0x04, 0x22, 0x4D, 0x18};\n+\n+  // JMC's IOToolkit hides this from us...\n+  private static final int ZIP_MAGIC[] = new int[] {80, 75, 3, 4};\n+  private static final int GZ_MAGIC[] = new int[] {31, 139};\n+\n+  private final InputStreamSupplier inputStreamSupplier;\n+  private final OutputStreamMappingFunction outputStreamMapper;\n+  private final RetryPolicy retryPolicy;\n+  private final RetryBackoff retryBackoff;\n+\n+  private long readBytes = 0;\n+  private long writtenBytes = 0;\n+\n+  /**\n+   * Create a new instance configured with 1 retry and constant 10ms backoff delay.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType, @Nonnull InputStreamSupplier inputStreamSupplier) {\n+    this(compressionType, inputStreamSupplier, r -> r <= 1, r -> 10);\n+  }\n+\n+  /**\n+   * Create a new instance configured with constant 10ms backoff delay.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   * @param retryPolicy {@linkplain RetryPolicy} instance\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType,\n+      @Nonnull InputStreamSupplier inputStreamSupplier,\n+      @Nonnull RetryPolicy retryPolicy) {\n+    this(compressionType, inputStreamSupplier, retryPolicy, r -> 10);\n+  }\n+\n+  /**\n+   * Create a new instance.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   * @param retryPolicy {@linkplain RetryPolicy} instance\n+   * @param retryBackoff {@linkplain RetryBackoff} instance\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType,\n+      @Nonnull InputStreamSupplier inputStreamSupplier,\n+      @Nonnull RetryPolicy retryPolicy,\n+      @Nonnull RetryBackoff retryBackoff) {\n+    this.inputStreamSupplier = () -> ensureMarkSupported(inputStreamSupplier.get());\n+    this.outputStreamMapper = getOutputStreamMapper(compressionType);\n+    this.retryPolicy = retryPolicy;\n+    this.retryBackoff = retryBackoff;\n+  }\n+\n+  @Override\n+  public long contentLength() throws IOException {\n+    // uploading chunked streaming data -> the length is unknown\n+    return -1;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public MediaType contentType() {\n+    return OCTET_STREAM;\n+  }\n+\n+  @Override\n+  public void writeTo(BufferedSink bufferedSink) throws IOException {\n+    Throwable lastException = null;\n+    boolean shouldRetry = false;\n+    int retry = 1;\n+    do {\n+      /*\n+       * Here we do attempt a simple retry functionality in case the input stream can not be obtained -\n+       * which is usually caused by the JFR recording being not finalized on disk in our case.\n+       * The number of times this should be re-attempted as well as the backoff between the attempts\n+       * can be defined per CompressingRequestBody instance.\n+       *\n+       * However, the failures in reading the input stream and writing the data out to the Okio sink\n+       * will not be retried because doing so can result in corrupted data uploads. Instead, the client\n+       * should use the OkHttpClient callback to get notified about failed requests and handle the retries\n+       * at the request level.\n+       */\n+      try (ByteCountingInputStream inputStream =\n+          new ByteCountingInputStream(inputStreamSupplier.get())) {\n+        // Got the input stream so clear the 'lastException'\n+        lastException = null;\n+        try {\n+          ByteCountingOutputStream outputStream =\n+              new ByteCountingOutputStream(bufferedSink.outputStream());\n+          attemptWrite(inputStream, outputStream);\n+          readBytes = inputStream.getReadBytes();\n+          writtenBytes = outputStream.getWrittenBytes();\n+        } catch (Throwable t) {\n+          // Only the failures while obtaining the input stream are retriable.\n+          // Any failure during reading that input stream must make this write to fail as well.\n+          lastException = t;\n+          shouldRetry = false;\n+        }\n+      } catch (Throwable t) {\n+        // Only the failures while obtaining the input stream are retriable.\n+        lastException = t;\n+        shouldRetry = true;\n+      }\n+      if (shouldRetry && retryPolicy.shouldRetry(retry)) {\n+        long backoffMs = retryBackoff.backoff(retry);\n+        try {\n+          Thread.sleep(backoffMs);\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IOException(e);\n+        }\n+        retry++;\n+      } else {\n+        shouldRetry = false;\n+      }\n+    } while (shouldRetry);\n+    if (lastException != null) {\n+      throw lastException instanceof IOException\n+          ? (IOException) lastException\n+          : new IOException(lastException);\n+    }\n+  }\n+\n+  long getReadBytes() {\n+    return readBytes;\n+  }\n+\n+  long getWrittenBytes() {\n+    return writtenBytes;\n+  }\n+\n+  private void attemptWrite(@Nonnull InputStream inputStream, @Nonnull OutputStream outputStream)\n+      throws IOException {\n+    OutputStream sinkStream =\n+        isCompressed(inputStream)\n+            ? outputStream\n+            : new BufferedOutputStream(\n+                outputStreamMapper.apply(\n+                    new BufferedOutputStream(outputStream) {\n+                      @Override\n+                      public void close() throws IOException {\n+                        // Do not propagate close; call 'flush()' instead.\n+                        // Compression streams must be 'closed' because they finalize the\n+                        // compression\n+                        // in that method.\n+                        flush();\n+                      }\n+                    }));\n+    BufferedSink sink = Okio.buffer(Okio.sink(sinkStream));\n+    try (Source source = Okio.buffer(Okio.source(inputStream))) {\n+      sink.writeAll(source);\n+    }\n+    // a bit of cargo-culting to make sure that all writes have really-really been flushed\n+    sink.emit();\n+    sink.flush();\n+    sinkStream.close();", "originalCommit": "f6ee868ae29772ce4e5f3eb4e068609b8b6239a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fe007159518919347e2871197f8c582e999825bc", "chunk": "diff --git a/dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java b/dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java\nindex 151f5367f6..afff3ef869 100644\n--- a/dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java\n+++ b/dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java\n\n@@ -200,7 +200,7 @@ final class CompressingRequestBody extends RequestBody {\n \n   private void attemptWrite(@Nonnull InputStream inputStream, @Nonnull OutputStream outputStream)\n       throws IOException {\n-    OutputStream sinkStream =\n+    try (OutputStream sinkStream =\n         isCompressed(inputStream)\n             ? outputStream\n             : new BufferedOutputStream(\n"}}, {"oid": "fe007159518919347e2871197f8c582e999825bc", "url": "https://github.com/DataDog/dd-trace-java/commit/fe007159518919347e2871197f8c582e999825bc", "message": "Try-with-resources", "committedDate": "2021-03-15T17:15:17Z", "type": "commit"}]}