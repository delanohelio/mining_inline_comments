{"pr_number": 13729, "pr_title": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "pr_createdAt": "2020-10-21T14:01:24Z", "pr_url": "https://github.com/apache/flink/pull/13729", "timeline": [{"oid": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "url": "https://github.com/apache/flink/commit/f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "message": "[FLINK-19644][hive] Support read specific partition of Hive table in temporal join", "committedDate": "2020-10-22T03:15:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1OTA4Mg==", "url": "https://github.com/apache/flink/pull/13729#discussion_r509859082", "bodyText": "Why do this? Why not Filesystem also have this lookup capability?", "author": "JingsongLi", "createdAt": "2020-10-22T03:32:31Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java", "diffHunk": "@@ -47,29 +59,46 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Properties;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_PARTITION;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getPartitionByPartitionSpecs;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getTableProps;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.toHiveTablePartition;\n+import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.validateAndParsePartitionSpecs;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup table function for Hive connector tables.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+public class HiveLookupFunction<T extends InputSplit> extends TableFunction<RowData> {", "originalCommit": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MDIxNQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r509860215", "bodyText": "The  FileSystemLookupFunction  only used in HiveTableSource.", "author": "leonardBang", "createdAt": "2020-10-22T03:37:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1OTA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDAyOTAxMQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r510029011", "bodyText": "But we can let Filesystem supports this", "author": "JingsongLi", "createdAt": "2020-10-22T09:48:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1OTA4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDAyOTY5Mg==", "url": "https://github.com/apache/flink/pull/13729#discussion_r510029692", "bodyText": "I think we should a better code design, to make the components more decoupled.", "author": "JingsongLi", "createdAt": "2020-10-22T09:49:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1OTA4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 53%\nrename from flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex fd8c8e6221..e8fc29a991 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -59,46 +47,29 @@ import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.Properties;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n-import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.connectors.hive.HiveTableFactory.LOOKUP_JOIN_PARTITION;\n-import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getPartitionByPartitionSpecs;\n-import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.getTableProps;\n-import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.toHiveTablePartition;\n-import static org.apache.flink.table.catalog.hive.util.HivePartitionUtils.validateAndParsePartitionSpecs;\n-import static org.apache.flink.util.Preconditions.checkNotNull;\n-\n /**\n- * Lookup table function for Hive connector tables.\n+ * Lookup table function for filesystem connector tables.\n  */\n-public class HiveLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n \n \tprivate static final long serialVersionUID = 1L;\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(HiveLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\t// parameters to build an HiveTableInputFormat\n-\tprivate final JobConfWrapper confWrapper;\n-\tprivate final String hiveVersion;\n-\tprivate final ObjectPath tablePath;\n-\tprivate final HiveShim hiveShim;\n-\tprivate final DataType[] fieldTypes;\n-\tprivate final String[] fieldNames;\n-\tprivate final int[] selectedFields;\n-\tprivate final List<String> partitionKeys;\n-\tprivate final boolean useMapRedReader;\n-\n+\tprivate final InputFormat<RowData, T> inputFormat;\n \t// names and types of the records returned by the input format\n \tprivate final String[] producedNames;\n \tprivate final DataType[] producedTypes;\n+\tprivate final Duration cacheTTL;\n+\n \t// indices of lookup columns in the record returned by input format\n \tprivate final int[] lookupCols;\n \t// use Row as key for the cache\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDAyODc1OQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r510028759", "bodyText": "Can we add a copy(Partitions) method to HiveTableInputFormat? Then we don't need move so many logicals.", "author": "JingsongLi", "createdAt": "2020-10-22T09:48:13Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java", "diffHunk": "@@ -134,9 +188,48 @@ public void eval(Object... values) {\n \t\t}\n \t}\n \n-\t@VisibleForTesting\n-\tpublic Duration getCacheTTL() {\n-\t\treturn cacheTTL;\n+\tprivate HiveTableInputFormat getHiveTableInputFormat() {", "originalCommit": "f028c8d52466fbd6fd5a9cbf3ed85dcbc65fd2ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 53%\nrename from flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex fd8c8e6221..e8fc29a991 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -188,48 +134,9 @@ public class HiveLookupFunction<T extends InputSplit> extends TableFunction<RowD\n \t\t}\n \t}\n \n-\tprivate HiveTableInputFormat getHiveTableInputFormat() {\n-\t\tList<HiveTablePartition> lookupPartitions = new ArrayList<>();\n-\t\ttry (HiveMetastoreClientWrapper client = HiveMetastoreClientFactory.create(\n-\t\t\t\tnew HiveConf(confWrapper.conf(), HiveConf.class), hiveVersion)) {\n-\t\t\tString dbName = tablePath.getDatabaseName();\n-\t\t\tString tableName = tablePath.getObjectName();\n-\t\t\tTable hiveTable = client.getTable(dbName, tableName);\n-\n-\t\t\tif (partitionKeys != null && partitionKeys.size() > 0) {\n-\t\t\t\tList<Partition> allPartitions = client.listPartitions(dbName, tableName, (short) -1);\n-\t\t\t\tList<Partition> partitions = getPartitionByPartitionSpecs(allPartitions, partitionKeys, partitionSpecs);\n-\n-\t\t\t\tfinal String defaultPartitionName = confWrapper.conf().get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n-\t\t\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n-\t\t\t\tfor (Partition partition : partitions) {\n-\t\t\t\t\tHiveTablePartition hiveTablePartition = toHiveTablePartition(\n-\t\t\t\t\t\t\tpartitionKeys,\n-\t\t\t\t\t\t\tfieldNames,\n-\t\t\t\t\t\t\tfieldTypes,\n-\t\t\t\t\t\t\thiveShim,\n-\t\t\t\t\t\t\ttableProps,\n-\t\t\t\t\t\t\tdefaultPartitionName,\n-\t\t\t\t\t\t\tpartition);\n-\t\t\t\t\tlookupPartitions.add(hiveTablePartition);\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tlookupPartitions.add(new HiveTablePartition(hiveTable.getSd(), tableProps));\n-\t\t\t}\n-\t\t} catch (TException e) {\n-\t\t\tthrow new FlinkHiveException(\"Failed to collect all partitions from hive metaStore\", e);\n-\t\t}\n-\n-\t\treturn new HiveTableInputFormat(\n-\t\t\t\tconfWrapper.conf(),\n-\t\t\t\thiveVersion,\n-\t\t\t\tfieldTypes,\n-\t\t\t\tfieldNames,\n-\t\t\t\tpartitionKeys,\n-\t\t\t\tselectedFields,\n-\t\t\t\tlookupPartitions,\n-\t\t\t\tuseMapRedReader,\n-\t\t\t\t-1L);\n+\t@VisibleForTesting\n+\tpublic Duration getCacheTTL() {\n+\t\treturn cacheTTL;\n \t}\n \n \tprivate void checkCacheReload() {\n"}}, {"oid": "bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "url": "https://github.com/apache/flink/commit/bc53ef3b7595acbc03bdaf1e01360cb802ff5d52", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "committedDate": "2020-11-03T16:58:19Z", "type": "forcePushed"}, {"oid": "9124af4f501b2afeaefa94621a82612846d98783", "url": "https://github.com/apache/flink/commit/9124af4f501b2afeaefa94621a82612846d98783", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "committedDate": "2020-11-04T06:38:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1MzM2Mg==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517153362", "bodyText": "Revert name changing.", "author": "JingsongLi", "createdAt": "2020-11-04T07:56:00Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1MzQ2MA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517153460", "bodyText": "lookupCols is never be used", "author": "JingsongLi", "createdAt": "2020-11-04T07:56:15Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NTk2OA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517155968", "bodyText": "Can you use int[] lookupKeys?", "author": "JingsongLi", "createdAt": "2020-11-04T08:01:27Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzQxNA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517157414", "bodyText": "I think it is better to change to reloadInterval?", "author": "JingsongLi", "createdAt": "2020-11-04T08:04:28Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,\n-\t\t\tString[] producedNames,\n-\t\t\tDataType[] producedTypes,\n \t\t\tDuration cacheTTL) {", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzY5OQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517157699", "bodyText": "I think it is better to remove this default value", "author": "JingsongLi", "createdAt": "2020-11-04T08:05:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -74,24 +74,37 @@\n \t\t\t\t\t\t\t\" NOTES: Please make sure that each partition/file should be written\" +\n \t\t\t\t\t\t\t\" atomically, otherwise the reader may get incomplete data.\");\n \n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_INCLUDE =\n+\t\t\tkey(\"streaming-source.partition.include\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"all\")\n+\t\t\t\t\t.withDescription(\"Option to set the partitions to read, the supported values \" +\n+\t\t\t\t\t\t\t\"are \\\"all\\\" and \\\"latest\\\",\" +\n+\t\t\t\t\t\t\t\" the \\\"all\\\" means read all partitions; the \\\"latest\\\" means read latest \" +\n+\t\t\t\t\t\t\t\"partition in order of streaming-source.partition.order, the \\\"latest\\\" only works\" +\n+\t\t\t\t\t\t\t\" when the streaming hive source table used as temporal table. \" +\n+\t\t\t\t\t\t\t\"By default the option is \\\"all\\\".\\n.\");\n+\n \tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n \t\t\tkey(\"streaming-source.monitor-interval\")\n \t\t\t\t\t.durationType()\n \t\t\t\t\t.defaultValue(Duration.ofMinutes(1))", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\nindex 7be6a39c26..7818a82fda 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\n\n@@ -88,28 +88,30 @@ public class FileSystemOptions {\n \tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n \t\t\tkey(\"streaming-source.monitor-interval\")\n \t\t\t\t\t.durationType()\n-\t\t\t\t\t.defaultValue(Duration.ofMinutes(1))\n+\t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"Time interval for consecutively monitoring partition/file.\");\n \n \tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_ORDER =\n \t\t\tkey(\"streaming-source.partition-order\")\n \t\t\t\t\t.stringType()\n-\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.defaultValue(\"partition-name\")\n \t\t\t\t\t.withDeprecatedKeys(\"streaming-source.consume-order\")\n \t\t\t\t\t.withDescription(\"The partition order of streaming source,\" +\n-\t\t\t\t\t\t\t\" support create-time and partition-time.\" +\n-\t\t\t\t\t\t\t\" create-time compare partition/file creation time, this is not the\" +\n+\t\t\t\t\t\t\t\" support create-time, partition-time and partition-name.\" +\n+\t\t\t\t\t\t\t\" create-time compares partition/file creation time, this is not the\" +\n \t\t\t\t\t\t\t\" partition create time in Hive metaStore, but the folder/file modification\" +\n \t\t\t\t\t\t\t\" time in filesystem, if the partition folder somehow gets updated,\" +\n \t\t\t\t\t\t\t\" e.g. add new file into folder, it can affect how the data is consumed.\" +\n-\t\t\t\t\t\t\t\" partition-time compare time represented by partition name.\\n\" +\n+\t\t\t\t\t\t\t\" partition-time compares the time extracted from partition name.\" +\n+\t\t\t\t\t\t\t\" partition-name compares partition name's alphabetical order, partition-name\" +\n+\t\t\t\t\t\t\t\" is only supported in temporal table join, is not supported in hive streaming reading.\" +\n \t\t\t\t\t\t\t\"For non-partition table, this value should always be 'create-time'.\" +\n \t\t\t\t\t\t\t\"The option is equality with deprecated option \\\"streaming-source.consume-order\\\".\");\n \n \tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_START_OFFSET =\n \t\t\tkey(\"streaming-source.consume-start-offset\")\n \t\t\t\t\t.stringType()\n-\t\t\t\t\t.defaultValue(\"1970-00-00\")\n+\t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"Start offset for streaming consuming.\" +\n \t\t\t\t\t\t\t\" How to parse and compare offsets depends on your order.\" +\n \t\t\t\t\t\t\t\" For create-time and partition-time, should be a timestamp\" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1ODQzMg==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517158432", "bodyText": "You can just pass a RowType here", "author": "JingsongLi", "createdAt": "2020-11-04T08:06:34Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1ODU5Ng==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517158596", "bodyText": "Use InternalSerializers.create", "author": "JingsongLi", "createdAt": "2020-11-04T08:06:54Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -51,125 +45,113 @@\n import java.util.stream.IntStream;\n \n /**\n- * Lookup table function for filesystem connector tables.\n+ * Lookup function for filesystem connector tables.\n+ *\n+ * <p>The hive connector and filesystem connector share read/write files code.\n+ * Currently, only this function only used in hive connector.\n  */\n-public class FileSystemLookupFunction<T extends InputSplit> extends TableFunction<RowData> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n+public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n \t// interval between retries\n \tprivate static final Duration RETRY_INTERVAL = Duration.ofSeconds(10);\n \n-\tprivate final InputFormat<RowData, T> inputFormat;\n-\t// names and types of the records returned by the input format\n-\tprivate final String[] producedNames;\n-\tprivate final DataType[] producedTypes;\n+\tprivate final PartitionFetcher<P> partitionFetcher;\n+\tprivate final PartitionReader<P, RowData> partitionReader;\n+\tprivate final int[] lookupCols;\n+\tprivate final RowData.FieldGetter[] lookupFieldGetters;\n \tprivate final Duration cacheTTL;\n+\tprivate final TypeSerializer<RowData> serializer;\n+\tprivate final DataType[] fieldTypes;\n+\tprivate final String[] fieldNames;\n \n-\t// indices of lookup columns in the record returned by input format\n-\tprivate final int[] lookupCols;\n-\t// use Row as key for the cache\n-\tprivate transient Map<Row, List<RowData>> cache;\n+\t// cache for lookup data\n+\tprivate transient Map<RowData, List<RowData>> cache;\n \t// timestamp when cache expires\n \tprivate transient long nextLoadTime;\n-\t// serializer to copy RowData\n-\tprivate transient TypeSerializer<RowData> serializer;\n-\t// converters to convert data from internal to external in order to generate keys for the cache\n-\tprivate final DataFormatConverter[] converters;\n \n-\tpublic FileSystemLookupFunction(\n-\t\t\tInputFormat<RowData, T> inputFormat,\n+\tpublic FilesystemLookupFunction(\n+\t\t\tPartitionFetcher<P> partitionFetcher,\n+\t\t\tPartitionReader<P, RowData> partitionReader,\n+\t\t\tDataType[] fieldTypes,\n+\t\t\tString[] fieldNames,\n \t\t\tString[] lookupKeys,\n-\t\t\tString[] producedNames,\n-\t\t\tDataType[] producedTypes,\n \t\t\tDuration cacheTTL) {\n-\t\tlookupCols = new int[lookupKeys.length];\n-\t\tconverters = new DataFormatConverter[lookupKeys.length];\n-\t\tMap<String, Integer> nameToIndex = IntStream.range(0, producedNames.length).boxed().collect(\n-\t\t\t\tCollectors.toMap(i -> producedNames[i], i -> i));\n+\t\tthis.cacheTTL = cacheTTL;\n+\t\tthis.partitionFetcher = partitionFetcher;\n+\t\tthis.partitionReader = partitionReader;\n+\t\tthis.fieldTypes = fieldTypes;\n+\t\tthis.fieldNames = fieldNames;\n+\t\tthis.lookupCols = new int[lookupKeys.length];\n+\t\tthis.lookupFieldGetters = new RowData.FieldGetter[lookupKeys.length];\n+\t\tMap<String, Integer> nameToIndex = IntStream.range(0, fieldNames.length).boxed().collect(\n+\t\t\t\tCollectors.toMap(i -> fieldNames[i], i -> i));\n \t\tfor (int i = 0; i < lookupKeys.length; i++) {\n \t\t\tInteger index = nameToIndex.get(lookupKeys[i]);\n \t\t\tPreconditions.checkArgument(index != null, \"Lookup keys %s not selected\", Arrays.toString(lookupKeys));\n-\t\t\tconverters[i] = DataFormatConverters.getConverterForDataType(producedTypes[index]);\n+\t\t\tlookupFieldGetters[i] = RowData.createFieldGetter(fieldTypes[index].getLogicalType(), index);\n \t\t\tlookupCols[i] = index;\n \t\t}\n-\t\tthis.inputFormat = inputFormat;\n-\t\tthis.producedNames = producedNames;\n-\t\tthis.producedTypes = producedTypes;\n-\t\tthis.cacheTTL = cacheTTL;\n-\t}\n-\n-\t@Override\n-\tpublic TypeInformation<RowData> getResultType() {\n-\t\treturn InternalTypeInfo.ofFields(\n-\t\t\t\tArrays.stream(producedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),\n-\t\t\t\tproducedNames);\n+\t\tthis.serializer = getResultType().createSerializer(new ExecutionConfig());", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -19,40 +19,35 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.functions.FunctionContext;\n import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.FlinkRuntimeException;\n-import org.apache.flink.util.Preconditions;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n \n /**\n  * Lookup function for filesystem connector tables.\n  *\n  * <p>The hive connector and filesystem connector share read/write files code.\n- * Currently, only this function only used in hive connector.\n+ * Currently, this function only used in hive connector.\n  */\n-public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n+public class FileSystemLookupFunction<P> extends TableFunction<RowData> {\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FilesystemLookupFunction.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FileSystemLookupFunction.class);\n \n \t// the max number of retries before throwing exception, in case of failure to load the table into cache\n \tprivate static final int MAX_RETRIES = 3;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1OTU4NA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517159584", "bodyText": "I think it is better to keep Context here, and we don't need open and close.\n(All open and close are the same)", "author": "JingsongLi", "createdAt": "2020-11-04T08:09:03Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Fetcher to fetch the suitable partitions of a filesystem table.\n+ *\n+ * @param <P> The type to describe a partition.\n+ */\n+@Internal\n+public interface PartitionFetcher<P> extends Serializable {\n+\n+\t/**\n+\t * Open the resources of the fetcher.\n+\t */\n+\tvoid open() throws Exception;\n+\n+\t/**\n+\t * Fetch the suitable partitions, call this method should guarantee the fetcher has opened.\n+\t */\n+\tList<P> fetch() throws Exception;", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java\nindex 7344277dec..330b638124 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionFetcher.java\n\n@@ -19,35 +19,25 @@\n package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.Internal;\n-import org.apache.flink.api.java.tuple.Tuple2;\n \n import java.io.Serializable;\n import java.util.List;\n import java.util.Optional;\n-import java.util.function.Supplier;\n \n /**\n  * Fetcher to fetch the suitable partitions of a filesystem table.\n  *\n  * @param <P> The type to describe a partition.\n  */\n+\n @Internal\n+@FunctionalInterface\n public interface PartitionFetcher<P> extends Serializable {\n \n-\t/**\n-\t * Open the resources of the fetcher.\n-\t */\n-\tvoid open() throws Exception;\n-\n \t/**\n \t * Fetch the suitable partitions, call this method should guarantee the fetcher has opened.\n \t */\n-\tList<P> fetch() throws Exception;\n-\n-\t/**\n-\t * Close the resources of the fetcher, call this method should guarantee the fetcher has opened.\n-\t */\n-\tvoid close() throws Exception;\n+\tList<P> fetch(Context<P> context) throws Exception;\n \n \t/**\n \t * Context for fetch partitions, partition information is stored in hive meta store.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MTU3Ng==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517171576", "bodyText": "Just @Nullable OUT read() is OK", "author": "JingsongLi", "createdAt": "2020-11-04T08:31:26Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Reader that reads all records from given partitions.\n+ *\n+ *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n+ *\n+ * @param <P> The type of partition.\n+ * @param <OUT> The type of returned record.\n+ */\n+@Internal\n+public interface PartitionReader<P, OUT> extends Closeable, Serializable {\n+\n+\t/**\n+\t * Opens the reader with given partitions.\n+\t * @throws IOException\n+\t */\n+\tvoid open(List<P> partitions) throws IOException;\n+\n+\t/**\n+\t * Method used to check the partitions have read finished or not.\n+\t *\n+\t *<p>When this method is called, the reader it guaranteed to be opened.\n+\t *\n+\t * @return True if the partitions has read finished.\n+\t * @throws IOException\n+\t */\n+\tboolean hasNext() throws IOException;", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\nindex 09da49deb5..5b758399bd 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\n\n@@ -20,13 +20,15 @@ package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.Internal;\n \n+import javax.annotation.Nullable;\n+\n import java.io.Closeable;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.List;\n \n /**\n- * Reader that reads all records from given partitions.\n+ * Reader that reads record from given partitions.\n  *\n  *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n  *\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjUzNA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517172534", "bodyText": "Remove this comment, ide will warn this.", "author": "JingsongLi", "createdAt": "2020-11-04T08:33:20Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.filesystem;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Reader that reads all records from given partitions.\n+ *\n+ *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n+ *\n+ * @param <P> The type of partition.\n+ * @param <OUT> The type of returned record.\n+ */\n+@Internal\n+public interface PartitionReader<P, OUT> extends Closeable, Serializable {\n+\n+\t/**\n+\t * Opens the reader with given partitions.\n+\t * @throws IOException\n+\t */\n+\tvoid open(List<P> partitions) throws IOException;\n+\n+\t/**\n+\t * Method used to check the partitions have read finished or not.\n+\t *\n+\t *<p>When this method is called, the reader it guaranteed to be opened.\n+\t *\n+\t * @return True if the partitions has read finished.\n+\t * @throws IOException", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\nindex 09da49deb5..5b758399bd 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/PartitionReader.java\n\n@@ -20,13 +20,15 @@ package org.apache.flink.table.filesystem;\n \n import org.apache.flink.annotation.Internal;\n \n+import javax.annotation.Nullable;\n+\n import java.io.Closeable;\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.List;\n \n /**\n- * Reader that reads all records from given partitions.\n+ * Reader that reads record from given partitions.\n  *\n  *<P>This reader should only use in non-parallel instance, e.g. : used by lookup function.\n  *\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MzA1MA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517173050", "bodyText": "partitionFetcher never null", "author": "JingsongLi", "createdAt": "2020-11-04T08:34:18Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java", "diffHunk": "@@ -187,11 +169,34 @@ private void checkCacheReload() {\n \t\t}\n \t}\n \n-\tprivate Row extractKey(RowData row) {\n-\t\tRow key = new Row(lookupCols.length);\n+\tprivate RowData extractLookupKey(RowData row) {\n+\t\tGenericRowData key = new GenericRowData(lookupCols.length);\n \t\tfor (int i = 0; i < lookupCols.length; i++) {\n-\t\t\tkey.setField(i, converters[i].toExternal(row, lookupCols[i]));\n+\t\t\tkey.setField(i, lookupFieldGetters[i].getFieldOrNull(row));\n \t\t}\n \t\treturn key;\n \t}\n+\n+\t@Override\n+\tpublic void close() throws Exception {\n+\t\tif (this.partitionFetcher != null) {", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nsimilarity index 69%\nrename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\nrename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\nindex 149f7dbfe8..a02b2bf76b 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FilesystemLookupFunction.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java\n\n@@ -170,8 +156,8 @@ public class FilesystemLookupFunction<P> extends TableFunction<RowData> {\n \t}\n \n \tprivate RowData extractLookupKey(RowData row) {\n-\t\tGenericRowData key = new GenericRowData(lookupCols.length);\n-\t\tfor (int i = 0; i < lookupCols.length; i++) {\n+\t\tGenericRowData key = new GenericRowData(lookupFieldGetters.length);\n+\t\tfor (int i = 0; i < lookupFieldGetters.length; i++) {\n \t\t\tkey.setField(i, lookupFieldGetters[i].getFieldOrNull(row));\n \t\t}\n \t\treturn key;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4NjMzNg==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517186336", "bodyText": "I think we should support name comparator.", "author": "JingsongLi", "createdAt": "2020-11-04T08:57:14Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n+import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableCacheTTL;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tList<String> keyNames = new ArrayList<>();\n+\t\tTableSchema schema = getTableSchema();\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyNames.add(schema.getFieldName(key[0]).get());\n+\t\t}\n+\t\treturn getLookupFunction(keyNames.toArray(new String[0]));\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tif (monitorInterval.equals(STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue())) {\n+\t\t\t\tmonitorInterval = DEFAULT_LOOKUP_MONITOR_INTERVAL;\n+\t\t\t}\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableCacheTTL = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableCacheTTL = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(String[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context context = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table\n+\t\t\tpartitionFetcher = new FileSystemNonPartitionedTableFetcher(context);\n+\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemLatestPartitionFetcher(context);\n+\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemAllPartitionFetcher(context);\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FilesystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tpartitionReader,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableCacheTTL);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void initialize() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName, partition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Long>> getAllPartValueToTimeList() {\n+\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex d930f0d192..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,60 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n-import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n-import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n-import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n-import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n-import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n-import java.util.function.Supplier;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4NjQ1MQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517186451", "bodyText": "extractTimestamp is a inner method", "author": "JingsongLi", "createdAt": "2020-11-04T08:57:25Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n+import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n+import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableCacheTTL;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tList<String> keyNames = new ArrayList<>();\n+\t\tTableSchema schema = getTableSchema();\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyNames.add(schema.getFieldName(key[0]).get());\n+\t\t}\n+\t\treturn getLookupFunction(keyNames.toArray(new String[0]));\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tif (monitorInterval.equals(STREAMING_SOURCE_MONITOR_INTERVAL.defaultValue())) {\n+\t\t\t\tmonitorInterval = DEFAULT_LOOKUP_MONITOR_INTERVAL;\n+\t\t\t}\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableCacheTTL = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableCacheTTL = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(String[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context context = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table\n+\t\t\tpartitionFetcher = new FileSystemNonPartitionedTableFetcher(context);\n+\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemLatestPartitionFetcher(context);\n+\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table\n+\t\t\tpartitionFetcher = new FileSystemAllPartitionFetcher(context);\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FilesystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tpartitionReader,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableCacheTTL);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void initialize() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName, partition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Long>> getAllPartValueToTimeList() {\n+\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);\n+\t\t\tList<Tuple2<List<String>, Long>> partValueList = new ArrayList<>();\n+\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\tList<String> partValues = extractPartitionValues(\n+\t\t\t\t\t\tnew org.apache.flink.core.fs.Path(status.getPath().toString()));\n+\t\t\t\tlong timestamp = extractTimestamp(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tpartValues,\n+\t\t\t\t\t\t// to UTC millisecond.\n+\t\t\t\t\t\t() -> TimestampData.fromTimestamp(\n+\t\t\t\t\t\t\t\tnew Timestamp(status.getModificationTime())).getMillisecond());\n+\t\t\t\tpartValueList.add(new Tuple2<>(partValues, timestamp));\n+\t\t\t}\n+\n+\t\t\treturn partValueList;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long extractTimestamp(", "originalCommit": "9124af4f501b2afeaefa94621a82612846d98783", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex d930f0d192..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,60 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n-import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n-import org.apache.flink.table.filesystem.FileSystemAllPartitionFetcher;\n-import org.apache.flink.table.filesystem.FileSystemLatestPartitionFetcher;\n-import org.apache.flink.table.filesystem.FileSystemNonPartitionedTableFetcher;\n-import org.apache.flink.table.filesystem.FilesystemLookupFunction;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n-import java.util.function.Supplier;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc0OTAzMQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517749031", "bodyText": "Please implement Comparable", "author": "JingsongLi", "createdAt": "2020-11-05T02:23:06Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {\n+\t\t\tList<Tuple2<List<String>, Comparable>> partValueList = new ArrayList<>();\n+\t\t\tswitch (consumeOrder) {\n+\t\t\t\tcase PARTITION_NAME_ORDER:\n+\t\t\t\t\tList<String> partitionNames = metaStoreClient.listPartitionNames(\n+\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\tShort.MAX_VALUE);\n+\t\t\t\t\tfor (String partitionName : partitionNames) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(new org.apache.flink.core.fs.Path(partitionName));\n+\t\t\t\t\t\tComparable comparable = partValues.toString();", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex 68b76c2174..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,56 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1MDYzMw==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517750633", "bodyText": "Why not listPartitionNames?", "author": "JingsongLi", "createdAt": "2020-11-05T02:25:15Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {\n+\t\t\tList<Tuple2<List<String>, Comparable>> partValueList = new ArrayList<>();\n+\t\t\tswitch (consumeOrder) {\n+\t\t\t\tcase PARTITION_NAME_ORDER:\n+\t\t\t\t\tList<String> partitionNames = metaStoreClient.listPartitionNames(\n+\t\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\t\tShort.MAX_VALUE);\n+\t\t\t\t\tfor (String partitionName : partitionNames) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(new org.apache.flink.core.fs.Path(partitionName));\n+\t\t\t\t\t\tComparable comparable = partValues.toString();\n+\t\t\t\t\t\tpartValueList.add(new Tuple2<>(partValues, comparable));\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CREATE_TIME_ORDER:\n+\t\t\t\t\tFileStatus[] statuses = HivePartitionUtils.getFileStatusRecurse(tableLocation, partitionKeys.size(), fs);\n+\t\t\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\t\t\tList<String> partValues = extractPartitionValues(\n+\t\t\t\t\t\t\t\tnew org.apache.flink.core.fs.Path(status.getPath().toString()));\n+\t\t\t\t\t\tComparable comparable = TimestampData.fromTimestamp(new Timestamp(status.getModificationTime()))\n+\t\t\t\t\t\t\t\t.getMillisecond();\n+\t\t\t\t\t\tpartValueList.add(new Tuple2<>(partValues, comparable));\n+\t\t\t\t\t}\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase PARTITION_TIME_ORDER:\n+\t\t\t\t\tList<Partition> partitions = metaStoreClient.listPartitions(", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex 68b76c2174..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,56 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1MjUxMQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517752511", "bodyText": "You can return a ComparablePartition interface in Context.", "author": "JingsongLi", "createdAt": "2020-11-05T02:27:54Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.of(new HiveTablePartition(tableSd, tableProps));\n+\t\t\t}\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<Tuple2<List<String>, Comparable>> getPartValueWithComparableObjList() throws Exception {", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex 68b76c2174..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,56 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1Mzk5Nw==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517753997", "bodyText": "Remove this useless method, you can merge this to getPartition", "author": "JingsongLi", "createdAt": "2020-11-05T02:30:00Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else if (isStreamingSource()) {\n+\t\t\t// streaming-read partitioned table, the fetcher fetches the latest partition of the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\t// fetch latest partitions for partitioned table\n+\t\t\t\tif (allPartValueToTime.size() > 0) {\n+\t\t\t\t\t//sort in desc order\n+\t\t\t\t\tallPartValueToTime.sort((o1, o2) -> o2.f1.compareTo(o1.f1));\n+\t\t\t\t\tTuple2<List<String>, Comparable> maxPartition = allPartValueToTime.get(0);\n+\t\t\t\t\tcontext.getPartition(maxPartition.f0).ifPresent(partValueList::add);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\t\tString.format(\"At least one partition is required when set '%s' to 'latest' in temporal join,\" +\n+\t\t\t\t\t\t\t\t\t\t\t\" but actual partition number is '%s'\",\n+\t\t\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(), allPartValueToTime.size()));\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t} else {\n+\t\t\t// bounded-read partitioned table, the fetcher fetches all partitions of the given filesystem table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tList<Tuple2<List<String>, Comparable>> allPartValueToTime = context.getPartValueWithComparableObjList();\n+\t\t\t\tfor (Tuple2<List<String>, Comparable> partValueToTime : allPartValueToTime) {\n+\t\t\t\t\tcontext.getPartition(partValueToTime.f0).ifPresent(partValueList::add);\n+\t\t\t\t}\n+\t\t\t\treturn partValueList;\n+\t\t\t};\n+\t\t}\n+\n+\t\tPartitionReader<HiveTablePartition, RowData> partitionReader = new HiveInputFormatPartitionReader(\n+\t\t\t\tjobConf,\n+\t\t\t\thiveVersion,\n+\t\t\t\ttablePath,\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tprojectedFields,\n+\t\t\t\tflinkConf.get(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));\n+\n+\t\treturn new FileSystemLookupFunction<>(\n+\t\t\t\tpartitionFetcher,\n+\t\t\t\tfetcherContext,\n+\t\t\t\tpartitionReader,\n+\t\t\t\t(RowType) getProducedTableSchema().toRowDataType().getLogicalType(),\n+\t\t\t\tkeys,\n+\t\t\t\thiveTableReloadInterval);\n+\t}\n+\n+\t/**\n+\t * PartitionFetcher.Context for {@link HiveTablePartition}.\n+\t */\n+\tstatic class HiveTablePartitionFetcherContext implements PartitionFetcher.Context<HiveTablePartition> {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\t\tprivate final ObjectPath tablePath;\n+\t\tprivate final HiveShim hiveShim;\n+\t\tprivate final JobConfWrapper confWrapper;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final Configuration configuration;\n+\t\tprivate final String defaultPartitionName;\n+\n+\t\tprivate transient IMetaStoreClient metaStoreClient;\n+\t\tprivate transient StorageDescriptor tableSd;\n+\t\tprivate transient Properties tableProps;\n+\t\tprivate transient PartitionTimeExtractor extractor;\n+\t\tprivate transient ConsumeOrder consumeOrder;\n+\t\tprivate transient Path tableLocation;\n+\t\tprivate transient Table table;\n+\t\tprivate transient FileSystem fs;\n+\n+\t\tpublic HiveTablePartitionFetcherContext(\n+\t\t\t\tObjectPath tablePath,\n+\t\t\t\tHiveShim hiveShim,\n+\t\t\t\tJobConfWrapper confWrapper,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tConfiguration configuration,\n+\t\t\t\tString defaultPartitionName) {\n+\t\t\tthis.tablePath = tablePath;\n+\t\t\tthis.hiveShim = hiveShim;\n+\t\t\tthis.confWrapper = confWrapper;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.configuration = configuration;\n+\t\t\tthis.defaultPartitionName = defaultPartitionName;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open() throws Exception {\n+\t\t\tmetaStoreClient = hiveShim.getHiveMetastoreClient(new HiveConf(confWrapper.conf(), HiveConf.class));\n+\t\t\ttable = metaStoreClient.getTable(tablePath.getDatabaseName(), tablePath.getObjectName());\n+\t\t\ttableSd = table.getSd();\n+\t\t\ttableProps = HiveReflectionUtils.getTableMetadata(hiveShim, table);\n+\n+\t\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_PARTITION_ORDER);\n+\t\t\tconsumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\t\tString extractorKind = configuration.get(PARTITION_TIME_EXTRACTOR_KIND);\n+\t\t\tString extractorClass = configuration.get(PARTITION_TIME_EXTRACTOR_CLASS);\n+\t\t\tString extractorPattern = configuration.get(PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN);\n+\n+\t\t\textractor = PartitionTimeExtractor.create(\n+\t\t\t\t\tThread.currentThread().getContextClassLoader(),\n+\t\t\t\t\textractorKind,\n+\t\t\t\t\textractorClass,\n+\t\t\t\t\textractorPattern);\n+\t\t\ttableLocation = new Path(table.getSd().getLocation());\n+\t\t\tfs = tableLocation.getFileSystem(confWrapper.conf());\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getPartition(List<String> partValues) throws Exception {\n+\t\t\tif (partitionKeys.isEmpty()) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t\ttry {\n+\t\t\t\tPartition partition = metaStoreClient.getPartition(\n+\t\t\t\t\t\ttablePath.getDatabaseName(),\n+\t\t\t\t\t\ttablePath.getObjectName(),\n+\t\t\t\t\t\tpartValues);\n+\t\t\t\tHiveTablePartition hiveTablePartition = HivePartitionUtils.toHiveTablePartition(\n+\t\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\t\tfieldNames,\n+\t\t\t\t\t\tfieldTypes,\n+\t\t\t\t\t\thiveShim,\n+\t\t\t\t\t\ttableProps,\n+\t\t\t\t\t\tdefaultPartitionName,\n+\t\t\t\t\t\tpartition);\n+\t\t\t\treturn Optional.of(hiveTablePartition);\n+\t\t\t} catch (NoSuchObjectException e) {\n+\t\t\t\treturn Optional.empty();\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Optional<HiveTablePartition> getNonPartitionedTablePartition() {", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex 68b76c2174..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,56 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc1NzkzNQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517757935", "bodyText": "Do we need to change this default value to partition-name?", "author": "JingsongLi", "createdAt": "2020-11-05T02:35:58Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java", "diffHunk": "@@ -74,24 +74,39 @@\n \t\t\t\t\t\t\t\" NOTES: Please make sure that each partition/file should be written\" +\n \t\t\t\t\t\t\t\" atomically, otherwise the reader may get incomplete data.\");\n \n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_INCLUDE =\n+\t\t\tkey(\"streaming-source.partition.include\")\n+\t\t\t\t\t.stringType()\n+\t\t\t\t\t.defaultValue(\"all\")\n+\t\t\t\t\t.withDescription(\"Option to set the partitions to read, the supported values \" +\n+\t\t\t\t\t\t\t\"are \\\"all\\\" and \\\"latest\\\",\" +\n+\t\t\t\t\t\t\t\" the \\\"all\\\" means read all partitions; the \\\"latest\\\" means read latest \" +\n+\t\t\t\t\t\t\t\"partition in order of streaming-source.partition.order, the \\\"latest\\\" only works\" +\n+\t\t\t\t\t\t\t\" when the streaming hive source table used as temporal table. \" +\n+\t\t\t\t\t\t\t\"By default the option is \\\"all\\\".\\n.\");\n+\n \tpublic static final ConfigOption<Duration> STREAMING_SOURCE_MONITOR_INTERVAL =\n \t\t\tkey(\"streaming-source.monitor-interval\")\n \t\t\t\t\t.durationType()\n-\t\t\t\t\t.defaultValue(Duration.ofMinutes(1))\n+\t\t\t\t\t.noDefaultValue()\n \t\t\t\t\t.withDescription(\"Time interval for consecutively monitoring partition/file.\");\n \n-\tpublic static final ConfigOption<String> STREAMING_SOURCE_CONSUME_ORDER =\n-\t\t\tkey(\"streaming-source.consume-order\")\n+\tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_ORDER =\n+\t\t\tkey(\"streaming-source.partition-order\")\n \t\t\t\t\t.stringType()\n \t\t\t\t\t.defaultValue(\"create-time\")", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\nindex bc966edd67..7818a82fda 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java\n\n@@ -94,7 +94,7 @@ public class FileSystemOptions {\n \tpublic static final ConfigOption<String> STREAMING_SOURCE_PARTITION_ORDER =\n \t\t\tkey(\"streaming-source.partition-order\")\n \t\t\t\t\t.stringType()\n-\t\t\t\t\t.defaultValue(\"create-time\")\n+\t\t\t\t\t.defaultValue(\"partition-name\")\n \t\t\t\t\t.withDeprecatedKeys(\"streaming-source.consume-order\")\n \t\t\t\t\t.withDescription(\"The partition order of streaming source,\" +\n \t\t\t\t\t\t\t\" support create-time, partition-time and partition-name.\" +\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc2MjU5NA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517762594", "bodyText": "Absent should throw exception", "author": "JingsongLi", "createdAt": "2020-11-05T02:53:03Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,377 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.sql.Timestamp;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n+import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive dimension table and always lookup the latest partition data, in this case\n+ * hive table source is a continuous read source but currently we implements it by LookupFunction. Because currently\n+ * TableSource can not tell the downstream when the latest partition has been read finished. This is a temporarily\n+ * workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\"all\".equals(partitionInclude),\n+\t\t\t\t\tString.format(\"The only supported %s for lookup is '%s' in batch source,\" +\n+\t\t\t\t\t\t\t\" but actual is '%s'\", STREAMING_SOURCE_PARTITION_INCLUDE.key(), \"all\", partitionInclude));\n+\n+\t\t\thiveTableReloadInterval = configuration.get(LOOKUP_JOIN_CACHE_TTL);\n+\t\t}\n+\t}\n+\n+\tprivate TableFunction<RowData> getLookupFunction(int[] keys) {\n+\n+\t\tfinal String defaultPartitionName = jobConf.get(HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,\n+\t\t\t\tHiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal);\n+\n+\t\tPartitionFetcher.Context<HiveTablePartition> fetcherContext = new HiveTablePartitionFetcherContext(\n+\t\t\t\ttablePath,\n+\t\t\t\thiveShim,\n+\t\t\t\tnew JobConfWrapper(jobConf),\n+\t\t\t\tcatalogTable.getPartitionKeys(),\n+\t\t\t\tgetProducedTableSchema().getFieldDataTypes(),\n+\t\t\t\tgetProducedTableSchema().getFieldNames(),\n+\t\t\t\tconfiguration,\n+\t\t\t\tdefaultPartitionName);\n+\n+\t\tPartitionFetcher<HiveTablePartition> partitionFetcher;\n+\t\tif (catalogTable.getPartitionKeys().isEmpty()) {\n+\t\t\t// non-partitioned table, the fetcher fetches the partition which represents the given table.\n+\t\t\tpartitionFetcher = context -> {\n+\t\t\t\tList<HiveTablePartition> partValueList = new ArrayList<>();\n+\t\t\t\tcontext.getNonPartitionedTablePartition().ifPresent(partValueList::add);", "originalCommit": "13202dd2477bfb3730e2cb039311b1a0784368dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\nindex 68b76c2174..7660de358e 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java\n\n@@ -19,56 +19,39 @@\n package org.apache.flink.connectors.hive;\n \n import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.hive.client.HiveShim;\n import org.apache.flink.table.catalog.hive.util.HivePartitionUtils;\n-import org.apache.flink.table.catalog.hive.util.HiveReflectionUtils;\n import org.apache.flink.table.connector.source.LookupTableSource;\n import org.apache.flink.table.connector.source.TableFunctionProvider;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.data.TimestampData;\n import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n import org.apache.flink.table.filesystem.PartitionFetcher;\n import org.apache.flink.table.filesystem.PartitionReader;\n-import org.apache.flink.table.filesystem.PartitionTimeExtractor;\n import org.apache.flink.table.functions.TableFunction;\n import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.util.Preconditions;\n \n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n-import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.mapred.JobConf;\n \n-import java.sql.Timestamp;\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n-import java.util.Properties;\n \n-import static org.apache.flink.table.filesystem.DefaultPartTimeExtractor.toMills;\n import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_CLASS;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_KIND;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.PARTITION_TIME_EXTRACTOR_TIMESTAMP_PATTERN;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n-import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_ORDER;\n-import static org.apache.flink.table.utils.PartitionPathUtils.extractPartitionValues;\n \n /**\n  * Hive Table Source that has lookup ability.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzgzOTAxOQ==", "url": "https://github.com/apache/flink/pull/13729#discussion_r517839019", "bodyText": "Better to org.apache.flink.connectors.hive.util, because this is for read and write instead of for HiveCatalog.", "author": "JingsongLi", "createdAt": "2020-11-05T07:26:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HivePartitionUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.catalog.hive.util;", "originalCommit": "8768964c023e7d241203485203ad5216958365a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a41ac978fceda679c3cd3f09a26011ed8d16029", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HivePartitionUtils.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/HivePartitionUtils.java\nsimilarity index 98%\nrename from flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HivePartitionUtils.java\nrename to flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/HivePartitionUtils.java\nindex dd99e52ff7..7550c09b3a 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HivePartitionUtils.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/util/HivePartitionUtils.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.catalog.hive.util;\n+package org.apache.flink.connectors.hive.util;\n \n import org.apache.flink.connectors.hive.FlinkHiveException;\n import org.apache.flink.connectors.hive.HiveTablePartition;\n"}}, {"oid": "aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "url": "https://github.com/apache/flink/commit/aefdb5f32451d2a47b8cd57b46bc559ccde5b797", "message": "rebase", "committedDate": "2020-11-06T08:33:41Z", "type": "forcePushed"}, {"oid": "4a41ac978fceda679c3cd3f09a26011ed8d16029", "url": "https://github.com/apache/flink/commit/4a41ac978fceda679c3cd3f09a26011ed8d16029", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "committedDate": "2020-11-06T10:19:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1MTc4OA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r518751788", "bodyText": "Does this mean we require STREAMING_SOURCE_ENABLE to be set in order to load latest partition in temporal join? IMHO this limitation is not very friendly because many users still want to be able to use their hive tables in batch analysis. I think we should only require STREAMING_SOURCE_ENABLE if users want to consume data in a streaming fashion, i.e. periodically monitor files/partitions and fetch data incrementally.", "author": "lirui-apache", "createdAt": "2020-11-06T13:30:52Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveLookupTableSource.java", "diffHunk": "@@ -0,0 +1,285 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader;\n+import org.apache.flink.connectors.hive.read.HivePartitionFetcherContextBase;\n+import org.apache.flink.connectors.hive.util.HivePartitionUtils;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.hive.client.HiveShim;\n+import org.apache.flink.table.connector.source.LookupTableSource;\n+import org.apache.flink.table.connector.source.TableFunctionProvider;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.filesystem.FileSystemLookupFunction;\n+import org.apache.flink.table.filesystem.PartitionFetcher;\n+import org.apache.flink.table.filesystem.PartitionReader;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.mapred.JobConf;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.apache.flink.table.filesystem.FileSystemOptions.LOOKUP_JOIN_CACHE_TTL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_CONSUME_START_OFFSET;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_MONITOR_INTERVAL;\n+import static org.apache.flink.table.filesystem.FileSystemOptions.STREAMING_SOURCE_PARTITION_INCLUDE;\n+\n+/**\n+ * Hive Table Source that has lookup ability.\n+ *\n+ * <p>Hive Table source has both lookup and continuous read ability, when it acts as continuous read source\n+ * it does not have the lookup ability but can be a temporal table just like other stream sources.\n+ * When it acts as bounded table, it has the lookup ability.\n+ *\n+ * <p>A common user case is use hive table as dimension table and always lookup the latest partition data, in this\n+ * case, hive table source is a continuous read source but currently we implements it by LookupFunction. Because\n+ * currently TableSource can not tell the downstream when the latest partition has been read finished. This is a\n+ * temporarily workaround and will re-implement in the future.\n+ */\n+public class HiveLookupTableSource extends HiveTableSource implements LookupTableSource {\n+\n+\tprivate static final Duration DEFAULT_LOOKUP_MONITOR_INTERVAL = Duration.ofHours(1L);\n+\tprivate final Configuration configuration;\n+\tprivate Duration hiveTableReloadInterval;\n+\n+\tpublic HiveLookupTableSource(\n+\t\t\tJobConf jobConf,\n+\t\t\tReadableConfig flinkConf,\n+\t\t\tObjectPath tablePath,\n+\t\t\tCatalogTable catalogTable) {\n+\t\tsuper(jobConf, flinkConf, tablePath, catalogTable);\n+\t\tthis.configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tvalidateLookupConfigurations();\n+\t}\n+\n+\t@Override\n+\tpublic LookupRuntimeProvider getLookupRuntimeProvider(LookupContext context) {\n+\t\treturn TableFunctionProvider.of(getLookupFunction(context.getKeys()));\n+\t}\n+\n+\t@VisibleForTesting\n+\tTableFunction<RowData> getLookupFunction(int[][] keys) {\n+\t\tint[] keyIndices = new int[keys.length];\n+\t\tint i = 0;\n+\t\tfor (int[] key : keys) {\n+\t\t\tif (key.length > 1) {\n+\t\t\t\tthrow new UnsupportedOperationException(\"Hive lookup can not support nested key now.\");\n+\t\t\t}\n+\t\t\tkeyIndices[i] = key[0];\n+\t\t\ti++;\n+\t\t}\n+\t\treturn getLookupFunction(keyIndices);\n+\t}\n+\n+\tprivate void validateLookupConfigurations() {\n+\t\tString partitionInclude = configuration.get(STREAMING_SOURCE_PARTITION_INCLUDE);\n+\t\tif (isStreamingSource()) {\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t!configuration.contains(STREAMING_SOURCE_CONSUME_START_OFFSET),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"The '%s' is not supported when set '%s' to 'latest'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key()));\n+\n+\t\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL) == null\n+\t\t\t\t\t? DEFAULT_LOOKUP_MONITOR_INTERVAL\n+\t\t\t\t\t: configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n+\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\tmonitorInterval.toMillis() >= DEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"Currently the value of '%s' is required bigger or equal to default value '%s' \" +\n+\t\t\t\t\t\t\t\t\t\"when set '%s' to 'latest', but actual is '%s'\",\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.key(),\n+\t\t\t\t\t\t\tDEFAULT_LOOKUP_MONITOR_INTERVAL.toMillis(),\n+\t\t\t\t\t\t\tSTREAMING_SOURCE_PARTITION_INCLUDE.key(),\n+\t\t\t\t\t\t\tmonitorInterval.toMillis())\n+\t\t\t);\n+\n+\t\t\thiveTableReloadInterval = monitorInterval;\n+\t\t} else {\n+\t\t\tPreconditions.checkArgument(", "originalCommit": "4a41ac978fceda679c3cd3f09a26011ed8d16029", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODgwMDAzOA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r518800038", "bodyText": "Yes, because we think using partition project is better to load specific partition in batch analysis, and from the semantic, a batch table does not have the latest partition but streaming hive table has. And for batch, user can still use LOOKUP_JOIN_CACHE_TTL to set the reload interval.", "author": "leonardBang", "createdAt": "2020-11-06T14:50:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1MTc4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA5MzAwMA==", "url": "https://github.com/apache/flink/pull/13729#discussion_r519093000", "bodyText": "Problem is, when STREAMING_SOURCE_ENABLE is set, the table can no longer be used in a batch job. And given that not many users are using the real \"streaming read\", it basically means such tables can only be used in temporal join. I'm fine if this is by design, but still feel it's unfriendly and unnecessary.\nIMO, any table can have \"latest partition\" as long as the partitions are comparable in some way. Whether the data should be read in batch or streaming mode is an orthogonal concept.", "author": "lirui-apache", "createdAt": "2020-11-07T03:33:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODc1MTc4OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "url": "https://github.com/apache/flink/commit/924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "committedDate": "2020-11-07T04:13:20Z", "type": "commit"}, {"oid": "924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "url": "https://github.com/apache/flink/commit/924b755349e6ef14f84d26a83a9addb4dd9dc6c6", "message": "[FLINK-19644][hive] Support read latest partition of Hive table in temporal join", "committedDate": "2020-11-07T04:13:20Z", "type": "forcePushed"}]}