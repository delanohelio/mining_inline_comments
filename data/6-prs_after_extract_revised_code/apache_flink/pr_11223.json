{"pr_number": 11223, "pr_title": "[FLINK-16281][Table SQL / Ecosystem] parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink.", "pr_createdAt": "2020-02-26T13:40:52Z", "pr_url": "https://github.com/apache/flink/pull/11223", "timeline": [{"oid": "64692a3605478258174ec0e1b88754843170a01e", "url": "https://github.com/apache/flink/commit/64692a3605478258174ec0e1b88754843170a01e", "message": "[FLINK-16281][Table SQL / Ecosystem] parameter 'maxRetryTimes' can not work in JDBCUpsertTableSink.", "committedDate": "2020-02-26T13:28:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTEzOQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384521139", "bodyText": "List<Row> is sufficient here, we don't need to save a Tuple.", "author": "libenchao", "createdAt": "2020-02-26T14:20:46Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java", "diffHunk": "@@ -38,6 +40,7 @@\n \tprivate final String insertSQL;\n \tprivate final int[] fieldTypes;\n \n+\tprivate transient List<Tuple2<Boolean, Row>> rows;", "originalCommit": "64692a3605478258174ec0e1b88754843170a01e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTcxOQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384521719", "bodyText": "And for the name, maybe cachedRows or batchedRows be better?", "author": "libenchao", "createdAt": "2020-02-26T14:21:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyMTEzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "2cb96534f083b219b8652650425b2a6f8fbc1baa", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\nindex 892de6cb170..154d37c1239 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n\n@@ -40,7 +40,7 @@ public class AppendOnlyWriter implements JDBCWriter {\n \tprivate final String insertSQL;\n \tprivate final int[] fieldTypes;\n \n-\tprivate transient List<Tuple2<Boolean, Row>> rows;\n+\tprivate transient List<Tuple2<Boolean, Row>> tuples;\n \tprivate transient PreparedStatement statement;\n \n \tpublic AppendOnlyWriter(String insertSQL, int[] fieldTypes) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNjgxMg==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384526812", "bodyText": "notExistedTable ?", "author": "libenchao", "createdAt": "2020-02-26T14:29:44Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -52,6 +53,7 @@\n \tpublic static final String DB_URL = \"jdbc:derby:memory:upsert\";\n \tpublic static final String OUTPUT_TABLE1 = \"upsertSink\";\n \tpublic static final String OUTPUT_TABLE2 = \"appendSink\";\n+\tpublic static final String NOT_EXISTS_TABLE = \"notExistsTable\";", "originalCommit": "64692a3605478258174ec0e1b88754843170a01e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\nindex d3175295ab1..56117115b8c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n\n@@ -53,7 +53,7 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {\n \tpublic static final String DB_URL = \"jdbc:derby:memory:upsert\";\n \tpublic static final String OUTPUT_TABLE1 = \"upsertSink\";\n \tpublic static final String OUTPUT_TABLE2 = \"appendSink\";\n-\tpublic static final String NOT_EXISTS_TABLE = \"notExistsTable\";\n+\tpublic static final String NOT_EXISTS_TABLE = \"notExistedTable\";\n \n \t@Before\n \tpublic void before() throws ClassNotFoundException, SQLException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNzUwMA==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384527500", "bodyText": "why do you test \"table not exists\" for fixing \"multiple flushing not work\" issue ?", "author": "libenchao", "createdAt": "2020-02-26T14:30:48Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -211,4 +213,31 @@ public void testAppend() throws Exception {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n+\n+\t@Test(expected = JobExecutionException.class)\n+\tpublic void testTableNotExists() throws Exception {", "originalCommit": "64692a3605478258174ec0e1b88754843170a01e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDkwMDA2Nw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384900067", "bodyText": "Hi\uff0c@libenchao\nAs this issue describe\uff0cthis test will hang rather than throw an Exception when the parameter connector.write.max-retries > 1\uff0c so I add this case to check that exception will throw properly after retry 3 times.", "author": "leonardBang", "createdAt": "2020-02-27T03:50:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNzUwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDkwODU2Ng==", "url": "https://github.com/apache/flink/pull/11223#discussion_r384908566", "bodyText": "get it.", "author": "libenchao", "createdAt": "2020-02-27T04:32:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDUyNzUwMA=="}], "type": "inlineReview", "revised_code": {"commit": "70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\nindex d3175295ab1..d311ed6d7b5 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n\n@@ -213,31 +212,4 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n-\n-\t@Test(expected = JobExecutionException.class)\n-\tpublic void testTableNotExists() throws Exception {\n-\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\t\tenv.getConfig().enableObjectReuse();\n-\t\tenv.getConfig().setParallelism(1);\n-\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env);\n-\n-\t\tTable t = tEnv.fromDataStream(get4TupleDataStream(env), \"id, num, text, ts\");\n-\n-\t\ttEnv.registerTable(\"T\", t);\n-\n-\t\ttEnv.sqlUpdate(\n-\t\t\t\"CREATE TABLE upsertSink (\" +\n-\t\t\t\t\"  id INT,\" +\n-\t\t\t\t\"  num BIGINT,\" +\n-\t\t\t\t\"  ts TIMESTAMP(3)\" +\n-\t\t\t\t\") WITH (\" +\n-\t\t\t\t\"  'connector.type'='jdbc',\" +\n-\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n-\t\t\t\t\"  'connector.table'='\" + NOT_EXISTS_TABLE + \"',\" +\n-\t\t\t\t\"  'connector.write.max-retries'='3'\" +\n-\t\t\t\t\")\");\n-\n-\t\ttEnv.sqlUpdate(\"INSERT INTO upsertSink SELECT id, num, ts FROM T WHERE id IN (2, 10, 20)\");\n-\t\tenv.execute();\n-\t}\n }\n"}}, {"oid": "2cb96534f083b219b8652650425b2a6f8fbc1baa", "url": "https://github.com/apache/flink/commit/2cb96534f083b219b8652650425b2a6f8fbc1baa", "message": "using deep copy to buffer data", "committedDate": "2020-02-27T03:26:40Z", "type": "commit"}, {"oid": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "url": "https://github.com/apache/flink/commit/46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "message": "address comment", "committedDate": "2020-02-27T03:55:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NTUxMg==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385045512", "bodyText": "See UpsertWriter.addRecord.\nTuple2<Boolean, Row> tuple2 = objectReuse ? new Tuple2<>(record.f0, Row.copy(record.f1)) : record;", "author": "JingsongLi", "createdAt": "2020-02-27T10:41:03Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java", "diffHunk": "@@ -47,19 +50,28 @@ public AppendOnlyWriter(String insertSQL, int[] fieldTypes) {\n \n \t@Override\n \tpublic void open(Connection connection) throws SQLException {\n+\t\tthis.cachedRows = new ArrayList<>();\n \t\tthis.statement = connection.prepareStatement(insertSQL);\n \t}\n \n \t@Override\n-\tpublic void addRecord(Tuple2<Boolean, Row> record) throws SQLException {\n+\tpublic void addRecord(Tuple2<Boolean, Row> record) {\n \t\tcheckArgument(record.f0, \"Append mode can not receive retract/delete message.\");\n-\t\tsetRecordToStatement(statement, fieldTypes, record.f1);\n-\t\tstatement.addBatch();\n+\t\t//deep copy, add record to buffer\n+\t\tRow row = Row.copy(record.f1);", "originalCommit": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\nindex 5f37603ccbc..ff07874a185 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n\n@@ -57,9 +57,7 @@ public class AppendOnlyWriter implements JDBCWriter {\n \t@Override\n \tpublic void addRecord(Tuple2<Boolean, Row> record) {\n \t\tcheckArgument(record.f0, \"Append mode can not receive retract/delete message.\");\n-\t\t//deep copy, add record to buffer\n-\t\tRow row = Row.copy(record.f1);\n-\t\tcachedRows.add(row);\n+\t\tcachedRows.add(record.f1);\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NTg2NA==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385045864", "bodyText": "Both two writers need cache records. This copy could be extract to JDBCUpsertOutputFormat.", "author": "JingsongLi", "createdAt": "2020-02-27T10:41:43Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java", "diffHunk": "@@ -47,19 +50,28 @@ public AppendOnlyWriter(String insertSQL, int[] fieldTypes) {\n \n \t@Override\n \tpublic void open(Connection connection) throws SQLException {\n+\t\tthis.cachedRows = new ArrayList<>();\n \t\tthis.statement = connection.prepareStatement(insertSQL);\n \t}\n \n \t@Override\n-\tpublic void addRecord(Tuple2<Boolean, Row> record) throws SQLException {\n+\tpublic void addRecord(Tuple2<Boolean, Row> record) {\n \t\tcheckArgument(record.f0, \"Append mode can not receive retract/delete message.\");\n-\t\tsetRecordToStatement(statement, fieldTypes, record.f1);\n-\t\tstatement.addBatch();\n+\t\t//deep copy, add record to buffer\n+\t\tRow row = Row.copy(record.f1);", "originalCommit": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE1NzE4Ng==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385157186", "bodyText": "nice tips", "author": "leonardBang", "createdAt": "2020-02-27T14:23:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NTg2NA=="}], "type": "inlineReview", "revised_code": {"commit": "70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\nindex 5f37603ccbc..ff07874a185 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java\n\n@@ -57,9 +57,7 @@ public class AppendOnlyWriter implements JDBCWriter {\n \t@Override\n \tpublic void addRecord(Tuple2<Boolean, Row> record) {\n \t\tcheckArgument(record.f0, \"Append mode can not receive retract/delete message.\");\n-\t\t//deep copy, add record to buffer\n-\t\tRow row = Row.copy(record.f1);\n-\t\tcachedRows.add(row);\n+\t\tcachedRows.add(record.f1);\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MDc0OQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385050749", "bodyText": "Can you mock a PreparedStatement and add some unit tests?", "author": "JingsongLi", "createdAt": "2020-02-27T10:50:35Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/writer/AppendOnlyWriter.java", "diffHunk": "@@ -38,6 +40,7 @@\n \tprivate final String insertSQL;\n \tprivate final int[] fieldTypes;\n \n+\tprivate transient List<Row> cachedRows;\n \tprivate transient PreparedStatement statement;", "originalCommit": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE1OTMyNQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385159325", "bodyText": "Could you give more tips? add some unit  tests for  checking/validating what.", "author": "leonardBang", "createdAt": "2020-02-27T14:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MDc0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTUzMjcwNw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385532707", "bodyText": "I mean you can add a unit test for AppendOnlyWriter.", "author": "JingsongLi", "createdAt": "2020-02-28T06:50:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MDc0OQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MTEzMw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385051133", "bodyText": "What is this test for? I ran it on master, it passed.", "author": "JingsongLi", "createdAt": "2020-02-27T10:51:17Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -211,4 +213,31 @@ public void testAppend() throws Exception {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n+\n+\t@Test(expected = JobExecutionException.class)\n+\tpublic void testTableNotExists() throws Exception {", "originalCommit": "46b60757e8d5c7b38f6c2fa04cf1b01dd4712a25", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTE4MDA5Mg==", "url": "https://github.com/apache/flink/pull/11223#discussion_r385180092", "bodyText": "As this issue describe\uff0cthis test will hang rather than throw an Exception when the parameter connector.write.max-retries > 1\uff0c so I add this case to check that exception will throw properly after retry 3 times.\nI found this test passed in master\uff0cbecause this  'table not exists Exception' was thrown in open() function not in flush().\n@Override public void open(Connection connection) throws SQLException { this.cachedRows = new ArrayList<>(); this.statement = connection.prepareStatement(insertSQL); } \nwhen I used 'com.mysql.jdbc.Driver' it will be thrown in in flush() rather than open() function, the difference should be that we use 'org.apache.derby.jdbc.EmbeddedDriver' in ITcase.\nSo\uff0c I will drop this function and try other tests to cover this issue update.", "author": "leonardBang", "createdAt": "2020-02-27T15:20:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MTEzMw=="}], "type": "inlineReview", "revised_code": {"commit": "70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\nindex 56117115b8c..d311ed6d7b5 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n\n@@ -213,31 +212,4 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {\n \t\t\t\tRow.of(20, 6, Timestamp.valueOf(\"1970-01-01 00:00:00.02\"))\n \t\t}, DB_URL, OUTPUT_TABLE2, new String[]{\"id\", \"num\", \"ts\"});\n \t}\n-\n-\t@Test(expected = JobExecutionException.class)\n-\tpublic void testTableNotExists() throws Exception {\n-\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\t\tenv.getConfig().enableObjectReuse();\n-\t\tenv.getConfig().setParallelism(1);\n-\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env);\n-\n-\t\tTable t = tEnv.fromDataStream(get4TupleDataStream(env), \"id, num, text, ts\");\n-\n-\t\ttEnv.registerTable(\"T\", t);\n-\n-\t\ttEnv.sqlUpdate(\n-\t\t\t\"CREATE TABLE upsertSink (\" +\n-\t\t\t\t\"  id INT,\" +\n-\t\t\t\t\"  num BIGINT,\" +\n-\t\t\t\t\"  ts TIMESTAMP(3)\" +\n-\t\t\t\t\") WITH (\" +\n-\t\t\t\t\"  'connector.type'='jdbc',\" +\n-\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n-\t\t\t\t\"  'connector.table'='\" + NOT_EXISTS_TABLE + \"',\" +\n-\t\t\t\t\"  'connector.write.max-retries'='3'\" +\n-\t\t\t\t\")\");\n-\n-\t\ttEnv.sqlUpdate(\"INSERT INTO upsertSink SELECT id, num, ts FROM T WHERE id IN (2, 10, 20)\");\n-\t\tenv.execute();\n-\t}\n }\n"}}, {"oid": "70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "url": "https://github.com/apache/flink/commit/70e3c71eaafa144ebc5a2fe8e19c9f205a48a277", "message": "address comment", "committedDate": "2020-02-27T14:38:49Z", "type": "commit"}, {"oid": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "url": "https://github.com/apache/flink/commit/be5bed759e39f445d24dad3f2a0109ddddd7ce43", "message": "add unit test", "committedDate": "2020-02-28T12:55:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzA0NQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386317045", "bodyText": "Don't need member. Just pass null.", "author": "JingsongLi", "createdAt": "2020-03-02T10:42:02Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.sql.BatchUpdateException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.Statement;\n+\n+import static org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.toRow;\n+import static org.mockito.Mockito.doReturn;\n+\n+/**\n+ * Test for the {@link AppendOnlyWriter}.\n+ */\n+public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\tprivate JDBCUpsertOutputFormat format;\n+\tprivate String[] fieldNames;\n+\tprivate String[] keyFields;", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\nindex 195cdfc5377..86c61e55e1c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n\n@@ -40,14 +40,13 @@ import static org.mockito.Mockito.doReturn;\n  * Test for the {@link AppendOnlyWriter}.\n  */\n public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\n \tprivate JDBCUpsertOutputFormat format;\n \tprivate String[] fieldNames;\n-\tprivate String[] keyFields;\n \n \t@Before\n \tpublic void setup() {\n \t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n-\t\tkeyFields = null;\n \t}\n \n \t@Test(expected = BatchUpdateException.class)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzE0NQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386317145", "bodyText": "close this format?", "author": "JingsongLi", "createdAt": "2020-03-02T10:42:12Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.sql.BatchUpdateException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.Statement;\n+\n+import static org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.toRow;\n+import static org.mockito.Mockito.doReturn;\n+\n+/**\n+ * Test for the {@link AppendOnlyWriter}.\n+ */\n+public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\tprivate JDBCUpsertOutputFormat format;", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\nindex 195cdfc5377..86c61e55e1c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n\n@@ -40,14 +40,13 @@ import static org.mockito.Mockito.doReturn;\n  * Test for the {@link AppendOnlyWriter}.\n  */\n public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\n \tprivate JDBCUpsertOutputFormat format;\n \tprivate String[] fieldNames;\n-\tprivate String[] keyFields;\n \n \t@Before\n \tpublic void setup() {\n \t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n-\t\tkeyFields = null;\n \t}\n \n \t@Test(expected = BatchUpdateException.class)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzI3Mw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386317273", "bodyText": "Add a empty line above.", "author": "JingsongLi", "createdAt": "2020-03-02T10:42:27Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.sql.BatchUpdateException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.Statement;\n+\n+import static org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.toRow;\n+import static org.mockito.Mockito.doReturn;\n+\n+/**\n+ * Test for the {@link AppendOnlyWriter}.\n+ */\n+public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\tprivate JDBCUpsertOutputFormat format;", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\nindex 195cdfc5377..86c61e55e1c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n\n@@ -40,14 +40,13 @@ import static org.mockito.Mockito.doReturn;\n  * Test for the {@link AppendOnlyWriter}.\n  */\n public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\n \tprivate JDBCUpsertOutputFormat format;\n \tprivate String[] fieldNames;\n-\tprivate String[] keyFields;\n \n \t@Before\n \tpublic void setup() {\n \t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n-\t\tkeyFields = null;\n \t}\n \n \t@Test(expected = BatchUpdateException.class)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzkxOQ==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386317919", "bodyText": "Remove close, already have try.", "author": "JingsongLi", "createdAt": "2020-03-02T10:43:43Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.sql.BatchUpdateException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.Statement;\n+\n+import static org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.toRow;\n+import static org.mockito.Mockito.doReturn;\n+\n+/**\n+ * Test for the {@link AppendOnlyWriter}.\n+ */\n+public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\tprivate JDBCUpsertOutputFormat format;\n+\tprivate String[] fieldNames;\n+\tprivate String[] keyFields;\n+\n+\t@Before\n+\tpublic void setup() {\n+\t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n+\t\tkeyFields = null;\n+\t}\n+\n+\t@Test(expected = BatchUpdateException.class)\n+\tpublic void testMaxRetry() throws Exception {\n+\t\tformat = JDBCUpsertOutputFormat.builder()\n+\t\t\t.setOptions(JDBCOptions.builder()\n+\t\t\t\t.setDBUrl(DB_URL)\n+\t\t\t\t.setTableName(OUTPUT_TABLE)\n+\t\t\t\t.build())\n+\t\t\t.setFieldNames(fieldNames)\n+\t\t\t.setKeyFields(keyFields)\n+\t\t\t.build();\n+\t\tRuntimeContext context = Mockito.mock(RuntimeContext.class);\n+\t\tExecutionConfig config = Mockito.mock(ExecutionConfig.class);\n+\t\tdoReturn(config).when(context).getExecutionConfig();\n+\t\tdoReturn(true).when(config).isObjectReuseEnabled();\n+\t\tformat.setRuntimeContext(context);\n+\t\tformat.open(0, 1);\n+\n+\t\t// alter table schema to trigger retry logic after failure.\n+\t\talterTable();\n+\t\tfor (TestEntry entry : TEST_DATA) {\n+\t\t\tformat.writeRecord(Tuple2.of(true, toRow(entry)));\n+\t\t}\n+\n+\t\t// after retry default times, throws a BatchUpdateException.\n+\t\tformat.flush();\n+\t}\n+\n+\tprivate void alterTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (Connection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"ALTER  TABLE \" + OUTPUT_TABLE + \" DROP COLUMN \" + fieldNames[1]);\n+\t\t\tstat.close();", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjM2NDM4Mw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386364383", "bodyText": "I see many this kind of style that call resource close() function in try(){} body in our code,  I just want to align, should I keep this?", "author": "leonardBang", "createdAt": "2020-03-02T12:29:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzkxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\nindex 195cdfc5377..86c61e55e1c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n\n@@ -40,14 +40,13 @@ import static org.mockito.Mockito.doReturn;\n  * Test for the {@link AppendOnlyWriter}.\n  */\n public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\n \tprivate JDBCUpsertOutputFormat format;\n \tprivate String[] fieldNames;\n-\tprivate String[] keyFields;\n \n \t@Before\n \tpublic void setup() {\n \t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n-\t\tkeyFields = null;\n \t}\n \n \t@Test(expected = BatchUpdateException.class)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxNzk0Nw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386317947", "bodyText": "Remove close, already have try.", "author": "JingsongLi", "createdAt": "2020-03-02T10:43:46Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.java.io.jdbc.writer.AppendOnlyWriter;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+import java.sql.BatchUpdateException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.Statement;\n+\n+import static org.apache.flink.api.java.io.jdbc.JDBCOutputFormatTest.toRow;\n+import static org.mockito.Mockito.doReturn;\n+\n+/**\n+ * Test for the {@link AppendOnlyWriter}.\n+ */\n+public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\tprivate JDBCUpsertOutputFormat format;\n+\tprivate String[] fieldNames;\n+\tprivate String[] keyFields;\n+\n+\t@Before\n+\tpublic void setup() {\n+\t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n+\t\tkeyFields = null;\n+\t}\n+\n+\t@Test(expected = BatchUpdateException.class)\n+\tpublic void testMaxRetry() throws Exception {\n+\t\tformat = JDBCUpsertOutputFormat.builder()\n+\t\t\t.setOptions(JDBCOptions.builder()\n+\t\t\t\t.setDBUrl(DB_URL)\n+\t\t\t\t.setTableName(OUTPUT_TABLE)\n+\t\t\t\t.build())\n+\t\t\t.setFieldNames(fieldNames)\n+\t\t\t.setKeyFields(keyFields)\n+\t\t\t.build();\n+\t\tRuntimeContext context = Mockito.mock(RuntimeContext.class);\n+\t\tExecutionConfig config = Mockito.mock(ExecutionConfig.class);\n+\t\tdoReturn(config).when(context).getExecutionConfig();\n+\t\tdoReturn(true).when(config).isObjectReuseEnabled();\n+\t\tformat.setRuntimeContext(context);\n+\t\tformat.open(0, 1);\n+\n+\t\t// alter table schema to trigger retry logic after failure.\n+\t\talterTable();\n+\t\tfor (TestEntry entry : TEST_DATA) {\n+\t\t\tformat.writeRecord(Tuple2.of(true, toRow(entry)));\n+\t\t}\n+\n+\t\t// after retry default times, throws a BatchUpdateException.\n+\t\tformat.flush();\n+\t}\n+\n+\tprivate void alterTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (Connection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"ALTER  TABLE \" + OUTPUT_TABLE + \" DROP COLUMN \" + fieldNames[1]);\n+\t\t\tstat.close();\n+\t\t\tconn.close();\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clear() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DELETE FROM \" + OUTPUT_TABLE);\n+\n+\t\t\tstat.close();", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\nindex 195cdfc5377..86c61e55e1c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCAppenOnlyWriterTest.java\n\n@@ -40,14 +40,13 @@ import static org.mockito.Mockito.doReturn;\n  * Test for the {@link AppendOnlyWriter}.\n  */\n public class JDBCAppenOnlyWriterTest extends JDBCTestBase {\n+\n \tprivate JDBCUpsertOutputFormat format;\n \tprivate String[] fieldNames;\n-\tprivate String[] keyFields;\n \n \t@Before\n \tpublic void setup() {\n \t\tfieldNames = new String[]{\"id\", \"title\", \"author\", \"price\", \"qty\"};\n-\t\tkeyFields = null;\n \t}\n \n \t@Test(expected = BatchUpdateException.class)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjMxODA4Mw==", "url": "https://github.com/apache/flink/pull/11223#discussion_r386318083", "bodyText": "Please remove this", "author": "JingsongLi", "createdAt": "2020-03-02T10:44:05Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java", "diffHunk": "@@ -52,6 +52,7 @@\n \tpublic static final String DB_URL = \"jdbc:derby:memory:upsert\";\n \tpublic static final String OUTPUT_TABLE1 = \"upsertSink\";\n \tpublic static final String OUTPUT_TABLE2 = \"appendSink\";\n+\tpublic static final String NOT_EXISTS_TABLE = \"notExistedTable\";", "originalCommit": "be5bed759e39f445d24dad3f2a0109ddddd7ce43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\nindex d311ed6d7b5..240c45d4d3c 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCUpsertTableSinkITCase.java\n\n@@ -52,7 +52,6 @@ public class JDBCUpsertTableSinkITCase extends AbstractTestBase {\n \tpublic static final String DB_URL = \"jdbc:derby:memory:upsert\";\n \tpublic static final String OUTPUT_TABLE1 = \"upsertSink\";\n \tpublic static final String OUTPUT_TABLE2 = \"appendSink\";\n-\tpublic static final String NOT_EXISTS_TABLE = \"notExistedTable\";\n \n \t@Before\n \tpublic void before() throws ClassNotFoundException, SQLException {\n"}}, {"oid": "1f118045b3f009e3a10a8f06b8e35ec95562fbef", "url": "https://github.com/apache/flink/commit/1f118045b3f009e3a10a8f06b8e35ec95562fbef", "message": "minor update", "committedDate": "2020-03-02T12:39:09Z", "type": "commit"}, {"oid": "16f8eb6f78a933588c092306dc726aa640c2faa2", "url": "https://github.com/apache/flink/commit/16f8eb6f78a933588c092306dc726aa640c2faa2", "message": "minor update", "committedDate": "2020-03-02T16:15:43Z", "type": "commit"}]}