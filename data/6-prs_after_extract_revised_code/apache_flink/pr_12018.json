{"pr_number": 12018, "pr_title": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema", "pr_createdAt": "2020-05-07T09:14:16Z", "pr_url": "https://github.com/apache/flink/pull/12018", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxMzU1MA==", "url": "https://github.com/apache/flink/pull/12018#discussion_r422913550", "bodyText": "The changes in this file are purely an orthogonal refactoring, right? Could you put these in a separate commit, along with the removal of the emitRecord() method on the fetcher that is only used in tests?", "author": "aljoscha", "createdAt": "2020-05-11T09:38:24Z", "path": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java", "diffHunk": "@@ -108,13 +112,13 @@ public void testSkipCorruptedRecord() throws Exception {\n \n \t\tfinal KafkaTopicPartitionState<Object> partitionStateHolder = fetcher.subscribedPartitionStates().get(0);\n \n-\t\tfetcher.emitRecord(1L, partitionStateHolder, 1L);\n-\t\tfetcher.emitRecord(2L, partitionStateHolder, 2L);\n+\t\temitRecord(fetcher, 1L, partitionStateHolder, 1L);", "originalCommit": "9cf30a56133e39d0e895f99796201d99a8447249", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4NTkxOA==", "url": "https://github.com/apache/flink/pull/12018#discussion_r422985918", "bodyText": "There are two related changes:\n\nI changed a bit the way no results are emitted (null before/empty queue now)\nI changed one test case slightly to emit multiple results in a single emitRecordsWithTimestamps\n\nI think after extracting the refactoring to a separate commit it is visible more clearly.", "author": "dawidwys", "createdAt": "2020-05-11T11:54:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxMzU1MA=="}], "type": "inlineReview", "revised_code": {"commit": "53e37282adc57dff328367a84f15132c80ccd54d", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java b/flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java\nindex e662a062c5..16fb3dd347 100644\n--- a/flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java\n+++ b/flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/internals/AbstractFetcherTest.java\n\n@@ -117,8 +113,8 @@ public class AbstractFetcherTest {\n \t\tassertEquals(2L, sourceContext.getLatestElement().getValue().longValue());\n \t\tassertEquals(2L, partitionStateHolder.getOffset());\n \n-\t\t// emit no records\n-\t\tfetcher.emitRecordsWithTimestamps(emptyQueue(), partitionStateHolder, 3L, Long.MIN_VALUE);\n+\t\t// emit null record\n+\t\temitRecord(fetcher, null, partitionStateHolder, 3L);\n \t\tassertEquals(2L, sourceContext.getLatestElement().getValue().longValue()); // the null record should be skipped\n \t\tassertEquals(3L, partitionStateHolder.getOffset()); // the offset in state still should have advanced\n \t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxNDYwMQ==", "url": "https://github.com/apache/flink/pull/12018#discussion_r422914601", "bodyText": "So we're not emitting all the records of the batch after one of them signals \"end of stream\"? I recall that an initial version still emitted all records.", "author": "aljoscha", "createdAt": "2020-05-11T09:40:18Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java", "diffHunk": "@@ -228,4 +226,33 @@ protected void doCommitInternalOffsetsToKafka(\n \t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n \t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n \t}\n+\n+\tprivate class KafkaCollector implements Collector<T> {\n+\t\tprivate final Queue<T> records = new ArrayDeque<>();\n+\n+\t\tprivate boolean endOfStreamSignalled = false;\n+\n+\t\t@Override\n+\t\tpublic void collect(T record) {\n+\t\t\t// do not emit subsequent elements if the end of the stream reached", "originalCommit": "9cf30a56133e39d0e895f99796201d99a8447249", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk3MDc2OQ==", "url": "https://github.com/apache/flink/pull/12018#discussion_r422970769", "bodyText": "Actually, that is the behaviour I had from the very beginning. Do you think it makes more sense to emit all but the end of the stream record?\nSide note. I find this method very confusing. I just realized that there is no cross partition alignment on this method. The whole task will be brought down if any of the assigned partitions signals the end of stream, irrespective of the state of the remaining partitions. Honestly I'd be in favour of dropping this method at some point in the future.", "author": "dawidwys", "createdAt": "2020-05-11T11:23:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxNDYwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzAzMjM1NA==", "url": "https://github.com/apache/flink/pull/12018#discussion_r423032354", "bodyText": "Oh boy, I guess the interplay of this end record and multiple partitions was not considered... \ud83d\ude48", "author": "aljoscha", "createdAt": "2020-05-11T13:18:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxNDYwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzA0MTUxOA==", "url": "https://github.com/apache/flink/pull/12018#discussion_r423041518", "bodyText": "It has no additional implications on this PR as it has those problems already.", "author": "dawidwys", "createdAt": "2020-05-11T13:32:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjkxNDYwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "53e37282adc57dff328367a84f15132c80ccd54d", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java\nindex ac8d27c7a9..2826007107 100644\n--- a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java\n+++ b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka010Fetcher.java\n\n@@ -226,33 +228,4 @@ public class Kafka010Fetcher<T> extends AbstractFetcher<T, TopicPartition> {\n \t\t// record the work to be committed by the main consumer thread and make sure the consumer notices that\n \t\tconsumerThread.setOffsetsToCommit(offsetsToCommit, commitCallback);\n \t}\n-\n-\tprivate class KafkaCollector implements Collector<T> {\n-\t\tprivate final Queue<T> records = new ArrayDeque<>();\n-\n-\t\tprivate boolean endOfStreamSignalled = false;\n-\n-\t\t@Override\n-\t\tpublic void collect(T record) {\n-\t\t\t// do not emit subsequent elements if the end of the stream reached\n-\t\t\tif (endOfStreamSignalled || deserializer.isEndOfStream(record)) {\n-\t\t\t\tendOfStreamSignalled = true;\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t\trecords.add(record);\n-\t\t}\n-\n-\t\tpublic Queue<T> getRecords() {\n-\t\t\treturn records;\n-\t\t}\n-\n-\t\tpublic boolean isEndOfStreamSignalled() {\n-\t\t\treturn endOfStreamSignalled;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void close() {\n-\n-\t\t}\n-\t}\n }\n"}}, {"oid": "53e37282adc57dff328367a84f15132c80ccd54d", "url": "https://github.com/apache/flink/commit/53e37282adc57dff328367a84f15132c80ccd54d", "message": "[hotfix][kafka] Remove unused method AbstractFetcher#emitRecord", "committedDate": "2020-05-11T11:48:41Z", "type": "commit"}, {"oid": "e7e6890ac5b32caacc424dd8cf7417d4569873e9", "url": "https://github.com/apache/flink/commit/e7e6890ac5b32caacc424dd8cf7417d4569873e9", "message": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema\n\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path.", "committedDate": "2020-05-11T11:50:47Z", "type": "commit"}, {"oid": "e7e6890ac5b32caacc424dd8cf7417d4569873e9", "url": "https://github.com/apache/flink/commit/e7e6890ac5b32caacc424dd8cf7417d4569873e9", "message": "[FLINK-17307] Add collector to deserialize in KafkaDeserializationSchema\n\nThis PR adds a way to emit multiple records from\nKafkaDeserializationSchema. This is possible through a collector, which\nwill buffer deserialized records in a queue and then emit all records\natomically. The queue is reused for all incoming Kafka records to\nminimize creating new objects on the hot path.", "committedDate": "2020-05-11T11:50:47Z", "type": "forcePushed"}]}