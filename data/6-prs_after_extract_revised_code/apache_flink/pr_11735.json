{"pr_number": 11735, "pr_title": "[FLINK-16802][hive] Set schema info in JobConf for Hive readers", "pr_createdAt": "2020-04-14T11:35:29Z", "pr_url": "https://github.com/apache/flink/pull/11735", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412729427", "bodyText": "Need this branch?\nWe can just:\njobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\njobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));", "author": "JingsongLi", "createdAt": "2020-04-22T07:13:57Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n+\t\tif (numPartCol == 0) {", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc0OTUzOA==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412749538", "bodyText": "Yeah I added the branch to avoid array copies if the table is not partitioned. But perhaps terseness is more desirable here.", "author": "lirui-apache", "createdAt": "2020-04-22T07:45:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjcyOTQyNw=="}], "type": "inlineReview", "revised_code": {"commit": "b77e52194cbab40028c15a6ab4a747f5e900e37c", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\nindex 4b70ed1f5a0..ff62b67e9ac 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n\n@@ -148,18 +148,13 @@ public class HiveTableInputFormat extends HadoopInputFormatCommonBase<BaseRow, H\n \t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n \t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n \t\t// set schema evolution -- excluding partition cols\n-\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n-\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n-\t\tif (numPartCol == 0) {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, jobConf.get(IOConstants.COLUMNS));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, jobConf.get(IOConstants.COLUMNS_TYPES));\n-\t\t} else {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));\n-\t\t}\n+\t\tint numNonPartCol = fieldNames.length - partitionKeys.size();\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, numNonPartCol)));\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, numNonPartCol)));\n+\n \t\t// in older versions, parquet reader also expects the selected col indices in conf, excluding part cols\n \t\tString readColIDs = Arrays.stream(selectedFields)\n-\t\t\t\t.filter(i -> i < firstPartColIndex)\n+\t\t\t\t.filter(i -> i < numNonPartCol)\n \t\t\t\t.mapToObj(String::valueOf)\n \t\t\t\t.collect(Collectors.joining(\",\"));\n \t\tjobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIDs);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczMzYwMQ==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412733601", "bodyText": "partitionKeys never null", "author": "JingsongLi", "createdAt": "2020-04-22T07:20:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b77e52194cbab40028c15a6ab4a747f5e900e37c", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\nindex 4b70ed1f5a0..ff62b67e9ac 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n\n@@ -148,18 +148,13 @@ public class HiveTableInputFormat extends HadoopInputFormatCommonBase<BaseRow, H\n \t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n \t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n \t\t// set schema evolution -- excluding partition cols\n-\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n-\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n-\t\tif (numPartCol == 0) {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, jobConf.get(IOConstants.COLUMNS));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, jobConf.get(IOConstants.COLUMNS_TYPES));\n-\t\t} else {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));\n-\t\t}\n+\t\tint numNonPartCol = fieldNames.length - partitionKeys.size();\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, numNonPartCol)));\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, numNonPartCol)));\n+\n \t\t// in older versions, parquet reader also expects the selected col indices in conf, excluding part cols\n \t\tString readColIDs = Arrays.stream(selectedFields)\n-\t\t\t\t.filter(i -> i < firstPartColIndex)\n+\t\t\t\t.filter(i -> i < numNonPartCol)\n \t\t\t\t.mapToObj(String::valueOf)\n \t\t\t\t.collect(Collectors.joining(\",\"));\n \t\tjobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIDs);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjczNDMwMw==", "url": "https://github.com/apache/flink/pull/11735#discussion_r412734303", "bodyText": "numNonPartCol?", "author": "JingsongLi", "createdAt": "2020-04-22T07:21:38Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java", "diffHunk": "@@ -122,12 +131,40 @@ public void open(HiveTableInputSplit split) throws IOException {\n \t\t\tthis.reader = new HiveVectorizedParquetSplitReader(\n \t\t\t\t\thiveVersion, jobConf, fieldNames, fieldTypes, selectedFields, split);\n \t\t} else {\n-\t\t\tthis.reader = new HiveMapredSplitReader(jobConf, partitionKeys, fieldTypes, selectedFields, split,\n+\t\t\tJobConf clonedConf = new JobConf(jobConf);\n+\t\t\taddSchemaToConf(clonedConf);\n+\t\t\tthis.reader = new HiveMapredSplitReader(clonedConf, partitionKeys, fieldTypes, selectedFields, split,\n \t\t\t\t\tHiveShimLoader.loadHiveShim(hiveVersion));\n \t\t}\n \t\tcurrentReadCount = 0L;\n \t}\n \n+\t// Hive readers may rely on the schema info in configuration\n+\tprivate void addSchemaToConf(JobConf jobConf) {\n+\t\t// set columns/types -- including partition cols\n+\t\tList<String> typeStrs = Arrays.stream(fieldTypes)\n+\t\t\t\t.map(t -> HiveTypeUtil.toHiveTypeInfo(t, true).toString())\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n+\t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n+\t\t// set schema evolution -- excluding partition cols\n+\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n+\t\tint firstPartColIndex = fieldNames.length - numPartCol;", "originalCommit": "60d86e4a86f0afefe1bfbf398b1d28f139ca97f2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b77e52194cbab40028c15a6ab4a747f5e900e37c", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\nindex 4b70ed1f5a0..ff62b67e9ac 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableInputFormat.java\n\n@@ -148,18 +148,13 @@ public class HiveTableInputFormat extends HadoopInputFormatCommonBase<BaseRow, H\n \t\tjobConf.set(IOConstants.COLUMNS, String.join(\",\", fieldNames));\n \t\tjobConf.set(IOConstants.COLUMNS_TYPES, String.join(\",\", typeStrs));\n \t\t// set schema evolution -- excluding partition cols\n-\t\tint numPartCol = partitionKeys != null ? partitionKeys.size() : 0;\n-\t\tint firstPartColIndex = fieldNames.length - numPartCol;\n-\t\tif (numPartCol == 0) {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, jobConf.get(IOConstants.COLUMNS));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, jobConf.get(IOConstants.COLUMNS_TYPES));\n-\t\t} else {\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, firstPartColIndex)));\n-\t\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, firstPartColIndex)));\n-\t\t}\n+\t\tint numNonPartCol = fieldNames.length - partitionKeys.size();\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS, String.join(\",\", Arrays.copyOfRange(fieldNames, 0, numNonPartCol)));\n+\t\tjobConf.set(SCHEMA_EVOLUTION_COLUMNS_TYPES, String.join(\",\", typeStrs.subList(0, numNonPartCol)));\n+\n \t\t// in older versions, parquet reader also expects the selected col indices in conf, excluding part cols\n \t\tString readColIDs = Arrays.stream(selectedFields)\n-\t\t\t\t.filter(i -> i < firstPartColIndex)\n+\t\t\t\t.filter(i -> i < numNonPartCol)\n \t\t\t\t.mapToObj(String::valueOf)\n \t\t\t\t.collect(Collectors.joining(\",\"));\n \t\tjobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIDs);\n"}}, {"oid": "b77e52194cbab40028c15a6ab4a747f5e900e37c", "url": "https://github.com/apache/flink/commit/b77e52194cbab40028c15a6ab4a747f5e900e37c", "message": "address comments", "committedDate": "2020-04-22T07:49:16Z", "type": "forcePushed"}, {"oid": "f9fa39640ee8e4082f0e3d7a7963080d56689fa9", "url": "https://github.com/apache/flink/commit/f9fa39640ee8e4082f0e3d7a7963080d56689fa9", "message": "[FLINK-16802][hive] Set schema info in JobConf for Hive readers", "committedDate": "2020-04-23T08:55:15Z", "type": "commit"}, {"oid": "67355f513149f11b717932a98f406b8be92bfbe2", "url": "https://github.com/apache/flink/commit/67355f513149f11b717932a98f406b8be92bfbe2", "message": "address comments", "committedDate": "2020-04-23T08:55:15Z", "type": "commit"}, {"oid": "67355f513149f11b717932a98f406b8be92bfbe2", "url": "https://github.com/apache/flink/commit/67355f513149f11b717932a98f406b8be92bfbe2", "message": "address comments", "committedDate": "2020-04-23T08:55:15Z", "type": "forcePushed"}]}