{"pr_number": 13447, "pr_title": "[FLINK-19297][network] Make ResultPartitionWriter record-oriented", "pr_createdAt": "2020-09-22T03:06:12Z", "pr_url": "https://github.com/apache/flink/pull/13447", "timeline": [{"oid": "620bd4aeecbb133a360cefae8c873ded05e5b720", "url": "https://github.com/apache/flink/commit/620bd4aeecbb133a360cefae8c873ded05e5b720", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-22T03:19:40Z", "type": "forcePushed"}, {"oid": "30010e1ebc1bc65cf5151a544134947567baac4d", "url": "https://github.com/apache/flink/commit/30010e1ebc1bc65cf5151a544134947567baac4d", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-22T06:04:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcwODAzMA==", "url": "https://github.com/apache/flink/pull/13447#discussion_r492708030", "bodyText": "Just to double check: We do not want this to be the default behavior in BufferWritingResultPartition, because this would finish the partial buffers for streaming/pipelined cases as well, which we don't want.\nI think this logic may be confusing for future developers. What we could do is the following:\n\nBufferWritingResultPartition leaves the void flush(int) and flushAll() methods abstract.\nInstead it offers protected void flushSubpartition(int partition, boolean finishProducers) and protected void flushAllSubpartitions(boolean finishProducers). That makes it clear that there is a producer that may or may not be finished, so the caller has to be aware of this behavior.\nThe BoundedBlockingResultPartition then implements flushAll() { flushAllSubpartitions(true); } and the PipelinedResultPartition implements flushAll() { flushAllSubpartitions(false); }", "author": "StephanEwen", "createdAt": "2020-09-22T12:52:58Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BoundedBlockingResultPartition.java", "diffHunk": "@@ -63,6 +63,22 @@ public BoundedBlockingResultPartition(\n \t\t\tbufferPoolFactory);\n \t}\n \n+\t@Override\n+\tpublic void flush(int targetSubpartition) {\n+\t\tfinishBroadcastBufferBuilder();", "originalCommit": "f98909f2c50173758f29167fcd176010a937d284", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE1OTgzNg==", "url": "https://github.com/apache/flink/pull/13447#discussion_r493159836", "bodyText": "This logic is already in BoundedBlockingResultPartition and should have no impact on streaming/pipelined cases. I guess the source file name is misread?", "author": "wsry", "createdAt": "2020-09-23T02:27:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcwODAzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQyOTE4Mg==", "url": "https://github.com/apache/flink/pull/13447#discussion_r493429182", "bodyText": "Maybe it was like that before, but we can still make it more intuitive to understand.", "author": "StephanEwen", "createdAt": "2020-09-23T10:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcwODAzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQ5ODE1MQ==", "url": "https://github.com/apache/flink/pull/13447#discussion_r493498151", "bodyText": "Fixed", "author": "wsry", "createdAt": "2020-09-23T11:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcwODAzMA=="}], "type": "inlineReview", "revised_code": {"commit": "a0f1eaefc83dd9c183e43215bdbc3d1e4e06b5d4", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BoundedBlockingResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BoundedBlockingResultPartition.java\nindex 09fd32f478..b98b5683aa 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BoundedBlockingResultPartition.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/BoundedBlockingResultPartition.java\n\n@@ -65,18 +65,12 @@ public class BoundedBlockingResultPartition extends BufferWritingResultPartition\n \n \t@Override\n \tpublic void flush(int targetSubpartition) {\n-\t\tfinishBroadcastBufferBuilder();\n-\t\tfinishSubpartitionBufferBuilder(targetSubpartition);\n-\n-\t\tsuper.flush(targetSubpartition);\n+\t\tflushSubpartition(targetSubpartition, true);\n \t}\n \n \t@Override\n \tpublic void flushAll() {\n-\t\tfinishBroadcastBufferBuilder();\n-\t\tfinishSubpartitionBufferBuilders();\n-\n-\t\tsuper.flushAll();\n+\t\tflushAllSubpartitions(true);\n \t}\n \n \tprivate static ResultPartitionType checkResultPartitionType(ResultPartitionType type) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcyMDMxOQ==", "url": "https://github.com/apache/flink/pull/13447#discussion_r492720319", "bodyText": "It would be really great if this method were not public. Ideally we can remove this completely, because all tests that use this bypass some crucial logic of this class and may result in meaningless tests.\nThis method is used in three places:\n\nThe occurrence in SingleInputGateTest can be replaced with emitting a record.\nThe occurrence in TestPartitionProducer could be removed by adjusting TestProducerSource to produce ByteBuffer instead of BufferConsumer, which looks like a nice change that might even simplify things.\nIf the change for PartitionTestUtils could in theory be kept, and the visibility of the method be reduced to package-private.", "author": "StephanEwen", "createdAt": "2020-09-22T13:11:02Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.java", "diffHunk": "@@ -109,89 +94,58 @@\n \t\t}\n \t}\n \n-\tprotected void emit(T record, int targetChannel) throws IOException, InterruptedException {\n+\tprotected void emit(T record, int targetSubpartition) throws IOException {\n \t\tcheckErroneous();\n \n-\t\tserializer.serializeRecord(record);\n-\n-\t\t// Make sure we don't hold onto the large intermediate serialization buffer for too long\n-\t\tcopyFromSerializerToTargetChannel(targetChannel);\n-\t}\n-\n-\t/**\n-\t * @param targetChannel\n-\t * @return <tt>true</tt> if the intermediate serialization buffer should be pruned\n-\t */\n-\tprotected boolean copyFromSerializerToTargetChannel(int targetChannel) throws IOException, InterruptedException {\n-\t\t// We should reset the initial position of the intermediate serialization buffer before\n-\t\t// copying, so the serialization results can be copied to multiple target buffers.\n-\t\tserializer.reset();\n-\n-\t\tboolean pruneTriggered = false;\n-\t\tBufferBuilder bufferBuilder = getBufferBuilder(targetChannel);\n-\t\tSerializationResult result = serializer.copyToBufferBuilder(bufferBuilder);\n-\t\twhile (result.isFullBuffer()) {\n-\t\t\tfinishBufferBuilder(bufferBuilder);\n-\n-\t\t\t// If this was a full record, we are done. Not breaking out of the loop at this point\n-\t\t\t// will lead to another buffer request before breaking out (that would not be a\n-\t\t\t// problem per se, but it can lead to stalls in the pipeline).\n-\t\t\tif (result.isFullRecord()) {\n-\t\t\t\tpruneTriggered = true;\n-\t\t\t\temptyCurrentBufferBuilder(targetChannel);\n-\t\t\t\tbreak;\n-\t\t\t}\n-\n-\t\t\tbufferBuilder = requestNewBufferBuilder(targetChannel);\n-\t\t\tresult = serializer.copyToBufferBuilder(bufferBuilder);\n-\t\t}\n-\t\tcheckState(!serializer.hasSerializedData(), \"All data should be written at once\");\n+\t\ttargetPartition.emitRecord(serializeRecord(serializer, record), targetSubpartition);\n \n \t\tif (flushAlways) {\n-\t\t\tflushTargetPartition(targetChannel);\n+\t\t\ttargetPartition.flush(targetSubpartition);\n \t\t}\n-\t\treturn pruneTriggered;\n \t}\n \n \tpublic void broadcastEvent(AbstractEvent event) throws IOException {\n \t\tbroadcastEvent(event, false);\n \t}\n \n \tpublic void broadcastEvent(AbstractEvent event, boolean isPriorityEvent) throws IOException {\n-\t\ttry (BufferConsumer eventBufferConsumer = EventSerializer.toBufferConsumer(event)) {\n-\t\t\tfor (int targetChannel = 0; targetChannel < numberOfChannels; targetChannel++) {\n-\t\t\t\ttryFinishCurrentBufferBuilder(targetChannel);\n-\n-\t\t\t\t// Retain the buffer so that it can be recycled by each channel of targetPartition\n-\t\t\t\ttargetPartition.addBufferConsumer(eventBufferConsumer.copy(), targetChannel, isPriorityEvent);\n-\t\t\t}\n+\t\ttargetPartition.broadcastEvent(event, isPriorityEvent);\n \n-\t\t\tif (flushAlways) {\n-\t\t\t\tflushAll();\n-\t\t\t}\n+\t\tif (flushAlways) {\n+\t\t\tflushAll();\n \t\t}\n \t}\n \n-\tpublic void flushAll() {\n-\t\ttargetPartition.flushAll();\n+\t@VisibleForTesting\n+\tpublic static ByteBuffer serializeRecord(", "originalCommit": "f95779273fd0c8ca9ec4eeaf8a4dc232ecb6788a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQ3MDI1Mg==", "url": "https://github.com/apache/flink/pull/13447#discussion_r493470252", "bodyText": "I  removed BufferWritingResultPartition#addBufferConsumer completely.", "author": "wsry", "createdAt": "2020-09-23T11:21:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcyMDMxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "fe617726316ca41ecb446cedc718aac77d8aeb31", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.java\nindex b58ef38a13..59204cc7fe 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/RecordWriter.java\n\n@@ -120,19 +120,14 @@ public abstract class RecordWriter<T extends IOReadableWritable> implements Avai\n \tpublic static ByteBuffer serializeRecord(\n \t\t\tDataOutputSerializer serializer,\n \t\t\tIOReadableWritable record) throws IOException {\n-\t\tserializer.clear();\n-\n \t\t// the initial capacity should be no less than 4 bytes\n-\t\tserializer.skipBytesToWrite(4);\n+\t\tserializer.setPositionUnsafe(4);\n \n \t\t// write data\n \t\trecord.write(serializer);\n \n \t\t// write length\n-\t\tint len = serializer.length() - 4;\n-\t\tserializer.setPosition(0);\n-\t\tserializer.writeInt(len);\n-\t\tserializer.skipBytesToWrite(len);\n+\t\tserializer.writeIntUnsafe(serializer.length() - 4, 0);\n \n \t\treturn serializer.wrapAsByteBuffer();\n \t}\n"}}, {"oid": "fe617726316ca41ecb446cedc718aac77d8aeb31", "url": "https://github.com/apache/flink/commit/fe617726316ca41ecb446cedc718aac77d8aeb31", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T07:59:19Z", "type": "forcePushed"}, {"oid": "4afb45bc2cee67d99fffda71792b292e6bfbf7c2", "url": "https://github.com/apache/flink/commit/4afb45bc2cee67d99fffda71792b292e6bfbf7c2", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T11:19:47Z", "type": "forcePushed"}, {"oid": "a0f1eaefc83dd9c183e43215bdbc3d1e4e06b5d4", "url": "https://github.com/apache/flink/commit/a0f1eaefc83dd9c183e43215bdbc3d1e4e06b5d4", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T11:52:06Z", "type": "forcePushed"}, {"oid": "b9b964ea58a463c3b742f22c27f9dfc998715277", "url": "https://github.com/apache/flink/commit/b9b964ea58a463c3b742f22c27f9dfc998715277", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T12:04:39Z", "type": "forcePushed"}, {"oid": "1fcc9db08d65a5d05bf0a9d010d6308103c132e0", "url": "https://github.com/apache/flink/commit/1fcc9db08d65a5d05bf0a9d010d6308103c132e0", "message": "[FLINK-19320][task] Remove RecordWriter#clearBuffers\n\nPreviously, RecordWriter#clearBuffers was used to recycle the partially filled buffer in the serializer. However, currently the serializer does not contain any network buffer any more. The method now is used to finish the current BufferBuilders and only some tests and BatchTask use it. Actually, these usage should be replaced by RecordWriter#close which dose the same thing. So this patch removes RecordWriter#clearBuffers and the corresponding test cases.", "committedDate": "2020-09-23T17:40:07Z", "type": "commit"}, {"oid": "d4914aebfa6692ea2ba6216065bcc6a7c0b128b9", "url": "https://github.com/apache/flink/commit/d4914aebfa6692ea2ba6216065bcc6a7c0b128b9", "message": "[hotfix] Remove outdated description in Javadoc of RecordWriter\n\nThis closes #13447", "committedDate": "2020-09-23T17:40:07Z", "type": "commit"}, {"oid": "bd18b11e160b674d37e7ed538f84bde0ad82ba18", "url": "https://github.com/apache/flink/commit/bd18b11e160b674d37e7ed538f84bde0ad82ba18", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T18:33:21Z", "type": "forcePushed"}, {"oid": "65520f125fed528a36e8808e2dcc2a3437b041ed", "url": "https://github.com/apache/flink/commit/65520f125fed528a36e8808e2dcc2a3437b041ed", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T18:54:07Z", "type": "forcePushed"}, {"oid": "1e3ac706c5fb08997d5e088cccd0ab5fda7e1cc2", "url": "https://github.com/apache/flink/commit/1e3ac706c5fb08997d5e088cccd0ab5fda7e1cc2", "message": "[FLINK-19312][network] Introduce BufferWritingResultPartition which wraps the ResultSubpartition related logic\n\nIn the current abstraction, buffers are written to and read from ResultSubpartitions, which is a hash-style data writing and reading implementation. This is in contrast to implementations where records are appended to a joint structure, from which the data is drawn after the write phase is finished, for example the sort-based partitioning which clusters data belonging to different channels by sorting channel index. In the future, sort-merge based ResultPartitionWriter will be implemented which can not share the current hash-style ResultSubpartition related logics. So this patch migrates these logics from ResultPartition to the new BufferWritingResultPartition, after which ResultPartition is free of ResultSubpartition and can be reused by the future sort-merge based ResultPartitionWriter implementation.", "committedDate": "2020-09-23T19:31:08Z", "type": "commit"}, {"oid": "da7a6d2a4ecf76fad05303f2e39e33efb94cf0b6", "url": "https://github.com/apache/flink/commit/da7a6d2a4ecf76fad05303f2e39e33efb94cf0b6", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T19:35:38Z", "type": "forcePushed"}, {"oid": "e19cb540b3ca9421c7c0022458aff96c9e1bd6d2", "url": "https://github.com/apache/flink/commit/e19cb540b3ca9421c7c0022458aff96c9e1bd6d2", "message": "[FLINK-19297][network] Make ResultPartitionWriter record-oriented\n\nCurrently, the ResultPartitionWriter is buffer-oriented, that is, RecordWriter can only add buffers of different channels to ResultPartitionWriter and the buffer boundary serves as a nature boundary of data belonging to different channels. However, this abstraction is not flexible enough to handle new implementations like sort-based partitioning where records are appended a joint structure shared by all channels and sorting is used to cluster data belonging to different channels. This patch makes ResultPartitionWriter record-oriented by adding new record-oriented interfaces to and removing the old buffer-oriented interfaces from ResultPartitionWriter. After this change, the future sort-merge based ResultPartitionWriter can be implemented easily.", "committedDate": "2020-09-23T19:52:50Z", "type": "commit"}, {"oid": "97b8643b5e9f44e52911c05a35e6d6699aea4440", "url": "https://github.com/apache/flink/commit/97b8643b5e9f44e52911c05a35e6d6699aea4440", "message": "[hotfix] Remove unused RecordWriterTest#TrackingBufferRecycler\n\nThis closes #13447", "committedDate": "2020-09-23T19:54:32Z", "type": "commit"}, {"oid": "cb4341d6daa1329ce85090bcc480f7f7ed04d10a", "url": "https://github.com/apache/flink/commit/cb4341d6daa1329ce85090bcc480f7f7ed04d10a", "message": "[FLINK-19302][network] Fix flushing BoundedBlockingResultPartition\n\nCurrently, when flushing the BoundedBlockingSubpartition, the unfinished BufferConsumer will be closed and recycled, however the corresponding BufferBuilder is not finished and the writer can keep coping records to it which can lead to loss of data. This patch fix the issue by finishing the corresponding BufferBuilders first when flushing a BoundedBlockingResultPartition.", "committedDate": "2020-09-23T19:54:54Z", "type": "commit"}, {"oid": "063c529b916e3bf9f6bc2be39f04be87bb84eb84", "url": "https://github.com/apache/flink/commit/063c529b916e3bf9f6bc2be39f04be87bb84eb84", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T19:55:09Z", "type": "commit"}, {"oid": "063c529b916e3bf9f6bc2be39f04be87bb84eb84", "url": "https://github.com/apache/flink/commit/063c529b916e3bf9f6bc2be39f04be87bb84eb84", "message": "[FLINK-19323][network] Small optimization of RecordWriter#serializeRecord\n\nCurrently, when serializing a record, the serializer will first skip 4 bytes for length filed and serialize the record. Then it gets the serialized record length and skips back to position 0 to write the length field. After that, it skip again to the tail of the serialized data. This patch avoids the last two skips by writing length field to position 0 directly.", "committedDate": "2020-09-23T19:55:09Z", "type": "forcePushed"}]}