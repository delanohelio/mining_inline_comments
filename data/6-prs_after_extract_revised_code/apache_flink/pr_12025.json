{"pr_number": 12025, "pr_title": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read", "pr_createdAt": "2020-05-07T14:09:32Z", "pr_url": "https://github.com/apache/flink/pull/12025", "timeline": [{"oid": "04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "url": "https://github.com/apache/flink/commit/04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "message": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read", "committedDate": "2020-05-14T13:38:06Z", "type": "commit"}, {"oid": "04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "url": "https://github.com/apache/flink/commit/04db04d8d1b41ca05618c5b09b83b3031bf7ee11", "message": "[FLINK-17435] [hive] Hive non-partitioned source supports streaming read", "committedDate": "2020-05-14T13:38:06Z", "type": "forcePushed"}, {"oid": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "url": "https://github.com/apache/flink/commit/d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "message": "fix checkstyle error", "committedDate": "2020-05-15T02:08:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNTY3Mg==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425535672", "bodyText": "Above should have a blank line.", "author": "JingsongLi", "createdAt": "2020-05-15T02:44:57Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\nindex 2cdf649d0a..0dc3448346 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n\n@@ -18,6 +18,8 @@\n \n package org.apache.flink.connectors.hive;\n \n+import java.util.Arrays;\n+\n /**\n  * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjA0MA==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536040", "bodyText": "ConsumeOrder.values() -> values()", "author": "JingsongLi", "createdAt": "2020-05-15T02:46:27Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\nindex 2cdf649d0a..0dc3448346 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n\n@@ -18,6 +18,8 @@\n \n package org.apache.flink.connectors.hive;\n \n+import java.util.Arrays;\n+\n /**\n  * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjExMg==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536112", "bodyText": "equalsIgnoreCase", "author": "JingsongLi", "createdAt": "2020-05-15T02:46:47Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {\n+\t\t\tif (consumeOrder.order.equals(consumeOrderStr)) {", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\nindex 2cdf649d0a..0dc3448346 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n\n@@ -18,6 +18,8 @@\n \n package org.apache.flink.connectors.hive;\n \n+import java.util.Arrays;\n+\n /**\n  * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjI1OQ==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536259", "bodyText": "throw exception, we should not pass a null to operators.", "author": "JingsongLi", "createdAt": "2020-05-15T02:47:23Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive;\n+\n+/**\n+ * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n+ */\n+public enum ConsumeOrder {\n+\n+\t/**\n+\t * create-time compare partition/file creation time,\n+\t * this is not the partition create time in Hive metaStore,\n+\t * but the folder/file create time in filesystem.\n+\t */\n+\tCREATE_TIME_ORDER(\"create-time\"),\n+\n+\t/**\n+\t * partition-time compare time represented by partition name.\n+\t */\n+\tPARTITION_TIME_ORDER(\"partition-time\");\n+\n+\tprivate final String order;\n+\tConsumeOrder(String order) {\n+\t\tthis.order = order;\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn order;\n+\t}\n+\n+\t/**\n+\t * Get {@link ConsumeOrder} from consume order string.\n+\t */\n+\tpublic static ConsumeOrder getConsumeOrder(String consumeOrderStr) {\n+\t\tfor (ConsumeOrder consumeOrder : ConsumeOrder.values()) {\n+\t\t\tif (consumeOrder.order.equals(consumeOrderStr)) {\n+\t\t\t\treturn consumeOrder;\n+\t\t\t}\n+\t\t}\n+\t\treturn null;", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\nindex 2cdf649d0a..0dc3448346 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/ConsumeOrder.java\n\n@@ -18,6 +18,8 @@\n \n package org.apache.flink.connectors.hive;\n \n+import java.util.Arrays;\n+\n /**\n  * {@link ConsumeOrder} defines the orders to continuously consume stream source.\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNjcwMA==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425536700", "bodyText": "Configuration configuration = new Configuration();\noptions.forEach(configuration::setString)\n\nWe can use Configuration to get option.", "author": "JingsongLi", "createdAt": "2020-05-15T02:49:19Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());\n+\n+\t\tfinal Map<String, String> properties = catalogTable.getOptions();", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\nindex ddb9c44128..46d91b68f2 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n\n@@ -272,33 +263,21 @@ public class HiveTableSource implements\n \t\t\tTypeInformation<RowData> typeInfo,\n \t\t\tHiveTableInputFormat inputFormat,\n \t\t\tHiveTablePartition hiveTable) {\n-\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n-\t\t\t\tinputFormat,\n-\t\t\t\thiveTable);\n-\t\tfileInputFormat.setFilePath(getFilePath());\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(inputFormat, hiveTable);\n \n-\t\tfinal Map<String, String> properties = catalogTable.getOptions();\n-\n-\t\tString consumeOrderStr = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.defaultValue());\n+\t\tConfiguration configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_CONSUME_ORDER);\n \t\tConsumeOrder consumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n \t\tif (consumeOrder != ConsumeOrder.CREATE_TIME_ORDER) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unsupported consumer order: \" + consumeOrder);\n+\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\t\"Only \" + ConsumeOrder.CREATE_TIME_ORDER + \" is supported for non partition table.\");\n \t\t}\n \n-\t\tString consumeOffset = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.defaultValue());\n-\t\tlong currentReadTime = Long.MIN_VALUE;\n-\t\tif (consumeOffset != null) {\n-\t\t\tcurrentReadTime = toMills(consumeOffset);\n-\t\t}\n+\t\tString consumeOffset = configuration.get(STREAMING_SOURCE_CONSUME_START_OFFSET);\n+\t\tlong currentReadTime = toMills(consumeOffset);\n \n-\t\tString monitorIntervalStr = properties.get(STREAMING_SOURCE_MONITOR_INTERVAL.key());\n-\t\tDuration monitorInterval = monitorIntervalStr != null ?\n-\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n-\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n \n \t\tContinuousFileMonitoringFunction<RowData> monitoringFunction =\n \t\t\t\tnew ContinuousFileMonitoringFunction<>(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNzExNA==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425537114", "bodyText": "never null.", "author": "JingsongLi", "createdAt": "2020-05-15T02:50:56Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());\n+\n+\t\tfinal Map<String, String> properties = catalogTable.getOptions();\n+\n+\t\tString consumeOrderStr = properties.getOrDefault(\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.key(),\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.defaultValue());\n+\t\tConsumeOrder consumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n+\t\tif (consumeOrder != ConsumeOrder.CREATE_TIME_ORDER) {\n+\t\t\tthrow new UnsupportedOperationException(\"Unsupported consumer order: \" + consumeOrder);\n+\t\t}\n+\n+\t\tString consumeOffset = properties.getOrDefault(\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n+\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.defaultValue());\n+\t\tlong currentReadTime = Long.MIN_VALUE;\n+\t\tif (consumeOffset != null) {", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\nindex ddb9c44128..46d91b68f2 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n\n@@ -272,33 +263,21 @@ public class HiveTableSource implements\n \t\t\tTypeInformation<RowData> typeInfo,\n \t\t\tHiveTableInputFormat inputFormat,\n \t\t\tHiveTablePartition hiveTable) {\n-\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n-\t\t\t\tinputFormat,\n-\t\t\t\thiveTable);\n-\t\tfileInputFormat.setFilePath(getFilePath());\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(inputFormat, hiveTable);\n \n-\t\tfinal Map<String, String> properties = catalogTable.getOptions();\n-\n-\t\tString consumeOrderStr = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.defaultValue());\n+\t\tConfiguration configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_CONSUME_ORDER);\n \t\tConsumeOrder consumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n \t\tif (consumeOrder != ConsumeOrder.CREATE_TIME_ORDER) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unsupported consumer order: \" + consumeOrder);\n+\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\t\"Only \" + ConsumeOrder.CREATE_TIME_ORDER + \" is supported for non partition table.\");\n \t\t}\n \n-\t\tString consumeOffset = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.defaultValue());\n-\t\tlong currentReadTime = Long.MIN_VALUE;\n-\t\tif (consumeOffset != null) {\n-\t\t\tcurrentReadTime = toMills(consumeOffset);\n-\t\t}\n+\t\tString consumeOffset = configuration.get(STREAMING_SOURCE_CONSUME_START_OFFSET);\n+\t\tlong currentReadTime = toMills(consumeOffset);\n \n-\t\tString monitorIntervalStr = properties.get(STREAMING_SOURCE_MONITOR_INTERVAL.key());\n-\t\tDuration monitorInterval = monitorIntervalStr != null ?\n-\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n-\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n \n \t\tContinuousFileMonitoringFunction<RowData> monitoringFunction =\n \t\t\t\tnew ContinuousFileMonitoringFunction<>(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzNzY0MQ==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425537641", "bodyText": "Remove getFilePath, we can get path from allHivePartitions(should be a single object when no partitions.).", "author": "JingsongLi", "createdAt": "2020-05-15T02:53:10Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -261,6 +267,57 @@ private boolean isStreamingSource() {\n \t\treturn new DataStreamSource<>(source);\n \t}\n \n+\tprivate DataStream<RowData> createStreamSourceForNonPartitionTable(\n+\t\t\tStreamExecutionEnvironment execEnv,\n+\t\t\tTypeInformation<RowData> typeInfo,\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTable) {\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n+\t\t\t\tinputFormat,\n+\t\t\t\thiveTable);\n+\t\tfileInputFormat.setFilePath(getFilePath());", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\nindex ddb9c44128..46d91b68f2 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n\n@@ -272,33 +263,21 @@ public class HiveTableSource implements\n \t\t\tTypeInformation<RowData> typeInfo,\n \t\t\tHiveTableInputFormat inputFormat,\n \t\t\tHiveTablePartition hiveTable) {\n-\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(\n-\t\t\t\tinputFormat,\n-\t\t\t\thiveTable);\n-\t\tfileInputFormat.setFilePath(getFilePath());\n+\t\tHiveTableFileInputFormat fileInputFormat = new HiveTableFileInputFormat(inputFormat, hiveTable);\n \n-\t\tfinal Map<String, String> properties = catalogTable.getOptions();\n-\n-\t\tString consumeOrderStr = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_ORDER.defaultValue());\n+\t\tConfiguration configuration = new Configuration();\n+\t\tcatalogTable.getOptions().forEach(configuration::setString);\n+\t\tString consumeOrderStr = configuration.get(STREAMING_SOURCE_CONSUME_ORDER);\n \t\tConsumeOrder consumeOrder = ConsumeOrder.getConsumeOrder(consumeOrderStr);\n \t\tif (consumeOrder != ConsumeOrder.CREATE_TIME_ORDER) {\n-\t\t\tthrow new UnsupportedOperationException(\"Unsupported consumer order: \" + consumeOrder);\n+\t\t\tthrow new UnsupportedOperationException(\n+\t\t\t\t\t\"Only \" + ConsumeOrder.CREATE_TIME_ORDER + \" is supported for non partition table.\");\n \t\t}\n \n-\t\tString consumeOffset = properties.getOrDefault(\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.key(),\n-\t\t\t\tSTREAMING_SOURCE_CONSUME_START_OFFSET.defaultValue());\n-\t\tlong currentReadTime = Long.MIN_VALUE;\n-\t\tif (consumeOffset != null) {\n-\t\t\tcurrentReadTime = toMills(consumeOffset);\n-\t\t}\n+\t\tString consumeOffset = configuration.get(STREAMING_SOURCE_CONSUME_START_OFFSET);\n+\t\tlong currentReadTime = toMills(consumeOffset);\n \n-\t\tString monitorIntervalStr = properties.get(STREAMING_SOURCE_MONITOR_INTERVAL.key());\n-\t\tDuration monitorInterval = monitorIntervalStr != null ?\n-\t\t\t\tTimeUtils.parseDuration(monitorIntervalStr) :\n-\t\t\t\tSTREAMING_SOURCE_MONITOR_INTERVAL.defaultValue();\n+\t\tDuration monitorInterval = configuration.get(STREAMING_SOURCE_MONITOR_INTERVAL);\n \n \t\tContinuousFileMonitoringFunction<RowData> monitoringFunction =\n \t\t\t\tnew ContinuousFileMonitoringFunction<>(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODEwOQ==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538109", "bodyText": "Please add comments:\n\nOnly support FileInputSplit\nOnly support renaming inserting\ngetSplit use flinks instead format.", "author": "JingsongLi", "createdAt": "2020-05-15T02:55:05Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+/**\n+ * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTU0NjIyNw==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425546227", "bodyText": "\"Only support renaming inserting\" is not the limitation of HiveTableFileInputFormat, it's the limit of ContinuousFileMonitoringFunction, I have add some documents in FileSystemOptions.", "author": "godfreyhe", "createdAt": "2020-05-15T03:28:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODEwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\nindex 155d8f4375..eb1fb41283 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\n\n@@ -33,17 +33,21 @@ import java.net.URI;\n \n /**\n  * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.\n+ *\n+ * <p>We only use a {@link HiveTableInputFormat} to read the data of a {@link FileInputSplit}.\n+ * `createInputSplits`, `getInputSplitAssigner` will use {@link FileInputFormat}'s logic.\n  */\n public class HiveTableFileInputFormat extends FileInputFormat<RowData> {\n \n-\tprivate HiveTableInputFormat inputFormat;\n-\tprivate HiveTablePartition hiveTablePartition;\n+\tprivate final HiveTableInputFormat inputFormat;\n+\tprivate final HiveTablePartition hiveTablePartition;\n \n \tpublic HiveTableFileInputFormat(\n \t\t\tHiveTableInputFormat inputFormat,\n \t\t\tHiveTablePartition hiveTablePartition) {\n \t\tthis.inputFormat = inputFormat;\n \t\tthis.hiveTablePartition = hiveTablePartition;\n+\t\tsetFilePath(hiveTablePartition.getStorageDescriptor().getLocation());\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODI5MQ==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538291", "bodyText": "A better way is first super., second do own works.\nsame below.", "author": "JingsongLi", "createdAt": "2020-05-15T02:55:47Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connectors.hive.read;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.api.common.io.FileInputFormat;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connectors.hive.HiveTablePartition;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.table.data.RowData;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+/**\n+ * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.\n+ */\n+public class HiveTableFileInputFormat extends FileInputFormat<RowData> {\n+\n+\tprivate HiveTableInputFormat inputFormat;\n+\tprivate HiveTablePartition hiveTablePartition;\n+\n+\tpublic HiveTableFileInputFormat(\n+\t\t\tHiveTableInputFormat inputFormat,\n+\t\t\tHiveTablePartition hiveTablePartition) {\n+\t\tthis.inputFormat = inputFormat;\n+\t\tthis.hiveTablePartition = hiveTablePartition;\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit fileSplit) throws IOException {\n+\t\tURI uri = fileSplit.getPath().toUri();\n+\t\tHiveTableInputSplit split = new HiveTableInputSplit(\n+\t\t\t\tfileSplit.getSplitNumber(),\n+\t\t\t\tnew FileSplit(new Path(uri), fileSplit.getStart(), fileSplit.getLength(), (String[]) null),\n+\t\t\t\tinputFormat.getJobConf(),\n+\t\t\t\thiveTablePartition\n+\t\t);\n+\t\tinputFormat.open(split);\n+\t}\n+\n+\t@Override\n+\tpublic boolean reachedEnd() throws IOException {\n+\t\treturn inputFormat.reachedEnd();\n+\t}\n+\n+\t@Override\n+\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\treturn inputFormat.nextRecord(reuse);\n+\t}\n+\n+\t@Override\n+\tpublic void configure(Configuration parameters) {\n+\t\tinputFormat.configure(parameters);", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\nindex 155d8f4375..eb1fb41283 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveTableFileInputFormat.java\n\n@@ -33,17 +33,21 @@ import java.net.URI;\n \n /**\n  * A {@link FileInputFormat} that wraps a {@link HiveTableInputFormat}.\n+ *\n+ * <p>We only use a {@link HiveTableInputFormat} to read the data of a {@link FileInputSplit}.\n+ * `createInputSplits`, `getInputSplitAssigner` will use {@link FileInputFormat}'s logic.\n  */\n public class HiveTableFileInputFormat extends FileInputFormat<RowData> {\n \n-\tprivate HiveTableInputFormat inputFormat;\n-\tprivate HiveTablePartition hiveTablePartition;\n+\tprivate final HiveTableInputFormat inputFormat;\n+\tprivate final HiveTablePartition hiveTablePartition;\n \n \tpublic HiveTableFileInputFormat(\n \t\t\tHiveTableInputFormat inputFormat,\n \t\t\tHiveTablePartition hiveTablePartition) {\n \t\tthis.inputFormat = inputFormat;\n \t\tthis.hiveTablePartition = hiveTablePartition;\n+\t\tsetFilePath(hiveTablePartition.getStorageDescriptor().getLocation());\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUzODY2Ng==", "url": "https://github.com/apache/flink/pull/12025#discussion_r425538666", "bodyText": "useMapredReader only works in parquet and orc, you should use them.", "author": "JingsongLi", "createdAt": "2020-05-15T02:57:15Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java", "diffHunk": "@@ -531,6 +534,76 @@ public void testStreamPartitionRead() throws Exception {\n \t\tjob.cancel();\n \t}\n \n+\t@Test(timeout = 30000)\n+\tpublic void testNonPartitionStreamingSourceWithMapredReader() throws Exception {\n+\t\ttestNonPartitionStreamingSource(true, \"test_mapred_reader\");\n+\t}\n+\n+\t@Test(timeout = 30000)\n+\tpublic void testNonPartitionStreamingSourceWithVectorizedReader() throws Exception {\n+\t\ttestNonPartitionStreamingSource(false, \"test_vectorized_reader\");\n+\t}\n+\n+\tprivate void testNonPartitionStreamingSource(Boolean useMapredReader, String tblName) throws Exception {\n+\t\tfinal String catalogName = \"hive\";\n+\t\tfinal String dbName = \"source_db\";\n+\t\thiveShell.execute(\"CREATE TABLE source_db.\" + tblName + \" (\" +\n+\t\t\t\t\"  a INT,\" +\n+\t\t\t\t\"  b CHAR(1) \" +\n+\t\t\t\t\") TBLPROPERTIES (\" +\n+\t\t\t\t\"  'streaming-source.enable'='true',\" +", "originalCommit": "d4b70dbd62d22edbaf0c3ff54ca61a35ac93c04e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java\nindex 165801106c..a8f3ddc88e 100644\n--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java\n+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceTest.java\n\n@@ -550,7 +550,7 @@ public class HiveTableSourceTest {\n \t\thiveShell.execute(\"CREATE TABLE source_db.\" + tblName + \" (\" +\n \t\t\t\t\"  a INT,\" +\n \t\t\t\t\"  b CHAR(1) \" +\n-\t\t\t\t\") TBLPROPERTIES (\" +\n+\t\t\t\t\") stored as parquet TBLPROPERTIES (\" +\n \t\t\t\t\"  'streaming-source.enable'='true',\" +\n \t\t\t\t\"  'streaming-source.monitor-interval'='100ms'\" +\n \t\t\t\t\")\");\n"}}, {"oid": "c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "url": "https://github.com/apache/flink/commit/c38c037f31687b4d20ef0e183d42cc5f64d3bb45", "message": "address comments", "committedDate": "2020-05-15T03:45:34Z", "type": "commit"}]}