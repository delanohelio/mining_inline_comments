{"pr_number": 10762, "pr_title": "[FLINK-15115][kafka] Drop Kafka 0.8/0.9", "pr_createdAt": "2020-01-03T14:33:04Z", "pr_url": "https://github.com/apache/flink/pull/10762", "timeline": [{"oid": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "url": "https://github.com/apache/flink/commit/6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-03T14:33:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2MA==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318960", "bodyText": "Should we throw UnsupportedOperationException here in order to avoid NPE somewhere else in the code?", "author": "tillrohrmann", "createdAt": "2020-01-10T16:28:12Z", "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;", "originalCommit": "18dd7d344258743705da311508a1fc7d0873da45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMzgyMQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365613821", "bodyText": "Tests still passes with all methods that return something throwing an exception instead, so we'll go with that.", "author": "zentol", "createdAt": "2020-01-12T21:52:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2MA=="}], "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nindex 436b42bcd21..5575b752c95 100644\n--- a/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n+++ b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n\n@@ -923,7 +923,7 @@ public class KafkaConsumerThreadTest {\n \n \t\t@Override\n \t\tpublic Set<String> subscription() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMxODk2OQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365318969", "bodyText": "Same here with the UnsupportedOperationException.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:28:13Z", "path": "flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -875,109 +878,194 @@ void reassignPartitions(List<KafkaTopicPartitionState<TopicPartition>> newPartit\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate static KafkaConsumer<byte[], byte[]> createMockConsumer(\n+\tprivate static TestConsumer createMockConsumer(\n \t\t\tfinal Map<TopicPartition, Long> mockConsumerAssignmentAndPosition,\n \t\t\tfinal Map<TopicPartition, Long> mockRetrievedPositions,\n \t\t\tfinal boolean earlyWakeup,\n \t\t\tfinal OneShotLatch midAssignmentLatch,\n \t\t\tfinal OneShotLatch continueAssignmentLatch) {\n \n-\t\tfinal KafkaConsumer<byte[], byte[]> mockConsumer = mock(KafkaConsumer.class);\n+\t\treturn new TestConsumer(mockConsumerAssignmentAndPosition, mockRetrievedPositions, earlyWakeup, midAssignmentLatch, continueAssignmentLatch);\n+\t}\n \n-\t\twhen(mockConsumer.assignment()).thenAnswer(new Answer<Object>() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tif (midAssignmentLatch != null) {\n-\t\t\t\t\tmidAssignmentLatch.trigger();\n-\t\t\t\t}\n+\tprivate static class TestConsumer implements Consumer<byte[], byte[]> {\n+\t\tprivate final Map<TopicPartition, Long> mockConsumerAssignmentAndPosition;\n+\t\tprivate final Map<TopicPartition, Long> mockRetrievedPositions;\n+\t\tprivate final boolean earlyWakeup;\n+\t\tprivate final OneShotLatch midAssignmentLatch;\n+\t\tprivate final OneShotLatch continueAssignmentLatch;\n+\n+\t\tprivate int numWakeupCalls = 0;\n+\n+\t\tprivate TestConsumer(Map<TopicPartition, Long> mockConsumerAssignmentAndPosition, Map<TopicPartition, Long> mockRetrievedPositions, boolean earlyWakeup, OneShotLatch midAssignmentLatch, OneShotLatch continueAssignmentLatch) {\n+\t\t\tthis.mockConsumerAssignmentAndPosition = mockConsumerAssignmentAndPosition;\n+\t\t\tthis.mockRetrievedPositions = mockRetrievedPositions;\n+\t\t\tthis.earlyWakeup = earlyWakeup;\n+\t\t\tthis.midAssignmentLatch = midAssignmentLatch;\n+\t\t\tthis.continueAssignmentLatch = continueAssignmentLatch;\n+\t\t}\n \n-\t\t\t\tif (continueAssignmentLatch != null) {\n+\t\t@Override\n+\t\tpublic Set<TopicPartition> assignment() {\n+\t\t\tif (midAssignmentLatch != null) {\n+\t\t\t\tmidAssignmentLatch.trigger();\n+\t\t\t}\n+\n+\t\t\tif (continueAssignmentLatch != null) {\n+\t\t\t\ttry {\n \t\t\t\t\tcontinueAssignmentLatch.await();\n+\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n \t\t\t\t}\n-\t\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n \t\t\t}\n-\t\t});\n+\t\t\treturn mockConsumerAssignmentAndPosition.keySet();\n+\t\t}\n \n-\t\twhen(mockConsumer.poll(anyLong())).thenReturn(mock(ConsumerRecords.class));\n+\t\t@Override\n+\t\tpublic Set<String> subscription() {\n+\t\t\treturn null;\n+\t\t}\n \n-\t\tif (!earlyWakeup) {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenAnswer(new Answer<Object>() {\n-\t\t\t\t@Override\n-\t\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\treturn mockConsumerAssignmentAndPosition.get(invocationOnMock.getArgument(0));\n-\t\t\t\t}\n-\t\t\t});\n-\t\t} else {\n-\t\t\twhen(mockConsumer.position(any(TopicPartition.class))).thenThrow(new WakeupException());\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list) {\n \t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\t\t@Override\n+\t\tpublic void subscribe(List<String> list, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tList<TopicPartition> assignedPartitions = invocationOnMock.getArgument(0);\n-\t\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n+\t\t@Override\n+\t\tpublic void assign(List<TopicPartition> assignedPartitions) {\n+\t\t\tmockConsumerAssignmentAndPosition.clear();\n+\n+\t\t\tfor (TopicPartition assigned : assignedPartitions) {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(assigned, null);\n \t\t\t}\n-\t\t}).when(mockConsumer).assign(anyListOf(TopicPartition.class));\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n-\t\t\t\tlong position = invocationOnMock.getArgument(1);\n+\t\t@Override\n+\t\tpublic void subscribe(Pattern pattern, ConsumerRebalanceListener consumerRebalanceListener) {\n+\t\t}\n \n-\t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n-\t\t\t\t} else {\n-\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n-\t\t\t\t}\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t}).when(mockConsumer).seek(any(TopicPartition.class), anyLong());\n+\t\t@Override\n+\t\tpublic void unsubscribe() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic ConsumerRecords<byte[], byte[]> poll(long l) {\n+\t\t\treturn mock(ConsumerRecords.class);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitSync(Map<TopicPartition, OffsetAndMetadata> map) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync() {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void commitAsync(OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t@Override\n+\t\tpublic void commitAsync(Map<TopicPartition, OffsetAndMetadata> map, OffsetCommitCallback offsetCommitCallback) {\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seek(TopicPartition partition, long position) {\n+\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n+\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t} else {\n+\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, position);\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void seekToBeginning(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToBeginning(any(TopicPartition.class));\n-\n-\t\tdoAnswer(new Answer() {\n-\t\t\t@Override\n-\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\tTopicPartition partition = invocationOnMock.getArgument(0);\n+\t\t}\n \n+\t\t@Override\n+\t\tpublic void seekToEnd(TopicPartition... partitions) {\n+\t\t\tfor (TopicPartition partition : partitions) {\n \t\t\t\tif (!mockConsumerAssignmentAndPosition.containsKey(partition)) {\n-\t\t\t\t\tthrow new Exception(\"the current mock assignment does not contain partition \" + partition);\n+\t\t\t\t\tthrow new RuntimeException(\"the current mock assignment does not contain partition \" + partition);\n \t\t\t\t} else {\n \t\t\t\t\tLong mockRetrievedPosition = mockRetrievedPositions.get(partition);\n \t\t\t\t\tif (mockRetrievedPosition == null) {\n-\t\t\t\t\t\tthrow new Exception(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n+\t\t\t\t\t\tthrow new RuntimeException(\"mock consumer needed to retrieve a position, but no value was provided in the mock values for retrieval\");\n \t\t\t\t\t} else {\n \t\t\t\t\t\tmockConsumerAssignmentAndPosition.put(partition, mockRetrievedPositions.get(partition));\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\treturn null;\n \t\t\t}\n-\t\t}).when(mockConsumer).seekToEnd(any(TopicPartition.class));\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic long position(TopicPartition topicPartition) {\n+\t\t\tif (!earlyWakeup) {\n+\t\t\t\treturn mockConsumerAssignmentAndPosition.get(topicPartition);\n+\t\t\t} else {\n+\t\t\t\tthrow new WakeupException();\n+\t\t\t}\n+\t\t}\n \n-\t\treturn mockConsumer;\n+\t\t@Override\n+\t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<MetricName, ? extends Metric> metrics() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<PartitionInfo> partitionsFor(String s) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<String, List<PartitionInfo>> listTopics() {\n+\t\t\treturn null;\n+\t\t}", "originalCommit": "18dd7d344258743705da311508a1fc7d0873da45", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nindex 436b42bcd21..5575b752c95 100644\n--- a/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n+++ b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n\n@@ -923,7 +923,7 @@ public class KafkaConsumerThreadTest {\n \n \t\t@Override\n \t\tpublic Set<String> subscription() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365322652", "bodyText": "ConnectorDescriptorValidator.java contains a reference to Kafka 0.8 in line 47.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:35:36Z", "path": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java", "diffHunk": "@@ -38,7 +38,6 @@\n public class KafkaValidator extends ConnectorDescriptorValidator {\n \n \tpublic static final String CONNECTOR_TYPE_VALUE_KAFKA = \"kafka\";\n-\tpublic static final String CONNECTOR_VERSION_VALUE_08 = \"0.8\";", "originalCommit": "3e0a89f84b48d5daa31fac2a2f6e916f4ae596e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMzAwNQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365323005", "bodyText": "KafkaShortRetentionTestBase.java contains in line 253 Kafka 0.8 specific code. In line 254 the same class contains Kafka 0.9 specific code.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyMjY1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java\nindex f2ac226fcf0..24ecfef8efa 100644\n--- a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java\n+++ b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/table/descriptors/KafkaValidator.java\n\n@@ -38,6 +38,7 @@ import static org.apache.flink.table.descriptors.StreamTableDescriptorValidator.\n public class KafkaValidator extends ConnectorDescriptorValidator {\n \n \tpublic static final String CONNECTOR_TYPE_VALUE_KAFKA = \"kafka\";\n+\tpublic static final String CONNECTOR_VERSION_VALUE_08 = \"0.8\";\n \tpublic static final String CONNECTOR_VERSION_VALUE_09 = \"0.9\";\n \tpublic static final String CONNECTOR_VERSION_VALUE_010 = \"0.10\";\n \tpublic static final String CONNECTOR_VERSION_VALUE_011 = \"0.11\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODQ4MQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328481", "bodyText": "Can we avoid null and instead pass in an empty collection? If not, then let's add @Nullable annotation.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:47:51Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -173,7 +205,38 @@ public FlinkKafkaConsumer010(Pattern subscriptionPattern, DeserializationSchema<\n \t */\n \t@PublicEvolving\n \tpublic FlinkKafkaConsumer010(Pattern subscriptionPattern, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(subscriptionPattern, deserializer, props);\n+\t\tthis(null, subscriptionPattern, deserializer, props);", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMjg3NQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365612875", "bodyText": "The KafkTopicsDescriptor actually relies on this being null. I'll add Nullable to the private constructor for the time being.", "author": "zentol", "createdAt": "2020-01-12T21:39:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODQ4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\nindex a71ef7df8ac..c36f1841836 100644\n--- a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\n+++ b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\n\n@@ -205,38 +173,7 @@ public class FlinkKafkaConsumer010<T> extends FlinkKafkaConsumerBase<T> {\n \t */\n \t@PublicEvolving\n \tpublic FlinkKafkaConsumer010(Pattern subscriptionPattern, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tthis(null, subscriptionPattern, deserializer, props);\n-\t}\n-\n-\tprivate FlinkKafkaConsumer010(\n-\t\t\tList<String> topics,\n-\t\t\tPattern subscriptionPattern,\n-\t\t\tKafkaDeserializationSchema<T> deserializer,\n-\t\t\tProperties props) {\n-\n-\t\tsuper(\n-\t\t\t\ttopics,\n-\t\t\t\tsubscriptionPattern,\n-\t\t\t\tdeserializer,\n-\t\t\t\tgetLong(\n-\t\t\t\t\tcheckNotNull(props, \"props\"),\n-\t\t\t\t\tKEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),\n-\t\t\t\t!getBoolean(props, KEY_DISABLE_METRICS, false));\n-\n-\t\tthis.properties = props;\n-\t\tsetDeserializer(this.properties);\n-\n-\t\t// configure the polling timeout\n-\t\ttry {\n-\t\t\tif (properties.containsKey(KEY_POLL_TIMEOUT)) {\n-\t\t\t\tthis.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));\n-\t\t\t} else {\n-\t\t\t\tthis.pollTimeout = DEFAULT_POLL_TIMEOUT;\n-\t\t\t}\n-\t\t}\n-\t\tcatch (Exception e) {\n-\t\t\tthrow new IllegalArgumentException(\"Cannot parse poll timeout for '\" + KEY_POLL_TIMEOUT + '\\'', e);\n-\t\t}\n+\t\tsuper(subscriptionPattern, deserializer, props);\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODY4OQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365328689", "bodyText": "Same here with the null value for Pattern. I think it would be good to avoid it if possible.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:48:20Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java", "diffHunk": "@@ -130,7 +162,7 @@ public FlinkKafkaConsumer010(List<String> topics, DeserializationSchema<T> deser\n \t *           The properties that are used to configure both the fetcher and the offset handler.\n \t */\n \tpublic FlinkKafkaConsumer010(List<String> topics, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tsuper(topics, deserializer, props);\n+\t\tthis(topics, null, deserializer, props);", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTYxMjk4Nw==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365612987", "bodyText": "Will add Nullable with the same reasoning as for the list of topics.", "author": "zentol", "createdAt": "2020-01-12T21:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMyODY4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\nindex a71ef7df8ac..c36f1841836 100644\n--- a/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\n+++ b/flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java\n\n@@ -162,7 +130,7 @@ public class FlinkKafkaConsumer010<T> extends FlinkKafkaConsumerBase<T> {\n \t *           The properties that are used to configure both the fetcher and the offset handler.\n \t */\n \tpublic FlinkKafkaConsumer010(List<String> topics, KafkaDeserializationSchema<T> deserializer, Properties props) {\n-\t\tthis(topics, null, deserializer, props);\n+\t\tsuper(topics, deserializer, props);\n \t}\n \n \t/**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTA3MQ==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331071", "bodyText": "I would suggest to either fail or to return an empty map but not null.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:53:28Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nsimilarity index 97%\nrename from flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nrename to flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nindex 2a6a59914e4..5575b752c95 100644\n--- a/flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n+++ b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n\n@@ -1028,60 +1028,36 @@ public class KafkaConsumerThreadTest {\n \n \t\t@Override\n \t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic Map<MetricName, ? extends Metric> metrics() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic List<PartitionInfo> partitionsFor(String s) {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic Map<String, List<PartitionInfo>> listTopics() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n-\t\tpublic Set<TopicPartition> paused() {\n-\t\t\treturn null;\n+\t\tpublic void pause(TopicPartition... topicPartitions) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(Collection<TopicPartition> collection) {\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void resume(Collection<TopicPartition> collection) {\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> collection) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> collection) {\n-\t\t\treturn null;\n+\t\tpublic void resume(TopicPartition... topicPartitions) {\n \t\t}\n \n \t\t@Override\n \t\tpublic void close() {\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void close(long l, TimeUnit timeUnit) {\n-\t\t}\n-\n \t\t@Override\n \t\tpublic void wakeup() {\n \t\t\tnumWakeupCalls++;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTMzMTE0Mw==", "url": "https://github.com/apache/flink/pull/10762#discussion_r365331143", "bodyText": "Same here with null.", "author": "tillrohrmann", "createdAt": "2020-01-10T16:53:37Z", "path": "flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java", "diffHunk": "@@ -1047,17 +1047,41 @@ public OffsetAndMetadata committed(TopicPartition topicPartition) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(TopicPartition... topicPartitions) {\n+\t\tpublic Set<TopicPartition> paused() {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void pause(Collection<TopicPartition> collection) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void resume(Collection<TopicPartition> collection) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void resume(TopicPartition... topicPartitions) {\n+\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> collection) {\n+\t\t\treturn null;", "originalCommit": "6bfb6d18af5fdfbddadf6da53adab78567d64aa9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "9db1c796618532ffa1ca3542714d56957aa1bffc", "chunk": "diff --git a/flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nsimilarity index 97%\nrename from flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nrename to flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\nindex 2a6a59914e4..5575b752c95 100644\n--- a/flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n+++ b/flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThreadTest.java\n\n@@ -1028,60 +1028,36 @@ public class KafkaConsumerThreadTest {\n \n \t\t@Override\n \t\tpublic OffsetAndMetadata committed(TopicPartition topicPartition) {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic Map<MetricName, ? extends Metric> metrics() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic List<PartitionInfo> partitionsFor(String s) {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n \t\tpublic Map<String, List<PartitionInfo>> listTopics() {\n-\t\t\treturn null;\n+\t\t\tthrow new UnsupportedOperationException();\n \t\t}\n \n \t\t@Override\n-\t\tpublic Set<TopicPartition> paused() {\n-\t\t\treturn null;\n+\t\tpublic void pause(TopicPartition... topicPartitions) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic void pause(Collection<TopicPartition> collection) {\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void resume(Collection<TopicPartition> collection) {\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes(Map<TopicPartition, Long> map) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, Long> beginningOffsets(Collection<TopicPartition> collection) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Map<TopicPartition, Long> endOffsets(Collection<TopicPartition> collection) {\n-\t\t\treturn null;\n+\t\tpublic void resume(TopicPartition... topicPartitions) {\n \t\t}\n \n \t\t@Override\n \t\tpublic void close() {\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void close(long l, TimeUnit timeUnit) {\n-\t\t}\n-\n \t\t@Override\n \t\tpublic void wakeup() {\n \t\t\tnumWakeupCalls++;\n"}}, {"oid": "9db1c796618532ffa1ca3542714d56957aa1bffc", "url": "https://github.com/apache/flink/commit/9db1c796618532ffa1ca3542714d56957aa1bffc", "message": "[FLINK-15115][kafka] Reduce mocking\n\nReplace the mocked KafkaConsumer with a custom Consumer implementation. Migrate all users toward the Consumer interface instead of the concrete KafkaConsumer implementation.\nUsing an actual implementation highlights API changes when migrating to later custom versions and prevents issues due to subtle mocking gotchas.", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "url": "https://github.com/apache/flink/commit/9bebf0bc9b4d1a726d53504f48d3ecfcc2441b37", "message": "[FLINK-15115][kafka] Drop Kafka 0.8", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "url": "https://github.com/apache/flink/commit/2b78e6f3398952c5d5c79822375cc56ee7b96d8a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9 SQL jar", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "url": "https://github.com/apache/flink/commit/458b80ac0f12dd117ee23fa8ff4f18d74c1a46e5", "message": "[hotfix][kafka][legal] Correct version in NOTICE", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-13T11:38:18Z", "type": "commit"}, {"oid": "0604a77cf764e1c36abb91352c9ce9410d7e883a", "url": "https://github.com/apache/flink/commit/0604a77cf764e1c36abb91352c9ce9410d7e883a", "message": "[FLINK-15115][kafka] Drop Kafka 0.9", "committedDate": "2020-01-13T11:38:18Z", "type": "forcePushed"}]}