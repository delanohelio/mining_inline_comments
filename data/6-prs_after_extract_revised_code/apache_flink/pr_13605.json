{"pr_number": 13605, "pr_title": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table", "pr_createdAt": "2020-10-13T06:19:37Z", "pr_url": "https://github.com/apache/flink/pull/13605", "timeline": [{"oid": "76ff23e27070dc54e6af85dd91c2742457621aa9", "url": "https://github.com/apache/flink/commit/76ff23e27070dc54e6af85dd91c2742457621aa9", "message": "[FLINK-19599][table] Introduce BulkFormatFactory to integrate new FileSource to table", "committedDate": "2020-10-14T03:32:19Z", "type": "forcePushed"}, {"oid": "315b69fafef254ff273bc36c713d28166260e4c6", "url": "https://github.com/apache/flink/commit/315b69fafef254ff273bc36c713d28166260e4c6", "message": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table", "committedDate": "2020-10-27T07:29:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjgzOA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513162838", "bodyText": "Why UTC_TIMEZONE option is not in the set?", "author": "wuchong", "createdAt": "2020-10-28T03:45:19Z", "path": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetFileSystemFormatFactory.java", "diffHunk": "@@ -93,124 +117,17 @@ private static Configuration getParquetConfiguration(ReadableConfig options) {\n \t}\n \n \t@Override\n-\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n-\t\treturn new ParquetInputFormat(\n-\t\t\t\tcontext.getPaths(),\n-\t\t\t\tcontext.getSchema().getFieldNames(),\n-\t\t\t\tcontext.getSchema().getFieldDataTypes(),\n-\t\t\t\tcontext.getProjectFields(),\n-\t\t\t\tcontext.getDefaultPartName(),\n-\t\t\t\tcontext.getPushedDownLimit(),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE));\n+\tpublic String factoryIdentifier() {\n+\t\treturn \"parquet\";\n \t}\n \n \t@Override\n-\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n-\t\treturn Optional.of(ParquetRowDataBuilder.createWriterFactory(\n-\t\t\t\tRowType.of(Arrays.stream(context.getFormatFieldTypes())\n-\t\t\t\t\t\t\t\t.map(DataType::getLogicalType)\n-\t\t\t\t\t\t\t\t.toArray(LogicalType[]::new),\n-\t\t\t\t\t\tcontext.getFormatFieldNames()),\n-\t\t\t\tgetParquetConfiguration(context.getFormatOptions()),\n-\t\t\t\tcontext.getFormatOptions().get(UTC_TIMEZONE)));\n+\tpublic Set<ConfigOption<?>> requiredOptions() {\n+\t\treturn new HashSet<>();\n \t}\n \n \t@Override\n-\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n-\t\treturn Optional.empty();\n-\t}\n-\n-\t/**\n-\t * An implementation of {@link ParquetInputFormat} to read {@link RowData} records\n-\t * from Parquet files.\n-\t */\n-\tpublic static class ParquetInputFormat extends FileInputFormat<RowData> {\n-\n-\t\tprivate static final long serialVersionUID = 1L;\n-\n-\t\tprivate final String[] fullFieldNames;\n-\t\tprivate final DataType[] fullFieldTypes;\n-\t\tprivate final int[] selectedFields;\n-\t\tprivate final String partDefaultName;\n-\t\tprivate final boolean utcTimestamp;\n-\t\tprivate final SerializableConfiguration conf;\n-\t\tprivate final long limit;\n-\n-\t\tprivate transient ParquetColumnarRowSplitReader reader;\n-\t\tprivate transient long currentReadCount;\n-\n-\t\tpublic ParquetInputFormat(\n-\t\t\t\tPath[] paths,\n-\t\t\t\tString[] fullFieldNames,\n-\t\t\t\tDataType[] fullFieldTypes,\n-\t\t\t\tint[] selectedFields,\n-\t\t\t\tString partDefaultName,\n-\t\t\t\tlong limit,\n-\t\t\t\tConfiguration conf,\n-\t\t\t\tboolean utcTimestamp) {\n-\t\t\tsuper.setFilePaths(paths);\n-\t\t\tthis.limit = limit;\n-\t\t\tthis.partDefaultName = partDefaultName;\n-\t\t\tthis.fullFieldNames = fullFieldNames;\n-\t\t\tthis.fullFieldTypes = fullFieldTypes;\n-\t\t\tthis.selectedFields = selectedFields;\n-\t\t\tthis.conf = new SerializableConfiguration(conf);\n-\t\t\tthis.utcTimestamp = utcTimestamp;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void open(FileInputSplit fileSplit) throws IOException {\n-\t\t\t// generate partition specs.\n-\t\t\tList<String> fieldNameList = Arrays.asList(fullFieldNames);\n-\t\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(\n-\t\t\t\t\tfileSplit.getPath());\n-\t\t\tLinkedHashMap<String, Object> partObjects = new LinkedHashMap<>();\n-\t\t\tpartSpec.forEach((k, v) -> partObjects.put(k, restorePartValueFromType(\n-\t\t\t\t\tpartDefaultName.equals(v) ? null : v,\n-\t\t\t\t\tfullFieldTypes[fieldNameList.indexOf(k)])));\n-\n-\t\t\tthis.reader = ParquetSplitReaderUtil.genPartColumnarRowReader(\n-\t\t\t\t\tutcTimestamp,\n-\t\t\t\t\ttrue,\n-\t\t\t\t\tconf.conf(),\n-\t\t\t\t\tfullFieldNames,\n-\t\t\t\t\tfullFieldTypes,\n-\t\t\t\t\tpartObjects,\n-\t\t\t\t\tselectedFields,\n-\t\t\t\t\tDEFAULT_SIZE,\n-\t\t\t\t\tnew Path(fileSplit.getPath().toString()),\n-\t\t\t\t\tfileSplit.getStart(),\n-\t\t\t\t\tfileSplit.getLength());\n-\t\t\tthis.currentReadCount = 0L;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean supportsMultiPaths() {\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean reachedEnd() throws IOException {\n-\t\t\tif (currentReadCount >= limit) {\n-\t\t\t\treturn true;\n-\t\t\t} else {\n-\t\t\t\treturn reader.reachedEnd();\n-\t\t\t}\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic RowData nextRecord(RowData reuse) {\n-\t\t\tcurrentReadCount++;\n-\t\t\treturn reader.nextRecord();\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void close() throws IOException {\n-\t\t\tif (reader != null) {\n-\t\t\t\tthis.reader.close();\n-\t\t\t}\n-\t\t\tthis.reader = null;\n-\t\t}\n+\tpublic Set<ConfigOption<?>> optionalOptions() {\n+\t\treturn new HashSet<>();", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMTM4MA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513201380", "bodyText": "We can't enumerate all of its options, so we can't validate them, so we simply don't add any", "author": "JingsongLi", "createdAt": "2020-10-28T06:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjgzOA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NDkzOA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513164938", "bodyText": "Remove this comment?", "author": "wuchong", "createdAt": "2020-10-28T03:53:08Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {\n+\t// interface is used for discovery but is already fully specified by the generics", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\nindex c299ecbbda7..b1da157a701 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\n\n@@ -36,7 +36,6 @@ public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowD\n \t/**\n \t * Creates a {@link BulkDecodingFormat} from the given context and format options.\n \t */\n-\t@Override\n \tBulkDecodingFormat<RowData> createDecodingFormat(\n \t\t\tDynamicTableFactory.Context context, ReadableConfig formatOptions);\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTM2OA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165368", "bodyText": "How about naming this BulkReaderFormatFactory which is more align with BulkWriterFormatFactory.", "author": "wuchong", "createdAt": "2020-10-28T03:54:47Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.ReadableConfig;\n+import org.apache.flink.connector.file.src.reader.BulkFormat;\n+import org.apache.flink.table.connector.format.BulkDecodingFormat;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkFormat} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowData>> {", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\nindex c299ecbbda7..b1da157a701 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkFormatFactory.java\n\n@@ -36,7 +36,6 @@ public interface BulkFormatFactory extends DecodingFormatFactory<BulkFormat<RowD\n \t/**\n \t * Creates a {@link BulkDecodingFormat} from the given context and format options.\n \t */\n-\t@Override\n \tBulkDecodingFormat<RowData> createDecodingFormat(\n \t\t\tDynamicTableFactory.Context context, ReadableConfig formatOptions);\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NTQ2OA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513165468", "bodyText": "How about naming this BulkWriterFormatFactory which is more align with BulkReaderFormatFactory.", "author": "wuchong", "createdAt": "2020-10-28T03:55:10Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link BulkWriter.Factory} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface BulkWriterFactory extends EncodingFormatFactory<BulkWriter.Factory<RowData>> {", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFormatFactory.java\nsimilarity index 92%\nrename from flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java\nrename to flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFormatFactory.java\nindex ddcd7027f44..90cc8772a1d 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFactory.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/BulkWriterFormatFactory.java\n\n@@ -28,6 +28,6 @@ import org.apache.flink.table.data.RowData;\n  * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n  */\n @Internal\n-public interface BulkWriterFactory extends EncodingFormatFactory<BulkWriter.Factory<RowData>> {\n+public interface BulkWriterFormatFactory extends EncodingFormatFactory<BulkWriter.Factory<RowData>> {\n \t// interface is used for discovery but is already fully specified by the generics\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NjIzMA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513166230", "bodyText": "Do we really need this factory? It seems duplicate with the SerializationSchema. I'm afraid we will introduce a lot of duplicate codes if we can't reuse existing SerializationFormatFactorys.", "author": "wuchong", "createdAt": "2020-10-28T03:58:34Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.factories;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.table.data.RowData;\n+\n+/**\n+ * Base interface for configuring a {@link Encoder} for file system connector.\n+ *\n+ * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ */\n+@Internal\n+public interface EncoderFactory extends EncodingFormatFactory<Encoder<RowData>> {", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE5OTYzNA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513199634", "bodyText": "I think you are right, we can try to wrap SerializationSchema and DeserializationSchema", "author": "JingsongLi", "createdAt": "2020-10-28T06:04:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NjIzMA=="}], "type": "inlineReview", "revised_code": {"commit": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java b/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/functions/Timestamper.java\nsimilarity index 60%\nrename from flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java\nrename to flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/functions/Timestamper.java\nindex 01a6450297a..bcb6f44cbb1 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/EncoderFactory.java\n+++ b/flink-libraries/flink-state-processing-api/src/main/java/org/apache/flink/state/api/functions/Timestamper.java\n\n@@ -16,18 +16,17 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.factories;\n+package org.apache.flink.state.api.functions;\n \n-import org.apache.flink.annotation.Internal;\n-import org.apache.flink.api.common.serialization.Encoder;\n-import org.apache.flink.table.data.RowData;\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.common.functions.Function;\n \n /**\n- * Base interface for configuring a {@link Encoder} for file system connector.\n- *\n- * @see FactoryUtil#createTableFactoryHelper(DynamicTableFactory, DynamicTableFactory.Context)\n+ * Assigns an event time timestamp to the given record. This class\n+ * does not create watermarks.\n  */\n-@Internal\n-public interface EncoderFactory extends EncodingFormatFactory<Encoder<RowData>> {\n-\t// interface is used for discovery but is already fully specified by the generics\n+@PublicEvolving\n+@FunctionalInterface\n+public interface Timestamper<T> extends Function {\n+\tlong timestamp(T element);\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513167654", "bodyText": "Why not reuse FactoryUtil.TableFactoryHelper#discoverOptionalEncodingFormat?", "author": "wuchong", "createdAt": "2020-10-28T04:04:30Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java", "diffHunk": "@@ -53,15 +62,75 @@\n \t\tcontext.getCatalogTable().getOptions().forEach(tableOptions::setString);\n \t\tthis.schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n \t\tthis.partitionKeys = context.getCatalogTable().getPartitionKeys();\n-\t\tthis.path = new Path(context.getCatalogTable().getOptions().getOrDefault(PATH.key(), PATH.defaultValue()));\n-\t\tthis.defaultPartName = context.getCatalogTable().getOptions().getOrDefault(\n-\t\t\t\tPARTITION_DEFAULT_NAME.key(), PARTITION_DEFAULT_NAME.defaultValue());\n+\t\tthis.path = new Path(tableOptions.get(PATH));\n+\t\tthis.defaultPartName = tableOptions.get(PARTITION_DEFAULT_NAME);\n \t}\n \n-\tstatic FileSystemFormatFactory createFormatFactory(ReadableConfig tableOptions) {\n+\tReadableConfig formatOptions(String identifier) {\n+\t\treturn new DelegatingConfiguration(tableOptions, identifier + \".\");\n+\t}\n+\n+\tFileSystemFormatFactory createFormatFactory() {\n \t\treturn FactoryUtil.discoverFactory(\n \t\t\t\tThread.currentThread().getContextClassLoader(),\n \t\t\t\tFileSystemFormatFactory.class,\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n+\n+\t@SuppressWarnings(\"rawtypes\")\n+\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDQ4Ng==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200486", "bodyText": "See below comments:\n\t/**\n\t * Unlike {@link FactoryUtil#discoverFactory}, it will not throw an exception if it cannot\n\t * find the factory.\n\t */\n\nDo you think we should modify FactoryUtil.TableFactoryHelper#discoverOptionalEncodingFormat?", "author": "JingsongLi", "createdAt": "2020-10-28T06:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIyMTYxNQ==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513221615", "bodyText": "I will move these logic to FileSystemTableFactory and use FactoryUtil to create formats.", "author": "JingsongLi", "createdAt": "2020-10-28T07:11:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NzY1NA=="}], "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java\nindex 4d7ad2de66a..8b6e4b99344 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/AbstractFileSystemTable.java\n\n@@ -77,47 +75,22 @@ abstract class AbstractFileSystemTable {\n \t\t\t\ttableOptions.get(FactoryUtil.FORMAT));\n \t}\n \n-\t@SuppressWarnings(\"rawtypes\")\n-\t<F extends EncodingFormatFactory<?>> Optional<EncodingFormat> discoverOptionalEncodingFormat(\n-\t\t\tClass<F> formatFactoryClass) {\n-\t\treturn discoverOptionalFormatFactory(formatFactoryClass)\n-\t\t\t\t.map(formatFactory -> {\n-\t\t\t\t\tString format = formatFactory.factoryIdentifier();\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\treturn formatFactory.createEncodingFormat(context, formatOptions(format));\n-\t\t\t\t\t} catch (Throwable t) {\n-\t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\n-\t\t\t\t\t\t\t\t\t\t\"Error creating sink format '%s' in option space '%s'.\",\n-\t\t\t\t\t\t\t\t\t\tformatFactory.factoryIdentifier(),\n-\t\t\t\t\t\t\t\t\t\tformat),\n-\t\t\t\t\t\t\t\tt);\n-\t\t\t\t\t}\n-\t\t\t\t});\n+\t@SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+\tOptional<BulkFormatFactory> createBulkFormatFactory() {\n+\t\treturn tryCreateFormatFactory(BulkFormatFactory.class);\n \t}\n \n-\tOptional<BulkDecodingFormat<RowData>> discoverBulkDecodingFormat() {\n-\t\treturn discoverOptionalFormatFactory(BulkFormatFactory.class)\n-\t\t\t\t.map(formatFactory -> {\n-\t\t\t\t\tString format = formatFactory.factoryIdentifier();\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\treturn formatFactory.createDecodingFormat(context, formatOptions(format));\n-\t\t\t\t\t} catch (Throwable t) {\n-\t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\n-\t\t\t\t\t\t\t\t\t\t\"Error creating scan format '%s' in option space '%s'.\",\n-\t\t\t\t\t\t\t\t\t\tformatFactory.factoryIdentifier(),\n-\t\t\t\t\t\t\t\t\t\tformat),\n-\t\t\t\t\t\t\t\tt);\n-\t\t\t\t\t}\n-\t\t\t\t});\n+\t@SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+\tOptional<BulkWriterFactory> createBulkWriterFactory() {\n+\t\treturn tryCreateFormatFactory(BulkWriterFactory.class);\n \t}\n \n-\t/**\n-\t * Unlike {@link FactoryUtil#discoverFactory}, it will not throw an exception if it cannot\n-\t * find the factory.\n-\t */\n-\tprivate <T extends Factory> Optional<T> discoverOptionalFormatFactory(Class<T> clazz) {\n+\t@SuppressWarnings({\"unchecked\", \"rawtypes\"})\n+\tOptional<EncoderFactory> createEncoderFactory() {\n+\t\treturn tryCreateFormatFactory(EncoderFactory.class);\n+\t}\n+\n+\t<T extends Factory> Optional<T> tryCreateFormatFactory(Class<T> clazz) {\n \t\tString format = tableOptions.get(FactoryUtil.FORMAT);\n \t\tif (format == null) {\n \t\t\tthrow new ValidationException(String.format(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MTE2NQ==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513171165", "bodyText": "Why this is still needed? Do we need to migrate all the formats to use EncodingFormatFactory and DecodingFormatFactory before we can remove these code?  If yes, could you create an issue for that?", "author": "wuchong", "createdAt": "2020-10-28T04:18:55Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java", "diffHunk": "@@ -199,15 +202,32 @@ private Path toStagingPath() {\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n-\tprivate OutputFormatFactory<RowData> createOutputFormatFactory() {\n-\t\tObject writer = createWriter();\n+\tprivate OutputFormatFactory<RowData> createOutputFormatFactory(Context sinkContext) {\n+\t\tObject writer = createWriter(sinkContext);\n \t\treturn writer instanceof Encoder ?\n \t\t\t\tpath -> createEncoderOutputFormat((Encoder<RowData>) writer, path) :\n \t\t\t\tpath -> createBulkWriterOutputFormat((BulkWriter.Factory<RowData>) writer, path);\n \t}\n \n-\tprivate Object createWriter() {\n-\t\tFileSystemFormatFactory formatFactory = createFormatFactory(tableOptions);\n+\tprivate DataType getFormatDataType() {\n+\t\tTableSchema.Builder builder = TableSchema.builder();\n+\t\tschema.getTableColumns().forEach(column -> {\n+\t\t\tif (!partitionKeys.contains(column.getName())) {\n+\t\t\t\tbuilder.add(column);\n+\t\t\t}\n+\t\t});\n+\t\treturn builder.build().toRowDataType();\n+\t}\n+\n+\tprivate Object createWriter(Context sinkContext) {\n+\t\t@SuppressWarnings(\"rawtypes\")\n+\t\tOptional<EncodingFormat> encodingFormat = discoverOptionalEncodingFormat(BulkWriterFactory.class)\n+\t\t\t\t.map(Optional::of).orElseGet(() -> discoverOptionalEncodingFormat(EncoderFactory.class));\n+\t\tif (encodingFormat.isPresent()) {\n+\t\t\treturn encodingFormat.get().createRuntimeEncoder(sinkContext, getFormatDataType());\n+\t\t}\n+\n+\t\tFileSystemFormatFactory formatFactory = createFormatFactory();", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDIxMA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200210", "bodyText": "https://issues.apache.org/jira/browse/FLINK-19845", "author": "JingsongLi", "createdAt": "2020-10-28T06:07:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MTE2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java\nindex 0d176b853a2..bff208aac6c 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSink.java\n\n@@ -209,6 +208,13 @@ public class FileSystemTableSink extends AbstractFileSystemTable implements\n \t\t\t\tpath -> createBulkWriterOutputFormat((BulkWriter.Factory<RowData>) writer, path);\n \t}\n \n+\tprivate <I> I createEncoder(EncodingFormatFactory<I> factory, Context sinkContext) {\n+\t\tReadableConfig formatOptions = formatOptions(factory.factoryIdentifier());\n+\t\treturn factory\n+\t\t\t\t.createEncodingFormat(context, formatOptions)\n+\t\t\t\t.createRuntimeEncoder(sinkContext, getFormatDataType());\n+\t}\n+\n \tprivate DataType getFormatDataType() {\n \t\tTableSchema.Builder builder = TableSchema.builder();\n \t\tschema.getTableColumns().forEach(column -> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513173969", "bodyText": "Why we should avoid using ContinuousFileMonitoringFunction here? and why not return SourceFunctionProvider of InputFormatSourceFunction directly?", "author": "wuchong", "createdAt": "2020-10-28T04:30:04Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -88,7 +92,16 @@ private FileSystemTableSource(\n \t}\n \n \t@Override\n-\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext runtimeProviderContext) {\n+\tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n+\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\n+\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n+\t\t\t// When this table has no partition, just return a empty source.\n+\t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n+\t\t} else if (bulkDecodingFormat.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t}\n+\n \t\treturn new DataStreamScanProvider() {\n \t\t\t@Override\n \t\t\tpublic DataStream<RowData> produceDataStream(StreamExecutionEnvironment execEnv) {", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDg5Nw==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513200897", "bodyText": "The ContinuousFileMonitoringFunction can not accept multiple paths. Default StreamEnv.createInput will create continuous function.", "author": "JingsongLi", "createdAt": "2020-10-28T06:09:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNTU2Mw==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513205563", "bodyText": "Then can we return SourceFunctionProvider.of(InputFormatSourceFunction) instead of DataStreamScanProvider here?", "author": "wuchong", "createdAt": "2020-10-28T06:25:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNzM5NA==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513207394", "bodyText": "Indeed, we can!", "author": "JingsongLi", "createdAt": "2020-10-28T06:30:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3Mzk2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\nindex 8e553fccb28..975d081e338 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\n\n@@ -93,13 +94,13 @@ public class FileSystemTableSource extends AbstractFileSystemTable implements\n \n \t@Override\n \tpublic ScanRuntimeProvider getScanRuntimeProvider(ScanContext scanContext) {\n-\t\tOptional<BulkDecodingFormat<RowData>> bulkDecodingFormat = discoverBulkDecodingFormat();\n+\t\tOptional<BulkFormatFactory> bulkFormatOptional = createBulkFormatFactory();\n \n \t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n \t\t\t// When this table has no partition, just return a empty source.\n \t\t\treturn InputFormatProvider.of(new CollectionInputFormat<>(new ArrayList<>(), null));\n-\t\t} else if (bulkDecodingFormat.isPresent()) {\n-\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkDecodingFormat.get(), scanContext));\n+\t\t} else if (bulkFormatOptional.isPresent()) {\n+\t\t\treturn SourceProvider.of(createBulkFormatSource(bulkFormatOptional.get(), scanContext));\n \t\t}\n \n \t\treturn new DataStreamScanProvider() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513174783", "bodyText": "I think Long.MAX_VALUE can't represent no limit, right?", "author": "wuchong", "createdAt": "2020-10-28T04:33:04Z", "path": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java", "diffHunk": "@@ -108,13 +121,39 @@ public boolean isBounded() {\n \t\t};\n \t}\n \n-\tprivate InputFormat<RowData, ?> getInputFormat() {\n-\t\t// When this table has no partition, just return a empty source.\n-\t\tif (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n-\t\t\treturn new CollectionInputFormat<>(new ArrayList<>(), null);\n+\tprivate FileSource<RowData> createBulkFormatSource(\n+\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n+\t\tdecodingFormat.applyLimit(pushedDownLimit());\n+\t\tdecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n+\t\t\t\tscanContext, getProducedDataType());\n+\t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n+\t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n+\t\treturn builder.build();\n+\t}\n+\n+\tprivate Path[] paths() {\n+\t\tif (partitionKeys.isEmpty()) {\n+\t\t\treturn new Path[] {path};\n+\t\t} else {\n+\t\t\treturn getOrFetchPartitions().stream()\n+\t\t\t\t\t.map(FileSystemTableSource.this::toFullLinkedPartSpec)\n+\t\t\t\t\t.map(PartitionPathUtils::generatePartitionPath)\n+\t\t\t\t\t.map(n -> new Path(path, n))\n+\t\t\t\t\t.toArray(Path[]::new);\n \t\t}\n+\t}\n+\n+\tprivate long pushedDownLimit() {\n+\t\treturn limit == null ? Long.MAX_VALUE : limit;", "originalCommit": "05116d4768052821a37adfa725e60e40f4f71176", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMTA5Mg==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513201092", "bodyText": "We can keep null value", "author": "JingsongLi", "createdAt": "2020-10-28T06:10:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwODYxOQ==", "url": "https://github.com/apache/flink/pull/13605#discussion_r513208619", "bodyText": "We can do something like this:\n\t\tif (limit != null) {\n\t\t\tdecodingFormat.applyLimit(limit);\n\t\t}\n\t\tif (filters != null && filters.size() > 0) {\n\t\t\tdecodingFormat.applyFilters(filters);\n\t\t}", "author": "JingsongLi", "createdAt": "2020-10-28T06:35:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3NDc4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "chunk": "diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\nindex 8e553fccb28..975d081e338 100644\n--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\n+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemTableSource.java\n\n@@ -122,11 +123,12 @@ public class FileSystemTableSource extends AbstractFileSystemTable implements\n \t}\n \n \tprivate FileSource<RowData> createBulkFormatSource(\n-\t\t\tBulkDecodingFormat<RowData> decodingFormat, ScanContext scanContext) {\n-\t\tdecodingFormat.applyLimit(pushedDownLimit());\n-\t\tdecodingFormat.applyFilters(pushedDownFilters());\n-\t\tBulkFormat<RowData> bulkFormat = decodingFormat.createRuntimeDecoder(\n-\t\t\t\tscanContext, getProducedDataType());\n+\t\t\tBulkFormatFactory bulkFormatFactory, ScanContext scanContext) {\n+\t\tReadableConfig formatOptions = formatOptions(bulkFormatFactory.factoryIdentifier());\n+\t\tBulkDecodingFormat<RowData> bulkDecodingFormat = bulkFormatFactory.createDecodingFormat(context, formatOptions);\n+\t\tbulkDecodingFormat.applyLimit(pushedDownLimit());\n+\t\tbulkDecodingFormat.applyFilters(pushedDownFilters());\n+\t\tBulkFormat<RowData> bulkFormat = bulkDecodingFormat.createRuntimeDecoder(scanContext, getProducedDataType());\n \t\tFileSource.FileSourceBuilder<RowData> builder = FileSource\n \t\t\t\t.forBulkFileFormat(bulkFormat, paths());\n \t\treturn builder.build();\n"}}, {"oid": "b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "url": "https://github.com/apache/flink/commit/b56cf8831300d22fa4777f3f65dd6236ca39f8b5", "message": "[FLINK-19599][table] Introduce Filesystem format factories to integrate new FileSource to table", "committedDate": "2020-10-28T07:14:55Z", "type": "commit"}, {"oid": "6fdf4fe5293ba62a0dd946bfd050b19e575799f6", "url": "https://github.com/apache/flink/commit/6fdf4fe5293ba62a0dd946bfd050b19e575799f6", "message": "Refactor create format factory", "committedDate": "2020-10-28T07:14:55Z", "type": "commit"}, {"oid": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "url": "https://github.com/apache/flink/commit/61fed9445fbd820423b73ef520d5f9ffc9e0606d", "message": "Address comments", "committedDate": "2020-10-28T07:14:56Z", "type": "commit"}, {"oid": "61fed9445fbd820423b73ef520d5f9ffc9e0606d", "url": "https://github.com/apache/flink/commit/61fed9445fbd820423b73ef520d5f9ffc9e0606d", "message": "Address comments", "committedDate": "2020-10-28T07:14:56Z", "type": "forcePushed"}, {"oid": "47e39b41aa88aa9887046123bcc561dad3340374", "url": "https://github.com/apache/flink/commit/47e39b41aa88aa9887046123bcc561dad3340374", "message": "Pass SerializationSchemaFormat into source/sink", "committedDate": "2020-10-28T08:02:46Z", "type": "commit"}, {"oid": "fa0dc913f56aa2567c30073c044f688f9ed74fee", "url": "https://github.com/apache/flink/commit/fa0dc913f56aa2567c30073c044f688f9ed74fee", "message": "Fix exceptions", "committedDate": "2020-10-28T08:21:48Z", "type": "commit"}, {"oid": "96468e31f7614ef314c655f97a63fcabe83505a8", "url": "https://github.com/apache/flink/commit/96468e31f7614ef314c655f97a63fcabe83505a8", "message": "checkstyle", "committedDate": "2020-10-28T11:14:39Z", "type": "commit"}, {"oid": "5ef88c8f400b6c2ef5f5ca6bbd750c17f27137c7", "url": "https://github.com/apache/flink/commit/5ef88c8f400b6c2ef5f5ca6bbd750c17f27137c7", "message": "Move TestCsvFileSystemFormatFactory to table runtime", "committedDate": "2020-10-29T02:38:32Z", "type": "commit"}]}