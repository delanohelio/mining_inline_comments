{"pr_number": 13631, "pr_title": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushD\u2026", "pr_createdAt": "2020-10-14T10:39:52Z", "pr_url": "https://github.com/apache/flink/pull/13631", "timeline": [{"oid": "cb66dc97754abc1468b03508a41a4f3a34ce9572", "url": "https://github.com/apache/flink/commit/cb66dc97754abc1468b03508a41a4f3a34ce9572", "message": "[FLINK-19693][table sql/planner]Support SupportsNestedProjectionPushDown in planner", "committedDate": "2020-10-14T11:39:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQxODI3OA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r505418278", "bodyText": "nit: add a test about complex expressions, such as  deepNested.nested1.name + nested.value", "author": "godfreyhe", "createdAt": "2020-10-15T10:00:29Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -107,4 +96,13 @@ private void testNestedProject(boolean nestedProjectionSupported) {\n \t\tutil().verifyPlan(sqlQuery);\n \t}\n \n+\t@Test\n+\tpublic void testComplicatedNestedProject() {\n+\t\tString sqlQuery = \"SELECT id,\" +\n+\t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n+\t\t\t\t\"    deepNested.nested2 AS nested2,\\n\" +\n+\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"FROM NestedTable\";\n+\t\tutil().verifyPlan(sqlQuery);", "originalCommit": "cb66dc97754abc1468b03508a41a4f3a34ce9572", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjAzNzEyOA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506037128", "bodyText": "Add test with calculation in query.", "author": "fsk119", "createdAt": "2020-10-16T03:59:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQxODI3OA=="}], "type": "inlineReview", "revised_code": {"commit": "4b3b9898ceeebc31777e2aa96d04b445cf578d87", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\nindex 6f3044aba0d..91a28f66c6d 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n\n@@ -89,9 +89,9 @@ public class PushProjectIntoTableSourceScanRuleTest extends PushProjectIntoLegac\n \tpublic void testNestedProject() {\n \t\tString sqlQuery = \"SELECT id,\\n\" +\n \t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n-\t\t\t\t\"    nested.`value` AS nestedValue,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.flag AS nestedFlag,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"    nested.`value.` AS nestedValue,\\n\" +\n+\t\t\t\t\"    deepNested.`nested2.`.flag AS nestedFlag,\\n\" +\n+\t\t\t\t\"    deepNested.`nested2.`.num AS nestedNum\\n\" +\n \t\t\t\t\"FROM NestedTable\";\n \t\tutil().verifyPlan(sqlQuery);\n \t}\n"}}, {"oid": "4b3b9898ceeebc31777e2aa96d04b445cf578d87", "url": "https://github.com/apache/flink/commit/4b3b9898ceeebc31777e2aa96d04b445cf578d87", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests", "committedDate": "2020-10-16T03:56:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxNzM5Ng==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506917396", "bodyText": "how about resolve the conflicts through adding postfix ?", "author": "godfreyhe", "createdAt": "2020-10-17T09:53:50Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java", "diffHunk": "@@ -75,10 +79,46 @@ public static TableSchema projectSchema(TableSchema tableSchema, int[][] project\n \t\tcheckArgument(containsPhysicalColumnsOnly(tableSchema), \"Projection is only supported for physical columns.\");\n \t\tTableSchema.Builder schemaBuilder = TableSchema.builder();\n \t\tList<TableColumn> tableColumns = tableSchema.getTableColumns();\n+\t\tMap<String, String> nameDomain = new HashMap<>();\n+\t\tString exceptionTemplate = \"Get name conflicts for origin fields %s and %s with new name `%s`. \" +\n+\t\t\t\t\"When pushing projection into scan, we will concatenate top level names with delimiter '_'. \" +\n+\t\t\t\t\"Please rename the origin field names when creating table.\";\n+\t\tString originFullyQualifiedName;\n+\t\tString newName;\n \t\tfor (int[] fieldPath : projectedFields) {\n-\t\t\tcheckArgument(fieldPath.length == 1, \"Nested projection push down is not supported yet.\");\n-\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n-\t\t\tschemaBuilder.field(column.getName(), column.getType());\n+\t\t\tif (fieldPath.length == 1) {\n+\t\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n+\t\t\t\tnewName = column.getName();\n+\t\t\t\toriginFullyQualifiedName = String.format(\"`%s`\", column.getName());\n+\t\t\t\tif (nameDomain.containsKey(column.getName())) {\n+\t\t\t\t\tthrow new TableException(", "originalCommit": "d1b917bb33f837bf04e8351affe7e108911e79b4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a7fe7dec5d7c869ccbc644bba45df241dd05e953", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java\nindex 36110ecbbad..01f8d665c6d 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/TableSchemaUtils.java\n\n@@ -73,54 +71,23 @@ public class TableSchemaUtils {\n \t * Creates a new {@link TableSchema} with the projected fields from another {@link TableSchema}.\n \t * The new {@link TableSchema} doesn't contain any primary key or watermark information.\n \t *\n+\t * <p>When extracting the fields from the origin schema, the fields may get name conflicts in the\n+\t * new schema. Considering that the path to the fields is unique in schema, use the path as the\n+\t * new name to resolve the name conflicts in the new schema. If name conflicts still exists, it\n+\t * will add postfix in the fashion \"$%d\" to resolve.\n+\t *\n \t * @see org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown\n \t */\n \tpublic static TableSchema projectSchema(TableSchema tableSchema, int[][] projectedFields) {\n \t\tcheckArgument(containsPhysicalColumnsOnly(tableSchema), \"Projection is only supported for physical columns.\");\n-\t\tTableSchema.Builder schemaBuilder = TableSchema.builder();\n-\t\tList<TableColumn> tableColumns = tableSchema.getTableColumns();\n-\t\tMap<String, String> nameDomain = new HashMap<>();\n-\t\tString exceptionTemplate = \"Get name conflicts for origin fields %s and %s with new name `%s`. \" +\n-\t\t\t\t\"When pushing projection into scan, we will concatenate top level names with delimiter '_'. \" +\n-\t\t\t\t\"Please rename the origin field names when creating table.\";\n-\t\tString originFullyQualifiedName;\n-\t\tString newName;\n-\t\tfor (int[] fieldPath : projectedFields) {\n-\t\t\tif (fieldPath.length == 1) {\n-\t\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n-\t\t\t\tnewName = column.getName();\n-\t\t\t\toriginFullyQualifiedName = String.format(\"`%s`\", column.getName());\n-\t\t\t\tif (nameDomain.containsKey(column.getName())) {\n-\t\t\t\t\tthrow new TableException(\n-\t\t\t\t\t\t\tString.format(exceptionTemplate,\n-\t\t\t\t\t\t\t\t\tnameDomain.get(newName), originFullyQualifiedName, newName));\n-\t\t\t\t}\n-\t\t\t\tschemaBuilder.field(newName, column.getType());\n-\t\t\t\tnameDomain.put(newName, originFullyQualifiedName);\n-\t\t\t} else {\n-\t\t\t\tTableColumn column = tableColumns.get(fieldPath[0]);\n-\t\t\t\tDataType dataType = column.getType();\n-\t\t\t\tStringBuilder nameBuilder = new StringBuilder(column.getName());\n-\t\t\t\tStringBuilder originQualifiedNameBuilder = new StringBuilder(\"`\" + column.getName() + \"`\");\n-\t\t\t\tfor (int i = 1; i < fieldPath.length; i++) {\n-\t\t\t\t\tRowType rowType = (RowType) dataType.getLogicalType();\n-\t\t\t\t\tnameBuilder.append('_').append(rowType.getFieldNames().get(fieldPath[i]));\n-\t\t\t\t\toriginQualifiedNameBuilder.append(\".`\")\n-\t\t\t\t\t\t\t.append(rowType.getFieldNames().get(fieldPath[i]))\n-\t\t\t\t\t\t\t.append(\"`\");\n-\t\t\t\t\tdataType = dataType.getChildren().get(fieldPath[i]);\n-\t\t\t\t}\n-\t\t\t\toriginFullyQualifiedName = originQualifiedNameBuilder.toString();\n-\t\t\t\tnewName = nameBuilder.toString();\n-\t\t\t\tif (nameDomain.containsKey(newName)) {\n-\t\t\t\t\tthrow new TableException(\n-\t\t\t\t\t\t\tString.format(exceptionTemplate, originFullyQualifiedName, nameDomain.get(newName), newName));\n-\t\t\t\t}\n-\t\t\t\tschemaBuilder.field(newName, dataType);\n-\t\t\t\tnameDomain.put(newName, originFullyQualifiedName);\n-\t\t\t}\n+\t\tTableSchema.Builder builder = TableSchema.builder();\n+\n+\t\tFieldsDataType fields = (FieldsDataType) DataTypeUtils.projectRow(tableSchema.toRowDataType(), projectedFields);\n+\t\tRowType topFields = (RowType) fields.getLogicalType();\n+\t\tfor (int i = 0; i < topFields.getFieldCount(); i++) {\n+\t\t\tbuilder.field(topFields.getFieldNames().get(i), fields.getChildren().get(i));\n \t\t}\n-\t\treturn schemaBuilder.build();\n+\t\treturn builder.build();\n \t}\n \n \t/**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjkxOTYyNQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r506919625", "bodyText": "how about keep the original test, and add a new test case the verify the case which field name contain dot", "author": "godfreyhe", "createdAt": "2020-10-17T09:59:09Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,41 +69,40 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n \n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n-\n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n-\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n-\t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, `nested2.` row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  nested row<name string, `value.` int>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n \t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\t}\n \n+\t@Override\n+\t@Test\n+\tpublic void testNestedProject() {\n \t\tString sqlQuery = \"SELECT id,\\n\" +\n \t\t\t\t\"    deepNested.nested1.name AS nestedName,\\n\" +\n-\t\t\t\t\"    nested.`value` AS nestedValue,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.flag AS nestedFlag,\\n\" +\n-\t\t\t\t\"    deepNested.nested2.num AS nestedNum\\n\" +\n+\t\t\t\t\"    nested.`value.` AS nestedValue,\\n\" +\n+\t\t\t\t\"    deepNested.`nested2.`.flag AS nestedFlag,\\n\" +", "originalCommit": "d1b917bb33f837bf04e8351affe7e108911e79b4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a7fe7dec5d7c869ccbc644bba45df241dd05e953", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\nindex 91a28f66c6d..a2a90e98eb3 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n\n@@ -73,15 +73,30 @@ public class PushProjectIntoTableSourceScanRuleTest extends PushProjectIntoLegac\n \t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n-\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, `nested2.` row<num int, flag boolean>>,\\n\" +\n-\t\t\t\t\t\t\"  nested row<name string, `value.` int>,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  `deepNestedWith.` row<`.value` int, nested row<name string, `.value` int>>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n \t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n-\t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl3);\n+\n+\t\tString ddl4 =\n+\t\t\t\t\"CREATE TABLE MetadataTable(\\n\" +\n+\t\t\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  metadata_1 int metadata,\\n\" +\n+\t\t\t\t\t\t\"  metadata_2 string metadata\\n\" +\n+\t\t\t\t\t\t\") WITH (\" +\n+\t\t\t\t\t\t\" 'connector' = 'values',\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\t\t\" 'readable-metadata' = 'metadata_1:INT, metadata_2:STRING, metadata_3:BIGINT'\" +\n+\t\t\t\t\t\t\")\";\n+\t\tutil().tableEnv().executeSql(ddl4);\n \t}\n \n \t@Override\n"}}, {"oid": "a7fe7dec5d7c869ccbc644bba45df241dd05e953", "url": "https://github.com/apache/flink/commit/a7fe7dec5d7c869ccbc644bba45df241dd05e953", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection.", "committedDate": "2020-10-26T02:44:09Z", "type": "forcePushed"}, {"oid": "1697a20356799dd5e2f8863110de1f4626664f12", "url": "https://github.com/apache/flink/commit/1697a20356799dd5e2f8863110de1f4626664f12", "message": "fix test", "committedDate": "2020-10-28T13:40:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIwNDU2MA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514204560", "bodyText": "nit: how about adding  _ before \"$\"", "author": "godfreyhe", "createdAt": "2020-10-29T12:00:49Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java", "diffHunk": "@@ -74,42 +76,46 @@\n \t *\n \t * <p>Note: Index paths allow for arbitrary deep nesting. For example, {@code [[0, 2, 1], ...]}\n \t * specifies to include the 2nd field of the 3rd field of the 1st field in the top-level row.\n+\t * Sometimes, it may get name conflicts when extract fields from the row field. Considering the\n+\t * the path is unique to extract fields, it makes sense to use the path to the fields with\n+\t * delimiter `_` as the new name of the field. For example, the new name of the field `b` in\n+\t * the row `a` is `a_b` rather than `b`. But it may still gets name conflicts in some situation,\n+\t * such as the field `a_b` in the top level schema. In such situation, it will use the postfix\n+\t * in the format '$%d' to resolve the name conflicts.\n \t */\n \tpublic static DataType projectRow(DataType dataType, int[][] indexPaths) {\n \t\tfinal List<RowField> updatedFields = new ArrayList<>();\n \t\tfinal List<DataType> updatedChildren = new ArrayList<>();\n+\t\tSet<String> nameDomain = new HashSet<>();\n+\t\tint duplicateCount = 0;\n \t\tfor (int[] indexPath : indexPaths) {\n-\t\t\tupdatedFields.add(selectChild(dataType.getLogicalType(), indexPath, 0));\n-\t\t\tupdatedChildren.add(selectChild(dataType, indexPath, 0));\n+\t\t\tDataType fieldType = dataType.getChildren().get(indexPath[0]);\n+\t\t\tLogicalType fieldLogicalType = fieldType.getLogicalType();\n+\t\t\tStringBuilder builder =\n+\t\t\t\t\tnew StringBuilder(((RowType) dataType.getLogicalType()).getFieldNames().get(indexPath[0]));\n+\t\t\tfor (int index = 1; index < indexPath.length; index++) {\n+\t\t\t\tPreconditions.checkArgument(\n+\t\t\t\t\t\thasRoot(fieldLogicalType, LogicalTypeRoot.ROW),\n+\t\t\t\t\t\t\"Row data type expected.\");\n+\t\t\t\tRowType rowtype = ((RowType) fieldLogicalType);\n+\t\t\t\tbuilder.append(\"_\").append(rowtype.getFieldNames().get(indexPath[index]));\n+\t\t\t\tfieldLogicalType = rowtype.getFields().get(indexPath[index]).getType();\n+\t\t\t\tfieldType = fieldType.getChildren().get(indexPath[index]);\n+\t\t\t}\n+\t\t\tString path = builder.toString();\n+\t\t\twhile (nameDomain.contains(path)) {\n+\t\t\t\tpath = builder.append(\"$\").append(duplicateCount++).toString();", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java\nindex 45a5404dbb2..427e2e54049 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/DataTypeUtils.java\n\n@@ -81,7 +81,7 @@ public final class DataTypeUtils {\n \t * delimiter `_` as the new name of the field. For example, the new name of the field `b` in\n \t * the row `a` is `a_b` rather than `b`. But it may still gets name conflicts in some situation,\n \t * such as the field `a_b` in the top level schema. In such situation, it will use the postfix\n-\t * in the format '$%d' to resolve the name conflicts.\n+\t * in the format '_$%d' to resolve the name conflicts.\n \t */\n \tpublic static DataType projectRow(DataType dataType, int[][] indexPaths) {\n \t\tfinal List<RowField> updatedFields = new ArrayList<>();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMDQ3OQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514220479", "bodyText": "what if the metadata columns have nested fields ?", "author": "godfreyhe", "createdAt": "2020-10-29T12:30:03Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -109,73 +112,67 @@ public void onMatch(RelOptRuleCall call) {\n \t\t\tusedFields = refFields;\n \t\t}\n \t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tif (!supportsNestedProjection && usedFields.length == fieldCount) {\n \t\t\treturn;\n \t\t}\n \n-\t\tfinal List<String> projectedFieldNames = IntStream.of(usedFields)\n-\t\t\t.mapToObj(fieldNames::get)\n-\t\t\t.collect(Collectors.toList());\n-\n \t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n \t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n \t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n \t\tfinal int physicalFieldCount = fieldCount - metadataKeys.size();\n \t\tfinal DynamicTableSource newSource = oldSource.copy();\n \n-\t\t// remove metadata columns from the projection push down and store it in a separate list\n-\t\t// the projection push down itself happens purely on physical columns\n-\t\tfinal int[] usedPhysicalFields;\n-\t\tfinal List<String> usedMetadataKeys;\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tusedPhysicalFields = IntStream.of(usedFields)\n-\t\t\t\t// select only physical columns\n-\t\t\t\t.filter(i -> i < physicalFieldCount)\n-\t\t\t\t.toArray();\n-\t\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(fieldCount - i - 1))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t\t// order the keys according to the source's declaration\n-\t\t\tusedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> usedFieldsCoordinates = new ArrayList<>();\n+\t\tfinal Map<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder = new HashMap<>();\n+\n+\t\tif (supportsNestedProjection) {\n+\t\t\tgetCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n+\t\t\t\t\tproject, oldSchema, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder);\n \t\t} else {\n-\t\t\tusedPhysicalFields = usedFields;\n-\t\t\tusedMetadataKeys = Collections.emptyList();\n+\t\t\tfor (int usedField : usedFields) {\n+\t\t\t\t// filter metadata columns", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg4NzkzNg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514887936", "bodyText": "add TODO.\nHere we only project the top level of the fields.", "author": "fsk119", "createdAt": "2020-10-30T06:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyMDQ3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex 4beaff0f00f..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -87,92 +84,106 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n+\t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n \t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n+\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n \n \t\tfinal boolean supportsNestedProjection =\n \t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n-\t\tfinal boolean supportsReadingMetaData = oldTableSourceTable.tableSource() instanceof SupportsReadingMetadata;\n-\n-\t\tfinal List<String> fieldNames = scan.getRowType().getFieldNames();\n-\t\tfinal int fieldCount = fieldNames.size();\n+\t\tList<String> fieldNames = scan.getRowType().getFieldNames();\n \n-\t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n-\t\tfinal int[] usedFields;\n+\t\tif (!supportsNestedProjection && refFields.length == fieldNames.size()) {\n+\t\t\t// just keep as same as the old plan\n+\t\t\t// TODO: refactor the affected plan\n+\t\t\treturn;\n+\t\t}\n \n+\t\tList<RexNode> oldProjectsWithPK = new ArrayList<>(project.getProjects());\n+\t\tFlinkTypeFactory flinkTypeFactory = (FlinkTypeFactory) oldTableSourceTable.getRelOptSchema().getTypeFactory();\n \t\tif (isUpsertSource(oldTableSourceTable)) {\n-\t\t\t// primary key fields are needed for upsert source\n-\t\t\tList<String> keyFields = oldTableSourceTable.catalogTable().getSchema()\n-\t\t\t\t.getPrimaryKey().get().getColumns();\n-\t\t\t// we should get source fields from scan node instead of CatalogTable,\n-\t\t\t// because projection may have been pushed down\n-\t\t\tList<String> sourceFields = scan.getRowType().getFieldNames();\n-\t\t\tint[] primaryKey = ScanUtil.getPrimaryKeyIndices(sourceFields, keyFields);\n-\t\t\tusedFields = mergeFields(refFields, primaryKey);\n-\t\t} else {\n-\t\t\tusedFields = refFields;\n+\t\t\t// add pk into projects\n+\t\t\toldSchema.getPrimaryKey().ifPresent(\n+\t\t\t\t\tpks -> {\n+\t\t\t\t\t\tfor (String name: pks.getColumns()) {\n+\t\t\t\t\t\t\tint index = fieldNames.indexOf(name);\n+\t\t\t\t\t\t\tTableColumn col = oldSchema.getTableColumn(index).get();\n+\t\t\t\t\t\t\toldProjectsWithPK.add(\n+\t\t\t\t\t\t\t\t\tnew RexInputRef(index,\n+\t\t\t\t\t\t\t\t\t\t\tflinkTypeFactory.createFieldTypeFromLogicalType(col.getType().getLogicalType())));\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t);\n \t\t}\n-\t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (!supportsNestedProjection && usedFields.length == fieldCount) {\n-\t\t\treturn;\n+\t\t// build used schema tree\n+\t\tRowType originType =\n+\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource);\n+\t\tRexNodeNestedField root = RexNodeNestedField.build(\n+\t\t\t\toldProjectsWithPK, flinkTypeFactory.buildRelNodeRowType(originType));\n+\t\tif (!supportsNestedProjection) {\n+\t\t\t// mark the fields in the top level as useall\n+\t\t\tfor (RexNodeNestedField child: root.fields().values()) {\n+\t\t\t\tchild.useAll_$eq(true);\n+\t\t\t}\n \t\t}\n-\n-\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n-\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n-\t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n-\t\tfinal int physicalFieldCount = fieldCount - metadataKeys.size();\n \t\tfinal DynamicTableSource newSource = oldSource.copy();\n-\n-\t\tfinal List<List<Integer>> usedFieldsCoordinates = new ArrayList<>();\n-\t\tfinal Map<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder = new HashMap<>();\n-\n-\t\tif (supportsNestedProjection) {\n-\t\t\tgetCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n-\t\t\t\t\tproject, oldSchema, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder);\n-\t\t} else {\n-\t\t\tfor (int usedField : usedFields) {\n-\t\t\t\t// filter metadata columns\n-\t\t\t\tif (usedField >= physicalFieldCount) {\n-\t\t\t\t\tcontinue;\n+\t\tfinal int[][] projectedFields;\n+\t\tfinal DataType newProducedDataType;\n+\t\tDataType producedDataType = TypeConversions.fromLogicalToDataType(originType);\n+\n+\t\tif (oldSource instanceof SupportsReadingMetadata) {\n+\t\t\t//TODO: supports nested projection for metadata\n+\t\t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n+\t\t\tList<RexNodeNestedField> usedMetaDataFields = new LinkedList<>();\n+\t\t\tint physicalCount = fieldNames.size() - metadataKeys.size();\n+\t\t\t// rm metadata in the tree\n+\t\t\tfor (int i = 0; i < metadataKeys.size(); i++) {\n+\t\t\t\tfinal RexInputRef key = new RexInputRef(i + physicalCount,\n+\t\t\t\t\t\tflinkTypeFactory.createFieldTypeFromLogicalType(originType.getChildren().get(i + physicalCount)));\n+\t\t\t\tOption<RexNodeNestedField> usedMetadata = root.deleteField(key.getName());\n+\t\t\t\tif (usedMetadata.isDefined()) {\n+\t\t\t\t\tusedMetaDataFields.add(usedMetadata.get());\n \t\t\t\t}\n-\t\t\t\tfieldCoordinatesToOrder.put(usedField,\n-\t\t\t\t\t\tCollections.singletonMap(Collections.singletonList(\"*\"), usedFieldsCoordinates.size()));\n-\t\t\t\tusedFieldsCoordinates.add(Collections.singletonList(usedField));\n \t\t\t}\n+\t\t\t// label the tree and get path\n+\t\t\tint[][] projectedPhysicalFields = RexNodeNestedField.labelAndConvert(root);\n+\t\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedPhysicalFields);\n+\t\t\t// push the metadata back for later rewrite and extract the location in the origin row\n+\t\t\tint order = root.order();\n+\t\t\tList<String> usedMetadataNames = new LinkedList<>();\n+\t\t\tfor (RexNodeNestedField metadata: usedMetaDataFields) {\n+\t\t\t\tmetadata.order_$eq(order++);\n+\t\t\t\troot.addField(metadata);\n+\t\t\t\tusedMetadataNames.add(metadataKeys.get(metadata.index() - physicalCount));\n+\t\t\t}\n+\t\t\troot.order_$eq(order);\n+\t\t\t// apply metadata push down\n+\t\t\tprojectedFields = Stream.concat(\n+\t\t\t\t\tStream.of(projectedPhysicalFields),\n+\t\t\t\t\tusedMetaDataFields.stream().map(field -> new int[]{field.index()})\n+\t\t\t).toArray(int[][]::new);\n+\t\t\tnewProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n+\t\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(\n+\t\t\t\t\tusedMetadataNames, newProducedDataType);\n+\t\t} else {\n+\t\t\tprojectedFields = RexNodeNestedField.labelAndConvert(root);\n+\t\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedFields);\n+\t\t\tnewProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n \t\t}\n \n-\t\tfinal int[][] projectedPhysicalFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(list -> list.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\t// push down physical projection\n-\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedPhysicalFields);\n-\n-\t\tDataType producedDataType = TypeConversions.fromLogicalToDataType(DynamicSourceUtils.createProducedType(oldSchema, oldSource));\n-\t\t// add metadata information into coordinates && mapping\n-\n-\t\tDataType newProducedDataType = supportsReadingMetaData ?\n-\t\t\t\tapplyUpdateMetadataAndGetNewDataType(\n-\t\t\t\t\t\tnewSource, producedDataType,  metadataKeys, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder) :\n-\t\t\t\tDataTypeUtils.projectRow(producedDataType, projectedPhysicalFields);\n-\n-\t\tFlinkTypeFactory flinkTypeFactory = (FlinkTypeFactory) oldTableSourceTable.getRelOptSchema().getTypeFactory();\n \t\tRelDataType newRowType = flinkTypeFactory.buildRelNodeRowType((RowType) newProducedDataType.getLogicalType());\n \n \t\t// project push down does not change the statistic, we can reuse origin statistic\n \t\tTableSourceTable newTableSourceTable = oldTableSourceTable.copy(\n \t\t\t\tnewSource, newRowType, new String[] {\n \t\t\t\t\t\t(\"project=[\" + String.join(\", \", newRowType.getFieldNames()) + \"]\") });\n-\n \t\tLogicalTableScan newScan = new LogicalTableScan(\n \t\t\t\tscan.getCluster(), scan.getTraitSet(), scan.getHints(), newTableSourceTable);\n \t\t// rewrite input field in projections\n-\t\tList<RexNode> newProjects = RexNodeRewriter.rewriteNestedProjectionWithNewFieldInput(\n-\t\t\t\tproject.getProjects(),\n-\t\t\t\tfieldCoordinatesToOrder,\n-\t\t\t\tnewRowType.getFieldList().stream().map(RelDataTypeField::getType).collect(Collectors.toList()),\n-\t\t\t\tcall.builder().getRexBuilder());\n-\n+\t\t// the origin projections are enough. Because the upsert source only uses pk info in the deduplication node.\n+\t\tList<RexNode> newProjects =\n+\t\t\t\tRexNodeNestedField.rewrite(project.getProjects(), root, call.builder().getRexBuilder());\n+\t\t// rewrite new source\n \t\tLogicalProject newProject = project.copy(\n \t\t\t\tproject.getTraitSet(),\n \t\t\t\tnewScan,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyNDc3OQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514224779", "bodyText": "the method name is too long, change to getExpandedFieldsAndOrderMapping ? add some comments the explain the arguments", "author": "godfreyhe", "createdAt": "2020-10-29T12:37:15Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates\n+\t\t\t\t.stream()\n+\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n \t\t\t\t.toArray(int[][]::new);\n \n-\t\t\t// create a new, final data type that includes all projections\n-\t\t\tfinal DataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n+\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n \n-\t\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n+\t\treturn newProducedDataType;\n+\t}\n+\n+\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex 4beaff0f00f..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -187,85 +198,6 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\t}\n \t}\n \n-\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n-\t\t\tDynamicTableSource newSource,\n-\t\t\tDataType producedDataType,\n-\t\t\tList<String> metadataKeys,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalFieldCount,\n-\t\t\tList<List<Integer>> usedFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t// order the keys according to the source's declaration\n-\t\tfinal List<String> usedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n-\n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n-\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n-\n-\t\tint[][] allFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n-\n-\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n-\t\treturn newProducedDataType;\n-\t}\n-\n-\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n-\t\t\tLogicalProject project,\n-\t\t\tTableSchema oldSchema,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalCount,\n-\t\t\tList<List<Integer>> usedPhysicalFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tList<String>[][] accessFieldNames =\n-\t\t\t\tRexNodeExtractor.extractRefNestedInputFields(project.getProjects(), usedFields);\n-\t\tint order = 0;\n-\t\tfor (int index = 0; index < usedFields.length; index++) {\n-\t\t\t// filter metadata columns\n-\t\t\tif (usedFields[index] >= physicalCount) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tint indexInOldSchema = usedFields[index];\n-\t\t\tMap<List<String>, Integer> mapping = new HashMap<>();\n-\t\t\tif (accessFieldNames[index][0].get(0).equals(\"*\")) {\n-\t\t\t\tusedPhysicalFieldsCoordinates.add(Collections.singletonList(indexInOldSchema));\n-\t\t\t\tmapping.put(Collections.singletonList(\"*\"), order++);\n-\t\t\t} else {\n-\t\t\t\tfor (List<String> fields : accessFieldNames[index]) {\n-\t\t\t\t\tLogicalType dataType = oldSchema.getFieldDataType(indexInOldSchema).get().getLogicalType();\n-\t\t\t\t\tList<Integer> coordinates = new LinkedList<>();\n-\t\t\t\t\tcoordinates.add(indexInOldSchema);\n-\t\t\t\t\tfor (String subFieldName : fields) {\n-\t\t\t\t\t\tRowType rowType = (RowType) dataType;\n-\t\t\t\t\t\tint fieldsIndex = rowType.getFieldIndex(subFieldName);\n-\t\t\t\t\t\tdataType = rowType.getTypeAt(fieldsIndex);\n-\t\t\t\t\t\tcoordinates.add(fieldsIndex);\n-\t\t\t\t\t}\n-\t\t\t\t\tusedPhysicalFieldsCoordinates.add(coordinates);\n-\t\t\t\t\tmapping.put(fields, order++);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfieldCoordinatesToOrder.put(indexInOldSchema, mapping);\n-\t\t}\n-\t}\n-\n \t/**\n \t * Returns true if the table is a upsert source when it is works in scan mode.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIyOTcxNg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514229716", "bodyText": "nit: It is better to close the position of the defined field to the position in which the field is used.", "author": "godfreyhe", "createdAt": "2020-10-29T12:45:34Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,28 +79,26 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n+\t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tfinal boolean supportsReadingMetaData = oldTableSourceTable.tableSource() instanceof SupportsReadingMetadata;", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex 4beaff0f00f..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -87,92 +84,106 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n+\t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n \t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n+\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n+\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n \n \t\tfinal boolean supportsNestedProjection =\n \t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n-\t\tfinal boolean supportsReadingMetaData = oldTableSourceTable.tableSource() instanceof SupportsReadingMetadata;\n-\n-\t\tfinal List<String> fieldNames = scan.getRowType().getFieldNames();\n-\t\tfinal int fieldCount = fieldNames.size();\n+\t\tList<String> fieldNames = scan.getRowType().getFieldNames();\n \n-\t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n-\t\tfinal int[] usedFields;\n+\t\tif (!supportsNestedProjection && refFields.length == fieldNames.size()) {\n+\t\t\t// just keep as same as the old plan\n+\t\t\t// TODO: refactor the affected plan\n+\t\t\treturn;\n+\t\t}\n \n+\t\tList<RexNode> oldProjectsWithPK = new ArrayList<>(project.getProjects());\n+\t\tFlinkTypeFactory flinkTypeFactory = (FlinkTypeFactory) oldTableSourceTable.getRelOptSchema().getTypeFactory();\n \t\tif (isUpsertSource(oldTableSourceTable)) {\n-\t\t\t// primary key fields are needed for upsert source\n-\t\t\tList<String> keyFields = oldTableSourceTable.catalogTable().getSchema()\n-\t\t\t\t.getPrimaryKey().get().getColumns();\n-\t\t\t// we should get source fields from scan node instead of CatalogTable,\n-\t\t\t// because projection may have been pushed down\n-\t\t\tList<String> sourceFields = scan.getRowType().getFieldNames();\n-\t\t\tint[] primaryKey = ScanUtil.getPrimaryKeyIndices(sourceFields, keyFields);\n-\t\t\tusedFields = mergeFields(refFields, primaryKey);\n-\t\t} else {\n-\t\t\tusedFields = refFields;\n+\t\t\t// add pk into projects\n+\t\t\toldSchema.getPrimaryKey().ifPresent(\n+\t\t\t\t\tpks -> {\n+\t\t\t\t\t\tfor (String name: pks.getColumns()) {\n+\t\t\t\t\t\t\tint index = fieldNames.indexOf(name);\n+\t\t\t\t\t\t\tTableColumn col = oldSchema.getTableColumn(index).get();\n+\t\t\t\t\t\t\toldProjectsWithPK.add(\n+\t\t\t\t\t\t\t\t\tnew RexInputRef(index,\n+\t\t\t\t\t\t\t\t\t\t\tflinkTypeFactory.createFieldTypeFromLogicalType(col.getType().getLogicalType())));\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t);\n \t\t}\n-\t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (!supportsNestedProjection && usedFields.length == fieldCount) {\n-\t\t\treturn;\n+\t\t// build used schema tree\n+\t\tRowType originType =\n+\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource);\n+\t\tRexNodeNestedField root = RexNodeNestedField.build(\n+\t\t\t\toldProjectsWithPK, flinkTypeFactory.buildRelNodeRowType(originType));\n+\t\tif (!supportsNestedProjection) {\n+\t\t\t// mark the fields in the top level as useall\n+\t\t\tfor (RexNodeNestedField child: root.fields().values()) {\n+\t\t\t\tchild.useAll_$eq(true);\n+\t\t\t}\n \t\t}\n-\n-\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n-\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n-\t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n-\t\tfinal int physicalFieldCount = fieldCount - metadataKeys.size();\n \t\tfinal DynamicTableSource newSource = oldSource.copy();\n-\n-\t\tfinal List<List<Integer>> usedFieldsCoordinates = new ArrayList<>();\n-\t\tfinal Map<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder = new HashMap<>();\n-\n-\t\tif (supportsNestedProjection) {\n-\t\t\tgetCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n-\t\t\t\t\tproject, oldSchema, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder);\n-\t\t} else {\n-\t\t\tfor (int usedField : usedFields) {\n-\t\t\t\t// filter metadata columns\n-\t\t\t\tif (usedField >= physicalFieldCount) {\n-\t\t\t\t\tcontinue;\n+\t\tfinal int[][] projectedFields;\n+\t\tfinal DataType newProducedDataType;\n+\t\tDataType producedDataType = TypeConversions.fromLogicalToDataType(originType);\n+\n+\t\tif (oldSource instanceof SupportsReadingMetadata) {\n+\t\t\t//TODO: supports nested projection for metadata\n+\t\t\tfinal List<String> metadataKeys = DynamicSourceUtils.createRequiredMetadataKeys(oldSchema, oldSource);\n+\t\t\tList<RexNodeNestedField> usedMetaDataFields = new LinkedList<>();\n+\t\t\tint physicalCount = fieldNames.size() - metadataKeys.size();\n+\t\t\t// rm metadata in the tree\n+\t\t\tfor (int i = 0; i < metadataKeys.size(); i++) {\n+\t\t\t\tfinal RexInputRef key = new RexInputRef(i + physicalCount,\n+\t\t\t\t\t\tflinkTypeFactory.createFieldTypeFromLogicalType(originType.getChildren().get(i + physicalCount)));\n+\t\t\t\tOption<RexNodeNestedField> usedMetadata = root.deleteField(key.getName());\n+\t\t\t\tif (usedMetadata.isDefined()) {\n+\t\t\t\t\tusedMetaDataFields.add(usedMetadata.get());\n \t\t\t\t}\n-\t\t\t\tfieldCoordinatesToOrder.put(usedField,\n-\t\t\t\t\t\tCollections.singletonMap(Collections.singletonList(\"*\"), usedFieldsCoordinates.size()));\n-\t\t\t\tusedFieldsCoordinates.add(Collections.singletonList(usedField));\n \t\t\t}\n+\t\t\t// label the tree and get path\n+\t\t\tint[][] projectedPhysicalFields = RexNodeNestedField.labelAndConvert(root);\n+\t\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedPhysicalFields);\n+\t\t\t// push the metadata back for later rewrite and extract the location in the origin row\n+\t\t\tint order = root.order();\n+\t\t\tList<String> usedMetadataNames = new LinkedList<>();\n+\t\t\tfor (RexNodeNestedField metadata: usedMetaDataFields) {\n+\t\t\t\tmetadata.order_$eq(order++);\n+\t\t\t\troot.addField(metadata);\n+\t\t\t\tusedMetadataNames.add(metadataKeys.get(metadata.index() - physicalCount));\n+\t\t\t}\n+\t\t\troot.order_$eq(order);\n+\t\t\t// apply metadata push down\n+\t\t\tprojectedFields = Stream.concat(\n+\t\t\t\t\tStream.of(projectedPhysicalFields),\n+\t\t\t\t\tusedMetaDataFields.stream().map(field -> new int[]{field.index()})\n+\t\t\t).toArray(int[][]::new);\n+\t\t\tnewProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n+\t\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(\n+\t\t\t\t\tusedMetadataNames, newProducedDataType);\n+\t\t} else {\n+\t\t\tprojectedFields = RexNodeNestedField.labelAndConvert(root);\n+\t\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedFields);\n+\t\t\tnewProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n \t\t}\n \n-\t\tfinal int[][] projectedPhysicalFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(list -> list.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\t// push down physical projection\n-\t\t((SupportsProjectionPushDown) newSource).applyProjection(projectedPhysicalFields);\n-\n-\t\tDataType producedDataType = TypeConversions.fromLogicalToDataType(DynamicSourceUtils.createProducedType(oldSchema, oldSource));\n-\t\t// add metadata information into coordinates && mapping\n-\n-\t\tDataType newProducedDataType = supportsReadingMetaData ?\n-\t\t\t\tapplyUpdateMetadataAndGetNewDataType(\n-\t\t\t\t\t\tnewSource, producedDataType,  metadataKeys, usedFields, physicalFieldCount, usedFieldsCoordinates, fieldCoordinatesToOrder) :\n-\t\t\t\tDataTypeUtils.projectRow(producedDataType, projectedPhysicalFields);\n-\n-\t\tFlinkTypeFactory flinkTypeFactory = (FlinkTypeFactory) oldTableSourceTable.getRelOptSchema().getTypeFactory();\n \t\tRelDataType newRowType = flinkTypeFactory.buildRelNodeRowType((RowType) newProducedDataType.getLogicalType());\n \n \t\t// project push down does not change the statistic, we can reuse origin statistic\n \t\tTableSourceTable newTableSourceTable = oldTableSourceTable.copy(\n \t\t\t\tnewSource, newRowType, new String[] {\n \t\t\t\t\t\t(\"project=[\" + String.join(\", \", newRowType.getFieldNames()) + \"]\") });\n-\n \t\tLogicalTableScan newScan = new LogicalTableScan(\n \t\t\t\tscan.getCluster(), scan.getTraitSet(), scan.getHints(), newTableSourceTable);\n \t\t// rewrite input field in projections\n-\t\tList<RexNode> newProjects = RexNodeRewriter.rewriteNestedProjectionWithNewFieldInput(\n-\t\t\t\tproject.getProjects(),\n-\t\t\t\tfieldCoordinatesToOrder,\n-\t\t\t\tnewRowType.getFieldList().stream().map(RelDataTypeField::getType).collect(Collectors.toList()),\n-\t\t\t\tcall.builder().getRexBuilder());\n-\n+\t\t// the origin projections are enough. Because the upsert source only uses pk info in the deduplication node.\n+\t\tList<RexNode> newProjects =\n+\t\t\t\tRexNodeNestedField.rewrite(project.getProjects(), root, call.builder().getRexBuilder());\n+\t\t// rewrite new source\n \t\tLogicalProject newProject = project.copy(\n \t\t\t\tproject.getTraitSet(),\n \t\t\t\tnewScan,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMjU1Mw==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514232553", "bodyText": "use for to make it clearer", "author": "godfreyhe", "createdAt": "2020-10-29T12:50:16Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex 4beaff0f00f..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -187,85 +198,6 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\t}\n \t}\n \n-\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n-\t\t\tDynamicTableSource newSource,\n-\t\t\tDataType producedDataType,\n-\t\t\tList<String> metadataKeys,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalFieldCount,\n-\t\t\tList<List<Integer>> usedFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t// order the keys according to the source's declaration\n-\t\tfinal List<String> usedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n-\n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n-\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n-\n-\t\tint[][] allFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n-\n-\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n-\t\treturn newProducedDataType;\n-\t}\n-\n-\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n-\t\t\tLogicalProject project,\n-\t\t\tTableSchema oldSchema,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalCount,\n-\t\t\tList<List<Integer>> usedPhysicalFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tList<String>[][] accessFieldNames =\n-\t\t\t\tRexNodeExtractor.extractRefNestedInputFields(project.getProjects(), usedFields);\n-\t\tint order = 0;\n-\t\tfor (int index = 0; index < usedFields.length; index++) {\n-\t\t\t// filter metadata columns\n-\t\t\tif (usedFields[index] >= physicalCount) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tint indexInOldSchema = usedFields[index];\n-\t\t\tMap<List<String>, Integer> mapping = new HashMap<>();\n-\t\t\tif (accessFieldNames[index][0].get(0).equals(\"*\")) {\n-\t\t\t\tusedPhysicalFieldsCoordinates.add(Collections.singletonList(indexInOldSchema));\n-\t\t\t\tmapping.put(Collections.singletonList(\"*\"), order++);\n-\t\t\t} else {\n-\t\t\t\tfor (List<String> fields : accessFieldNames[index]) {\n-\t\t\t\t\tLogicalType dataType = oldSchema.getFieldDataType(indexInOldSchema).get().getLogicalType();\n-\t\t\t\t\tList<Integer> coordinates = new LinkedList<>();\n-\t\t\t\t\tcoordinates.add(indexInOldSchema);\n-\t\t\t\t\tfor (String subFieldName : fields) {\n-\t\t\t\t\t\tRowType rowType = (RowType) dataType;\n-\t\t\t\t\t\tint fieldsIndex = rowType.getFieldIndex(subFieldName);\n-\t\t\t\t\t\tdataType = rowType.getTypeAt(fieldsIndex);\n-\t\t\t\t\t\tcoordinates.add(fieldsIndex);\n-\t\t\t\t\t}\n-\t\t\t\t\tusedPhysicalFieldsCoordinates.add(coordinates);\n-\t\t\t\t\tmapping.put(fields, order++);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfieldCoordinatesToOrder.put(indexInOldSchema, mapping);\n-\t\t}\n-\t}\n-\n \t/**\n \t * Returns true if the table is a upsert source when it is works in scan mode.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDIzMzM5NA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514233394", "bodyText": "allFields -> projectedFields", "author": "godfreyhe", "createdAt": "2020-10-29T12:51:37Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -190,35 +187,82 @@ public void onMatch(RelOptRuleCall call) {\n \t\t}\n \t}\n \n-\tprivate void applyUpdatedMetadata(\n-\t\t\tDynamicTableSource oldSource,\n-\t\t\tTableSchema oldSchema,\n+\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\tDynamicTableSource newSource,\n+\t\t\tDataType producedDataType,\n \t\t\tList<String> metadataKeys,\n-\t\t\tList<String> usedMetadataKeys,\n+\t\t\tint[] usedFields,\n \t\t\tint physicalFieldCount,\n-\t\t\tint[][] projectedPhysicalFields) {\n-\t\tif (newSource instanceof SupportsReadingMetadata) {\n-\t\t\tfinal DataType producedDataType = TypeConversions.fromLogicalToDataType(\n-\t\t\t\tDynamicSourceUtils.createProducedType(oldSchema, oldSource));\n+\t\t\tList<List<Integer>> usedFieldsCoordinates,\n+\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n+\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n+\t\t\t\t// select only metadata columns\n+\t\t\t\t.filter(i -> i >= physicalFieldCount)\n+\t\t\t\t// map the indices to keys\n+\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t// order the keys according to the source's declaration\n+\t\tfinal List<String> usedMetadataKeys = metadataKeys\n+\t\t\t\t.stream()\n+\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n+\t\t\t\t.collect(Collectors.toList());\n \n-\t\t\tfinal int[][] projectedMetadataFields = usedMetadataKeys\n+\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n \t\t\t\t.stream()\n \t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> new int[]{ physicalFieldCount + i })\n-\t\t\t\t.toArray(int[][]::new);\n+\t\t\t\t.map(i -> {\n+\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n+\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n+\t\t\t\t.collect(Collectors.toList());\n+\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n \n-\t\t\tfinal int[][] projectedFields = Stream\n-\t\t\t\t.concat(\n-\t\t\t\t\tStream.of(projectedPhysicalFields),\n-\t\t\t\t\tStream.of(projectedMetadataFields)\n-\t\t\t\t)\n+\t\tint[][] allFields = usedFieldsCoordinates", "originalCommit": "1697a20356799dd5e2f8863110de1f4626664f12", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex 4beaff0f00f..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -187,85 +198,6 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\t}\n \t}\n \n-\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n-\t\t\tDynamicTableSource newSource,\n-\t\t\tDataType producedDataType,\n-\t\t\tList<String> metadataKeys,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalFieldCount,\n-\t\t\tList<List<Integer>> usedFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t// order the keys according to the source's declaration\n-\t\tfinal List<String> usedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n-\n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n-\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n-\n-\t\tint[][] allFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, allFields);\n-\n-\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n-\t\treturn newProducedDataType;\n-\t}\n-\n-\tprivate void getCoordinatesAndMappingOfPhysicalColumnWithNestedProjection(\n-\t\t\tLogicalProject project,\n-\t\t\tTableSchema oldSchema,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalCount,\n-\t\t\tList<List<Integer>> usedPhysicalFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tList<String>[][] accessFieldNames =\n-\t\t\t\tRexNodeExtractor.extractRefNestedInputFields(project.getProjects(), usedFields);\n-\t\tint order = 0;\n-\t\tfor (int index = 0; index < usedFields.length; index++) {\n-\t\t\t// filter metadata columns\n-\t\t\tif (usedFields[index] >= physicalCount) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tint indexInOldSchema = usedFields[index];\n-\t\t\tMap<List<String>, Integer> mapping = new HashMap<>();\n-\t\t\tif (accessFieldNames[index][0].get(0).equals(\"*\")) {\n-\t\t\t\tusedPhysicalFieldsCoordinates.add(Collections.singletonList(indexInOldSchema));\n-\t\t\t\tmapping.put(Collections.singletonList(\"*\"), order++);\n-\t\t\t} else {\n-\t\t\t\tfor (List<String> fields : accessFieldNames[index]) {\n-\t\t\t\t\tLogicalType dataType = oldSchema.getFieldDataType(indexInOldSchema).get().getLogicalType();\n-\t\t\t\t\tList<Integer> coordinates = new LinkedList<>();\n-\t\t\t\t\tcoordinates.add(indexInOldSchema);\n-\t\t\t\t\tfor (String subFieldName : fields) {\n-\t\t\t\t\t\tRowType rowType = (RowType) dataType;\n-\t\t\t\t\t\tint fieldsIndex = rowType.getFieldIndex(subFieldName);\n-\t\t\t\t\t\tdataType = rowType.getTypeAt(fieldsIndex);\n-\t\t\t\t\t\tcoordinates.add(fieldsIndex);\n-\t\t\t\t\t}\n-\t\t\t\t\tusedPhysicalFieldsCoordinates.add(coordinates);\n-\t\t\t\t\tmapping.put(fields, order++);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfieldCoordinatesToOrder.put(indexInOldSchema, mapping);\n-\t\t}\n-\t}\n-\n \t/**\n \t * Returns true if the table is a upsert source when it is works in scan mode.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwNzMzOQ==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514907339", "bodyText": "This line is unnecessary, use usedFieldsCoordinates.add(Collections.singletonList(physicalFieldCount + index)) at line 217", "author": "godfreyhe", "createdAt": "2020-10-30T07:11:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -207,27 +208,35 @@ private DataType applyUpdateMetadataAndGetNewDataType(\n \t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n \t\t\t\t.collect(Collectors.toList());\n \n-\t\tfinal List<List<Integer>> projectedMetadataFields = usedMetadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.map(metadataKeys::indexOf)\n-\t\t\t\t.map(i -> {\n-\t\t\t\t\tfieldCoordinatesToOrder.put(physicalFieldCount + i, Collections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\t\t\treturn Collections.singletonList(physicalFieldCount + i); })\n-\t\t\t\t.collect(Collectors.toList());\n+\t\tfinal List<List<Integer>> projectedMetadataFields = new ArrayList<>(usedMetadataKeys.size());", "originalCommit": "a5194ee25cf0c08ecd1d1e484b3f2e335ffb7656", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex d480cc2d681..da63c495b9a 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -187,94 +198,6 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\t}\n \t}\n \n-\tprivate DataType applyUpdateMetadataAndGetNewDataType(\n-\t\t\tDynamicTableSource newSource,\n-\t\t\tDataType producedDataType,\n-\t\t\tList<String> metadataKeys,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalFieldCount,\n-\t\t\tList<List<Integer>> usedFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\t//TODO: support nested projection for metadata\n-\t\tfinal List<String> usedMetadataKeysUnordered = IntStream.of(usedFields)\n-\t\t\t\t// select only metadata columns\n-\t\t\t\t.filter(i -> i >= physicalFieldCount)\n-\t\t\t\t// map the indices to keys\n-\t\t\t\t.mapToObj(i -> metadataKeys.get(i - physicalFieldCount))\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t// order the keys according to the source's declaration\n-\t\tfinal List<String> usedMetadataKeys = metadataKeys\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(usedMetadataKeysUnordered::contains)\n-\t\t\t\t.collect(Collectors.toList());\n-\n-\t\tfinal List<List<Integer>> projectedMetadataFields = new ArrayList<>(usedMetadataKeys.size());\n-\t\tfor (String key: usedMetadataKeys) {\n-\t\t\tint index = metadataKeys.indexOf(key);\n-\t\t\tfieldCoordinatesToOrder.put(\n-\t\t\t\t\tphysicalFieldCount + index,\n-\t\t\t\t\tCollections.singletonMap(Collections.singletonList(\"*\"), fieldCoordinatesToOrder.size()));\n-\t\t\tprojectedMetadataFields.add(Collections.singletonList(physicalFieldCount + index));\n-\t\t}\n-\t\tusedFieldsCoordinates.addAll(projectedMetadataFields);\n-\n-\t\tint[][] projectedFields = usedFieldsCoordinates\n-\t\t\t\t.stream()\n-\t\t\t\t.map(coordinates -> coordinates.stream().mapToInt(i -> i).toArray())\n-\t\t\t\t.toArray(int[][]::new);\n-\n-\t\tDataType newProducedDataType = DataTypeUtils.projectRow(producedDataType, projectedFields);\n-\n-\t\t((SupportsReadingMetadata) newSource).applyReadableMetadata(usedMetadataKeys, newProducedDataType);\n-\t\treturn newProducedDataType;\n-\t}\n-\n-\t/**\n-\t * \tGet coordinates and mapping of the physicalColumn if scan supports nested projection push down.\n-\t * \tIt will get the expanded the name of the reference and place the refs with the same top level\n-\t * \tin the same array. The order of the fields in the new schema is determined by the current.\n-\t *\n-\t * \t<p>NOTICE: Currently, to resolve the name conflicts we use list to restore the every level name.\n-\t */\n-\tprivate void getExpandedFieldAndOrderMapping(\n-\t\t\tLogicalProject project,\n-\t\t\tTableSchema oldSchema,\n-\t\t\tint[] usedFields,\n-\t\t\tint physicalCount,\n-\t\t\tList<List<Integer>> usedPhysicalFieldsCoordinates,\n-\t\t\tMap<Integer, Map<List<String>, Integer>> fieldCoordinatesToOrder) {\n-\t\tList<String>[][] accessFieldNames =\n-\t\t\t\tRexNodeExtractor.extractRefNestedInputFields(project.getProjects(), usedFields);\n-\t\tint order = 0;\n-\t\tfor (int index = 0; index < usedFields.length; index++) {\n-\t\t\t// filter metadata columns\n-\t\t\tif (usedFields[index] >= physicalCount) {\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tint indexInOldSchema = usedFields[index];\n-\t\t\tMap<List<String>, Integer> mapping = new HashMap<>();\n-\t\t\tif (accessFieldNames[index][0].get(0).equals(\"*\")) {\n-\t\t\t\tusedPhysicalFieldsCoordinates.add(Collections.singletonList(indexInOldSchema));\n-\t\t\t\tmapping.put(Collections.singletonList(\"*\"), order++);\n-\t\t\t} else {\n-\t\t\t\tfor (List<String> fields : accessFieldNames[index]) {\n-\t\t\t\t\tLogicalType dataType = oldSchema.getFieldDataType(indexInOldSchema).get().getLogicalType();\n-\t\t\t\t\tList<Integer> coordinates = new LinkedList<>();\n-\t\t\t\t\tcoordinates.add(indexInOldSchema);\n-\t\t\t\t\tfor (String subFieldName : fields) {\n-\t\t\t\t\t\tRowType rowType = (RowType) dataType;\n-\t\t\t\t\t\tint fieldsIndex = rowType.getFieldIndex(subFieldName);\n-\t\t\t\t\t\tdataType = rowType.getTypeAt(fieldsIndex);\n-\t\t\t\t\t\tcoordinates.add(fieldsIndex);\n-\t\t\t\t\t}\n-\t\t\t\t\tusedPhysicalFieldsCoordinates.add(coordinates);\n-\t\t\t\t\tmapping.put(fields, order++);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfieldCoordinatesToOrder.put(indexInOldSchema, mapping);\n-\t\t}\n-\t}\n-\n \t/**\n \t * Returns true if the table is a upsert source when it is works in scan mode.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MjUxNw==", "url": "https://github.com/apache/flink/pull/13631#discussion_r514952517", "bodyText": "MyTable does not support nested-projection-supported", "author": "godfreyhe", "createdAt": "2020-10-30T08:57:52Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java", "diffHunk": "@@ -70,34 +70,47 @@ public void setup() {\n \t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n \t\t\t\t\t\t\")\";\n \t\tutil().tableEnv().executeSql(ddl2);\n-\t}\n-\n-\t@Override\n-\tpublic void testNestedProject() {\n-\t\texpectedException().expect(TableException.class);\n-\t\texpectedException().expectMessage(\"Nested projection push down is unsupported now.\");\n-\t\ttestNestedProject(true);\n-\t}\n-\n-\t@Test\n-\tpublic void testNestedProjectDisabled() {\n-\t\ttestNestedProject(false);\n-\t}\n \n-\tprivate void testNestedProject(boolean nestedProjectionSupported) {\n-\t\tString ddl =\n+\t\tString ddl3 =\n \t\t\t\t\"CREATE TABLE NestedTable (\\n\" +\n \t\t\t\t\t\t\"  id int,\\n\" +\n \t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n \t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n+\t\t\t\t\t\t\"  `deepNestedWith.` row<`.value` int, nested row<name string, `.value` int>>,\\n\" +\n \t\t\t\t\t\t\"  name string\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n-\t\t\t\t\t\t\" 'nested-projection-supported' = '\" + nestedProjectionSupported + \"',\\n\" +\n-\t\t\t\t\t\t\"  'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true'\\n\" +\n+\t\t\t\t\t\t\")\";\n+\t\tutil().tableEnv().executeSql(ddl3);\n+\n+\t\tString ddl4 =\n+\t\t\t\t\"CREATE TABLE MetadataTable(\\n\" +\n+\t\t\t\t\t\t\"  id int,\\n\" +\n+\t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n+\t\t\t\t\t\t\"  metadata_1 int metadata,\\n\" +\n+\t\t\t\t\t\t\"  metadata_2 string metadata\\n\" +\n+\t\t\t\t\t\t\") WITH (\" +\n+\t\t\t\t\t\t\" 'connector' = 'values',\" +\n+\t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n+\t\t\t\t\t\t\" 'bounded' = 'true',\\n\" +\n+\t\t\t\t\t\t\" 'readable-metadata' = 'metadata_1:INT, metadata_2:STRING, metadata_3:BIGINT'\" +\n \t\t\t\t\t\t\")\";\n-\t\tutil().tableEnv().executeSql(ddl);\n+\t\tutil().tableEnv().executeSql(ddl4);\n+\t}\n \n+\t@Test\n+\tpublic void testProjectWithMapType() {\n+\t\tString sqlQuery =\n+\t\t\t\t\"SELECT a, d['e']\\n\" +\n+\t\t\t\t\t\t\"FROM MyTable\";", "originalCommit": "9898b59d09c7483e3e0c034372a4875c46844841", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb980798a022a35131fccd5c536a5f5519f38cba", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\nindex 7ff8d1dd0b9..3a3c1c50fda 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRuleTest.java\n\n@@ -77,7 +76,8 @@ public class PushProjectIntoTableSourceScanRuleTest extends PushProjectIntoLegac\n \t\t\t\t\t\t\"  deepNested row<nested1 row<name string, `value` int>, nested2 row<num int, flag boolean>>,\\n\" +\n \t\t\t\t\t\t\"  nested row<name string, `value` int>,\\n\" +\n \t\t\t\t\t\t\"  `deepNestedWith.` row<`.value` int, nested row<name string, `.value` int>>,\\n\" +\n-\t\t\t\t\t\t\"  name string\\n\" +\n+\t\t\t\t\t\t\"  name string,\\n\" +\n+\t\t\t\t\t\t\"  testMap Map<string, string>\\n\" +\n \t\t\t\t\t\t\") WITH (\\n\" +\n \t\t\t\t\t\t\" 'connector' = 'values',\\n\" +\n \t\t\t\t\t\t\" 'nested-projection-supported' = 'true',\" +\n"}}, {"oid": "cb980798a022a35131fccd5c536a5f5519f38cba", "url": "https://github.com/apache/flink/commit/cb980798a022a35131fccd5c536a5f5519f38cba", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections.", "committedDate": "2020-11-01T02:14:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NjM4Mg==", "url": "https://github.com/apache/flink/pull/13631#discussion_r515656382", "bodyText": "which plans are affected?", "author": "godfreyhe", "createdAt": "2020-11-01T18:40:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java", "diffHunk": "@@ -74,108 +76,114 @@ public boolean matches(RelOptRuleCall call) {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n-\t\tif (pushDownSource.supportsNestedProjection()) {\n-\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n-\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n-\t\t\t\t\t\"planner will push down the top-level columns.\");\n-\t\t} else {\n-\t\t\treturn true;\n-\t\t}\n+\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n \t}\n \n \t@Override\n \tpublic void onMatch(RelOptRuleCall call) {\n \t\tfinal LogicalProject project = call.rel(0);\n \t\tfinal LogicalTableScan scan = call.rel(1);\n \n-\t\tfinal List<String> fieldNames = scan.getRowType().getFieldNames();\n-\t\tfinal int fieldCount = fieldNames.size();\n-\n \t\tfinal int[] refFields = RexNodeExtractor.extractRefInputFields(project.getProjects());\n-\t\tfinal int[] usedFields;\n-\n \t\tTableSourceTable oldTableSourceTable = scan.getTable().unwrap(TableSourceTable.class);\n-\t\tif (isUpsertSource(oldTableSourceTable)) {\n-\t\t\t// primary key fields are needed for upsert source\n-\t\t\tList<String> keyFields = oldTableSourceTable.catalogTable().getSchema()\n-\t\t\t\t.getPrimaryKey().get().getColumns();\n-\t\t\t// we should get source fields from scan node instead of CatalogTable,\n-\t\t\t// because projection may have been pushed down\n-\t\t\tList<String> sourceFields = scan.getRowType().getFieldNames();\n-\t\t\tint[] primaryKey = ScanUtil.getPrimaryKeyIndices(sourceFields, keyFields);\n-\t\t\tusedFields = mergeFields(refFields, primaryKey);\n-\t\t} else {\n-\t\t\tusedFields = refFields;\n-\t\t}\n-\t\t// if no fields can be projected, we keep the original plan.\n-\t\tif (usedFields.length == fieldCount) {\n+\t\tfinal TableSchema oldSchema = oldTableSourceTable.catalogTable().getSchema();\n+\t\tfinal DynamicTableSource oldSource = oldTableSourceTable.tableSource();\n+\n+\t\tfinal boolean supportsNestedProjection =\n+\t\t\t\t((SupportsProjectionPushDown) oldTableSourceTable.tableSource()).supportsNestedProjection();\n+\t\tList<String> fieldNames = scan.getRowType().getFieldNames();\n+\n+\t\tif (!supportsNestedProjection && refFields.length == fieldNames.size()) {\n+\t\t\t// just keep as same as the old plan\n+\t\t\t// TODO: refactor the affected plan", "originalCommit": "aa094733a6678ec72712da928dd457e79f3f81a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcyNDcxOA==", "url": "https://github.com/apache/flink/pull/13631#discussion_r515724718", "bodyText": "The main problem is we will add digest in this situation. Maybe we can regard as an improvement for furture.", "author": "fsk119", "createdAt": "2020-11-02T03:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NjM4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "34b48e349af82ba98ca546d4d3c8798b0521a1e7", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\nindex da63c495b9a..9db75d4b01c 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/rules/logical/PushProjectIntoTableSourceScanRule.java\n\n@@ -76,7 +74,14 @@ public class PushProjectIntoTableSourceScanRule extends RelOptRule {\n \t\tif (tableSourceTable == null || !(tableSourceTable.tableSource() instanceof SupportsProjectionPushDown)) {\n \t\t\treturn false;\n \t\t}\n-\t\treturn Arrays.stream(tableSourceTable.extraDigests()).noneMatch(digest -> digest.startsWith(\"project=[\"));\n+\t\tSupportsProjectionPushDown pushDownSource = (SupportsProjectionPushDown) tableSourceTable.tableSource();\n+\t\tif (pushDownSource.supportsNestedProjection()) {\n+\t\t\tthrow new TableException(\"Nested projection push down is unsupported now. \\n\" +\n+\t\t\t\t\t\"Please disable nested projection (SupportsProjectionPushDown#supportsNestedProjection returns false), \" +\n+\t\t\t\t\t\"planner will push down the top-level columns.\");\n+\t\t} else {\n+\t\t\treturn true;\n+\t\t}\n \t}\n \n \t@Override\n"}}, {"oid": "34b48e349af82ba98ca546d4d3c8798b0521a1e7", "url": "https://github.com/apache/flink/commit/34b48e349af82ba98ca546d4d3c8798b0521a1e7", "message": "[FLINK-19639][table sql/planner]Support SupportsNestedProjectionPushDown in planner", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "1a6af35c7056199c7f925687e7cff8dc525a1071", "url": "https://github.com/apache/flink/commit/1a6af35c7056199c7f925687e7cff8dc525a1071", "message": "fix godfrey's comment:\n1. use qualified name list to get the projectedFields and build new projections;\n2. add more tests", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "url": "https://github.com/apache/flink/commit/679a5eb6a3e649aa2dceadef8a4e6c9455b91983", "message": "minor fix", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "c624950f08de80ee7dc1e0c23d85fc832f779ad2", "url": "https://github.com/apache/flink/commit/c624950f08de80ee7dc1e0c23d85fc832f779ad2", "message": "fix godfrey's comment:\n1. use qualified name as the projected column name\n2. fix suggestions", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "url": "https://github.com/apache/flink/commit/62edbb5ff58d100105d1bb1f85e44ef11d04cf05", "message": "use postfix to solve name conflicts.", "committedDate": "2020-11-03T05:43:58Z", "type": "commit"}, {"oid": "7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "url": "https://github.com/apache/flink/commit/7ac21c27e3ba812bc0fc73941a494638d8cc6e80", "message": "1. use postfix \"$%d\" to resolve the name conflicts\n2. rewrite the rule to support metadata push down:\n2.1 we will check the source and extract the physical part of the schema. If the source supports nested projection push down, we use `RexNodeExtractor.extractRefNestedInputFields` to extract data else we add the physical part info into the coordinates info.\n2.2 If the source supports metadata push down, we add the metadata info into the coordinates.\n2.3 with the final coordinates, we write the projection.", "committedDate": "2020-11-03T05:48:43Z", "type": "commit"}, {"oid": "2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "url": "https://github.com/apache/flink/commit/2e8ef24b27c38b03f0e6917972a4ab3f14947f04", "message": "fix line too long", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "url": "https://github.com/apache/flink/commit/503bf618e2ea4cb46c1b929f6e5c1bbdcc5ab170", "message": "fix test", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "url": "https://github.com/apache/flink/commit/1c148a77f2f5b68a80dc7e7ecb9a6014d9e760ea", "message": "address feedback:\n1. rename the func name and add comments;\n2. add test: projection push down with map type;\n3. other minor fix;", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "url": "https://github.com/apache/flink/commit/e24f5d693ec0ad3f687fcb622d64ec9f78d20f11", "message": "delete unused method", "committedDate": "2020-11-03T05:48:47Z", "type": "commit"}, {"oid": "9515a90a341fdb1173d20025f6ad2915364489b6", "url": "https://github.com/apache/flink/commit/9515a90a341fdb1173d20025f6ad2915364489b6", "message": "1. introduce the new extractor and rewriter:\n2. fix failed test: the order of fileds in the new schema is determined by the hashmap rather that the order in the projections.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "url": "https://github.com/apache/flink/commit/40d2a12d81458dc803fe9cfd6eba45d0f3098a8a", "message": "fix failed test", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "ad207e1b987cee8de52adac5e3dd2c8eee96d185", "url": "https://github.com/apache/flink/commit/ad207e1b987cee8de52adac5e3dd2c8eee96d185", "message": "use LinkedHashMap to reduce the cost of the reorder and roll back the modification of the test.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "url": "https://github.com/apache/flink/commit/421d5bb5e7f784a38064941dc2bc3fdc6b1f2ed7", "message": "fix test", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "url": "https://github.com/apache/flink/commit/95f0efef8a8dbbd0caf5e90cce810b448fbe5c74", "message": "fix test and address the feedbacks.", "committedDate": "2020-11-03T05:48:48Z", "type": "commit"}, {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems", "committedDate": "2020-11-03T05:51:50Z", "type": "commit"}, {"oid": "0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "url": "https://github.com/apache/flink/commit/0707f7489676ee1a7e709479e47c6a3da5dd0e8b", "message": "address godfrey's comment:\n1. add RexNodeNestedFields that works as tableschema\n2. fix failed test and some small problems", "committedDate": "2020-11-03T05:51:50Z", "type": "forcePushed"}, {"oid": "4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "url": "https://github.com/apache/flink/commit/4d844d385b7a556ee1f3f53b36b0b1211136e5b3", "message": "address feedback: refactor the NestedSchema && NestedColumn", "committedDate": "2020-11-03T12:19:24Z", "type": "commit"}, {"oid": "4bc536663a8a89fee69b58494c76b8e6f5e98c25", "url": "https://github.com/apache/flink/commit/4bc536663a8a89fee69b58494c76b8e6f5e98c25", "message": "minor fix", "committedDate": "2020-11-03T12:25:04Z", "type": "commit"}, {"oid": "95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "url": "https://github.com/apache/flink/commit/95b1f48e2fa7c49f8193ef35738787e01bdf2fb5", "message": "minor fix", "committedDate": "2020-11-03T13:18:01Z", "type": "commit"}, {"oid": "5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "url": "https://github.com/apache/flink/commit/5a3617336c68ff9311adf5d93a07c9eb5ff39dbf", "message": "minor fix", "committedDate": "2020-11-03T13:43:49Z", "type": "commit"}, {"oid": "bb84157d91d0f564a199c6af00209db8f51a7dda", "url": "https://github.com/apache/flink/commit/bb84157d91d0f564a199c6af00209db8f51a7dda", "message": "delete JLong", "committedDate": "2020-11-03T13:53:45Z", "type": "commit"}, {"oid": "fd8961181c3838b57b78d28c22029d581031763e", "url": "https://github.com/apache/flink/commit/fd8961181c3838b57b78d28c22029d581031763e", "message": "fix tab problems", "committedDate": "2020-11-03T15:29:11Z", "type": "commit"}, {"oid": "5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "url": "https://github.com/apache/flink/commit/5f9d0abfa8ebf2ca3f29d1eeb69f898a6b4de915", "message": "rename the file and class", "committedDate": "2020-11-04T02:16:44Z", "type": "commit"}]}