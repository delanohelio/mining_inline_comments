{"pr_number": 13315, "pr_title": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "pr_createdAt": "2020-09-03T10:05:19Z", "pr_url": "https://github.com/apache/flink/pull/13315", "timeline": [{"oid": "2e0da845caf00006355b898bdcff29e3b7ac56b3", "url": "https://github.com/apache/flink/commit/2e0da845caf00006355b898bdcff29e3b7ac56b3", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-03T09:35:06Z", "type": "commit"}, {"oid": "b9e54a84b88a37bf16c73597f8bbbea8ab169f99", "url": "https://github.com/apache/flink/commit/b9e54a84b88a37bf16c73597f8bbbea8ab169f99", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-04T13:10:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYwNDA0NA==", "url": "https://github.com/apache/flink/pull/13315#discussion_r483604044", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tthrow new TableException(String.format(\"Cannot write on the ACID table %s.\", identifier.asSummaryString()));\n          \n          \n            \n            \t\t\t\tthrow new FlinkHiveException(String.format(\"Writing ACID table %s is not supported\", identifier.asSummaryString()));", "author": "lirui-apache", "createdAt": "2020-09-04T13:07:52Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -328,4 +330,16 @@ public void setStaticPartition(Map<String, String> partitionSpec) {\n \tpublic void setOverwrite(boolean overwrite) {\n \t\tthis.overwrite = overwrite;\n \t}\n+\n+\tprivate void checkAcidTable() {\n+\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n+\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n+\t\t\tif (tableIsTransactional == null) {\n+\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n+\t\t\t}\n+\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n+\t\t\t\tthrow new TableException(String.format(\"Cannot write on the ACID table %s.\", identifier.asSummaryString()));", "originalCommit": "2ed647fc736a248d536d6f5422e0a50a7119045f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\nindex 48b2ab35411..19e5831e5a7 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\n\n@@ -331,14 +332,44 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS\n \t\tthis.overwrite = overwrite;\n \t}\n \n-\tprivate void checkAcidTable() {\n-\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n-\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n-\t\t\tif (tableIsTransactional == null) {\n-\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n-\t\t\t}\n-\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n-\t\t\t\tthrow new TableException(String.format(\"Cannot write on the ACID table %s.\", identifier.asSummaryString()));\n+\t/**\n+\t * Getting size of the file is too expensive. See {@link HiveBulkWriterFactory#create}.\n+\t * We can't check for every element, which will cause great pressure on DFS.\n+\t * Therefore, in this implementation, only check the file size in\n+\t * {@link #shouldRollOnProcessingTime}, which can effectively avoid DFS pressure.\n+\t */\n+\tprivate static class HiveRollingPolicy extends CheckpointRollingPolicy<RowData, String> {\n+\n+\t\tprivate final long rollingFileSize;\n+\t\tprivate final long rollingTimeInterval;\n+\n+\t\tprivate HiveRollingPolicy(\n+\t\t\t\tlong rollingFileSize,\n+\t\t\t\tlong rollingTimeInterval) {\n+\t\t\tPreconditions.checkArgument(rollingFileSize > 0L);\n+\t\t\tPreconditions.checkArgument(rollingTimeInterval > 0L);\n+\t\t\tthis.rollingFileSize = rollingFileSize;\n+\t\t\tthis.rollingTimeInterval = rollingTimeInterval;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean shouldRollOnCheckpoint(PartFileInfo<String> partFileState) {\n+\t\t\treturn true;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean shouldRollOnEvent(PartFileInfo<String> partFileState, RowData element) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean shouldRollOnProcessingTime(\n+\t\t\t\tPartFileInfo<String> partFileState, long currentTime) {\n+\t\t\ttry {\n+\t\t\t\treturn currentTime - partFileState.getCreationTime() >= rollingTimeInterval ||\n+\t\t\t\t\t\tpartFileState.getSize() > rollingFileSize;\n+\t\t\t} catch (IOException e) {\n+\t\t\t\tthrow new UncheckedIOException(e);\n \t\t\t}\n \t\t}\n \t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYwNDU2Mg==", "url": "https://github.com/apache/flink/pull/13315#discussion_r483604562", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tthrow new TableException(String.format(\"Cannot read on the ACID table %s.\", tablePath));\n          \n          \n            \n            \t\t\t\tthrow new FlinkHiveException(String.format(\"Reading ACID table %s is not supported\", tablePath));", "author": "lirui-apache", "createdAt": "2020-09-04T13:08:56Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java", "diffHunk": "@@ -556,4 +558,16 @@ public String explainSource() {\n \tpublic boolean isAsyncEnabled() {\n \t\treturn false;\n \t}\n+\n+\tprivate void checkAcidTable() {\n+\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n+\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n+\t\t\tif (tableIsTransactional == null) {\n+\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n+\t\t\t}\n+\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n+\t\t\t\tthrow new TableException(String.format(\"Cannot read on the ACID table %s.\", tablePath));", "originalCommit": "2ed647fc736a248d536d6f5422e0a50a7119045f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\nindex be80d6b0486..7b73713263b 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSource.java\n\n@@ -558,16 +558,4 @@ public class HiveTableSource implements\n \tpublic boolean isAsyncEnabled() {\n \t\treturn false;\n \t}\n-\n-\tprivate void checkAcidTable() {\n-\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n-\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n-\t\t\tif (tableIsTransactional == null) {\n-\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n-\t\t\t}\n-\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n-\t\t\t\tthrow new TableException(String.format(\"Cannot read on the ACID table %s.\", tablePath));\n-\t\t\t}\n-\t\t}\n-\t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYwNTAxOQ==", "url": "https://github.com/apache/flink/pull/13315#discussion_r483605019", "bodyText": "Let's extract these into a util method", "author": "lirui-apache", "createdAt": "2020-09-04T13:09:46Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java", "diffHunk": "@@ -330,6 +332,18 @@ public void setOverwrite(boolean overwrite) {\n \t\tthis.overwrite = overwrite;\n \t}\n \n+\tprivate void checkAcidTable() {\n+\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n+\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n+\t\t\tif (tableIsTransactional == null) {\n+\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n+\t\t\t}\n+\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {", "originalCommit": "451016cf861606ffe150929a9ad096d7aa665259", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9e54a84b88a37bf16c73597f8bbbea8ab169f99", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\nindex 418b8fc1b18..48b2ab35411 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/HiveTableSink.java\n\n@@ -340,48 +339,6 @@ public class HiveTableSink implements AppendStreamTableSink, PartitionableTableS\n \t\t\t}\n \t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n \t\t\t\tthrow new TableException(String.format(\"Cannot write on the ACID table %s.\", identifier.asSummaryString()));\n-      }\n-    }\n-  }\n-  \n-\t/**\n-\t * Getting size of the file is too expensive. See {@link HiveBulkWriterFactory#create}.\n-\t * We can't check for every element, which will cause great pressure on DFS.\n-\t * Therefore, in this implementation, only check the file size in\n-\t * {@link #shouldRollOnProcessingTime}, which can effectively avoid DFS pressure.\n-\t */\n-\tprivate static class HiveRollingPolicy extends CheckpointRollingPolicy<RowData, String> {\n-\n-\t\tprivate final long rollingFileSize;\n-\t\tprivate final long rollingTimeInterval;\n-\n-\t\tprivate HiveRollingPolicy(\n-\t\t\t\tlong rollingFileSize,\n-\t\t\t\tlong rollingTimeInterval) {\n-\t\t\tPreconditions.checkArgument(rollingFileSize > 0L);\n-\t\t\tPreconditions.checkArgument(rollingTimeInterval > 0L);\n-\t\t\tthis.rollingFileSize = rollingFileSize;\n-\t\t\tthis.rollingTimeInterval = rollingTimeInterval;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean shouldRollOnCheckpoint(PartFileInfo<String> partFileState) {\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean shouldRollOnEvent(PartFileInfo<String> partFileState, RowData element) {\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic boolean shouldRollOnProcessingTime(\n-\t\t\t\tPartFileInfo<String> partFileState, long currentTime) {\n-\t\t\ttry {\n-\t\t\t\treturn currentTime - partFileState.getCreationTime() >= rollingTimeInterval ||\n-\t\t\t\t\t\tpartFileState.getSize() > rollingFileSize;\n-\t\t\t} catch (IOException e) {\n-\t\t\t\tthrow new UncheckedIOException(e);\n \t\t\t}\n \t\t}\n \t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYwNzY3NQ==", "url": "https://github.com/apache/flink/pull/13315#discussion_r483607675", "bodyText": "No need to insert data", "author": "lirui-apache", "createdAt": "2020-09-04T13:14:49Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java", "diffHunk": "@@ -574,6 +575,43 @@ public void testStreamCompressTextTable() throws Exception {\n \t\ttestCompressTextTable(false);\n \t}\n \n+\tprivate void testTransactionalTable(boolean batch) {\n+\t\tTableEnvironment tableEnv = batch ?\n+\t\t\tgetTableEnvWithHiveCatalog() :\n+\t\t\tgetStreamTableEnvWithHiveCatalog();\n+\t\ttableEnv.executeSql(\"create database db1\");\n+\t\ttry {\n+\t\t\ttableEnv.executeSql(\"create table db1.src (x string,y string)\");\n+\t\t\thiveShell.execute(\"create table db1.dest (x string,y string) clustered by (x) into 3 buckets stored as orc tblproperties ('transactional'='true')\");\n+\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"src\")\n+\t\t\t\t.addRow(new Object[]{\"a\", \"b\"})\n+\t\t\t\t.addRow(new Object[]{\"c\", \"d\"})\n+\t\t\t\t.commit();", "originalCommit": "b9e54a84b88a37bf16c73597f8bbbea8ab169f99", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java\nindex 739c6b8d966..31015ddc35b 100644\n--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java\n+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/TableEnvHiveConnectorITCase.java\n\n@@ -583,19 +582,17 @@ public class TableEnvHiveConnectorITCase {\n \t\ttry {\n \t\t\ttableEnv.executeSql(\"create table db1.src (x string,y string)\");\n \t\t\thiveShell.execute(\"create table db1.dest (x string,y string) clustered by (x) into 3 buckets stored as orc tblproperties ('transactional'='true')\");\n-\t\t\tHiveTestUtils.createTextTableInserter(hiveShell, \"db1\", \"src\")\n-\t\t\t\t.addRow(new Object[]{\"a\", \"b\"})\n-\t\t\t\t.addRow(new Object[]{\"c\", \"d\"})\n-\t\t\t\t.commit();\n \t\t\ttry {\n \t\t\t\tTableEnvUtil.execInsertSqlAndWaitResult(tableEnv, \"insert into db1.src select * from db1.dest\");\n-\t\t\t} catch (TableException e) {\n-\t\t\t\tassertEquals(\"Cannot read on the ACID table db1.dest.\", e.getMessage());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tassertTrue(e instanceof FlinkHiveException);\n+\t\t\t\tassertEquals(\"Reading or writing ACID table db1.dest is not supported.\", e.getMessage());\n \t\t\t}\n \t\t\ttry {\n \t\t\t\tTableEnvUtil.execInsertSqlAndWaitResult(tableEnv, \"insert into db1.dest select * from db1.src\");\n-\t\t\t} catch (TableException e) {\n-\t\t\t\tassertEquals(\"Cannot write on the ACID table test-catalog.db1.dest.\", e.getMessage());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tassertTrue(e instanceof FlinkHiveException);\n+\t\t\t\tassertEquals(\"Reading or writing ACID table db1.dest is not supported.\", e.getMessage());\n \t\t\t}\n \t\t} finally {\n \t\t\ttableEnv.executeSql(\"drop database db1 cascade\");\n"}}, {"oid": "6f796f529a1dc3aed87785eaeb251725c4dd17c2", "url": "https://github.com/apache/flink/commit/6f796f529a1dc3aed87785eaeb251725c4dd17c2", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-04T13:15:44Z", "type": "forcePushed"}, {"oid": "2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "url": "https://github.com/apache/flink/commit/2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-05T06:27:53Z", "type": "commit"}, {"oid": "2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "url": "https://github.com/apache/flink/commit/2c4a9a348dcec07dc2b990fe770132c1b5ed61a7", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-05T06:27:53Z", "type": "forcePushed"}, {"oid": "e3afac861f83784588ae5deeddbc549c4e54d09c", "url": "https://github.com/apache/flink/commit/e3afac861f83784588ae5deeddbc549c4e54d09c", "message": "Merge branch 'master' of github.com:SteNicholas/flink into hive-acid-exception\n\n* 'master' of github.com:SteNicholas/flink: (30 commits)\n  [FLINK-19036][docs-zh] Translate page 'Application Profiling & Debugging' of 'Debugging & Monitoring' into Chinese\n  [FLINK-18984][python][docs] Add tutorial documentation for Python DataStream API (#13203)\n  [FLINK-15974][python] Support to use the Python UDF directly in the Python Table API (#13325)\n  [FLINK-19121][hive] Avoid accessing HDFS frequently in HiveBulkWriterFactory\n  [FLINK-18959][Runtime] Try to revert MiniDispatcher for archiveExecutionGraph and shutdown cluster upon cancel.\n  [FLINK-18536][kinesis] Adding enhanced fan-out related configurations.\n  [FLINK-19108][table] Stop expanding the identifiers with scope aliased by the system with 'EXPR$' prefix\n  [FLINK-19118][python] Support Expression in the operations of Python Table API (#13304)\n  [FLINK-14087][datastream] Clone the StreamPartitioner to avoid being shared at runtime.\n  [FLINK-18513][Kinesis] Omitting AWS SDK services from shaded jar\n  [hotfix] Exclude test AWS credentials profile from license checks\n  [FLINK-18513][Kinesis] Inverting dependency control of KinesisProxyV2\n  [FLINK-18513][Kinesis] Adding explicit CBOR dependency to fix runtime issue\n  [FLINK-18513][Kinesis] Updated NOTICE file to reflect bundled dependencies\n  [FLINK-18513][Kinesis] Add AWS SDK v2.x dependency and KinesisProxyV2\n  [hotfix][task] Add SuppressWarnings to StreamMultipleInputProcessor\n  [FLINK-18905][hotfix][task] Simplify exception handling in StreamTask#dispatchOperatorEvent\n  [FLINK-18905][task] Allow SourceOperator chaining with MultipleInputStreamTask\n  [FLINK-18905][task/datastream] Convert OneInputStreamOperator to Input\n  [FLINK-18905][hotfix] Extract common OutputTag#isResponsibleFor with explicit Nonnull check\n  ...", "committedDate": "2020-09-05T06:31:58Z", "type": "commit"}, {"oid": "6d8f174b90ce3efd668bb7d53709d55eab2cbfa0", "url": "https://github.com/apache/flink/commit/6d8f174b90ce3efd668bb7d53709d55eab2cbfa0", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-08T10:59:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyMTMxMQ==", "url": "https://github.com/apache/flink/pull/13315#discussion_r485421311", "bodyText": "catalogTable and tablePath never null, you can take a look to the constructor of HiveTableSource", "author": "JingsongLi", "createdAt": "2020-09-09T08:11:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java", "diffHunk": "@@ -403,6 +404,25 @@ public static Table instantiateHiveTable(ObjectPath tablePath, CatalogBaseTable\n \t\t\t\t.collect(Collectors.toMap(t -> t.f0, t -> t.f1));\n \t}\n \n+\t/**\n+\t * Check whether to read or write on the hive ACID table.\n+\t *\n+\t * @param catalogTable Hive catalog table.\n+\t * @param tablePath    Identifier table path.\n+\t * @throws FlinkHiveException Thrown, if the source or sink table is transactional.\n+\t */\n+\tpublic static void checkAcidTable(CatalogTable catalogTable, ObjectPath tablePath) {\n+\t\tif (catalogTable != null && catalogTable.getOptions() != null) {", "originalCommit": "6d8f174b90ce3efd668bb7d53709d55eab2cbfa0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyNzI1Nw==", "url": "https://github.com/apache/flink/pull/13315#discussion_r485427257", "bodyText": "@JingsongLi checkAcidTable is the util method for HiveTableUtil, which should add check for the parameter catalogTable.", "author": "SteNicholas", "createdAt": "2020-09-09T08:21:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyMTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyODg4MA==", "url": "https://github.com/apache/flink/pull/13315#discussion_r485428880", "bodyText": "If really want to have a check, check should be Preconditions.check instead of silence", "author": "JingsongLi", "createdAt": "2020-09-09T08:23:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyMTMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQ2MDgzMA==", "url": "https://github.com/apache/flink/pull/13315#discussion_r485460830", "bodyText": "@JingsongLi Yeah, you are right. I remove this check for catalogTable with options.", "author": "SteNicholas", "createdAt": "2020-09-09T09:11:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQyMTMxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "5ba3f117db423f0670562a740ed990937cb03d7f", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\nindex c0766fa2b38..db36ceb5f64 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n\n@@ -412,14 +412,12 @@ public class HiveTableUtil {\n \t * @throws FlinkHiveException Thrown, if the source or sink table is transactional.\n \t */\n \tpublic static void checkAcidTable(CatalogTable catalogTable, ObjectPath tablePath) {\n-\t\tif (catalogTable != null && catalogTable.getOptions() != null) {\n-\t\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n-\t\t\tif (tableIsTransactional == null) {\n-\t\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n-\t\t\t}\n-\t\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n-\t\t\t\tthrow new FlinkHiveException(String.format(\"Reading or writing ACID table %s is not supported.\", tablePath));\n-\t\t\t}\n+\t\tString tableIsTransactional = catalogTable.getOptions().get(\"transactional\");\n+\t\tif (tableIsTransactional == null) {\n+\t\t\ttableIsTransactional = catalogTable.getOptions().get(\"transactional\".toUpperCase());\n+\t\t}\n+\t\tif (tableIsTransactional != null && tableIsTransactional.equalsIgnoreCase(\"true\")) {\n+\t\t\tthrow new FlinkHiveException(String.format(\"Reading or writing ACID table %s is not supported.\", tablePath));\n \t\t}\n \t}\n \n"}}, {"oid": "5ba3f117db423f0670562a740ed990937cb03d7f", "url": "https://github.com/apache/flink/commit/5ba3f117db423f0670562a740ed990937cb03d7f", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-09T08:59:20Z", "type": "commit"}, {"oid": "5ba3f117db423f0670562a740ed990937cb03d7f", "url": "https://github.com/apache/flink/commit/5ba3f117db423f0670562a740ed990937cb03d7f", "message": "[FLINK-19070][hive] Hive connector should throw a meaningful exception if user reads/writes ACID tables", "committedDate": "2020-09-09T08:59:20Z", "type": "forcePushed"}]}