{"pr_number": 12202, "pr_title": "[FLINK-17791][table][streaming] Support collecting query results under all execution and network environments", "pr_createdAt": "2020-05-17T14:04:14Z", "pr_url": "https://github.com/apache/flink/pull/12202", "timeline": [{"oid": "dc4b338596d9d3dee5dae9b1dfa3bebe1a5d902d", "url": "https://github.com/apache/flink/commit/dc4b338596d9d3dee5dae9b1dfa3bebe1a5d902d", "message": "[FLINK-14807][table] Support collecting query results under all execution and network environments", "committedDate": "2020-05-18T01:59:42Z", "type": "commit"}, {"oid": "171a600b1bac7b86c34dc1f9679ccefc38037adf", "url": "https://github.com/apache/flink/commit/171a600b1bac7b86c34dc1f9679ccefc38037adf", "message": "[fix] Remove execute config options for collect as data stream can also use this collect iterator", "committedDate": "2020-05-18T06:33:16Z", "type": "commit"}, {"oid": "3a44fbe9824e576068a4f172ca5738a7dd5cf9d1", "url": "https://github.com/apache/flink/commit/3a44fbe9824e576068a4f172ca5738a7dd5cf9d1", "message": "[addition] Change implementation of DataStreamUtils#collect to the current implementation", "committedDate": "2020-05-18T06:37:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQxNDc4OA==", "url": "https://github.com/apache/flink/pull/12202#discussion_r426414788", "bodyText": "id is meaningless  here, use \"data stream collect\" ?", "author": "godfreyhe", "createdAt": "2020-05-18T07:19:10Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamUtils.java", "diffHunk": "@@ -47,44 +44,30 @@\n \t * Returns an iterator to iterate over the elements of the DataStream.\n \t * @return The iterator\n \t */\n-\tpublic static <OUT> Iterator<OUT> collect(DataStream<OUT> stream) throws IOException {\n-\n+\tpublic static <OUT> Iterator<OUT> collect(DataStream<OUT> stream) {\n \t\tTypeSerializer<OUT> serializer = stream.getType().createSerializer(\n-\t\t\t\tstream.getExecutionEnvironment().getConfig());\n+\t\t\tstream.getExecutionEnvironment().getConfig());\n+\t\tString id = UUID.randomUUID().toString();\n+\t\tString accumulatorName = \"dataStreamCollect_\" + id;\n \n-\t\tSocketStreamIterator<OUT> iter = new SocketStreamIterator<OUT>(serializer);\n+\t\tCollectSinkOperatorFactory<OUT> factory = new CollectSinkOperatorFactory<>(serializer, accumulatorName);\n+\t\tCollectSinkOperator<OUT> operator = (CollectSinkOperator<OUT>) factory.getOperator();\n+\t\tCollectResultIterator<OUT> iterator = new CollectResultIterator<>(\n+\t\t\toperator.getOperatorIdFuture(), serializer, accumulatorName);\n+\t\tCollectStreamSink<OUT> sink = new CollectStreamSink<>(stream, factory);\n+\t\tsink.name(\"Data stream collect sink\");\n \n-\t\t//Find out what IP of us should be given to CollectSink, that it will be able to connect to\n \t\tStreamExecutionEnvironment env = stream.getExecutionEnvironment();\n-\t\tInetAddress clientAddress;\n-\n-\t\tif (env instanceof RemoteStreamEnvironment) {\n-\t\t\tString host = ((RemoteStreamEnvironment) env).getHost();\n-\t\t\tint port = ((RemoteStreamEnvironment) env).getPort();\n-\t\t\ttry {\n-\t\t\t\tclientAddress = ConnectionUtils.findConnectingAddress(new InetSocketAddress(host, port), 2000, 400);\n-\t\t\t}\n-\t\t\tcatch (Exception e) {\n-\t\t\t\tthrow new IOException(\"Could not determine an suitable network address to \" +\n-\t\t\t\t\t\t\"receive back data from the streaming program.\", e);\n-\t\t\t}\n-\t\t} else if (env instanceof LocalStreamEnvironment) {\n-\t\t\tclientAddress = InetAddress.getLoopbackAddress();\n-\t\t} else {\n-\t\t\ttry {\n-\t\t\t\tclientAddress = InetAddress.getLocalHost();\n-\t\t\t} catch (UnknownHostException e) {\n-\t\t\t\tthrow new IOException(\"Could not determine this machines own local address to \" +\n-\t\t\t\t\t\t\"receive back data from the streaming program.\", e);\n-\t\t\t}\n-\t\t}\n-\n-\t\tDataStreamSink<OUT> sink = stream.addSink(new CollectSink<OUT>(clientAddress, iter.getPort(), serializer));\n-\t\tsink.setParallelism(1); // It would not work if multiple instances would connect to the same port\n+\t\tenv.addOperator(sink.getTransformation());\n \n-\t\t(new CallExecute(env, iter)).start();\n+\t\ttry {\n+\t\t\tJobClient jobClient = env.executeAsync(\"DataStreamCollect_\" + id);", "originalCommit": "3a44fbe9824e576068a4f172ca5738a7dd5cf9d1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d70dafb893db6a61dcbc1b614349e9164aafeab", "chunk": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamUtils.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamUtils.java\nindex ddee1f1685d..a4f1560710b 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamUtils.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamUtils.java\n\n@@ -47,8 +47,7 @@ public final class DataStreamUtils {\n \tpublic static <OUT> Iterator<OUT> collect(DataStream<OUT> stream) {\n \t\tTypeSerializer<OUT> serializer = stream.getType().createSerializer(\n \t\t\tstream.getExecutionEnvironment().getConfig());\n-\t\tString id = UUID.randomUUID().toString();\n-\t\tString accumulatorName = \"dataStreamCollect_\" + id;\n+\t\tString accumulatorName = \"dataStreamCollect_\" + UUID.randomUUID().toString();\n \n \t\tCollectSinkOperatorFactory<OUT> factory = new CollectSinkOperatorFactory<>(serializer, accumulatorName);\n \t\tCollectSinkOperator<OUT> operator = (CollectSinkOperator<OUT>) factory.getOperator();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQxNTc2Ng==", "url": "https://github.com/apache/flink/pull/12202#discussion_r426415766", "bodyText": "CollectStreamSink  is not only used for select query but also for DataStream collect, so make this comment more generic.", "author": "godfreyhe", "createdAt": "2020-05-18T07:21:10Z", "path": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectStreamSink.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.operators.collect;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.transformations.SinkTransformation;\n+\n+/**\n+ * A {@link DataStreamSink} which is used to collect query results.", "originalCommit": "3a44fbe9824e576068a4f172ca5738a7dd5cf9d1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d70dafb893db6a61dcbc1b614349e9164aafeab", "chunk": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectStreamSink.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectStreamSink.java\nindex 031047b14c1..53962365126 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectStreamSink.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/collect/CollectStreamSink.java\n\n@@ -24,7 +24,7 @@ import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.transformations.SinkTransformation;\n \n /**\n- * A {@link DataStreamSink} which is used to collect query results.\n+ * A {@link DataStreamSink} which is used to collect results of a data stream.\n  * It completely overwrites {@link DataStreamSink} so that its own transformation is manipulated.\n  */\n @Internal\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjQxNzAyMg==", "url": "https://github.com/apache/flink/pull/12202#discussion_r426417022", "bodyText": "we can extract the most code of BatchSelectTableSink and StreamSelectTableSink  into a base class (SelectTableSinkBase)", "author": "godfreyhe", "createdAt": "2020-05-18T07:23:40Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java", "diffHunk": "@@ -49,19 +44,24 @@\n  * once FLINK-14807 is finished, the implementation should be changed.\n  */\n public class BatchSelectTableSink implements StreamTableSink<Row>, SelectTableSink {\n+\n \tprivate final TableSchema tableSchema;\n-\tprivate final String accumulatorName;\n-\tprivate final TypeSerializer<Row> typeSerializer;\n-\tprivate JobClient jobClient;\n+\tprivate final CollectSinkOperatorFactory<Row> factory;\n+\tprivate final CollectResultIterator<Row> iterator;\n \n \t@SuppressWarnings(\"unchecked\")\n \tpublic BatchSelectTableSink(TableSchema tableSchema) {", "originalCommit": "3a44fbe9824e576068a4f172ca5738a7dd5cf9d1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d70dafb893db6a61dcbc1b614349e9164aafeab", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java\nindex 57b6b34a8e4..4ff80d459b1 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java\n\n@@ -18,76 +18,25 @@\n \n package org.apache.flink.table.planner.sinks;\n \n-import org.apache.flink.api.common.ExecutionConfig;\n-import org.apache.flink.api.common.typeutils.TypeSerializer;\n-import org.apache.flink.core.execution.JobClient;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n-import org.apache.flink.streaming.api.operators.collect.CollectResultIterator;\n-import org.apache.flink.streaming.api.operators.collect.CollectSinkOperator;\n-import org.apache.flink.streaming.api.operators.collect.CollectSinkOperatorFactory;\n-import org.apache.flink.streaming.api.operators.collect.CollectStreamSink;\n import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.api.internal.SelectTableSink;\n-import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter;\n import org.apache.flink.table.sinks.StreamTableSink;\n-import org.apache.flink.table.types.DataType;\n import org.apache.flink.types.Row;\n \n-import java.util.Iterator;\n-import java.util.UUID;\n \n /**\n  * A {@link SelectTableSink} for batch select job.\n- *\n- * <p><strong>NOTES:</strong> This is a temporary solution,\n- * once FLINK-14807 is finished, the implementation should be changed.\n  */\n-public class BatchSelectTableSink implements StreamTableSink<Row>, SelectTableSink {\n-\n-\tprivate final TableSchema tableSchema;\n-\tprivate final CollectSinkOperatorFactory<Row> factory;\n-\tprivate final CollectResultIterator<Row> iterator;\n+public class BatchSelectTableSink extends SelectTableSinkBase implements StreamTableSink<Row> {\n \n-\t@SuppressWarnings(\"unchecked\")\n \tpublic BatchSelectTableSink(TableSchema tableSchema) {\n-\t\tthis.tableSchema = SelectTableSinkSchemaConverter.convertTimeAttributeToRegularTimestamp(\n-\t\t\t\tSelectTableSinkSchemaConverter.changeDefaultConversionClass(tableSchema));\n-\n-\t\tTypeSerializer<Row> typeSerializer = (TypeSerializer<Row>) TypeInfoDataTypeConverter\n-\t\t\t\t.fromDataTypeToTypeInfo(this.tableSchema.toRowDataType())\n-\t\t\t\t.createSerializer(new ExecutionConfig());\n-\t\tString accumulatorName = \"tableResultCollect_\" + UUID.randomUUID();\n-\n-\t\tthis.factory = new CollectSinkOperatorFactory<>(typeSerializer, accumulatorName);\n-\t\tCollectSinkOperator<Row> operator = (CollectSinkOperator<Row>) factory.getOperator();\n-\t\tthis.iterator = new CollectResultIterator<>(operator.getOperatorIdFuture(), typeSerializer, accumulatorName);\n-\t}\n-\n-\t@Override\n-\tpublic DataType getConsumedDataType() {\n-\t\treturn tableSchema.toRowDataType();\n-\t}\n-\n-\t@Override\n-\tpublic TableSchema getTableSchema() {\n-\t\treturn tableSchema;\n+\t\tsuper(tableSchema);\n \t}\n \n \t@Override\n \tpublic DataStreamSink<?> consumeDataStream(DataStream<Row> dataStream) {\n-\t\tCollectStreamSink<Row> sink = new CollectStreamSink<>(dataStream, factory);\n-\t\tdataStream.getExecutionEnvironment().addOperator(sink.getTransformation());\n-\t\treturn sink.name(\"Batch select table sink\");\n-\t}\n-\n-\t@Override\n-\tpublic void setJobClient(JobClient jobClient) {\n-\t\titerator.setJobClient(jobClient);\n-\t}\n-\n-\t@Override\n-\tpublic Iterator<Row> getResultIterator() {\n-\t\treturn iterator;\n+\t\treturn super.consumeDataStream(dataStream);\n \t}\n }\n"}}, {"oid": "3d70dafb893db6a61dcbc1b614349e9164aafeab", "url": "https://github.com/apache/flink/commit/3d70dafb893db6a61dcbc1b614349e9164aafeab", "message": "[fix] Fix godfrey's comments", "committedDate": "2020-05-18T07:28:33Z", "type": "commit"}]}