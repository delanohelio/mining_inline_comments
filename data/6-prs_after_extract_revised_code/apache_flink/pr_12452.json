{"pr_number": 12452, "pr_title": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts", "pr_createdAt": "2020-06-03T02:41:00Z", "pr_url": "https://github.com/apache/flink/pull/12452", "timeline": [{"oid": "fef42fb8497d9822a12ab18114ecb0ea4f36509f", "url": "https://github.com/apache/flink/commit/fef42fb8497d9822a12ab18114ecb0ea4f36509f", "message": "Fix checkstyle", "committedDate": "2020-06-04T16:25:33Z", "type": "forcePushed"}, {"oid": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "url": "https://github.com/apache/flink/commit/3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts", "committedDate": "2020-06-05T08:02:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MjM5OA==", "url": "https://github.com/apache/flink/pull/12452#discussion_r435782398", "bodyText": "just 12 + pathBytes.length + inProgressBytes.length", "author": "JingsongLi", "createdAt": "2020-06-05T08:53:14Z", "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -142,14 +150,21 @@ public int getVersion() {\n \t\t\t\tthrow new UnsupportedOperationException(\"Only HadoopPathBasedPendingFileRecoverable is supported.\");\n \t\t\t}\n \n-\t\t\tPath path = ((HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable).getPath();\n+\t\t\tHadoopPathBasedPendingFileRecoverable hadoopRecoverable =\n+\t\t\t\t(HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable;\n+\t\t\tPath path = hadoopRecoverable.getPath();\n+\t\t\tPath inProgressPath = hadoopRecoverable.getInProgressPath();\n+\n \t\t\tbyte[] pathBytes = path.toUri().toString().getBytes(CHARSET);\n+\t\t\tbyte[] inProgressBytes = inProgressPath.toUri().toString().getBytes(CHARSET);\n \n-\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length];\n+\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length + 4 + inProgressBytes.length];", "originalCommit": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "248476005c601939c42dab33d7f689809090ff35", "chunk": "diff --git a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\nindex 9de8e4585ea..088e8792f6c 100644\n--- a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n+++ b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n\n@@ -152,13 +152,13 @@ public class HadoopPathBasedPartFileWriter<IN, BucketID> extends AbstractPartFil\n \n \t\t\tHadoopPathBasedPendingFileRecoverable hadoopRecoverable =\n \t\t\t\t(HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable;\n-\t\t\tPath path = hadoopRecoverable.getPath();\n-\t\t\tPath inProgressPath = hadoopRecoverable.getInProgressPath();\n+\t\t\tPath path = hadoopRecoverable.getTargetFilePath();\n+\t\t\tPath inProgressPath = hadoopRecoverable.getTempFilePath();\n \n \t\t\tbyte[] pathBytes = path.toUri().toString().getBytes(CHARSET);\n \t\t\tbyte[] inProgressBytes = inProgressPath.toUri().toString().getBytes(CHARSET);\n \n-\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length + 4 + inProgressBytes.length];\n+\t\t\tbyte[] targetBytes = new byte[12 + pathBytes.length + inProgressBytes.length];\n \t\t\tByteBuffer bb = ByteBuffer.wrap(targetBytes).order(ByteOrder.LITTLE_ENDIAN);\n \t\t\tbb.putInt(MAGIC_NUMBER);\n \t\t\tbb.putInt(pathBytes.length);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MzE2Mg==", "url": "https://github.com/apache/flink/pull/12452#discussion_r435783162", "bodyText": "I think you can add some comments to explain in HadoopPathBasedPartFileWriter, what we store in state.", "author": "JingsongLi", "createdAt": "2020-06-05T08:54:39Z", "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -142,14 +150,21 @@ public int getVersion() {\n \t\t\t\tthrow new UnsupportedOperationException(\"Only HadoopPathBasedPendingFileRecoverable is supported.\");\n \t\t\t}\n ", "originalCommit": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "248476005c601939c42dab33d7f689809090ff35", "chunk": "diff --git a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\nindex 9de8e4585ea..088e8792f6c 100644\n--- a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n+++ b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n\n@@ -152,13 +152,13 @@ public class HadoopPathBasedPartFileWriter<IN, BucketID> extends AbstractPartFil\n \n \t\t\tHadoopPathBasedPendingFileRecoverable hadoopRecoverable =\n \t\t\t\t(HadoopPathBasedPendingFileRecoverable) pendingFileRecoverable;\n-\t\t\tPath path = hadoopRecoverable.getPath();\n-\t\t\tPath inProgressPath = hadoopRecoverable.getInProgressPath();\n+\t\t\tPath path = hadoopRecoverable.getTargetFilePath();\n+\t\t\tPath inProgressPath = hadoopRecoverable.getTempFilePath();\n \n \t\t\tbyte[] pathBytes = path.toUri().toString().getBytes(CHARSET);\n \t\t\tbyte[] inProgressBytes = inProgressPath.toUri().toString().getBytes(CHARSET);\n \n-\t\t\tbyte[] targetBytes = new byte[8 + pathBytes.length + 4 + inProgressBytes.length];\n+\t\t\tbyte[] targetBytes = new byte[12 + pathBytes.length + inProgressBytes.length];\n \t\t\tByteBuffer bb = ByteBuffer.wrap(targetBytes).order(ByteOrder.LITTLE_ENDIAN);\n \t\t\tbb.putInt(MAGIC_NUMBER);\n \t\t\tbb.putInt(pathBytes.length);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4MzkxMA==", "url": "https://github.com/apache/flink/pull/12452#discussion_r435783910", "bodyText": "Can be consistent with HadoopFsRecoverable, Path targetFile, Path tempFile", "author": "JingsongLi", "createdAt": "2020-06-05T08:56:00Z", "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java", "diffHunk": "@@ -103,21 +103,29 @@ public void commitAfterRecovery() throws IOException {\n \n \t\tpublic PendingFileRecoverable getRecoverable() {\n \t\t\treturn new HadoopPathBasedPendingFileRecoverable(\n-\t\t\t\tfileCommitter.getTargetFilePath());\n+\t\t\t\tfileCommitter.getTargetFilePath(),\n+\t\t\t\tfileCommitter.getInProgressFilePath());\n \t\t}\n \t}\n \n \t@VisibleForTesting\n \tstatic class HadoopPathBasedPendingFileRecoverable implements PendingFileRecoverable {\n \t\tprivate final Path path;\n \n-\t\tpublic HadoopPathBasedPendingFileRecoverable(Path path) {\n+\t\tprivate final Path inProgressPath;\n+\n+\t\tpublic HadoopPathBasedPendingFileRecoverable(Path path, Path inProgressPath) {", "originalCommit": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "248476005c601939c42dab33d7f689809090ff35", "chunk": "diff --git a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\nindex 9de8e4585ea..088e8792f6c 100644\n--- a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n+++ b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/HadoopPathBasedPartFileWriter.java\n\n@@ -104,27 +104,27 @@ public class HadoopPathBasedPartFileWriter<IN, BucketID> extends AbstractPartFil\n \t\tpublic PendingFileRecoverable getRecoverable() {\n \t\t\treturn new HadoopPathBasedPendingFileRecoverable(\n \t\t\t\tfileCommitter.getTargetFilePath(),\n-\t\t\t\tfileCommitter.getInProgressFilePath());\n+\t\t\t\tfileCommitter.getTempFilePath());\n \t\t}\n \t}\n \n \t@VisibleForTesting\n \tstatic class HadoopPathBasedPendingFileRecoverable implements PendingFileRecoverable {\n-\t\tprivate final Path path;\n+\t\tprivate final Path targetFilePath;\n \n-\t\tprivate final Path inProgressPath;\n+\t\tprivate final Path tempFilePath;\n \n-\t\tpublic HadoopPathBasedPendingFileRecoverable(Path path, Path inProgressPath) {\n-\t\t\tthis.path = path;\n-\t\t\tthis.inProgressPath = inProgressPath;\n+\t\tpublic HadoopPathBasedPendingFileRecoverable(Path targetFilePath, Path tempFilePath) {\n+\t\t\tthis.targetFilePath = targetFilePath;\n+\t\t\tthis.tempFilePath = tempFilePath;\n \t\t}\n \n-\t\tpublic Path getPath() {\n-\t\t\treturn path;\n+\t\tpublic Path getTargetFilePath() {\n+\t\t\treturn targetFilePath;\n \t\t}\n \n-\t\tpublic Path getInProgressPath() {\n-\t\t\treturn inProgressPath;\n+\t\tpublic Path getTempFilePath() {\n+\t\t\treturn tempFilePath;\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc4NTc2Mw==", "url": "https://github.com/apache/flink/pull/12452#discussion_r435785763", "bodyText": "\".inprogress\" -> \".inprogress.\"", "author": "JingsongLi", "createdAt": "2020-06-05T08:59:13Z", "path": "flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java", "diffHunk": "@@ -96,12 +107,19 @@ private void rename(boolean assertFileExists) throws IOException {\n \t\t}\n \t}\n \n-\tprivate Path generateInProgressFilePath() {\n+\tprivate Path generateInProgressFilePath() throws IOException {\n \t\tcheckArgument(targetFilePath.isAbsolute(), \"Target file must be absolute\");\n \n+\t\tFileSystem fileSystem = FileSystem.get(targetFilePath.toUri(), configuration);\n+\n \t\tPath parent = targetFilePath.getParent();\n \t\tString name = targetFilePath.getName();\n \n-\t\treturn new Path(parent, \".\" + name + \".inprogress\");\n+\t\twhile (true) {\n+\t\t\tPath candidate = new Path(parent, \".\" + name + \".inprogress\" + UUID.randomUUID().toString());", "originalCommit": "3f7e6da089bea0dd3a2f2fc2bfafb88a13aa482c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "248476005c601939c42dab33d7f689809090ff35", "chunk": "diff --git a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java\nindex c88b49d7c51..2c690cd2506 100644\n--- a/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java\n+++ b/flink-formats/flink-hadoop-bulk/src/main/java/org/apache/flink/formats/hadoop/bulk/committer/HadoopRenameFileCommitter.java\n\n@@ -99,15 +98,15 @@ public class HadoopRenameFileCommitter implements HadoopFileCommitter {\n \n \t\ttry {\n \t\t\t// If file exists, it will be overwritten.\n-\t\t\tfileSystem.rename(inProgressFilePath, targetFilePath);\n+\t\t\tfileSystem.rename(tempFilePath, targetFilePath);\n \t\t} catch (IOException e) {\n \t\t\tthrow new IOException(\n-\t\t\t\tString.format(\"Could not commit file from %s to %s\", inProgressFilePath, targetFilePath),\n+\t\t\t\tString.format(\"Could not commit file from %s to %s\", tempFilePath, targetFilePath),\n \t\t\t\te);\n \t\t}\n \t}\n \n-\tprivate Path generateInProgressFilePath() throws IOException {\n+\tprivate Path generateTempFilePath() throws IOException {\n \t\tcheckArgument(targetFilePath.isAbsolute(), \"Target file must be absolute\");\n \n \t\tFileSystem fileSystem = FileSystem.get(targetFilePath.toUri(), configuration);\n"}}, {"oid": "248476005c601939c42dab33d7f689809090ff35", "url": "https://github.com/apache/flink/commit/248476005c601939c42dab33d7f689809090ff35", "message": "Remove the S3 tests", "committedDate": "2020-06-08T11:39:21Z", "type": "forcePushed"}, {"oid": "de8e31736af54c3a8e00a38994b949051aa1b98c", "url": "https://github.com/apache/flink/commit/de8e31736af54c3a8e00a38994b949051aa1b98c", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts", "committedDate": "2020-06-09T03:03:12Z", "type": "commit"}, {"oid": "de8e31736af54c3a8e00a38994b949051aa1b98c", "url": "https://github.com/apache/flink/commit/de8e31736af54c3a8e00a38994b949051aa1b98c", "message": "[FLINK-18056][fs-connector] Hadoop path-based file writer adds UUID to in-progress file to avoid conflicts", "committedDate": "2020-06-09T03:03:12Z", "type": "forcePushed"}]}