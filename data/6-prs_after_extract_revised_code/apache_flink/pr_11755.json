{"pr_number": 11755, "pr_title": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory", "pr_createdAt": "2020-04-15T12:47:24Z", "pr_url": "https://github.com/apache/flink/pull/11755", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTU3Mg==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665572", "bodyText": "Can we abstract RowCsvInputFormatTest to reuse code?", "author": "JingsongLi", "createdAt": "2020-04-22T04:43:55Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java", "diffHunk": "@@ -0,0 +1,798 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.dataformat.SqlTimestamp;\n+import org.apache.flink.table.dataformat.TypeGetterSetters;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.charset.StandardCharsets;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static junit.framework.TestCase.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test suites for {@link BaseRowCsvInputformat}.\n+ */\n+public class BaseRowCsvInputformatTest {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dff5cdb51db746c02151afd5b4624555def81fac", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java\ndeleted file mode 100644\nindex 3dafded1b9f..00000000000\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvInputformatTest.java\n+++ /dev/null\n\n@@ -1,798 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.formats.csv;\n-\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n-import org.apache.flink.table.api.DataTypes;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.dataformat.BaseRow;\n-import org.apache.flink.table.dataformat.GenericRow;\n-import org.apache.flink.table.dataformat.SqlTimestamp;\n-import org.apache.flink.table.dataformat.TypeGetterSetters;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n-\n-import org.junit.Test;\n-\n-import java.io.File;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.io.OutputStreamWriter;\n-import java.nio.charset.StandardCharsets;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.util.ArrayList;\n-import java.util.List;\n-\n-import static junit.framework.TestCase.assertEquals;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n-/**\n- * Test suites for {@link BaseRowCsvInputformat}.\n- */\n-public class BaseRowCsvInputformatTest {\n-\n-\tstatic final Path PATH = new Path(\"an/ignored/file/\");\n-\tprivate static final List<String> PARTITION_KEYS = new ArrayList();\n-\n-\t// static variables for testing the removal of \\r\\n to \\n\n-\tprivate static final String FIRST_PART = \"That is the first part\";\n-\tprivate static final String SECOND_PART = \"That is the second part\";\n-\n-\t@Test\n-\tpublic void ignoreInvalidLines() throws Exception {\n-\t\tString fileContent =\n-\t\t\t\"#description of the data\\n\" +\n-\t\t\t\t\"header1|header2|header3|\\n\" +\n-\t\t\t\t\"this is|1|2.0|\\n\" +\n-\t\t\t\t\"//a comment\\n\" +\n-\t\t\t\t\"a test|3|4.0|\\n\" +\n-\t\t\t\t\"#next|5|6.0|\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.DOUBLE()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setIgnoreParseErrors(false);\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tConfiguration parameters = new Configuration();\n-\t\tformat.configure(parameters);\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\t\ttry {\n-\t\t\tresult = format.nextRecord(result);\n-\t\t\tfail(\"RuntimeException was not thrown! (Row length mismatch. 3 fields expected but was 1)\");\n-\t\t} catch (IOException ignored) {\n-\t\t} // => ok\n-\n-\t\ttry {\n-\t\t\tresult = format.nextRecord(result);\n-\t\t\tfail(\"NumberFormatException was not thrown! (For input string: \\\"header2\\\")\");\n-\t\t} catch (IOException ignored) {\n-\t\t} // => ok\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"this is\", getField(result,  fieldTypes, 0));\n-\t\tassertEquals(1, getField(result, fieldTypes, 1));\n-\t\tassertEquals(2.0, getField(result, fieldTypes, 2));\n-\n-\t\ttry {\n-\t\t\tresult = format.nextRecord(result);\n-\t\t\tfail(\"RuntimeException was not thrown! (Row length mismatch. 3 fields expected but was 1)\");\n-\t\t} catch (IOException ignored) {\n-\t\t} // => ok\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"a test\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(3, getField(result, fieldTypes, 1));\n-\t\tassertEquals(4.0, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"#next\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(5, getField(result, fieldTypes, 1));\n-\t\tassertEquals(6.0, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\n-\t\t// re-open with lenient = true\n-\t\tbuilder.setIgnoreParseErrors(true);\n-\t\tformat = builder.build();\n-\t\tformat.configure(parameters);\n-\t\tformat.open(split);\n-\n-\t\tresult = new GenericRow(3);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"#description of the data\", getField(result, fieldTypes, 0));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"header1\", getField(result, fieldTypes, 0));\n-\t\tassertNull(getField(result, fieldTypes, 1));\n-\t\tassertNull(getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"this is\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(1, getField(result, fieldTypes, 1));\n-\t\tassertEquals(2.0, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"//a comment\", getField(result, fieldTypes, 0));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"a test\",  getField(result, fieldTypes, 0));\n-\t\tassertEquals(3,  getField(result, fieldTypes, 1));\n-\t\tassertEquals(4.0,  getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"#next\",  getField(result, fieldTypes, 0));\n-\t\tassertEquals(5,  getField(result, fieldTypes, 1));\n-\t\tassertEquals(6.0,  getField(result, fieldTypes, 2));\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t}\n-\n-\t@Test\n-\tpublic void ignorePrefixComments() throws Exception {\n-\t\tString fileContent =\n-\t\t\t\"#description of the data\\n\" +\n-\t\t\t\t\"#successive commented line\\n\" +\n-\t\t\t\t\"this is|1|2.0|\\n\" +\n-\t\t\t\t\"a test|3|4.0|\\n\" +\n-\t\t\t\t\"#next|5|6.0|\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.DOUBLE()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setAllowComments(true);\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"this is\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(1, getField(result, fieldTypes, 1));\n-\t\tassertEquals(2.0, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"a test\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(3, getField(result, fieldTypes, 1));\n-\t\tassertEquals(4.0, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t}\n-\n-\t@Test\n-\tpublic void readStringFields() throws Exception {\n-\t\tString fileContent = \"abc|def|ghijk\\nabc||hhg\\n|||\\n||\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"def\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"ghijk\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"hhg\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void readMixedQuotedStringFields() throws Exception {\n-\t\tString fileContent = \"@a|b|c@|def|@ghijk@\\nabc||@|hhg@\\n|||\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setQuoteCharacter('@');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"a|b|c\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"def\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"ghijk\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"|hhg\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testTailingEmptyFields() throws Exception {\n-\t\tString fileContent = \"abc|def|ghijk\\n\" +\n-\t\t\t\"abc|def|\\n\" +\n-\t\t\t\"abc||\\n\" +\n-\t\t\t\"|||\\n\" +\n-\t\t\t\"||\\n\" +\n-\t\t\t\"abc|def\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"def\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"ghijk\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"def\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"abc\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 0));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 1));\n-\t\tassertEquals(\"\", getField(result, fieldTypes, 2));\n-\n-\t\ttry {\n-\t\t\tformat.nextRecord(result);\n-\t\t\tfail(\"RuntimeException: Row length mismatch. 3 fields expected but was 2\");\n-\t\t} catch (IOException e) {}\n-\t}\n-\n-\t@Test\n-\tpublic void testIntegerFields() throws Exception {\n-\t\tString fileContent = \"111|222|333|444|555\\n666|777|888|999|000|\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(5);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(111, getField(result, fieldTypes, 0));\n-\t\tassertEquals(222, getField(result, fieldTypes, 1));\n-\t\tassertEquals(333, getField(result, fieldTypes, 2));\n-\t\tassertEquals(444, getField(result, fieldTypes, 3));\n-\t\tassertEquals(555, getField(result, fieldTypes, 4));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(666, getField(result, fieldTypes, 0));\n-\t\tassertEquals(777, getField(result, fieldTypes, 1));\n-\t\tassertEquals(888, getField(result, fieldTypes, 2));\n-\t\tassertEquals(999, getField(result, fieldTypes, 3));\n-\t\tassertEquals(0, getField(result, fieldTypes, 4));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testEmptyFields() throws Exception {\n-\t\tString fileContent =\n-\t\t\t\",,,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,\\n\" +\n-\t\t\t\t\",,,,,,,,\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.BOOLEAN(),\n-\t\t\tDataTypes.BYTES(),\n-\t\t\tDataTypes.DOUBLE(),\n-\t\t\tDataTypes.FLOAT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.BIGINT(),\n-\t\t\tDataTypes.SMALLINT(),\n-\t\t\tDataTypes.STRING()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter(',')\n-\t\t\t.setNullLiteral(\"\");\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(8);\n-\t\tint linesCnt = fileContent.split(\"\\n\").length;\n-\n-\t\tfor (int i = 0; i < linesCnt; i++) {\n-\t\t\tresult = format.nextRecord(result);\n-\t\t\tassertNull(getField(result, fieldTypes, i));\n-\t\t}\n-\n-\t\t// ensure no more rows\n-\t\tassertNull(format.nextRecord(result));\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testDoubleFields() throws Exception {\n-\t\tString fileContent = \"11.1|22.2|33.3|44.4|55.5\\n66.6|77.7|88.8|99.9|00.0|\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.DOUBLE(),\n-\t\t\tDataTypes.DOUBLE(),\n-\t\t\tDataTypes.DOUBLE(),\n-\t\t\tDataTypes.DOUBLE(),\n-\t\t\tDataTypes.DOUBLE()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(5);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(11.1, getField(result, fieldTypes, 0));\n-\t\tassertEquals(22.2, getField(result, fieldTypes, 1));\n-\t\tassertEquals(33.3, getField(result, fieldTypes, 2));\n-\t\tassertEquals(44.4, getField(result, fieldTypes, 3));\n-\t\tassertEquals(55.5, getField(result, fieldTypes, 4));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(66.6, getField(result, fieldTypes, 0));\n-\t\tassertEquals(77.7, getField(result, fieldTypes, 1));\n-\t\tassertEquals(88.8, getField(result, fieldTypes, 2));\n-\t\tassertEquals(99.9, getField(result, fieldTypes, 3));\n-\t\tassertEquals(0.0, getField(result, fieldTypes, 4));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testReadSparseWithPositionSetter() throws Exception {\n-\t\tString fileContent = \"111|222|333|444|555|666|777|888|999|000|\\n\" +\n-\t\t\t\"000|999|888|777|666|555|444|333|222|111|\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setSelectedFields(new int[]{0, 3, 7});\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\t\tresult = format.nextRecord(result);\n-\n-\t\tassertNotNull(result);\n-\t\tassertEquals(111, getField(result, fieldTypes, 0));\n-\t\tassertEquals(444, getField(result, fieldTypes, 1));\n-\t\tassertEquals(888, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(0, getField(result, fieldTypes, 0));\n-\t\tassertEquals(777, getField(result, fieldTypes, 1));\n-\t\tassertEquals(333, getField(result, fieldTypes, 2));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testWindowsLineEndRemoval() throws Exception {\n-\n-\t\t// check typical use case -- linux file is correct and it is set up to linux(\\n)\n-\t\ttestRemovingTrailingCR(\"\\n\");\n-\n-\t\t// check typical windows case -- windows file endings and file has windows file endings set up\n-\t\ttestRemovingTrailingCR(\"\\r\\n\");\n-\n-\t\t// check problematic case windows file -- windows file endings(\\r\\n)\n-\t\t// but linux line endings (\\n) set up\n-\t\ttestRemovingTrailingCR(\"\\r\\n\");\n-\n-\t\t// check problematic case linux file -- linux file endings (\\n)\n-\t\t// but windows file endings set up (\\r\\n)\n-\t\t// specific setup for windows line endings will expect \\r\\n because\n-\t\t// it has to be set up and is not standard.\n-\t}\n-\n-\t@Test\n-\tpublic void testQuotedStringParsingWithIncludeFields() throws Exception {\n-\t\tString fileContent = \"\\\"20:41:52-1-3-2015\\\"|\\\"Re: Taskmanager memory error in Eclipse\\\"|\" +\n-\t\t\t\"\\\"Blahblah <blah@blahblah.org>\\\"|\\\"blaaa\\\"|\\\"blubb\\\"\";\n-\t\tFile tempFile = File.createTempFile(\"CsvReaderQuotedString\", \"tmp\");\n-\t\ttempFile.deleteOnExit();\n-\t\ttempFile.setWritable(true);\n-\n-\t\tOutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tempFile));\n-\t\twriter.write(fileContent);\n-\t\twriter.close();\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, new Path(tempFile.toURI().toString()))\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setSelectedFields(new int[]{0, 2})\n-\t\t\t.setQuoteCharacter('\"');\n-\n-\t\tBaseRowCsvInputformat inputFormat = builder.build();\n-\t\tinputFormat.configure(new Configuration());\n-\n-\t\tFileInputSplit[] splits = inputFormat.createInputSplits(1);\n-\t\tinputFormat.open(splits[0]);\n-\n-\t\tBaseRow record = inputFormat.nextRecord(new GenericRow(2));\n-\t\tassertEquals(\"20:41:52-1-3-2015\", getField(record, fieldTypes, 0));\n-\t\tassertEquals(\"Blahblah <blah@blahblah.org>\", getField(record, fieldTypes, 1));\n-\t}\n-\n-\t@Test\n-\tpublic void testQuotedStringParsingWithEscapedQuotes() throws Exception {\n-\t\tString fileContent = \"\\\"\\\\\\\"Hello\\\\\\\" World\\\"|\\\"We are\\\\\\\" young\\\"\";\n-\t\tFile tempFile = File.createTempFile(\"CsvReaderQuotedString\", \"tmp\");\n-\t\ttempFile.deleteOnExit();\n-\t\ttempFile.setWritable(true);\n-\n-\t\tOutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tempFile));\n-\t\twriter.write(fileContent);\n-\t\twriter.close();\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.STRING()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, new Path(tempFile.toURI().toString()))\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setQuoteCharacter('\"')\n-\t\t\t.setEscapeCharacter('\\\\');\n-\n-\t\tBaseRowCsvInputformat inputFormat = builder.build();\n-\t\tinputFormat.configure(new Configuration());\n-\n-\t\tFileInputSplit[] splits = inputFormat.createInputSplits(1);\n-\t\tinputFormat.open(splits[0]);\n-\n-\t\tBaseRow record = inputFormat.nextRecord(new GenericRow(2));\n-\t\tassertEquals(\"\\\"Hello\\\" World\", getField(record, fieldTypes, 0));\n-\t\tassertEquals(\"We are\\\" young\", getField(record, fieldTypes, 1));\n-\t}\n-\n-\t@Test\n-\tpublic void testSqlTimeFields() throws Exception {\n-\t\tString fileContent = \"1990-10-14|02:42:25|1990-10-14 02:42:25.123|1990-1-4 2:2:5\\n\" +\n-\t\t\t\"1990-10-14|02:42:25|1990-10-14 02:42:25.123|1990-1-4 2:2:5.3\\n\";\n-\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.DATE(),\n-\t\t\tDataTypes.TIME(),\n-\t\t\tDataTypes.TIMESTAMP(),\n-\t\t\tDataTypes.TIMESTAMP()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|');\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(4);\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(LocalDate.parse(\"1990-10-14\").toEpochDay(), getField(result, fieldTypes, 0));\n-\t\tassertEquals(LocalTime.parse(\"02:42:25\").toSecondOfDay() * 1000L, getField(result, fieldTypes, 1));\n-\t\tassertEquals(LocalDateTime.parse(\"1990-10-14T02:42:25.123\"), getField(result, fieldTypes, 2));\n-\t\tassertEquals(LocalDateTime.parse(\"1990-01-04T02:02:05\"), getField(result, fieldTypes, 3));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(LocalDate.parse(\"1990-10-14\").toEpochDay(), getField(result, fieldTypes, 0));\n-\t\tassertEquals(LocalTime.parse(\"02:42:25\").toSecondOfDay() * 1000L, getField(result, fieldTypes, 1));\n-\t\tassertEquals(LocalDateTime.parse(\"1990-10-14T02:42:25.123\"), getField(result, fieldTypes, 2));\n-\t\tassertEquals(LocalDateTime.parse(\"1990-01-04T02:02:05.3\"), getField(result, fieldTypes, 3));\n-\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNull(result);\n-\t\tassertTrue(format.reachedEnd());\n-\t}\n-\n-\t@Test\n-\tpublic void testScanOrder() throws Exception {\n-\t\tString fileContent =\n-\t\t\t// first row\n-\t\t\t\"111|222|333|444|555|666|777|888|999|000|\\n\" +\n-\t\t\t\t// second row\n-\t\t\t\t\"000|999|888|777|666|555|444|333|222|111|\";\n-\t\tFileInputSplit split = createTempFile(fileContent);\n-\n-\t\tDataType[] fieldTypes = new DataType[]{\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.INT()};\n-\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, PATH)\n-\t\t\t.setFieldDelimiter('|')\n-\t\t\t.setSelectedFields(new int[]{7, 3, 0});\n-\n-\t\tBaseRowCsvInputformat format = builder.build();\n-\t\tformat.configure(new Configuration());\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(3);\n-\n-\t\t// check first row\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(888, getField(result, fieldTypes, 0));\n-\t\tassertEquals(444, getField(result, fieldTypes, 1));\n-\t\tassertEquals(111, getField(result, fieldTypes, 2));\n-\n-\t\t// check second row\n-\t\tresult = format.nextRecord(result);\n-\t\tassertNotNull(result);\n-\t\tassertEquals(333, getField(result, fieldTypes, 0));\n-\t\tassertEquals(777, getField(result, fieldTypes, 1));\n-\t\tassertEquals(0, getField(result, fieldTypes, 2));\n-\t}\n-\n-\tprivate static FileInputSplit createTempFile(String content) throws IOException {\n-\t\treturn createTempFile(content, 0, null);\n-\t}\n-\n-\tstatic FileInputSplit createTempFile(String content, long start, Long length) throws IOException {\n-\t\tFile tempFile = File.createTempFile(\"test_contents\", \"tmp\");\n-\t\ttempFile.deleteOnExit();\n-\t\tOutputStreamWriter wrt = new OutputStreamWriter(new FileOutputStream(tempFile), StandardCharsets.UTF_8);\n-\t\twrt.write(content);\n-\t\twrt.close();\n-\t\treturn new FileInputSplit(0, new Path(tempFile.toURI().toString()), start,\n-\t\t\tlength == null ? tempFile.length() : length, new String[]{\"localhost\"});\n-\t}\n-\n-\tprivate static void testRemovingTrailingCR(String lineBreakerInFile) throws IOException {\n-\t\tString fileContent = FIRST_PART + lineBreakerInFile + SECOND_PART + lineBreakerInFile;\n-\n-\t\t// create input file\n-\t\tFile tempFile = File.createTempFile(\"CsvInputFormatTest\", \"tmp\");\n-\t\ttempFile.deleteOnExit();\n-\t\ttempFile.setWritable(true);\n-\n-\t\tOutputStreamWriter wrt = new OutputStreamWriter(new FileOutputStream(tempFile));\n-\t\twrt.write(fileContent);\n-\t\twrt.close();\n-\n-\t\tDataType[] fieldTypes = new DataType[]{DataTypes.STRING()};\n-\t\tBaseRowCsvInputformat.Builder builder = BaseRowCsvInputformat\n-\t\t\t.builder(createTestSchema(fieldTypes), PARTITION_KEYS, new Path(tempFile.toURI().toString()));\n-\n-\t\tBaseRowCsvInputformat inputFormat = builder.build();\n-\n-\t\tinputFormat.configure(new Configuration());\n-\n-\t\tFileInputSplit[] splits = inputFormat.createInputSplits(1);\n-\t\tinputFormat.open(splits[0]);\n-\n-\t\tBaseRow result = inputFormat.nextRecord(new GenericRow(1));\n-\t\tassertNotNull(\"Expecting to not return null\", result);\n-\t\tassertEquals(FIRST_PART, getField(result, fieldTypes, 0));\n-\n-\t\tresult = inputFormat.nextRecord(result);\n-\t\tassertNotNull(\"Expecting to not return null\", result);\n-\t\tassertEquals(SECOND_PART, getField(result, fieldTypes, 0));\n-\t}\n-\n-\tprivate static TableSchema createTestSchema(DataType... dataTypes) {\n-\t\tTableSchema.Builder builder = new TableSchema.Builder();\n-\t\tfor (int i = 0; i < dataTypes.length; i++) {\n-\t\t\tbuilder.field(\"f\" + i, dataTypes[i]);\n-\t\t}\n-\t\treturn builder.build();\n-\t}\n-\n-\tprivate static Object getField(BaseRow row, DataType[] fieldTypes, int ordinal) {\n-\t\tif (row.isNullAt(ordinal)) {\n-\t\t\treturn null;\n-\t\t} else {\n-\t\t\tfinal LogicalType logicalType = fieldTypes[ordinal].getLogicalType();\n-\t\t\tswitch (logicalType.getTypeRoot()) {\n-\t\t\t\tcase CHAR:\n-\t\t\t\tcase VARCHAR:\n-\t\t\t\tcase BINARY:\n-\t\t\t\tcase VARBINARY:\n-\t\t\t\t\treturn TypeGetterSetters.get(row, ordinal, logicalType).toString();\n-\t\t\t\tcase DATE:\n-\t\t\t\tcase TIME_WITHOUT_TIME_ZONE:\n-\t\t\t\t\treturn ((Integer) TypeGetterSetters.get(row, ordinal, logicalType)).longValue();\n-\t\t\t\tcase TIMESTAMP_WITHOUT_TIME_ZONE:\n-\t\t\t\t\treturn ((SqlTimestamp) TypeGetterSetters.get(row, ordinal, logicalType)).toLocalDateTime();\n-\t\t\t\tdefault:\n-\t\t\t\t\treturn TypeGetterSetters.get(row, ordinal, logicalType);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NTc0NA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412665744", "bodyText": "After #11796 , please add streaming test too.", "author": "JingsongLi", "createdAt": "2020-04-22T04:44:28Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/**\n+ * Test fot {@link BaseRowCsvFileSystemFormatFactory}.\n+ */\n+@RunWith(Parameterized.class)\n+public class BaseRowCsvFilesystemITCase extends BatchFileSystemITCaseBase {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dff5cdb51db746c02151afd5b4624555def81fac", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\nsimilarity index 80%\nrename from flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java\nrename to flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\nindex ba8e226e8af..4bd01ea5c24 100644\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvFilesystemITCase.java\n+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\n\n@@ -11,10 +11,10 @@ import java.util.Collection;\n import java.util.List;\n \n /**\n- * Test fot {@link BaseRowCsvFileSystemFormatFactory}.\n+ * Test fot {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n  */\n @RunWith(Parameterized.class)\n-public class BaseRowCsvFilesystemITCase extends BatchFileSystemITCaseBase {\n+public class CsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {\n \n \tprivate final boolean configure;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NjYzMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412666630", "bodyText": "This class is for single row test? I think we don't need test single row, should be include in CsvRowDeSerializationSchemaTest.\nWe should add multi-rows test, or just add tests in RowCsvFilesystemITCase.java", "author": "JingsongLi", "createdAt": "2020-04-22T04:47:18Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java", "diffHunk": "@@ -0,0 +1,306 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Assert;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.ArrayList;\n+import java.util.function.Consumer;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.fail;\n+\n+/**\n+ * Test for {@link BaseRowCsvInputformat} and {@link BaseRowCsvEncoder}.\n+ */\n+public class BaseRowCsvDeSerializationTest extends TestLogger {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dff5cdb51db746c02151afd5b4624555def81fac", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java\ndeleted file mode 100644\nindex 802984b6954..00000000000\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/BaseRowCsvDeSerializationTest.java\n+++ /dev/null\n\n@@ -1,306 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.formats.csv;\n-\n-import org.apache.flink.api.common.serialization.Encoder;\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n-import org.apache.flink.table.api.DataTypes;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.dataformat.BaseRow;\n-import org.apache.flink.table.dataformat.DataFormatConverters;\n-import org.apache.flink.table.dataformat.GenericRow;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.types.Row;\n-import org.apache.flink.util.FileUtils;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.Assert;\n-import org.junit.ClassRule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-import java.io.File;\n-import java.io.FileOutputStream;\n-import java.io.IOException;\n-import java.io.OutputStreamWriter;\n-import java.math.BigDecimal;\n-import java.nio.charset.StandardCharsets;\n-import java.sql.Date;\n-import java.sql.Time;\n-import java.sql.Timestamp;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.util.ArrayList;\n-import java.util.function.Consumer;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.fail;\n-\n-/**\n- * Test for {@link BaseRowCsvInputformat} and {@link BaseRowCsvEncoder}.\n- */\n-public class BaseRowCsvDeSerializationTest extends TestLogger {\n-\n-\t@ClassRule\n-\tpublic static final TemporaryFolder TEMP_FOLDER = new TemporaryFolder();\n-\n-\t@Test\n-\tpublic void testSerializationProperties() throws Exception {\n-\t\tTableSchema schema = createTestSchema(DataTypes.STRING(), DataTypes.INT(), DataTypes.STRING());\n-\t\ttestField(schema, Row.of(\"Test\", 123, \"Hello\"), \"Test,123,Hello\\r\",\n-\t\t\t(serSchema) -> serSchema.setLineDelimiter(\"\\r\"));\n-\t\ttestField(schema, Row.of(\"Test\", 123, \"Hello\"), \"Test|123|Hello\\r\",\n-\t\t\t(serSchema) -> serSchema.setFieldDelimiter('|').disableQuoteCharacter().setLineDelimiter(\"\\r\"));\n-\t\ttestField(schema, Row.of(\"Test\", 123, \"2020-04-01 12:13:14\"), \"Test,123,2020-04-01 12:13:14\\r\",\n-\t\t\t(serSchema) -> serSchema.disableQuoteCharacter().setLineDelimiter(\"\\r\"));\n-\t\ttestField(schema, Row.of(\"Test\", 123, \"2020-04-01 12:13:14\"), \"Test,123,#2020-04-01 12:13:14#\\n\",\n-\t\t\t(serSchema) -> serSchema.setQuoteCharacter('#').setLineDelimiter(\"\\n\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testEmptyLineDelimiter() throws Exception {\n-\t\tTableSchema schema = createTestSchema(DataTypes.STRING(), DataTypes.INT(), DataTypes.STRING());\n-\t\ttestField(schema, Row.of(\"Test\", 123, \"Hello\"), \"Test,123,Hello\",\n-\t\t\t(serSchema) -> serSchema.setLineDelimiter(\"\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testSerializeDeserializeCustomizedProperties() throws Exception {\n-\t\tfinal Consumer<BaseRowCsvEncoder.Builder> serializationConfig = (builder) -> builder\n-\t\t\t.setEscapeCharacter('*')\n-\t\t\t.setQuoteCharacter('\\'')\n-\t\t\t.setArrayElementDelimiter(\":\")\n-\t\t\t.setFieldDelimiter(';');\n-\t\tfinal Consumer<BaseRowCsvInputformat.Builder> deserializationConfig = (deserSchemaBuilder) -> deserSchemaBuilder\n-\t\t\t.setEscapeCharacter('*')\n-\t\t\t.setQuoteCharacter('\\'')\n-\t\t\t.setArrayElementDelimiter(\":\")\n-\t\t\t.setFieldDelimiter(';');\n-\n-\t\ttestField(createTestSchema(DataTypes.STRING()), Row.of(\"123'4*\"), serializationConfig, deserializationConfig);\n-\t\ttestField(createTestSchema(DataTypes.STRING()), Row.of(\"a;b'c\"), serializationConfig, deserializationConfig);\n-\t\ttestField(createTestSchema(DataTypes.INT()), Row.of(2020), serializationConfig, deserializationConfig);\n-\t\ttestField(createTestSchema(DataTypes.STRING(), DataTypes.STRING()),\n-\t\t\tRow.of(\"1\", \"hello world\"), serializationConfig, deserializationConfig);\n-\t\ttestField(createTestSchema(DataTypes.STRING()), Row.of(\"null\"), serializationConfig, deserializationConfig);\n-\t}\n-\n-\t@Test\n-\tpublic void testSerialNullabeField() throws Exception {\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.BIGINT()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.STRING()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.BOOLEAN()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.DOUBLE()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.INT()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.BYTES()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.TIMESTAMP()), Row.of(\"BEGIN\", null));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.TIME()), Row.of(\"BEGIN\", null));\n-\t}\n-\n-\t@Test(expected = IllegalArgumentException.class)\n-\tpublic void testInvalidNesting() throws Exception {\n-\t\ttestField(createTestSchema(DataTypes.ROW(DataTypes.FIELD(\"f0\",\n-\t\t\tDataTypes.ROW(DataTypes.FIELD(\"f1\", DataTypes.STRING()))))),\n-\t\t\tRow.of(Row.of(\"null\")), \"null\\r\",\n-\t\t\t(builder) -> builder.setLineDelimiter(\"\\r\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testSimpleDataTypeSerializeDeserialize()throws Exception {\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING()), Row.of(\"This is a test\\n\\r.\"));\n-\t\ttestNullableField(createTestSchema(DataTypes.SMALLINT()), Row.of((short) 1000));\n-\t\ttestNullableField(createTestSchema(DataTypes.BOOLEAN()), Row.of(true));\n-\t\ttestNullableField(createTestSchema(DataTypes.INT()), Row.of(123));\n-\t\ttestNullableField(createTestSchema(DataTypes.BIGINT()), Row.of(12345678910L));\n-\t\ttestNullableField(createTestSchema(DataTypes.FLOAT()), Row.of(0.33333334f));\n-\t\ttestNullableField(createTestSchema(DataTypes.DOUBLE()), Row.of(0.33333333332d));\n-\t\ttestNullableField(createTestSchema(DataTypes.DECIMAL(38, 18)),\n-\t\t\tRow.of(new BigDecimal(\"1234.000000000000000001\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.DATE().bridgedTo(java.time.LocalDate.class)),\n-\t\t\tRow.of(LocalDate.parse(\"2020-04-01\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.TIME().bridgedTo(java.time.LocalTime.class)),\n-\t\t\tRow.of(LocalTime.parse(\"12:13:14\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.TIMESTAMP(3).bridgedTo(java.time.LocalDateTime.class)),\n-\t\t\tRow.of(LocalDateTime.parse(\"2020-04-01T12:13:14.123\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.DATE().bridgedTo(java.sql.Date.class)),\n-\t\t\tRow.of(Date.valueOf(\"2020-04-01\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.TIME().bridgedTo(java.sql.Time.class)),\n-\t\t\tRow.of(Time.valueOf(\"12:13:14\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.TIMESTAMP(3).bridgedTo(java.sql.Timestamp.class)),\n-\t\t\tRow.of(Timestamp.valueOf(\"2020-04-01 12:13:14.123\")));\n-\t\ttestNullableField(createTestSchema(DataTypes.STRING(), DataTypes.INT(), DataTypes.BOOLEAN()),\n-\t\t\tRow.of(\"Hello\", 42, false));\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeParseError() throws Exception {\n-\t\ttry {\n-\t\t\ttestDeserialization(false, false, \"Test,null,Test\"); // null not supported\n-\t\t\tfail(\"Missing field should cause failure.\");\n-\t\t} catch (IOException e) {\n-\t\t\t// valid exception\n-\t\t}\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeUnsupportedNull() throws Exception {\n-\t\t// unsupported null for integer\n-\t\tassertEquals(Row.of(\"Test\", null, \"Test\"), testDeserialization(true, false, \"Test,null,Test\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeIncompleteRow() throws Exception {\n-\t\t// last two columns are missing\n-\t\tassertEquals(Row.of(\"Test\", null, null), testDeserialization(true, false, \"Test\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeMoreColumnsThanExpected() throws Exception {\n-\t\t// one additional string column\n-\t\tassertNull(testDeserialization(true, false, \"Test,12,Test,Test\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeIgnoreComment() throws Exception {\n-\t\t// # is part of the string\n-\t\tassertEquals(Row.of(\"#Test\", 12, \"Test\"), testDeserialization(false, false, \"#Test,12,Test\"));\n-\t}\n-\n-\t@Test\n-\tpublic void testDeserializeAllowComment() throws Exception {\n-\t\t// entire row is ignored\n-\t\tassertNull(testDeserialization(true, true, \"#Test,12,Test\"));\n-\t}\n-\n-\tprivate void testNullableField(TableSchema schema, Row row) throws Exception {\n-\t\ttestField(schema, row,\n-\t\t\t(builder) -> builder.setNullLiteral(\"null\").setLineDelimiter(\"\"),\n-\t\t\t(builder) -> builder.setNullLiteral(\"null\").setLineDelimiter(\"\"));\n-\t}\n-\n-\tprivate void testField(\n-\t\tTableSchema schema,\n-\t\tRow row,\n-\t\tString expectedCsv,\n-\t\tConsumer<BaseRowCsvEncoder.Builder> serializationConfig) throws Exception {\n-\t\tBaseRowCsvEncoder.Builder builder =  BaseRowCsvEncoder.builder(schema);\n-\t\tserializationConfig.accept(builder);\n-\n-\t\t// serialization\n-\t\tFile tempFile = TEMP_FOLDER.newFile();\n-\t\tBaseRow baseRow = toBaseRow(schema, row);\n-\t\tEncoder<BaseRow> encoder = builder.build();\n-\t\tencoder.encode(baseRow, new FileOutputStream(tempFile));\n-\t\tString csvString = FileUtils.readFileUtf8(tempFile);\n-\t\tAssert.assertEquals(expectedCsv, csvString);\n-\t}\n-\n-\tprivate void testField(\n-\t\t\tTableSchema schema,\n-\t\t\tRow row,\n-\t\t\tConsumer<BaseRowCsvEncoder.Builder> serializationConfig,\n-\t\t\tConsumer<BaseRowCsvInputformat.Builder> deserializationConfig) throws Exception {\n-\t\tBaseRowCsvEncoder.Builder builder =  BaseRowCsvEncoder.builder(schema);\n-\t\tserializationConfig.accept(builder);\n-\n-\t\t// serialization\n-\t\tFile tempFile = TEMP_FOLDER.newFile();\n-\t\tBaseRow baseRow = toBaseRow(schema, row);\n-\t\tEncoder<BaseRow> encoder = builder.build();\n-\t\tencoder.encode(baseRow, new FileOutputStream(tempFile));\n-\n-\t\t// deserialization\n-\t\tFileInputSplit split = createLocalFileInputSplit(null, tempFile);\n-\t\tBaseRowCsvInputformat.Builder inputBuilder = BaseRowCsvInputformat\n-\t\t\t.builder(schema, new ArrayList<>(), Path.fromLocalFile(tempFile))\n-\t\t\t.setIgnoreParseErrors(false);\n-\t\tdeserializationConfig.accept(inputBuilder);\n-\n-\t\tBaseRowCsvInputformat format = inputBuilder.build();\n-\t\tConfiguration parameters = new Configuration();\n-\t\tformat.configure(parameters);\n-\t\tformat.open(split);\n-\n-\t\tBaseRow result = new GenericRow(row.getArity());\n-\t\ttry {\n-\t\t\tresult = format.nextRecord(result);\n-\t\t} catch (IOException ignored) {\n-\t\t}\n-\t\tAssert.assertEquals(baseRow, result);\n-\t}\n-\n-\tprivate Row testDeserialization(\n-\t\tboolean allowParsingErrors,\n-\t\tboolean allowComments,\n-\t\tString content) throws Exception {\n-\n-\t\tfinal DataType[] dataTypes = new DataType[]{\n-\t\t\tDataTypes.STRING(),\n-\t\t\tDataTypes.INT(),\n-\t\t\tDataTypes.STRING()};\n-\t\tfinal TableSchema tableSchema = createTestSchema(dataTypes);\n-\t\tFile tempFile = TEMP_FOLDER.newFile();\n-\t\tFileInputSplit split = createLocalFileInputSplit(content, tempFile);\n-\n-\t\tBaseRowCsvInputformat.Builder inputBuilder = BaseRowCsvInputformat\n-\t\t\t.builder(tableSchema, new ArrayList<>(), Path.fromLocalFile(tempFile))\n-\t\t\t.setIgnoreParseErrors(allowParsingErrors)\n-\t\t\t.setAllowComments(allowComments);\n-\n-\t\tBaseRowCsvInputformat format = inputBuilder.build();\n-\t\tformat.open(split);\n-\t\tBaseRow result = new GenericRow(dataTypes.length);\n-\t\tresult = format.nextRecord(result);\n-\n-\t\t// convert to Row for comparing clearly\n-\t\tDataFormatConverters.DataFormatConverter<BaseRow, Row> dataFormatConverter = DataFormatConverters\n-\t\t\t.getConverterForDataType(tableSchema.toRowDataType());\n-\t\treturn dataFormatConverter.toExternal(result);\n-\t}\n-\n-\tprivate BaseRow toBaseRow(TableSchema schema, Row row) {\n-\t\tDataType rowDatatype = schema.toRowDataType();\n-\n-\t\tDataFormatConverters.DataFormatConverter<BaseRow, Row> dataFormatConverter = DataFormatConverters\n-\t\t\t.getConverterForDataType(rowDatatype);\n-\t\treturn dataFormatConverter.toInternal(row);\n-\t}\n-\n-\tprivate TableSchema createTestSchema(DataType... dataTypes) {\n-\t\tTableSchema.Builder builder = new TableSchema.Builder();\n-\t\tfor (int i = 0; i < dataTypes.length; i++) {\n-\t\t\tbuilder.field(\"f\" + i, dataTypes[i]);\n-\t\t}\n-\t\treturn builder.build();\n-\t}\n-\n-\tprivate FileInputSplit createLocalFileInputSplit(String content, File file) throws Exception {\n-\t\tif (content != null) {\n-\t\t\tOutputStreamWriter wrt = new OutputStreamWriter(new FileOutputStream(file), StandardCharsets.UTF_8);\n-\t\t\twrt.write(content);\n-\t\t\twrt.close();\n-\t\t}\n-\t\treturn new FileInputSplit(0, new Path(file.toURI().toString()),\n-\t\t\t0, file.length(), new String[]{\"localhost\"});\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjY2NzEwMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r412667101", "bodyText": "use RowPartitionComputer.restorePartValueFromType instead", "author": "JingsongLi", "createdAt": "2020-04-22T04:48:50Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java", "diffHunk": "@@ -0,0 +1,310 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.dataformat.BaseRow;\n+import org.apache.flink.table.dataformat.BinaryString;\n+import org.apache.flink.table.dataformat.DataFormatConverters;\n+import org.apache.flink.table.dataformat.GenericRow;\n+import org.apache.flink.table.filesystem.PartitionPathUtils;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.createFieldRuntimeConverters;\n+import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Input format that reads csv file into {@link BaseRow}.\n+ */\n+public class BaseRowCsvInputformat extends AbstractCsvInputFormat<BaseRow> {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\n+\tprivate final List<TypeInformation> fieldTypes;\n+\tprivate final List<String> fieldNames;\n+\tprivate final int[] selectFields;\n+\tprivate final List<String> partitionKeys;\n+\tprivate final String defaultPartValue;\n+\tprivate final boolean ignoreParseErrors;\n+\tprivate final long limit;\n+\tprivate final List<String> csvFieldNames;\n+\tprivate final List<TypeInformation> csvFieldTypes;\n+\tprivate final List<String> csvSelectFieldNames;\n+\tprivate final List<TypeInformation> csvSelectTypes;\n+\tprivate final int[] csvFieldMapping;\n+\n+\tprivate transient CsvRowDeserializationSchema.RuntimeConverter runtimeConverter;\n+\tprivate transient List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n+\tprivate transient MappingIterator<JsonNode> iterator;\n+\tprivate transient boolean end;\n+\tprivate transient long emitted;\n+\t// reuse object for per record\n+\tprivate transient GenericRow rowData;\n+\n+\tprivate BaseRowCsvInputformat(\n+\t\tPath[] filePaths,\n+\t\tCsvSchema csvSchema,\n+\t\tRowTypeInfo rowType,\n+\t\tint[] selectFields,\n+\t\tList<String> partitionKeys,\n+\t\tString defaultPartValue,\n+\t\tboolean ignoreParseErrors,\n+\t\tlong limit) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n+\t\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n+\t\t\tthis.emitted = 0;\n+\t\t\t// partition field\n+\t\t\tthis.csvFieldNames = fieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvFieldTypes = csvFieldNames.stream()\n+\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\t// project field\n+\t\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n+\t\t\t\t.mapToObj(fieldNames::get)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t\tthis.csvSelectFieldNames = selectFieldNames.stream()\n+\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n+\t\t\tthis.csvSelectTypes = csvSelectFieldNames.stream()\n+\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n+\t\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n+\t}\n+\n+\t@Override\n+\tpublic void open(FileInputSplit split) throws IOException {\n+\t\tsuper.open(split);\n+\t\tthis.end = false;\n+\t\tthis.iterator = new CsvMapper()\n+\t\t\t.readerFor(JsonNode.class)\n+\t\t\t.with(csvSchema)\n+\t\t\t.readValues(csvInputStream);\n+\t\tprepareRuntimeConverter();\n+\t\tfillPartitionValueForRecord();\n+\t}\n+\n+\tprivate void fillPartitionValueForRecord() {\n+\t\trowData = new GenericRow(selectFields.length);\n+\t\tPath path = currentSplit.getPath();\n+\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n+\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\tint selectField = selectFields[i];\n+\t\t\tString name = fieldNames.get(selectField);\n+\t\t\tif (partitionKeys.contains(name)) {\n+\t\t\t\tString value = partSpec.get(name);\n+\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n+\t\t\t\trowData.setField(i, convertStringToInternal(value, fieldTypes.get(selectField)));\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate Object convertStringToInternal(String value, TypeInformation type) {", "originalCommit": "00fde88b6f1a151b95301a739699cbb41964d4bd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dff5cdb51db746c02151afd5b4624555def81fac", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java\ndeleted file mode 100644\nindex 90349b18845..00000000000\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/BaseRowCsvInputformat.java\n+++ /dev/null\n\n@@ -1,310 +0,0 @@\n-package org.apache.flink.formats.csv;\n-\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.common.typeinfo.Types;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n-import org.apache.flink.core.fs.FileInputSplit;\n-import org.apache.flink.core.fs.Path;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.api.ValidationException;\n-import org.apache.flink.table.dataformat.BaseRow;\n-import org.apache.flink.table.dataformat.BinaryString;\n-import org.apache.flink.table.dataformat.DataFormatConverters;\n-import org.apache.flink.table.dataformat.GenericRow;\n-import org.apache.flink.table.filesystem.PartitionPathUtils;\n-import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.utils.TypeConversions;\n-import org.apache.flink.types.Row;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n-import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n-\n-import java.io.IOException;\n-import java.util.Arrays;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.NoSuchElementException;\n-import java.util.stream.Collectors;\n-\n-import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.createFieldRuntimeConverters;\n-import static org.apache.flink.formats.csv.CsvRowDeserializationSchema.validateArity;\n-import static org.apache.flink.util.Preconditions.checkNotNull;\n-\n-/**\n- * Input format that reads csv file into {@link BaseRow}.\n- */\n-public class BaseRowCsvInputformat extends AbstractCsvInputFormat<BaseRow> {\n-\n-\tprivate static final long serialVersionUID = 1L;\n-\n-\tprivate final List<TypeInformation> fieldTypes;\n-\tprivate final List<String> fieldNames;\n-\tprivate final int[] selectFields;\n-\tprivate final List<String> partitionKeys;\n-\tprivate final String defaultPartValue;\n-\tprivate final boolean ignoreParseErrors;\n-\tprivate final long limit;\n-\tprivate final List<String> csvFieldNames;\n-\tprivate final List<TypeInformation> csvFieldTypes;\n-\tprivate final List<String> csvSelectFieldNames;\n-\tprivate final List<TypeInformation> csvSelectTypes;\n-\tprivate final int[] csvFieldMapping;\n-\n-\tprivate transient CsvRowDeserializationSchema.RuntimeConverter runtimeConverter;\n-\tprivate transient List<DataFormatConverters.DataFormatConverter> csvSelectConverters;\n-\tprivate transient MappingIterator<JsonNode> iterator;\n-\tprivate transient boolean end;\n-\tprivate transient long emitted;\n-\t// reuse object for per record\n-\tprivate transient GenericRow rowData;\n-\n-\tprivate BaseRowCsvInputformat(\n-\t\tPath[] filePaths,\n-\t\tCsvSchema csvSchema,\n-\t\tRowTypeInfo rowType,\n-\t\tint[] selectFields,\n-\t\tList<String> partitionKeys,\n-\t\tString defaultPartValue,\n-\t\tboolean ignoreParseErrors,\n-\t\tlong limit) {\n-\t\t\tsuper(filePaths, csvSchema);\n-\t\t\tthis.partitionKeys = partitionKeys;\n-\t\t\tthis.defaultPartValue = defaultPartValue;\n-\t\t\tthis.selectFields = selectFields;\n-\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n-\t\t\tthis.limit = limit;\n-\t\t\tthis.fieldTypes = Arrays.asList(rowType.getFieldTypes());\n-\t\t\tthis.fieldNames = Arrays.asList(rowType.getFieldNames());\n-\t\t\tthis.emitted = 0;\n-\t\t\t// partition field\n-\t\t\tthis.csvFieldNames = fieldNames.stream()\n-\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n-\t\t\tthis.csvFieldTypes = csvFieldNames.stream()\n-\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n-\t\t\t// project field\n-\t\t\tList<String> selectFieldNames = Arrays.stream(selectFields)\n-\t\t\t\t.mapToObj(fieldNames::get)\n-\t\t\t\t.collect(Collectors.toList());\n-\t\t\tthis.csvSelectFieldNames = selectFieldNames.stream()\n-\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n-\t\t\tthis.csvSelectTypes = csvSelectFieldNames.stream()\n-\t\t\t\t.map(name -> fieldTypes.get(fieldNames.indexOf(name))).collect(Collectors.toList());\n-\t\t\tthis.csvFieldMapping = csvSelectFieldNames.stream().mapToInt(selectFieldNames::indexOf).toArray();\n-\t}\n-\n-\t@Override\n-\tpublic void open(FileInputSplit split) throws IOException {\n-\t\tsuper.open(split);\n-\t\tthis.end = false;\n-\t\tthis.iterator = new CsvMapper()\n-\t\t\t.readerFor(JsonNode.class)\n-\t\t\t.with(csvSchema)\n-\t\t\t.readValues(csvInputStream);\n-\t\tprepareRuntimeConverter();\n-\t\tfillPartitionValueForRecord();\n-\t}\n-\n-\tprivate void fillPartitionValueForRecord() {\n-\t\trowData = new GenericRow(selectFields.length);\n-\t\tPath path = currentSplit.getPath();\n-\t\tLinkedHashMap<String, String> partSpec = PartitionPathUtils.extractPartitionSpecFromPath(path);\n-\t\tfor (int i = 0; i < selectFields.length; i++) {\n-\t\t\tint selectField = selectFields[i];\n-\t\t\tString name = fieldNames.get(selectField);\n-\t\t\tif (partitionKeys.contains(name)) {\n-\t\t\t\tString value = partSpec.get(name);\n-\t\t\t\tvalue = defaultPartValue.equals(value) ? null : value;\n-\t\t\t\trowData.setField(i, convertStringToInternal(value, fieldTypes.get(selectField)));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate Object convertStringToInternal(String value, TypeInformation type) {\n-\t\tif (type.equals(Types.INT)) {\n-\t\t\treturn Integer.parseInt(value);\n-\t\t} else if (type.equals(Types.LONG)) {\n-\t\t\treturn Long.parseLong(value);\n-\t\t} else if (type.equals(Types.STRING)) {\n-\t\t\treturn BinaryString.fromString(value);\n-\t\t} else {\n-\t\t\tthrow new UnsupportedOperationException(\"Unsupported partition type: \" + type);\n-\t\t}\n-\t}\n-\n-\tprivate void prepareRuntimeConverter() {\n-\t\tCsvRowDeserializationSchema.RuntimeConverter[] fieldConverters = createFieldRuntimeConverters(ignoreParseErrors,\n-\t\t\tcsvFieldTypes.toArray(new TypeInformation[0]));\n-\t\tthis.runtimeConverter = (node) -> {\n-\t\t\tfinal int nodeSize = node.size();\n-\n-\t\t\tvalidateArity(csvSchema.size(), nodeSize, ignoreParseErrors);\n-\n-\t\t\tRow row = new Row(csvSelectFieldNames.size());\n-\t\t\tfor (int i = 0; i < Math.min(csvSelectFieldNames.size(), nodeSize); i++) {\n-\t\t\t\t// Jackson only supports mapping by name in the first level\n-\t\t\t\trow.setField(i, fieldConverters[i].convert(node.get(csvSelectFieldNames.get(i))));\n-\t\t\t}\n-\t\t\treturn row;\n-\t\t};\n-\t\tthis.csvSelectConverters = csvSelectTypes.stream()\n-\t\t\t.map(TypeConversions::fromLegacyInfoToDataType)\n-\t\t\t.map(DataFormatConverters::getConverterForDataType)\n-\t\t\t.collect(Collectors.toList());\n-\t}\n-\n-\t@Override\n-\tpublic boolean reachedEnd() throws IOException {\n-\t\treturn emitted >= limit || end;\n-\t}\n-\n-\t@Override\n-\tpublic BaseRow nextRecord(BaseRow reuse) throws IOException {\n-\t\tGenericRow returnRecord = null;\n-\t\tdo {\n-\t\t\ttry {\n-\t\t\t\tJsonNode root = iterator.nextValue();\n-\t\t\t\tRow row  = (Row) runtimeConverter.convert(root);\n-\t\t\t\tif (row != null) {\n-\t\t\t\t\treturnRecord = rowData;\n-\t\t\t\t\tfor (int i = 0; i < csvSelectConverters.size(); i++) {\n-\t\t\t\t\t\treturnRecord.setField(\n-\t\t\t\t\t\t\tcsvFieldMapping[i],\n-\t\t\t\t\t\t\tcsvSelectConverters.get(i).toInternal(row.getField(i)));\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\treturnRecord =  null;\n-\t\t\t\t}\n-\n-\t\t\t} catch (NoSuchElementException e) {\n-\t\t\t\tend = true;\n-\t\t\t}\n-\t\t\tcatch (Throwable t) {\n-\t\t\t\tif (!ignoreParseErrors) {\n-\t\t\t\t\tthrow new IOException(\"Failed to deserialize CSV row.\", t);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} while (returnRecord == null && !reachedEnd());\n-\t\temitted++;\n-\t\treturn returnRecord;\n-\t}\n-\n-\t/**\n-\t * Create a builder.\n-\t */\n-\tpublic static Builder builder(TableSchema tableSchema, List<String> partitionKeys, Path... paths) {\n-\t\treturn new Builder(tableSchema, partitionKeys, paths);\n-\t}\n-\n-\t/**\n-\t * Builder for {@link BaseRowCsvInputformat}.\n-\t */\n-\tpublic static class Builder {\n-\t\tprivate Path[] paths;\n-\t\tprivate CsvSchema csvSchema;\n-\t\tprivate RowTypeInfo rowType;\n-\t\tprivate int[] selectedFields;\n-\t\tprivate List<String> partitionKeys;\n-\t\tprivate String defaultPartValue;\n-\t\tprivate boolean ignoreParseErrors;\n-\t\tprivate long limit;\n-\n-\t\tprivate Builder(TableSchema tableSchema, List<String> partitionKeys, Path... paths) {\n-\t\t\tcheckNotNull(tableSchema, \"Table schema must not be null.\");\n-\t\t\tcheckNotNull(partitionKeys, \"PartitionKeys must not be null.\");\n-\t\t\tcheckNotNull(paths, \"File paths must not be null.\");\n-\t\t\tthis.paths = paths;\n-\t\t\tthis.rowType = (RowTypeInfo) tableSchema.toRowType();\n-\t\t\tthis.partitionKeys = partitionKeys;\n-\t\t\tthis.csvSchema = getCsvSchema(tableSchema, partitionKeys);\n-\t\t}\n-\n-\t\tprivate CsvSchema getCsvSchema(TableSchema tableSchema, List<String> partitionKeys) {\n-\t\t\tfinal List<String> nonPartitionFields = Arrays.stream(tableSchema.getFieldNames())\n-\t\t\t\t.filter(name -> !partitionKeys.contains(name)).collect(Collectors.toList());\n-\t\t\tfinal DataType[] nonPartitionDataTypes = nonPartitionFields.stream()\n-\t\t\t\t.map(name -> tableSchema.getFieldDataType(name).orElseThrow(() ->\n-\t\t\t\t\tnew ValidationException(String.format(\"unknown field %s in table schema\", name))))\n-\t\t\t\t.toArray(DataType[]::new);\n-\t\t\tfinal TableSchema nonPartitionSchema = new TableSchema.Builder()\n-\t\t\t\t.fields(nonPartitionFields.toArray(new String[0]), nonPartitionDataTypes)\n-\t\t\t\t.build();\n-\t\t\treturn CsvRowSchemaConverter.convert((RowTypeInfo) nonPartitionSchema.toRowType());\n-\t\t}\n-\n-\t\tpublic Builder setFieldDelimiter(char delimiter) {\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setColumnSeparator(delimiter).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setLineDelimiter(String delimiter) {\n-\t\t\tPreconditions.checkNotNull(delimiter, \"Delimiter must not be null.\");\n-\t\t\tif (!delimiter.equals(\"\\n\") && !delimiter.equals(\"\\r\") && !delimiter.equals(\"\\r\\n\") && !delimiter.equals(\"\")) {\n-\t\t\t\tthrow new IllegalArgumentException(\n-\t\t\t\t\t\"Unsupported new line delimiter. Only \\\\n, \\\\r, \\\\r\\\\n, or empty string are supported.\");\n-\t\t\t}\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setLineSeparator(delimiter).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setAllowComments(boolean allowComments) {\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setAllowComments(allowComments).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setArrayElementDelimiter(String delimiter) {\n-\t\t\tcheckNotNull(delimiter, \"Array element delimiter must not be null.\");\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setArrayElementSeparator(delimiter).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setQuoteCharacter(char c) {\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setQuoteChar(c).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setEscapeCharacter(char c) {\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setEscapeChar(c).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setNullLiteral(String nullLiteral) {\n-\t\t\tcheckNotNull(nullLiteral, \"Null literal must not be null.\");\n-\t\t\tthis.csvSchema = this.csvSchema.rebuild().setNullValue(nullLiteral).build();\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setIgnoreParseErrors(boolean ignoreParseErrors) {\n-\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setSelectedFields(int[] selectedFields) {\n-\t\t\tthis.selectedFields = selectedFields;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setDefaultPartValues(String defaultPartValue) {\n-\t\t\tthis.defaultPartValue = defaultPartValue;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Builder setLimit(long limit) {\n-\t\t\tthis.limit = limit;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic BaseRowCsvInputformat build() {\n-\t\t\tif (selectedFields == null) {\n-\t\t\t\tselectedFields = new int[rowType.getFieldNames().length];\n-\t\t\t\tfor (int i = 0; i < rowType.getFieldNames().length; i++) {\n-\t\t\t\t\tselectedFields[i] = i;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn new BaseRowCsvInputformat(paths, csvSchema, rowType, selectedFields,\n-\t\t\t\tpartitionKeys, defaultPartValue, ignoreParseErrors, limit);\n-\t\t}\n-\t}\n-}\n"}}, {"oid": "dff5cdb51db746c02151afd5b4624555def81fac", "url": "https://github.com/apache/flink/commit/dff5cdb51db746c02151afd5b4624555def81fac", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory.", "committedDate": "2020-05-07T08:28:41Z", "type": "forcePushed"}, {"oid": "4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "url": "https://github.com/apache/flink/commit/4be08fc2530e1ed4eafe14e3c0f16780e01aa7d0", "message": "[FLINK-14267] Introduce Row Csv Encoder && [FLINK-14257] Integrate csv to file system connector", "committedDate": "2020-05-07T15:06:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIwMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441201", "bodyText": "Add empty line.", "author": "JingsongLi", "createdAt": "2020-05-09T02:08:43Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override", "originalCommit": "64d407d3530d308b4504282ae8fadb1503955f99", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b72a27e8fa3a6b21e5505435be096eea97d7da07", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nindex a3a659faee8..39ecb0cfa03 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n\n@@ -3,22 +3,21 @@ package org.apache.flink.formats.csv;\n import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.common.serialization.BulkWriter;\n import org.apache.flink.api.common.serialization.Encoder;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.core.fs.FileInputSplit;\n import org.apache.flink.core.fs.Path;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.descriptors.DescriptorProperties;\n import org.apache.flink.table.factories.FileSystemFormatFactory;\n import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.table.utils.PartitionPathUtils;\n \n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n \n import java.io.BufferedReader;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ0MTIxMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422441210", "bodyText": "Remove empty line", "author": "JingsongLi", "createdAt": "2020-05-09T02:08:52Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,354 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+", "originalCommit": "64d407d3530d308b4504282ae8fadb1503955f99", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b72a27e8fa3a6b21e5505435be096eea97d7da07", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nindex a3a659faee8..39ecb0cfa03 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n\n@@ -3,22 +3,21 @@ package org.apache.flink.formats.csv;\n import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.common.serialization.BulkWriter;\n import org.apache.flink.api.common.serialization.Encoder;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.core.fs.FileInputSplit;\n import org.apache.flink.core.fs.Path;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.descriptors.DescriptorProperties;\n import org.apache.flink.table.factories.FileSystemFormatFactory;\n import org.apache.flink.table.types.DataType;\n-import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.table.utils.PartitionPathUtils;\n \n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n \n import java.io.BufferedReader;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767499", "bodyText": "return null after end = true. And remove !reachedEnd() in while.\nThis can make codes clear and correct emitted number.", "author": "JingsongLi", "createdAt": "2020-05-11T04:03:15Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;", "originalCommit": "0fd33d570a8591ef7e5a523c29366a34aa6b986e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjk4NDUyMA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422984520", "bodyText": "I update the logic, please review again.", "author": "leonardBang", "createdAt": "2020-05-11T11:51:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzQ5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nindex 39ecb0cfa03..5ec26b6c454 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n\n@@ -23,7 +23,6 @@ import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.Csv\n import java.io.BufferedReader;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.io.OutputStream;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc2NzY4Ng==", "url": "https://github.com/apache/flink/pull/11755#discussion_r422767686", "bodyText": "Minor: this class can be a lambda.", "author": "JingsongLi", "createdAt": "2020-05-11T04:03:55Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,352 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setColumnSeparator(properties.getCharacter(FORMAT_FIELD_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_QUOTE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setQuoteChar(properties.getCharacter(FORMAT_QUOTE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ALLOW_COMMENTS)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setAllowComments(properties.getBoolean(FORMAT_ALLOW_COMMENTS))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ARRAY_ELEMENT_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setArrayElementSeparator(properties.getString(FORMAT_ARRAY_ELEMENT_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_ESCAPE_CHARACTER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setEscapeChar(properties.getCharacter(FORMAT_ESCAPE_CHARACTER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_NULL_LITERAL)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setNullValue(properties.getString(FORMAT_NULL_LITERAL))\n+\t\t\t\t.build();\n+\t\t}\n+\t\tif (properties.containsKey(FORMAT_LINE_DELIMITER)) {\n+\t\t\tcsvSchema = csvSchema.rebuild()\n+\t\t\t\t.setLineSeparator(properties.getString(FORMAT_LINE_DELIMITER))\n+\t\t\t\t.build();\n+\t\t}\n+\t\treturn csvSchema;\n+\t}\n+\n+\t@Override\n+\tpublic Optional<Encoder<RowData>> createEncoder(WriterContext context) {\n+\n+\t\tCsvRowDataSerializationSchema.Builder builder = new CsvRowDataSerializationSchema.Builder(\n+\t\t\tcontext.getFormatRowType());\n+\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_FIELD_DELIMITER)\n+\t\t\t.ifPresent(builder::setFieldDelimiter);\n+\n+\t\tproperties.getOptionalString(FORMAT_LINE_DELIMITER)\n+\t\t\t.ifPresent(builder::setLineDelimiter);\n+\n+\t\tif (properties.getOptionalBoolean(FORMAT_DISABLE_QUOTE_CHARACTER).orElse(false)) {\n+\t\t\tbuilder.disableQuoteCharacter();\n+\t\t} else {\n+\t\t\tproperties.getOptionalCharacter(FORMAT_QUOTE_CHARACTER).ifPresent(builder::setQuoteCharacter);\n+\t\t}\n+\n+\t\tproperties.getOptionalString(FORMAT_ARRAY_ELEMENT_DELIMITER)\n+\t\t\t.ifPresent(builder::setArrayElementDelimiter);\n+\n+\t\tproperties.getOptionalCharacter(FORMAT_ESCAPE_CHARACTER)\n+\t\t\t.ifPresent(builder::setEscapeCharacter);\n+\n+\t\tproperties.getOptionalString(FORMAT_NULL_LITERAL)\n+\t\t\t.ifPresent(builder::setNullLiteral);\n+\n+\t\treturn Optional.of(new CsvRowDataEncoder(builder.build()));\n+\t}\n+\n+\t@Override\n+\tpublic Optional<BulkWriter.Factory<RowData>> createBulkWriterFactory(WriterContext context) {\n+\t\treturn Optional.empty();\n+\t}\n+\n+\t@Override\n+\tpublic boolean supportsSchemaDerivation() {\n+\t\treturn true;\n+\t}\n+\n+\t@Override\n+\tpublic List<String> supportedProperties() {\n+\t\tfinal List<String> properties = new ArrayList<>();\n+\t\tproperties.add(FORMAT_FIELD_DELIMITER);\n+\t\tproperties.add(FORMAT_LINE_DELIMITER);\n+\t\tproperties.add(FORMAT_DISABLE_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_QUOTE_CHARACTER);\n+\t\tproperties.add(FORMAT_ALLOW_COMMENTS);\n+\t\tproperties.add(FORMAT_IGNORE_PARSE_ERRORS);\n+\t\tproperties.add(FORMAT_ARRAY_ELEMENT_DELIMITER);\n+\t\tproperties.add(FORMAT_ESCAPE_CHARACTER);\n+\t\tproperties.add(FORMAT_NULL_LITERAL);\n+\t\treturn properties;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, String> requiredContext() {\n+\t\tMap<String, String> context = new HashMap<>();\n+\t\tcontext.put(FORMAT, \"csv\");\n+\t\treturn context;\n+\t}\n+\n+\tprivate static DescriptorProperties getValidatedProperties(Map<String, String> propertiesMap) {\n+\t\tfinal DescriptorProperties properties = new DescriptorProperties(true);\n+\t\tproperties.putProperties(propertiesMap);\n+\n+\t\tproperties.validateString(FORMAT_FIELD_DELIMITER, true, 1, 1);\n+\t\tproperties.validateEnumValues(FORMAT_LINE_DELIMITER, true, Arrays.asList(\"\\r\", \"\\n\", \"\\r\\n\", \"\"));\n+\t\tproperties.validateBoolean(FORMAT_DISABLE_QUOTE_CHARACTER, true);\n+\t\tproperties.validateString(FORMAT_QUOTE_CHARACTER, true, 1, 1);\n+\t\tproperties.validateBoolean(FORMAT_ALLOW_COMMENTS, true);\n+\t\tproperties.validateBoolean(FORMAT_IGNORE_PARSE_ERRORS, true);\n+\t\tproperties.validateString(FORMAT_ARRAY_ELEMENT_DELIMITER, true, 1);\n+\t\tproperties.validateString(FORMAT_ESCAPE_CHARACTER, true, 1, 1);\n+\n+\t\treturn properties;\n+\t}\n+\n+\t/**\n+\t * InputFormat that reads csv record into {@link RowData}.\n+\t */\n+\tpublic static class CsvInputFormat extends AbstractCsvInputFormat<RowData> {\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\tprivate final RowType formatRowType;\n+\t\tprivate final DataType[] fieldTypes;\n+\t\tprivate final String[] fieldNames;\n+\t\tprivate final int[] selectFields;\n+\t\tprivate final List<String> partitionKeys;\n+\t\tprivate final String defaultPartValue;\n+\t\tprivate final long limit;\n+\t\tprivate final int[] csvSelectFieldToProjectFieldMapping;\n+\t\tprivate final int[] csvSelectFieldToCsvFieldMapping;\n+\t\tprivate final boolean ignoreParseErrors;\n+\n+\t\tprivate transient InputStreamReader inputStreamReader;\n+\t\tprivate transient BufferedReader reader;\n+\t\tprivate transient boolean end;\n+\t\tprivate transient long emitted;\n+\t\t// reuse object for per record\n+\t\tprivate transient GenericRowData rowData;\n+\t\tprivate transient DeserializationRuntimeConverter runtimeConverter;\n+\t\tprivate transient MappingIterator<JsonNode> iterator;\n+\n+\t\tpublic CsvInputFormat(\n+\t\t\t\tPath[] filePaths,\n+\t\t\t\tDataType[] fieldTypes,\n+\t\t\t\tString[] fieldNames,\n+\t\t\t\tCsvSchema csvSchema,\n+\t\t\t\tRowType formatRowType,\n+\t\t\t\tint[] selectFields,\n+\t\t\t\tList<String> partitionKeys,\n+\t\t\t\tString defaultPartValue,\n+\t\t\t\tlong limit,\n+\t\t\t\tint[] csvSelectFieldToProjectFieldMapping,\n+\t\t\t\tint[] csvSelectFieldToCsvFieldMapping,\n+\t\t\t\tboolean ignoreParseErrors) {\n+\t\t\tsuper(filePaths, csvSchema);\n+\t\t\tthis.fieldTypes = fieldTypes;\n+\t\t\tthis.fieldNames = fieldNames;\n+\t\t\tthis.formatRowType = formatRowType;\n+\t\t\tthis.partitionKeys = partitionKeys;\n+\t\t\tthis.defaultPartValue = defaultPartValue;\n+\t\t\tthis.selectFields = selectFields;\n+\t\t\tthis.limit = limit;\n+\t\t\tthis.emitted = 0;\n+\t\t\tthis.csvSelectFieldToProjectFieldMapping = csvSelectFieldToProjectFieldMapping;\n+\t\t\tthis.csvSelectFieldToCsvFieldMapping = csvSelectFieldToCsvFieldMapping;\n+\t\t\tthis.ignoreParseErrors = ignoreParseErrors;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void open(FileInputSplit split) throws IOException {\n+\t\t\tsuper.open(split);\n+\t\t\tthis.end = false;\n+\t\t\tthis.inputStreamReader = new InputStreamReader(csvInputStream);\n+\t\t\tthis.reader = new BufferedReader(inputStreamReader);\n+\t\t\tthis.rowData = PartitionPathUtils.fillPartitionValueForRecord(fieldNames, fieldTypes, selectFields,\n+\t\t\t\tpartitionKeys, currentSplit.getPath(), defaultPartValue);\n+\t\t\tthis.iterator = new CsvMapper()\n+\t\t\t\t.readerFor(JsonNode.class)\n+\t\t\t\t.with(csvSchema)\n+\t\t\t\t.readValues(csvInputStream);\n+\t\t\tprepareRuntimeConverter();\n+\t\t}\n+\n+\t\tprivate void prepareRuntimeConverter(){\n+\t\t\tCsvRowDataDeserializationSchema.Builder builder = new CsvRowDataDeserializationSchema.Builder(\n+\t\t\t\tformatRowType, new GenericTypeInfo<>(RowData.class))\n+\t\t\t\t.setIgnoreParseErrors(ignoreParseErrors);\n+\t\t\tthis.runtimeConverter = builder.build().createRowConverter(formatRowType, true);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean reachedEnd() throws IOException {\n+\t\t\treturn emitted >= limit || end;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic RowData nextRecord(RowData reuse) throws IOException {\n+\t\t\tGenericRowData csvRow = null;\n+\t\t\tdo {\n+\t\t\t\ttry {\n+\t\t\t\t\tJsonNode root = iterator.nextValue();\n+\t\t\t\t\tcsvRow = (GenericRowData) runtimeConverter.convert(root);\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tend = true;\n+\t\t\t\t} catch (Throwable t) {\n+\t\t\t\t\tif (!ignoreParseErrors) {\n+\t\t\t\t\t\tthrow new IOException(\"Failed to deserialize CSV row.\", t);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} while (csvRow == null && !reachedEnd());\n+\n+\t\t\tGenericRowData returnRecord = null;\n+\t\t\tif (csvRow != null) {\n+\t\t\t\treturnRecord = rowData;\n+\t\t\t\tfor (int i = 0; i < csvSelectFieldToCsvFieldMapping.length; i++) {\n+\t\t\t\t\treturnRecord.setField(csvSelectFieldToProjectFieldMapping[i],\n+\t\t\t\t\t\tcsvRow.getField(csvSelectFieldToCsvFieldMapping[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\temitted++;\n+\t\t\treturn returnRecord;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void close() throws IOException {\n+\t\t\tsuper.close();\n+\t\t\tif (reader != null) {\n+\t\t\t\treader.close();\n+\t\t\t\treader = null;\n+\t\t\t}\n+\t\t\tif (inputStreamReader != null) {\n+\t\t\t\tinputStreamReader.close();\n+\t\t\t\tinputStreamReader = null;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A {@link Encoder} writes {@link RowData} record into {@link java.io.OutputStream} with csv format.\n+\t */\n+\tpublic static class CsvRowDataEncoder implements Encoder<RowData> {", "originalCommit": "0fd33d570a8591ef7e5a523c29366a34aa6b986e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nindex 39ecb0cfa03..5ec26b6c454 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n\n@@ -23,7 +23,6 @@ import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.Csv\n import java.io.BufferedReader;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.io.OutputStream;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n"}}, {"oid": "b72a27e8fa3a6b21e5505435be096eea97d7da07", "url": "https://github.com/apache/flink/commit/b72a27e8fa3a6b21e5505435be096eea97d7da07", "message": "[FLINK-14257][Connectors / FileSystem]Integrate csv to FileSystemTableFactory", "committedDate": "2020-05-11T10:12:44Z", "type": "commit"}, {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "message": "rebase and address comment", "committedDate": "2020-05-11T11:52:11Z", "type": "commit"}, {"oid": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "url": "https://github.com/apache/flink/commit/73ca85b532ef3efcef5e7795db77c79d7bf82131", "message": "rebase and address comment", "committedDate": "2020-05-11T11:52:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzY2NA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423664", "bodyText": "CsvFileSystemFormatFactory", "author": "JingsongLi", "createdAt": "2020-05-12T02:18:04Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\nsimilarity index 85%\nrename from flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nrename to flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\nindex 5ec26b6c454..1053c78216f 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\n\n@@ -1,3 +1,21 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.flink.formats.csv;\n \n import org.apache.flink.api.common.io.InputFormat;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzgyNQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423825", "bodyText": "Use getOptionalChar.ifPresent getOptionalString.ifPresent", "author": "JingsongLi", "createdAt": "2020-05-12T02:18:39Z", "path": "flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java", "diffHunk": "@@ -0,0 +1,333 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.serialization.BulkWriter;\n+import org.apache.flink.api.common.serialization.Encoder;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.DeserializationRuntimeConverter;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.descriptors.DescriptorProperties;\n+import org.apache.flink.table.factories.FileSystemFormatFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.utils.PartitionPathUtils;\n+\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonNode;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.MappingIterator;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ALLOW_COMMENTS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ARRAY_ELEMENT_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_DISABLE_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_ESCAPE_CHARACTER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_FIELD_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_IGNORE_PARSE_ERRORS;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_LINE_DELIMITER;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_NULL_LITERAL;\n+import static org.apache.flink.table.descriptors.CsvValidator.FORMAT_QUOTE_CHARACTER;\n+import static org.apache.flink.table.descriptors.FormatDescriptorValidator.FORMAT;\n+\n+/**\n+ * CSV format factory for file system.\n+ */\n+public class CsvRowDataFileSystemFormatFactory implements FileSystemFormatFactory {\n+\n+\t@Override\n+\tpublic InputFormat<RowData, ?> createReader(ReaderContext context) {\n+\t\tDescriptorProperties properties = getValidatedProperties(context.getFormatProperties());\n+\n+\t\tRowType formatRowType = context.getFormatRowType();\n+\n+\t\tString[] fieldNames = context.getSchema().getFieldNames();\n+\t\tList<String> projectFields = Arrays.stream(context.getProjectFields())\n+\t\t\t.mapToObj(idx -> fieldNames[idx])\n+\t\t\t.collect(Collectors.toList());\n+\t\tList<String> csvFields = Arrays.stream(fieldNames)\n+\t\t\t.filter(field -> !context.getPartitionKeys().contains(field))\n+\t\t\t.collect(Collectors.toList());\n+\n+\t\tint[] csvSelectFieldToProjectFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(projectFields::indexOf)\n+\t\t\t.toArray();\n+\t\tint[] csvSelectFieldToCsvFieldMapping = context.getFormatProjectFields().stream()\n+\t\t\t.mapToInt(csvFields::indexOf)\n+\t\t\t.toArray();\n+\n+\t\tCsvSchema csvSchema = buildCsvSchema(formatRowType, properties);\n+\n+\t\tboolean ignoreParseErrors = properties.getOptionalBoolean(FORMAT_IGNORE_PARSE_ERRORS).orElse(false);\n+\n+\t\treturn new CsvInputFormat(\n+\t\t\tcontext.getPaths(),\n+\t\t\tcontext.getSchema().getFieldDataTypes(),\n+\t\t\tcontext.getSchema().getFieldNames(),\n+\t\t\tcsvSchema,\n+\t\t\tformatRowType,\n+\t\t\tcontext.getProjectFields(),\n+\t\t\tcontext.getPartitionKeys(),\n+\t\t\tcontext.getDefaultPartName(),\n+\t\t\tcontext.getPushedDownLimit(),\n+\t\t\tcsvSelectFieldToProjectFieldMapping,\n+\t\t\tcsvSelectFieldToCsvFieldMapping,\n+\t\t\tignoreParseErrors);\n+\t}\n+\n+\tprivate CsvSchema buildCsvSchema(RowType rowType, DescriptorProperties properties) {\n+\t\tCsvSchema csvSchema = CsvRowSchemaConverter.convert(rowType);\n+\t\t//format properties\n+\t\tif (properties.containsKey(FORMAT_FIELD_DELIMITER)) {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\nsimilarity index 85%\nrename from flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\nrename to flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\nindex 5ec26b6c454..1053c78216f 100644\n--- a/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvRowDataFileSystemFormatFactory.java\n+++ b/flink-formats/flink-csv/src/main/java/org/apache/flink/formats/csv/CsvFileSystemFormatFactory.java\n\n@@ -1,3 +1,21 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.flink.formats.csv;\n \n import org.apache.flink.api.common.io.InputFormat;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzk0MQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423423941", "bodyText": "CsvFilesystemBatchITCase\nOthers too.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:07Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nsimilarity index 64%\nrename from flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\nrename to flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nindex 001ea5f1550..671ba1292f6 100644\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\n+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\n\n@@ -1,3 +1,21 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.flink.formats.csv;\n \n import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAxOA==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424018", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:27Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nsimilarity index 64%\nrename from flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\nrename to flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nindex 001ea5f1550..671ba1292f6 100644\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\n+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\n\n@@ -1,3 +1,21 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.flink.formats.csv;\n \n import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDAzMQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424031", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:31Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java", "diffHunk": "@@ -0,0 +1,85 @@\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in batch mode.\n+ */\n+@RunWith(Enclosed.class)\n+public class CsvRowDataFilesystemBatchITCase {\n+\n+\t/**\n+\t * General IT cases for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class GeneralCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {\n+\n+\t\t@Override\n+\t\tpublic String[] formatProperties() {\n+\t\t\tList<String> ret = new ArrayList<>();\n+\t\t\tret.add(\"'format'='csv'\");\n+\t\t\tret.add(\"'format.field-delimiter'=';'\");\n+\t\t\tret.add(\"'format.quote-character'='#'\");\n+\t\t\treturn ret.toArray(new String[0]);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Enriched IT cases that including testParseError and testEscapeChar for CsvRowDataFilesystem in batch mode.\n+\t */\n+\tpublic static class EnrichedCsvRowDataFilesystemBatchITCase extends BatchFileSystemITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nsimilarity index 64%\nrename from flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\nrename to flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\nindex 001ea5f1550..671ba1292f6 100644\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemBatchITCase.java\n+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemBatchITCase.java\n\n@@ -1,3 +1,21 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n package org.apache.flink.formats.csv;\n \n import org.apache.flink.table.planner.runtime.batch.sql.BatchFileSystemITCaseBase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDA1MQ==", "url": "https://github.com/apache/flink/pull/11755#discussion_r423424051", "bodyText": "Ditto.", "author": "JingsongLi", "createdAt": "2020-05-12T02:19:36Z", "path": "flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.csv;\n+\n+import org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in stream mode.\n+ */\n+public class CsvRowDataFilesystemStreamITCase extends FsStreamingSinkITCaseBase {", "originalCommit": "73ca85b532ef3efcef5e7795db77c79d7bf82131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44142403791d63981b591fc062018b5d41dee7a8", "chunk": "diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java\nsimilarity index 87%\nrename from flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java\nrename to flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java\nindex 174adcc6be4..5f1069fe2ee 100644\n--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataFilesystemStreamITCase.java\n+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFilesystemStreamITCase.java\n\n@@ -24,9 +24,9 @@ import java.util.ArrayList;\n import java.util.List;\n \n /**\n- * ITCase to test csv format for {@link CsvRowDataFileSystemFormatFactory} in stream mode.\n+ * ITCase to test csv format for {@link CsvFileSystemFormatFactory} in stream mode.\n  */\n-public class CsvRowDataFilesystemStreamITCase extends FsStreamingSinkITCaseBase {\n+public class CsvFilesystemStreamITCase extends FsStreamingSinkITCaseBase {\n \t@Override\n \tpublic String[] additionalProperties() {\n \t\tList<String> ret = new ArrayList<>();\n"}}, {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "message": "address comments", "committedDate": "2020-05-12T04:06:03Z", "type": "commit"}, {"oid": "44142403791d63981b591fc062018b5d41dee7a8", "url": "https://github.com/apache/flink/commit/44142403791d63981b591fc062018b5d41dee7a8", "message": "address comments", "committedDate": "2020-05-12T04:06:03Z", "type": "forcePushed"}]}