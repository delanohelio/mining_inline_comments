{"pr_number": 13939, "pr_title": "[FLINK-19992][hive] Integrate new orc to Hive source", "pr_createdAt": "2020-11-05T09:01:05Z", "pr_url": "https://github.com/apache/flink/pull/13939", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3MzE2Mw==", "url": "https://github.com/apache/flink/pull/13939#discussion_r518573163", "bodyText": "We already have a computeSelectedFields method, can it be reused?", "author": "lirui-apache", "createdAt": "2020-11-06T07:45:04Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -107,24 +114,78 @@ public boolean isSplittable() {\n \t\treturn InternalTypeInfo.of(producedRowType);\n \t}\n \n+\tprivate RowType tableRowType() {\n+\t\tLogicalType[] types = Arrays.stream(fieldTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\treturn RowType.of(types, fieldNames);\n+\t}\n+\n+\tprivate int[] projectedFields() {", "originalCommit": "a7bfb90cbeffd83a836343f55f0535c7e3c487de", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "670c1f199ad98fb4814f172cdee5158dbb3ed97a", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\nindex 82335b9a5a..66e1f9aa2a 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\n\n@@ -119,16 +120,6 @@ public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSpli\n \t\treturn RowType.of(types, fieldNames);\n \t}\n \n-\tprivate int[] projectedFields() {\n-\t\tList<String> nameList = Arrays.asList(fieldNames);\n-\t\tint[] projectedFields = new int[producedRowType.getFieldCount()];\n-\t\tList<String> projectedNames = producedRowType.getFieldNames();\n-\t\tfor (int i = 0; i < projectedFields.length; i++) {\n-\t\t\tprojectedFields[i] = nameList.indexOf(projectedNames.get(i));\n-\t\t}\n-\t\treturn projectedFields;\n-\t}\n-\n \tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n \t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n \t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NDg2Ng==", "url": "https://github.com/apache/flink/pull/13939#discussion_r518574866", "bodyText": "Collections.emptyList()?", "author": "lirui-apache", "createdAt": "2020-11-06T07:49:11Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java", "diffHunk": "@@ -107,24 +114,78 @@ public boolean isSplittable() {\n \t\treturn InternalTypeInfo.of(producedRowType);\n \t}\n \n+\tprivate RowType tableRowType() {\n+\t\tLogicalType[] types = Arrays.stream(fieldTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new);\n+\t\treturn RowType.of(types, fieldNames);\n+\t}\n+\n+\tprivate int[] projectedFields() {\n+\t\tList<String> nameList = Arrays.asList(fieldNames);\n+\t\tint[] projectedFields = new int[producedRowType.getFieldCount()];\n+\t\tList<String> projectedNames = producedRowType.getFieldNames();\n+\t\tfor (int i = 0; i < projectedFields.length; i++) {\n+\t\t\tprojectedFields[i] = nameList.indexOf(projectedNames.get(i));\n+\t\t}\n+\t\treturn projectedFields;\n+\t}\n+\n \tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n \t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n-\t\t\tPartitionFieldExtractor<HiveSourceSplit> extractor = (PartitionFieldExtractor<HiveSourceSplit>)\n-\t\t\t\t\t(split1, fieldName, fieldType) -> split1.getHiveTablePartition().getPartitionSpec().get(fieldName);\n \t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n \t\t\t\t\tjobConfWrapper.conf(),\n \t\t\t\t\tproducedRowType,\n \t\t\t\t\tpartitionKeys,\n-\t\t\t\t\textractor,\n+\t\t\t\t\tPARTITION_FIELD_EXTRACTOR,\n \t\t\t\t\tDEFAULT_SIZE,\n \t\t\t\t\thiveVersion.startsWith(\"3\"),\n \t\t\t\t\tfalse\n \t\t\t);\n+\t\t} else if (!useMapRedReader && useOrcVectorizedRead(split.getHiveTablePartition())) {\n+\t\t\treturn createOrcFormat();\n \t\t} else {\n \t\t\treturn new HiveMapRedBulkFormat();\n \t\t}\n \t}\n \n+\tprivate OrcColumnarRowFileInputFormat<?, HiveSourceSplit> createOrcFormat() {\n+\t\treturn hiveVersion.startsWith(\"1.\") ?\n+\t\t\t\tOrcColumnarRowFileInputFormat.createPartitionedFormat(\n+\t\t\t\t\tOrcShim.createShim(hiveVersion),\n+\t\t\t\t\tjobConfWrapper.conf(),\n+\t\t\t\t\ttableRowType(),\n+\t\t\t\t\tpartitionKeys,\n+\t\t\t\t\tPARTITION_FIELD_EXTRACTOR,\n+\t\t\t\t\tprojectedFields(),\n+\t\t\t\t\tnew ArrayList<>(),", "originalCommit": "a7bfb90cbeffd83a836343f55f0535c7e3c487de", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "670c1f199ad98fb4814f172cdee5158dbb3ed97a", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\nindex 82335b9a5a..66e1f9aa2a 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/connectors/hive/read/HiveBulkFormatAdapter.java\n\n@@ -119,16 +120,6 @@ public class HiveBulkFormatAdapter implements BulkFormat<RowData, HiveSourceSpli\n \t\treturn RowType.of(types, fieldNames);\n \t}\n \n-\tprivate int[] projectedFields() {\n-\t\tList<String> nameList = Arrays.asList(fieldNames);\n-\t\tint[] projectedFields = new int[producedRowType.getFieldCount()];\n-\t\tList<String> projectedNames = producedRowType.getFieldNames();\n-\t\tfor (int i = 0; i < projectedFields.length; i++) {\n-\t\t\tprojectedFields[i] = nameList.indexOf(projectedNames.get(i));\n-\t\t}\n-\t\treturn projectedFields;\n-\t}\n-\n \tprivate BulkFormat<RowData, ? super HiveSourceSplit> createBulkFormatForSplit(HiveSourceSplit split) {\n \t\tif (!useMapRedReader && useParquetVectorizedRead(split.getHiveTablePartition())) {\n \t\t\treturn ParquetColumnarRowInputFormat.createPartitionedFormat(\n"}}, {"oid": "670c1f199ad98fb4814f172cdee5158dbb3ed97a", "url": "https://github.com/apache/flink/commit/670c1f199ad98fb4814f172cdee5158dbb3ed97a", "message": "Address comments", "committedDate": "2020-11-06T08:40:12Z", "type": "forcePushed"}, {"oid": "cd9245e045185c8740872596237a675547c1058d", "url": "https://github.com/apache/flink/commit/cd9245e045185c8740872596237a675547c1058d", "message": "[FLINK-19992][hive] Integrate new orc to Hive source", "committedDate": "2020-11-06T14:35:08Z", "type": "commit"}, {"oid": "cd9245e045185c8740872596237a675547c1058d", "url": "https://github.com/apache/flink/commit/cd9245e045185c8740872596237a675547c1058d", "message": "[FLINK-19992][hive] Integrate new orc to Hive source", "committedDate": "2020-11-06T14:35:08Z", "type": "forcePushed"}]}