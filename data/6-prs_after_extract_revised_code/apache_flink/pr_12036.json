{"pr_number": 12036, "pr_title": "[FLINK-17537][connectors / jdbc] Refactor flink-jdbc connector structure", "pr_createdAt": "2020-05-08T10:12:51Z", "pr_url": "https://github.com/apache/flink/pull/12036", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDk2NA==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422090964", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class GenericParameterValuesProvider extends JdbcGenericParameterValuesProvider implements ParameterValuesProvider{\n          \n          \n            \n            public class GenericParameterValuesProvider extends JdbcGenericParameterValuesProvider implements ParameterValuesProvider {", "author": "wuchong", "createdAt": "2020-05-08T11:27:28Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/GenericParameterValuesProvider.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc.split;\n+\n+import org.apache.flink.api.java.io.jdbc.JDBCInputFormat;\n+import org.apache.flink.connector.jdbc.split.JdbcGenericParameterValuesProvider;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * This splits generator actually does nothing but wrapping the query parameters\n+ * computed by the user before creating the {@link JDBCInputFormat} instance.\n+ *\n+ * @deprecated Please use {@link JdbcGenericParameterValuesProvider}.\n+ */\n+@Deprecated\n+public class GenericParameterValuesProvider extends JdbcGenericParameterValuesProvider implements ParameterValuesProvider{", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/GenericParameterValuesProvider.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/GenericParameterValuesProvider.java\nindex fbf92c6da1a..70b9596533a 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/GenericParameterValuesProvider.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/GenericParameterValuesProvider.java\n\n@@ -30,7 +30,7 @@ import java.io.Serializable;\n  * @deprecated Please use {@link JdbcGenericParameterValuesProvider}.\n  */\n @Deprecated\n-public class GenericParameterValuesProvider extends JdbcGenericParameterValuesProvider implements ParameterValuesProvider{\n+public class GenericParameterValuesProvider extends JdbcGenericParameterValuesProvider implements ParameterValuesProvider {\n \n \tpublic GenericParameterValuesProvider(Serializable[][] parameters) {\n \t\tsuper(parameters);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MTQwMA==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422091400", "bodyText": "remove this line.", "author": "wuchong", "createdAt": "2020-05-08T11:28:42Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/ParameterValuesProvider.java", "diffHunk": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc.split;\n+\n+import org.apache.flink.api.java.io.jdbc.JDBCInputFormat;\n+import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;\n+\n+/**\n+ * This interface is used by the {@link JDBCInputFormat} to compute the list of parallel query to run (i.e. splits).\n+ * Each query will be parameterized using a row of the matrix provided by each {@link ParameterValuesProvider}\n+ * implementation.\n+ *\n+ * @deprecated Please use {@link JdbcParameterValuesProvider}.\n+ * Flink proposes class name start with \"Jdbc\" rather than \"JDBC\".", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/ParameterValuesProvider.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/ParameterValuesProvider.java\nindex a59a05f55a2..f29132023b7 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/ParameterValuesProvider.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/ParameterValuesProvider.java\n\n@@ -27,7 +27,6 @@ import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;\n  * implementation.\n  *\n  * @deprecated Please use {@link JdbcParameterValuesProvider}.\n- * Flink proposes class name start with \"Jdbc\" rather than \"JDBC\".\n  */\n @Deprecated\n public interface ParameterValuesProvider extends JdbcParameterValuesProvider {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MTgyNQ==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422091825", "bodyText": "You can keep the original class Javadoc here.", "author": "wuchong", "createdAt": "2020-05-08T11:29:47Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.connector.jdbc.JdbcInputFormat;\n+import org.apache.flink.connector.jdbc.source.row.converter.JdbcRowConverter;\n+import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.sql.Connection;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+\n+/**\n+ * InputFormat to read data from a database and generate Rows.\n+ * The InputFormat has to be configured using the supplied InputFormatBuilder.", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java\nindex 9ff2ddf044e..5213bdd45f8 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java\n\n@@ -19,21 +19,69 @@\n package org.apache.flink.api.java.io.jdbc;\n \n import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.connector.jdbc.JdbcInputFormat;\n-import org.apache.flink.connector.jdbc.source.row.converter.JdbcRowConverter;\n+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;\n import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;\n+import org.apache.flink.types.Row;\n import org.apache.flink.util.Preconditions;\n \n import java.sql.Connection;\n+import java.sql.DriverManager;\n import java.sql.PreparedStatement;\n import java.sql.ResultSet;\n \n /**\n  * InputFormat to read data from a database and generate Rows.\n  * The InputFormat has to be configured using the supplied InputFormatBuilder.\n+ * A valid RowTypeInfo must be properly configured in the builder, e.g.:\n  *\n- *  @deprecated Please use {@link JdbcInputFormat}, Flink proposes class name start with \"Jdbc\" rather than \"JDBC\".\n+ * <pre><code>\n+ * TypeInformation<?>[] fieldTypes = new TypeInformation<?>[] {\n+ *\t\tBasicTypeInfo.INT_TYPE_INFO,\n+ *\t\tBasicTypeInfo.STRING_TYPE_INFO,\n+ *\t\tBasicTypeInfo.STRING_TYPE_INFO,\n+ *\t\tBasicTypeInfo.DOUBLE_TYPE_INFO,\n+ *\t\tBasicTypeInfo.INT_TYPE_INFO\n+ *\t};\n+ *\n+ * RowTypeInfo rowTypeInfo = new RowTypeInfo(fieldTypes);\n+ *\n+ * JDBCInputFormat jdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n+ *\t\t\t\t.setDrivername(\"org.apache.derby.jdbc.EmbeddedDriver\")\n+ *\t\t\t\t.setDBUrl(\"jdbc:derby:memory:ebookshop\")\n+ *\t\t\t\t.setQuery(\"select * from books\")\n+ *\t\t\t\t.setRowTypeInfo(rowTypeInfo)\n+ *\t\t\t\t.finish();\n+ * </code></pre>\n+ *\n+ * <p>In order to query the JDBC source in parallel, you need to provide a\n+ * parameterized query template (i.e. a valid {@link PreparedStatement}) and\n+ * a {@link ParameterValuesProvider} which provides binding values for the\n+ * query parameters. E.g.:\n+ *\n+ * <pre><code>\n+ *\n+ * Serializable[][] queryParameters = new String[2][1];\n+ * queryParameters[0] = new String[]{\"Kumar\"};\n+ * queryParameters[1] = new String[]{\"Tan Ah Teck\"};\n+ *\n+ * JDBCInputFormat jdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n+ *\t\t\t\t.setDrivername(\"org.apache.derby.jdbc.EmbeddedDriver\")\n+ *\t\t\t\t.setDBUrl(\"jdbc:derby:memory:ebookshop\")\n+ *\t\t\t\t.setQuery(\"select * from books WHERE author = ?\")\n+ *\t\t\t\t.setRowTypeInfo(rowTypeInfo)\n+ *\t\t\t\t.setParametersProvider(new GenericParameterValuesProvider(queryParameters))\n+ *\t\t\t\t.finish();\n+ * </code></pre>\n+ *\n+ * @see Row\n+ * @see ParameterValuesProvider\n+ * @see PreparedStatement\n+ * @see DriverManager\n+ *\n+ * @deprecated Please use {@link JdbcInputFormat}.\n  */\n @Deprecated\n public class JDBCInputFormat extends JdbcInputFormat {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5NTI4Mw==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422095283", "bodyText": "indent", "author": "wuchong", "createdAt": "2020-05-08T11:38:23Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java", "diffHunk": "@@ -80,7 +80,7 @@ default String quoteIdentifier(String identifier) {\n \t * the use of select + update/insert, this performance is poor.\n \t */\n \tdefault Optional<String> getUpsertStatement(\n-\t\t\tString tableName, String[] fieldNames, String[] uniqueKeyFields) {\n+\t\tString tableName, String[] fieldNames, String[] uniqueKeyFields) {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5NTc4MQ==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422095781", "bodyText": "Why removes the final?", "author": "wuchong", "createdAt": "2020-05-08T11:39:43Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java", "diffHunk": "@@ -39,27 +39,27 @@\n /**\n  * Default JDBC dialects.\n  */\n-public final class JDBCDialects {\n+public class JdbcDialects {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEyNDE3OQ==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422124179", "bodyText": "And what about moving each dialect in its own class (instead of having all the implementation insidide this class)? Or better: why don't load them via SPI (so we could support more dialects just adding external jars to Flink lib directory without requiring to have them in the official jdbc connector code)?", "author": "fpompermaier", "createdAt": "2020-05-08T12:50:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5NTc4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java\nindex 9386eb89f18..211bb02ed4c 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java\n\n@@ -39,7 +39,7 @@ import java.util.stream.Collectors;\n /**\n  * Default JDBC dialects.\n  */\n-public class JdbcDialects {\n+public final class JdbcDialects {\n \n \tprivate static final List<JdbcDialect> DIALECTS = Arrays.asList(\n \t\tnew DerbyDialect(),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5NzE2OQ==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422097169", "bodyText": "The package-visible is on purpose, so that users can't extend this class.", "author": "wuchong", "createdAt": "2020-05-08T11:43:23Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcBatchingOutputFormat.java", "diffHunk": "@@ -40,10 +39,13 @@\n import java.util.concurrent.TimeUnit;\n import java.util.function.Function;\n \n-import static org.apache.flink.api.java.io.jdbc.JDBCUtils.setRecordToStatement;\n+import static org.apache.flink.connector.jdbc.JdbcUtils.setRecordToStatement;\n import static org.apache.flink.util.Preconditions.checkNotNull;\n \n-class JdbcBatchingOutputFormat<In, JdbcIn, JdbcExec extends JdbcBatchStatementExecutor<JdbcIn>> extends AbstractJdbcOutputFormat<In> {\n+/**\n+ * A JDBC outputFormat support batching records before writing records to database.\n+ */\n+public class JdbcBatchingOutputFormat<In, JdbcIn, JdbcExec extends JdbcBatchStatementExecutor<JdbcIn>> extends AbstractJdbcOutputFormat<In> {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcBatchingOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java\nsimilarity index 83%\nrename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcBatchingOutputFormat.java\nrename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java\nindex 8b9f684f0bd..0af3d738439 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcBatchingOutputFormat.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java\n\n@@ -39,20 +47,31 @@ import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.TimeUnit;\n import java.util.function.Function;\n \n-import static org.apache.flink.connector.jdbc.JdbcUtils.setRecordToStatement;\n+import static org.apache.flink.connector.jdbc.utils.JdbcUtils.setRecordToStatement;\n import static org.apache.flink.util.Preconditions.checkNotNull;\n \n /**\n- * A JDBC outputFormat support batching records before writing records to database.\n+ * A JDBC outputFormat that supports batching records before writing records to database.\n  */\n+@Internal\n public class JdbcBatchingOutputFormat<In, JdbcIn, JdbcExec extends JdbcBatchStatementExecutor<JdbcIn>> extends AbstractJdbcOutputFormat<In> {\n-\tinterface RecordExtractor<F, T> extends Function<F, T>, Serializable {\n+\n+\t/**\n+\t * An interface to extract a value from given argument.\n+\t * @param <F> The type of given argument\n+\t * @param <T> The type of the return value\n+\t */\n+\tpublic interface RecordExtractor<F, T> extends Function<F, T>, Serializable {\n \t\tstatic <T> RecordExtractor<T, T> identity() {\n \t\t\treturn x -> x;\n \t\t}\n \t}\n \n-\tinterface StatementExecutorFactory<T extends JdbcBatchStatementExecutor<?>> extends Function<RuntimeContext, T>, Serializable {\n+\t/**\n+\t * A factory for creating {@link JdbcBatchStatementExecutor} instance.\n+\t * @param <T> The type of instance.\n+\t */\n+\tpublic interface StatementExecutorFactory<T extends JdbcBatchStatementExecutor<?>> extends Function<RuntimeContext, T>, Serializable {\n \t}\n \n \tprivate static final long serialVersionUID = 1L;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5OTI4Ng==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422099286", "bodyText": "Please remove this class.", "author": "wuchong", "createdAt": "2020-05-08T11:49:14Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java", "diffHunk": "@@ -37,15 +36,16 @@\n  * @see Row\n  * @see DriverManager\n  */\n+\n /**\n  * @deprecated use {@link JdbcBatchingOutputFormat}\n  */\n @Deprecated\n-public class JDBCOutputFormat extends AbstractJdbcOutputFormat<Row> {\n+public class JdbcOutputFormat extends AbstractJdbcOutputFormat<Row> {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java\nindex cf41922cef6..b57541c996b 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java\n\n@@ -18,133 +18,52 @@\n \n package org.apache.flink.connector.jdbc;\n \n+import org.apache.flink.annotation.Experimental;\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;\n+import org.apache.flink.connector.jdbc.internal.connection.JdbcConnectionProvider;\n+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;\n+import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;\n import org.apache.flink.types.Row;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.IOException;\n-import java.sql.PreparedStatement;\n-import java.sql.SQLException;\n+import java.util.function.Function;\n \n-import static org.apache.flink.connector.jdbc.JdbcUtils.setRecordToStatement;\n+import static org.apache.flink.connector.jdbc.utils.JdbcUtils.setRecordToStatement;\n \n /**\n  * OutputFormat to write Rows into a JDBC database.\n  * The OutputFormat has to be configured using the supplied OutputFormatBuilder.\n- *\n- * @see Row\n- * @see DriverManager\n  */\n-\n-/**\n- * @deprecated use {@link JdbcBatchingOutputFormat}\n- */\n-@Deprecated\n-public class JdbcOutputFormat extends AbstractJdbcOutputFormat<Row> {\n+@Experimental\n+public class JdbcOutputFormat extends JdbcBatchingOutputFormat<Row, Row, JdbcBatchStatementExecutor<Row>> {\n \n \tprivate static final long serialVersionUID = 1L;\n \n-\tprotected static final Logger LOG = LoggerFactory.getLogger(JdbcOutputFormat.class);\n-\n-\tfinal JdbcInsertOptions insertOptions;\n-\tprivate final JdbcExecutionOptions batchOptions;\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(JdbcOutputFormat.class);\n \n-\tprivate transient PreparedStatement upload;\n-\tprivate transient int batchCount = 0;\n-\n-\t/**\n-\t * @deprecated use {@link JdbcOutputFormatBuilder builder} instead.\n-\t */\n-\t@Deprecated\n-\tpublic JdbcOutputFormat(String username, String password, String drivername, String dbURL, String query, int batchInterval, int[] typesArray) {\n-\t\tthis(new SimpleJdbcConnectionProvider(new JdbcConnectionOptions.JdbcConnectionOptionsBuilder().withUrl(dbURL).withDriverName(drivername).withUsername(username).withPassword(password).build()),\n-\t\t\t\tnew JdbcInsertOptions(query, typesArray),\n-\t\t\t\tJdbcExecutionOptions.builder().withBatchSize(batchInterval).build());\n+\tprivate JdbcOutputFormat(JdbcConnectionProvider connectionProvider, String sql, int[] typesArray, int batchSize) {\n+\t\tsuper(\n+\t\t\tconnectionProvider,\n+\t\t\tnew JdbcExecutionOptions.Builder().withBatchSize(batchSize).build(),\n+\t\t\tctx -> createRowExecutor(sql, typesArray, ctx),\n+\t\t\tJdbcBatchingOutputFormat.RecordExtractor.identity());\n \t}\n \n-\tprotected JdbcOutputFormat(JdbcConnectionProvider connectionProvider, JdbcInsertOptions insertOptions, JdbcExecutionOptions batchOptions) {\n-\t\tsuper(connectionProvider);\n-\t\tthis.insertOptions = insertOptions;\n-\t\tthis.batchOptions = batchOptions;\n-\t}\n-\n-\t/**\n-\t * Connects to the target database and initializes the prepared statement.\n-\t *\n-\t * @param taskNumber The number of the parallel instance.\n-\t * @throws IOException Thrown, if the output could not be opened due to an\n-\t * I/O problem.\n-\t */\n-\t@Override\n-\tpublic void open(int taskNumber, int numTasks) throws IOException {\n-\t\tsuper.open(taskNumber, numTasks);\n-\t\ttry {\n-\t\t\tupload = connection.prepareStatement(insertOptions.getQuery());\n-\t\t} catch (SQLException sqe) {\n-\t\t\tthrow new IOException(\"open() failed.\", sqe);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void writeRecord(Row row) {\n-\t\ttry {\n-\t\t\tsetRecordToStatement(upload, insertOptions.getFieldTypes(), row);\n-\t\t\tupload.addBatch();\n-\t\t} catch (SQLException e) {\n-\t\t\tthrow new RuntimeException(\"Preparation of JDBC statement failed.\", e);\n-\t\t}\n-\n-\t\tbatchCount++;\n-\n-\t\tif (batchCount >= batchOptions.getBatchSize()) {\n-\t\t\t// execute batch\n-\t\t\tflush();\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void flush() {\n-\t\ttry {\n-\t\t\tupload.executeBatch();\n-\t\t\tbatchCount = 0;\n-\t\t} catch (SQLException e) {\n-\t\t\tthrow new RuntimeException(\"Execution of JDBC statement failed.\", e);\n-\t\t}\n-\t}\n-\n-\tint[] getTypesArray() {\n-\t\treturn insertOptions.getFieldTypes();\n-\t}\n-\n-\t/**\n-\t * Executes prepared statement and closes all resources of this instance.\n-\t *\n-\t */\n-\t@Override\n-\tpublic void close() {\n-\t\tif (upload != null) {\n-\t\t\tflush();\n-\t\t\ttry {\n-\t\t\t\tupload.close();\n-\t\t\t} catch (SQLException e) {\n-\t\t\t\tLOG.info(\"JDBC statement could not be closed: \" + e.getMessage());\n-\t\t\t} finally {\n-\t\t\t\tupload = null;\n-\t\t\t}\n-\t\t}\n-\n-\t\tsuper.close();\n+\tprivate static JdbcBatchStatementExecutor<Row> createRowExecutor(String sql, int[] typesArray, RuntimeContext ctx) {\n+\t\tJdbcStatementBuilder<Row> statementBuilder = (st, record) -> setRecordToStatement(st, typesArray, record);\n+\t\treturn JdbcBatchStatementExecutor.simple(\n+\t\t\tsql,\n+\t\t\tstatementBuilder,\n+\t\t\tctx.getExecutionConfig().isObjectReuseEnabled() ? Row::copy : Function.identity());\n \t}\n \n \tpublic static JdbcOutputFormatBuilder buildJdbcOutputFormat() {\n \t\treturn new JdbcOutputFormatBuilder();\n \t}\n \n-\tpublic int[] getFieldTypes() {\n-\t\treturn insertOptions.getFieldTypes();\n-\t}\n-\n \t/**\n \t * Builder for {@link JdbcOutputFormat}.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwMTQ5NA==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422101494", "bodyText": "I think keeping this static imports can make code more concise.", "author": "wuchong", "createdAt": "2020-05-08T11:54:45Z", "path": "flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcITCase.java", "diffHunk": "@@ -35,26 +33,22 @@\n import java.util.Arrays;\n import java.util.List;\n \n-import static org.apache.flink.api.java.io.jdbc.JdbcTestFixture.INPUT_TABLE;\n-import static org.apache.flink.api.java.io.jdbc.JdbcTestFixture.INSERT_TEMPLATE;\n-import static org.apache.flink.api.java.io.jdbc.JdbcTestFixture.TEST_DATA;", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcITCase.java\nindex 72d6474dd40..d7e04a109c6 100644\n--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcITCase.java\n+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcITCase.java\n\n@@ -33,6 +33,12 @@ import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n \n+import static org.apache.flink.connector.jdbc.JdbcConnectionOptions.JdbcConnectionOptionsBuilder;\n+import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;\n+import static org.apache.flink.connector.jdbc.JdbcTestFixture.INPUT_TABLE;\n+import static org.apache.flink.connector.jdbc.JdbcTestFixture.INSERT_TEMPLATE;\n+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TEST_DATA;\n+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TestEntry;\n import static org.junit.Assert.assertEquals;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNDM1Nw==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422104357", "bodyText": "remove this file? I think nobody is using it, and GenericJdbcSinkFunction is not accessible for users.", "author": "wuchong", "createdAt": "2020-05-08T12:02:15Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcSinkFunction.java", "diffHunk": "@@ -31,9 +31,9 @@\n  */\n @Deprecated\n class JdbcSinkFunction extends RichSinkFunction<Row> implements CheckpointedFunction {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcSinkFunction.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcSinkFunction.java\ndeleted file mode 100644\nindex 3cfba71ac22..00000000000\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcSinkFunction.java\n+++ /dev/null\n\n@@ -1,67 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.connector.jdbc;\n-\n-import org.apache.flink.api.common.functions.RuntimeContext;\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.runtime.state.FunctionInitializationContext;\n-import org.apache.flink.runtime.state.FunctionSnapshotContext;\n-import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n-import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n-import org.apache.flink.types.Row;\n-\n-/**\n- * @deprecated use {@link GenericJdbcSinkFunction}\n- */\n-@Deprecated\n-class JdbcSinkFunction extends RichSinkFunction<Row> implements CheckpointedFunction {\n-\tfinal JdbcOutputFormat outputFormat;\n-\n-\tJdbcSinkFunction(JdbcOutputFormat outputFormat) {\n-\t\tthis.outputFormat = outputFormat;\n-\t}\n-\n-\t@Override\n-\tpublic void invoke(Row value) throws Exception {\n-\t\toutputFormat.writeRecord(value);\n-\t}\n-\n-\t@Override\n-\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n-\t\toutputFormat.flush();\n-\t}\n-\n-\t@Override\n-\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n-\t}\n-\n-\t@Override\n-\tpublic void open(Configuration parameters) throws Exception {\n-\t\tsuper.open(parameters);\n-\t\tRuntimeContext ctx = getRuntimeContext();\n-\t\toutputFormat.setRuntimeContext(ctx);\n-\t\toutputFormat.open(ctx.getIndexOfThisSubtask(), ctx.getNumberOfParallelSubtasks());\n-\t}\n-\n-\t@Override\n-\tpublic void close() throws Exception {\n-\t\toutputFormat.close();\n-\t\tsuper.close();\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNDkwNA==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422104904", "bodyText": "Move to org.apache.flink.connector.jdbc.source.converter package? I think converter is always used for Row (or RowData in the future).", "author": "wuchong", "createdAt": "2020-05-08T12:03:47Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/source/row/converter/JdbcRowConverter.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.api.java.io.jdbc.source.row.converter;\n+package org.apache.flink.connector.jdbc.source.row.converter;", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/source/row/converter/JdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java\nsimilarity index 96%\nrename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/source/row/converter/JdbcRowConverter.java\nrename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java\nindex a2d45b7f6ba..9baa9adcbec 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/source/row/converter/JdbcRowConverter.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.connector.jdbc.source.row.converter;\n+package org.apache.flink.connector.jdbc.internal.converter;\n \n import org.apache.flink.types.Row;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNTIwMw==", "url": "https://github.com/apache/flink/pull/12036#discussion_r422105203", "bodyText": "How about to add an @Internal annotation on this to tell this is not ready to be exposed to users.", "author": "wuchong", "createdAt": "2020-05-08T12:04:31Z", "path": "flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java", "diffHunk": "@@ -31,7 +31,7 @@\n /**\n  * Handle the SQL dialect of jdbc driver.\n  */\n-public interface JDBCDialect extends Serializable {\n+public interface JdbcDialect extends Serializable {", "originalCommit": "173ded87a6bbdc639582e62245de653ce562e0ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "chunk": "diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java\nindex 60f017b7590..6ba85eb4eec 100644\n--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java\n+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java\n\n@@ -31,6 +32,7 @@ import java.util.stream.Collectors;\n /**\n  * Handle the SQL dialect of jdbc driver.\n  */\n+@Internal\n public interface JdbcDialect extends Serializable {\n \n \t/**\n"}}, {"oid": "cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "url": "https://github.com/apache/flink/commit/cb0d57d5d8792936e83463f5399bc5f15c5c69e9", "message": "address comments and recognize packages", "committedDate": "2020-05-12T03:13:58Z", "type": "forcePushed"}, {"oid": "adc67315f721dcedd256e938c642a18878829d0e", "url": "https://github.com/apache/flink/commit/adc67315f721dcedd256e938c642a18878829d0e", "message": "address comments and recognize packages", "committedDate": "2020-05-12T06:14:32Z", "type": "forcePushed"}, {"oid": "83806c39c17f56e280bc8cad215839a902742b9e", "url": "https://github.com/apache/flink/commit/83806c39c17f56e280bc8cad215839a902742b9e", "message": "address comments and recognize packages", "committedDate": "2020-05-12T06:22:47Z", "type": "forcePushed"}, {"oid": "b31fe9e8bb6095647e4267ad31e31908c9ca6063", "url": "https://github.com/apache/flink/commit/b31fe9e8bb6095647e4267ad31e31908c9ca6063", "message": "[FLINK-17537][connectors / jdbc] Refactor flink-jdbc connector structure\n(1) Use Jdbc instead of JDBC;\n(2) Move interfaces and classes to org.apache.flink.connector.jdbc;\n(3) Keep ancient JDBCOutputFormat, JDBCInputFormat and ParameterValuesProvider in old package;\n(4) Add tests/ITCase for ancient Classes and new classes;\n(5) rename flink-jdbc module to flink-connector-jdbc;\n(6) update docs.", "committedDate": "2020-05-12T08:40:33Z", "type": "forcePushed"}, {"oid": "72118ce7dff777ff8cc2f6889c9e8f9a097e4b79", "url": "https://github.com/apache/flink/commit/72118ce7dff777ff8cc2f6889c9e8f9a097e4b79", "message": "[FLINK-17537][connectors / jdbc] Refactor flink-jdbc connector structure\n(1) Use Jdbc instead of JDBC.\n(2) Move interfaces and classes to org.apache.flink.connector.jdbc.\n(3) Keep ancient JDBCOutputFormat, JDBCInputFormat and ParameterValuesProvider in old package.\n(4) Add tests/ITCase for ancient Classes and new classes.\n(5) rename flink-jdbc module to flink-connector-jdbc.\n(6) update docs.", "committedDate": "2020-05-12T14:33:42Z", "type": "forcePushed"}, {"oid": "1cd42176068c2a618b21be8700ca578c2f1b09a3", "url": "https://github.com/apache/flink/commit/1cd42176068c2a618b21be8700ca578c2f1b09a3", "message": "[FLINK-17537][connectors / jdbc] Refactor flink-jdbc connector structure\n(1) Use Jdbc instead of JDBC.\n(2) Move interfaces and classes to org.apache.flink.connector.jdbc.\n(3) Keep ancient JDBCOutputFormat, JDBCInputFormat and ParameterValuesProvider in old package.\n(4) Add tests/ITCase for ancient Classes and new classes.\n(5) rename flink-jdbc module to flink-connector-jdbc.\n(6) update docs.", "committedDate": "2020-05-13T03:26:24Z", "type": "commit"}, {"oid": "1cd42176068c2a618b21be8700ca578c2f1b09a3", "url": "https://github.com/apache/flink/commit/1cd42176068c2a618b21be8700ca578c2f1b09a3", "message": "[FLINK-17537][connectors / jdbc] Refactor flink-jdbc connector structure\n(1) Use Jdbc instead of JDBC.\n(2) Move interfaces and classes to org.apache.flink.connector.jdbc.\n(3) Keep ancient JDBCOutputFormat, JDBCInputFormat and ParameterValuesProvider in old package.\n(4) Add tests/ITCase for ancient Classes and new classes.\n(5) rename flink-jdbc module to flink-connector-jdbc.\n(6) update docs.", "committedDate": "2020-05-13T03:26:24Z", "type": "forcePushed"}]}