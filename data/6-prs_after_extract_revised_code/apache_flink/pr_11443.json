{"pr_number": 11443, "pr_title": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "pr_createdAt": "2020-03-18T14:23:06Z", "pr_url": "https://github.com/apache/flink/pull/11443", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2ODE5OA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r397768198", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ResourceManagerPartitionTrackerFactory", "author": "azagrebin", "createdAt": "2020-03-25T10:59:57Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+/**\n+ * Factory for {@link ResourceManagerPartitionTracker}.\n+ */\n+public interface ResourceManagerPartitionTrackerFactory {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java\ndeleted file mode 100644\nindex c88c5ac4b8..0000000000\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerFactory.java\n+++ /dev/null\n\n@@ -1,25 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-/**\n- * Factory for {@link ResourceManagerPartitionTracker}.\n- */\n-public interface ResourceManagerPartitionTrackerFactory {\n-\tResourceManagerPartitionTracker get(ClusterPartitionReleaser clusterPartitionReleaser);\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc2OTQ5OA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r397769498", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            @FunctionalInterface\n          \n          \n            \n            public interface ClusterPartitionReleaser {", "author": "azagrebin", "createdAt": "2020-03-25T11:02:22Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java b/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\nsimilarity index 64%\nrename from flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java\nrename to flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\nindex 3edf62ff35..7b7372e960 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java\n+++ b/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\n\n@@ -15,16 +15,19 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.runtime.io.network.partition;\n+package org.apache.flink.metrics.prometheus;\n \n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.metrics.reporter.MetricReporterFactory;\n \n-import java.util.Set;\n+import java.util.Properties;\n \n /**\n- * Interface for releasing cluster partitions on a task executor.\n+ * {@link MetricReporterFactory} for {@link PrometheusPushGatewayReporter}.\n  */\n-public interface ClusterPartitionReleaser {\n-\tvoid releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease);\n+public class PrometheusPushGatewayReporterFactory implements MetricReporterFactory {\n+\n+\t@Override\n+\tpublic PrometheusPushGatewayReporter createMetricReporter(Properties properties) {\n+\t\treturn new PrometheusPushGatewayReporter();\n+\t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2NTA3OQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r397865079", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public interface ClusterPartitionReleaser {\n          \n          \n            \n            public interface TaskExecutorClusterPartitionReleaser {\n          \n      \n    \n    \n  \n\nJust if we have also something to release external partitions over shuffle master/ClusterPartitionShuffleClient.", "author": "azagrebin", "createdAt": "2020-03-25T13:46:28Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+\n+import java.util.Set;\n+\n+/**\n+ * Interface for releasing cluster partitions on a task executor.\n+ */\n+public interface ClusterPartitionReleaser {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java b/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\nsimilarity index 64%\nrename from flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java\nrename to flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\nindex 3edf62ff35..7b7372e960 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ClusterPartitionReleaser.java\n+++ b/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusPushGatewayReporterFactory.java\n\n@@ -15,16 +15,19 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.runtime.io.network.partition;\n+package org.apache.flink.metrics.prometheus;\n \n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.metrics.reporter.MetricReporterFactory;\n \n-import java.util.Set;\n+import java.util.Properties;\n \n /**\n- * Interface for releasing cluster partitions on a task executor.\n+ * {@link MetricReporterFactory} for {@link PrometheusPushGatewayReporter}.\n  */\n-public interface ClusterPartitionReleaser {\n-\tvoid releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease);\n+public class PrometheusPushGatewayReporterFactory implements MetricReporterFactory {\n+\n+\t@Override\n+\tpublic PrometheusPushGatewayReporter createMetricReporter(Properties properties) {\n+\t\treturn new PrometheusPushGatewayReporter();\n+\t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2ODU1Mw==", "url": "https://github.com/apache/flink/pull/11443#discussion_r397868553", "bodyText": "a bit strange that this all resides in network package\nI would expect it to be somewhere in org.apache.flink.runtime.resourcemanager. partition.", "author": "azagrebin", "createdAt": "2020-03-25T13:50:55Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAyOTg5NQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400029895", "bodyText": "All partition tracking code is currently in this package. I agree that we may want to move all them elsewhere at some point.", "author": "zentol", "createdAt": "2020-03-30T08:56:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg2ODU1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java\ndeleted file mode 100644\nindex 96da998b8b..0000000000\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTracker.java\n+++ /dev/null\n\n@@ -1,67 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-\n-import java.util.Map;\n-import java.util.concurrent.CompletableFuture;\n-\n-/**\n- * Utility for tracking and releasing partitions on the ResourceManager.\n- */\n-public interface ResourceManagerPartitionTracker {\n-\n-\t/**\n-\t * Processes {@link ClusterPartitionReport} of a task executor. Updates the tracking information for the respective\n-\t * task executor. Any partition no longer being hosted on the task executor is considered lost, corrupting the\n-\t * corresponding data set.\n-\t * For any such data set this method issues partition release calls to all task executors that are hosting\n-\t * partitions of this data set.\n-\t *\n-\t * @param taskExecutorId origin of the report\n-\t * @param clusterPartitionReport partition report\n-\t */\n-\tvoid processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport);\n-\n-\t/**\n-\t * Processes the shutdown of task executor. Removes all tracking information for the given executor, determines\n-\t * datasets that may be corrupted by the shutdown (and implied loss of partitions).\n-\t * For any such data set this method issues partition release calls to all task executors that are hosting\n-\t * partitions of this data set, and issues release calls.\n-\t *\n-\t * @param taskExecutorId task executor that shut down\n-\t */\n-\tvoid processTaskExecutorShutdown(ResourceID taskExecutorId);\n-\n-\t/**\n-\t * Issues a release calls to all task executors that are hosting partitions of the given data set.\n-\t *\n-\t * @param dataSetId data set to release\n-\t */\n-\tCompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId);\n-\n-\t/**\n-\t * Returns all data sets for which all partitions are being tracked.\n-\t *\n-\t * @return completely tracked datasets\n-\t */\n-\tMap<IntermediateDataSetID, DataSetMetaInfo> listDataSets();\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyNzUzNA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398027534", "bodyText": "do we want to check whether internalReleasePartitions actually issued any releases?\nto avoid dangling futures in partitionReleaseCompletionFutures if dataSetId is not actually tracked at all", "author": "azagrebin", "createdAt": "2020-03-25T17:13:51Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzMzc5MQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400033791", "bodyText": "I will add a check at the start of the method that the partition is being tracked. if it isn't we will log something and return a completed future.", "author": "zentol", "createdAt": "2020-03-30T09:02:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyNzUzNA=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\ndeleted file mode 100644\nindex 794a393543..0000000000\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\n+++ /dev/null\n\n@@ -1,232 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Default {@link ResourceManagerPartitionTracker} implementation.\n- *\n- * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n- * executor state is the source of truth.\n- */\n-public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n-\n-\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n-\n-\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n-\n-\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n-\n-\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n-\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tPreconditions.checkNotNull(clusterPartitionReport);\n-\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n-\t}\n-\n-\t@Override\n-\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n-\n-\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n-\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n-\t\treturn partitionReleaseCompletionFuture;\n-\t}\n-\n-\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n-\t\t\t? processEmptyReport(taskExecutorId)\n-\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n-\n-\t\tupdateDataSetMetaData(clusterPartitionReport);\n-\n-\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n-\n-\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n-\t}\n-\n-\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n-\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n-\t}\n-\n-\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n-\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n-\t\tif (previouslyHostedDatasets == null) {\n-\t\t\t// default path for task executors that never have any cluster partitions\n-\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n-\t\t} else {\n-\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n-\t\t}\n-\t\treturn previouslyHostedDatasets;\n-\t}\n-\n-\t/**\n-\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n-\t * corrupted due to a loss of partitions.\n-\t *\n-\t * @param taskExecutorId ID of the hosting TaskExecutor\n-\t * @param reportEntries  IDs of data sets for which partitions are hosted\n-\t * @return corrupted data sets\n-\t */\n-\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n-\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n-\t\t\t.stream()\n-\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n-\t\t\t.collect(Collectors.toSet());\n-\n-\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n-\t\t\ttaskExecutorId,\n-\t\t\tcurrentlyHostedDatasets);\n-\n-\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n-\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n-\t\t\t.ofNullable(previouslyHostedDataSets)\n-\t\t\t.orElse(new HashSet<>(0));\n-\n-\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n-\t\treportEntries.forEach(hostedPartition -> {\n-\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n-\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n-\n-\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n-\t\t\tif (noPartitionLost) {\n-\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n-\t\t\t}\n-\t\t});\n-\n-\t\t// now only contains data sets for which a partition is no longer tracked\n-\t\treturn potentiallyCorruptedDataSets;\n-\t}\n-\n-\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n-\t\t// add meta info for new data sets\n-\t\tclusterPartitionReport.getEntries().forEach(entry ->\n-\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n-\t\t\t\tif (dataSetMetaInfo == null) {\n-\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n-\t\t\t\t} else {\n-\t\t\t\t\t// double check that the meta data is consistent\n-\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n-\t\t\t\t\treturn dataSetMetaInfo;\n-\t\t\t\t}\n-\t\t\t}));\n-\t}\n-\n-\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n-\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n-\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n-\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n-\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n-\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n-\t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n-\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n-\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n-\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n-\t\t});\n-\t\treturn releaseCalls;\n-\t}\n-\n-\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\n-\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n-\t\tif (trackedPartitions == null) {\n-\t\t\treturn Collections.emptySet();\n-\t\t} else {\n-\t\t\treturn trackedPartitions.keySet();\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n-\t\treturn dataSetMetaInfo.entrySet().stream()\n-\t\t\t.filter(entry -> {\n-\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n-\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n-\n-\t\t\t\tint numTrackedPartitions = 0;\n-\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n-\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n-\t\t\t\t}\n-\n-\t\t\t\treturn numTrackedPartitions == entry.getValue().getNumTotalPartitions();\n-\t\t\t})\n-\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\t}\n-\n-\tprivate static <K, V> void insert(K key1, V value, Map<K, Set<V>> collection) {\n-\t\tcollection.compute(key1, (key, values) -> {\n-\t\t\tif (values == null) {\n-\t\t\t\tvalues = new HashSet<>();\n-\t\t\t}\n-\t\t\tvalues.add(value);\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-\n-\tprivate static <K1, K2, V> void removeInnerKey(K1 key1, K2 value, Map<K1, Map<K2, V>> collection) {\n-\t\tcollection.computeIfPresent(key1, (key, values) -> {\n-\t\t\tvalues.remove(value);\n-\t\t\tif (values.isEmpty()) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyODEyNg==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398028126", "bodyText": "do we also want to GC the completed future from partitionReleaseCompletionFutures?", "author": "azagrebin", "createdAt": "2020-03-25T17:14:37Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNDEwMA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400034100", "bodyText": "yes, will change it to remove() the future instead of get().", "author": "zentol", "createdAt": "2020-03-30T09:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODAyODEyNg=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\ndeleted file mode 100644\nindex 794a393543..0000000000\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\n+++ /dev/null\n\n@@ -1,232 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Default {@link ResourceManagerPartitionTracker} implementation.\n- *\n- * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n- * executor state is the source of truth.\n- */\n-public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n-\n-\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n-\n-\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n-\n-\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n-\n-\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n-\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tPreconditions.checkNotNull(clusterPartitionReport);\n-\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n-\t}\n-\n-\t@Override\n-\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n-\n-\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n-\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n-\t\treturn partitionReleaseCompletionFuture;\n-\t}\n-\n-\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n-\t\t\t? processEmptyReport(taskExecutorId)\n-\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n-\n-\t\tupdateDataSetMetaData(clusterPartitionReport);\n-\n-\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n-\n-\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n-\t}\n-\n-\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n-\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n-\t}\n-\n-\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n-\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n-\t\tif (previouslyHostedDatasets == null) {\n-\t\t\t// default path for task executors that never have any cluster partitions\n-\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n-\t\t} else {\n-\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n-\t\t}\n-\t\treturn previouslyHostedDatasets;\n-\t}\n-\n-\t/**\n-\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n-\t * corrupted due to a loss of partitions.\n-\t *\n-\t * @param taskExecutorId ID of the hosting TaskExecutor\n-\t * @param reportEntries  IDs of data sets for which partitions are hosted\n-\t * @return corrupted data sets\n-\t */\n-\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n-\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n-\t\t\t.stream()\n-\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n-\t\t\t.collect(Collectors.toSet());\n-\n-\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n-\t\t\ttaskExecutorId,\n-\t\t\tcurrentlyHostedDatasets);\n-\n-\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n-\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n-\t\t\t.ofNullable(previouslyHostedDataSets)\n-\t\t\t.orElse(new HashSet<>(0));\n-\n-\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n-\t\treportEntries.forEach(hostedPartition -> {\n-\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n-\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n-\n-\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n-\t\t\tif (noPartitionLost) {\n-\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n-\t\t\t}\n-\t\t});\n-\n-\t\t// now only contains data sets for which a partition is no longer tracked\n-\t\treturn potentiallyCorruptedDataSets;\n-\t}\n-\n-\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n-\t\t// add meta info for new data sets\n-\t\tclusterPartitionReport.getEntries().forEach(entry ->\n-\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n-\t\t\t\tif (dataSetMetaInfo == null) {\n-\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n-\t\t\t\t} else {\n-\t\t\t\t\t// double check that the meta data is consistent\n-\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n-\t\t\t\t\treturn dataSetMetaInfo;\n-\t\t\t\t}\n-\t\t\t}));\n-\t}\n-\n-\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n-\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n-\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n-\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n-\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n-\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n-\t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n-\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n-\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n-\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n-\t\t});\n-\t\treturn releaseCalls;\n-\t}\n-\n-\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\n-\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n-\t\tif (trackedPartitions == null) {\n-\t\t\treturn Collections.emptySet();\n-\t\t} else {\n-\t\t\treturn trackedPartitions.keySet();\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n-\t\treturn dataSetMetaInfo.entrySet().stream()\n-\t\t\t.filter(entry -> {\n-\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n-\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n-\n-\t\t\t\tint numTrackedPartitions = 0;\n-\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n-\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n-\t\t\t\t}\n-\n-\t\t\t\treturn numTrackedPartitions == entry.getValue().getNumTotalPartitions();\n-\t\t\t})\n-\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\t}\n-\n-\tprivate static <K, V> void insert(K key1, V value, Map<K, Set<V>> collection) {\n-\t\tcollection.compute(key1, (key, values) -> {\n-\t\t\tif (values == null) {\n-\t\t\t\tvalues = new HashSet<>();\n-\t\t\t}\n-\t\t\tvalues.add(value);\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-\n-\tprivate static <K1, K2, V> void removeInnerKey(K1 key1, K2 value, Map<K1, Map<K2, V>> collection) {\n-\t\tcollection.computeIfPresent(key1, (key, values) -> {\n-\t\t\tvalues.remove(value);\n-\t\t\tif (values.isEmpty()) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA2MzcxOQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398063719", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\tint numTrackedPartitions = 0;\n          \n          \n            \n            \t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n          \n          \n            \n            \t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n          \n          \n            \n            \t\t\t\t}\n          \n          \n            \n            \t\t\t\tfinal int numTrackedPartitions = taskExecutorToPartitions.values().stream().mapToInt(Set::size).sum();\n          \n      \n    \n    \n  \n\nnit idea, as you seem to like streams :)", "author": "azagrebin", "createdAt": "2020-03-25T18:05:42Z", "path": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Default {@link ResourceManagerPartitionTracker} implementation.\n+ *\n+ * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n+ * executor state is the source of truth.\n+ */\n+public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n+\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n+\n+\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n+\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n+\n+\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n+\n+\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n+\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tPreconditions.checkNotNull(clusterPartitionReport);\n+\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n+\t}\n+\n+\t@Override\n+\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n+\t\tPreconditions.checkNotNull(taskExecutorId);\n+\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n+\n+\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n+\t}\n+\n+\t@Override\n+\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n+\n+\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n+\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n+\t\treturn partitionReleaseCompletionFuture;\n+\t}\n+\n+\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n+\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n+\t\t\t? processEmptyReport(taskExecutorId)\n+\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n+\n+\t\tupdateDataSetMetaData(clusterPartitionReport);\n+\n+\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n+\n+\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n+\t}\n+\n+\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n+\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n+\t}\n+\n+\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n+\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n+\t\tif (previouslyHostedDatasets == null) {\n+\t\t\t// default path for task executors that never have any cluster partitions\n+\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n+\t\t} else {\n+\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n+\t\t}\n+\t\treturn previouslyHostedDatasets;\n+\t}\n+\n+\t/**\n+\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n+\t * corrupted due to a loss of partitions.\n+\t *\n+\t * @param taskExecutorId ID of the hosting TaskExecutor\n+\t * @param reportEntries  IDs of data sets for which partitions are hosted\n+\t * @return corrupted data sets\n+\t */\n+\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n+\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n+\t\t\t.stream()\n+\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n+\t\t\t.collect(Collectors.toSet());\n+\n+\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n+\t\t\ttaskExecutorId,\n+\t\t\tcurrentlyHostedDatasets);\n+\n+\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n+\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n+\t\t\t.ofNullable(previouslyHostedDataSets)\n+\t\t\t.orElse(new HashSet<>(0));\n+\n+\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n+\t\treportEntries.forEach(hostedPartition -> {\n+\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n+\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n+\n+\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n+\t\t\tif (noPartitionLost) {\n+\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n+\t\t\t}\n+\t\t});\n+\n+\t\t// now only contains data sets for which a partition is no longer tracked\n+\t\treturn potentiallyCorruptedDataSets;\n+\t}\n+\n+\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n+\t\t// add meta info for new data sets\n+\t\tclusterPartitionReport.getEntries().forEach(entry ->\n+\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n+\t\t\t\tif (dataSetMetaInfo == null) {\n+\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n+\t\t\t\t} else {\n+\t\t\t\t\t// double check that the meta data is consistent\n+\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n+\t\t\t\t\treturn dataSetMetaInfo;\n+\t\t\t\t}\n+\t\t\t}));\n+\t}\n+\n+\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n+\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n+\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n+\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n+\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n+\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n+\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n+\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n+\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n+\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n+\t\t});\n+\t\treturn releaseCalls;\n+\t}\n+\n+\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n+\t\tPreconditions.checkNotNull(dataSetId);\n+\n+\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n+\t\tif (trackedPartitions == null) {\n+\t\t\treturn Collections.emptySet();\n+\t\t} else {\n+\t\t\treturn trackedPartitions.keySet();\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n+\t\treturn dataSetMetaInfo.entrySet().stream()\n+\t\t\t.filter(entry -> {\n+\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n+\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n+\n+\t\t\t\tint numTrackedPartitions = 0;\n+\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n+\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n+\t\t\t\t}", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNDY1OQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400034659", "bodyText": "I had the streams solution before but it was such an eye-sore that i opted for the simple loop approach instead.", "author": "zentol", "createdAt": "2020-03-30T09:04:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA2MzcxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\ndeleted file mode 100644\nindex 794a393543..0000000000\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImpl.java\n+++ /dev/null\n\n@@ -1,232 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Default {@link ResourceManagerPartitionTracker} implementation.\n- *\n- * <p>Internal tracking info must only be updated upon reception of a {@link ClusterPartitionReport}, as the task\n- * executor state is the source of truth.\n- */\n-public class ResourceManagerPartitionTrackerImpl implements ResourceManagerPartitionTracker {\n-\n-\tprivate static final Logger LOG = LoggerFactory.getLogger(ResourceManagerPartitionTrackerImpl.class);\n-\n-\tprivate final Map<ResourceID, Set<IntermediateDataSetID>> taskExecutorToDataSets = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, Map<ResourceID, Set<ResultPartitionID>>> dataSetToTaskExecutors = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, DataSetMetaInfo> dataSetMetaInfo = new HashMap<>();\n-\tprivate final Map<IntermediateDataSetID, CompletableFuture<Void>> partitionReleaseCompletionFutures = new HashMap<>();\n-\n-\tprivate final ClusterPartitionReleaser clusterPartitionReleaser;\n-\n-\tpublic ResourceManagerPartitionTrackerImpl(ClusterPartitionReleaser clusterPartitionReleaser) {\n-\t\tthis.clusterPartitionReleaser = clusterPartitionReleaser;\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tPreconditions.checkNotNull(clusterPartitionReport);\n-\t\tLOG.debug(\"Processing cluster partition report from task executor {}: {}.\", taskExecutorId, clusterPartitionReport);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, clusterPartitionReport);\n-\t}\n-\n-\t@Override\n-\tpublic void processTaskExecutorShutdown(ResourceID taskExecutorId) {\n-\t\tPreconditions.checkNotNull(taskExecutorId);\n-\t\tLOG.debug(\"Processing shutdown of task executor {}.\", taskExecutorId);\n-\n-\t\tinternalProcessClusterPartitionReport(taskExecutorId, new ClusterPartitionReport(Collections.emptyList()));\n-\t}\n-\n-\t@Override\n-\tpublic CompletableFuture<Void> releaseClusterPartitions(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\t\tLOG.debug(\"Releasing cluster partitions for data set {}.\", dataSetId);\n-\n-\t\tCompletableFuture<Void> partitionReleaseCompletionFuture = partitionReleaseCompletionFutures.computeIfAbsent(dataSetId, ignored -> new CompletableFuture<>());\n-\t\tinternalReleasePartitions(Collections.singleton(dataSetId));\n-\t\treturn partitionReleaseCompletionFuture;\n-\t}\n-\n-\tprivate void internalProcessClusterPartitionReport(ResourceID taskExecutorId, ClusterPartitionReport clusterPartitionReport) {\n-\t\tfinal Set<IntermediateDataSetID> dataSetsWithLostPartitions = clusterPartitionReport.getEntries().isEmpty()\n-\t\t\t? processEmptyReport(taskExecutorId)\n-\t\t\t: setHostedDataSetsAndCheckCorruption(taskExecutorId, clusterPartitionReport.getEntries());\n-\n-\t\tupdateDataSetMetaData(clusterPartitionReport);\n-\n-\t\tcheckForFullyLostDatasets(dataSetsWithLostPartitions);\n-\n-\t\tinternalReleasePartitions(dataSetsWithLostPartitions);\n-\t}\n-\n-\tprivate void internalReleasePartitions(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tMap<ResourceID, Set<IntermediateDataSetID>> releaseCalls = prepareReleaseCalls(dataSetsToRelease);\n-\t\treleaseCalls.forEach(clusterPartitionReleaser::releaseClusterPartitions);\n-\t}\n-\n-\tprivate Set<IntermediateDataSetID> processEmptyReport(ResourceID taskExecutorId) {\n-\t\tSet<IntermediateDataSetID> previouslyHostedDatasets = taskExecutorToDataSets.remove(taskExecutorId);\n-\t\tif (previouslyHostedDatasets == null) {\n-\t\t\t// default path for task executors that never have any cluster partitions\n-\t\t\tpreviouslyHostedDatasets = Collections.emptySet();\n-\t\t} else {\n-\t\t\tpreviouslyHostedDatasets.forEach(dataSetId -> removeInnerKey(dataSetId, taskExecutorId, dataSetToTaskExecutors));\n-\t\t}\n-\t\treturn previouslyHostedDatasets;\n-\t}\n-\n-\t/**\n-\t * Updates the data sets for which the given task executor is hosting partitions and returns data sets that were\n-\t * corrupted due to a loss of partitions.\n-\t *\n-\t * @param taskExecutorId ID of the hosting TaskExecutor\n-\t * @param reportEntries  IDs of data sets for which partitions are hosted\n-\t * @return corrupted data sets\n-\t */\n-\tprivate Set<IntermediateDataSetID> setHostedDataSetsAndCheckCorruption(ResourceID taskExecutorId, Collection<ClusterPartitionReport.ClusterPartitionReportEntry> reportEntries) {\n-\t\tfinal Set<IntermediateDataSetID> currentlyHostedDatasets = reportEntries\n-\t\t\t.stream()\n-\t\t\t.map(ClusterPartitionReport.ClusterPartitionReportEntry::getDataSetId)\n-\t\t\t.collect(Collectors.toSet());\n-\n-\t\tfinal Set<IntermediateDataSetID> previouslyHostedDataSets = taskExecutorToDataSets.put(\n-\t\t\ttaskExecutorId,\n-\t\t\tcurrentlyHostedDatasets);\n-\n-\t\t// previously tracked data sets may be corrupted since we may be tracking less partitions than before\n-\t\tfinal Set<IntermediateDataSetID> potentiallyCorruptedDataSets = Optional\n-\t\t\t.ofNullable(previouslyHostedDataSets)\n-\t\t\t.orElse(new HashSet<>(0));\n-\n-\t\t// update data set -> task executor mapping and find datasets for which lost a partition\n-\t\treportEntries.forEach(hostedPartition -> {\n-\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorHosts = dataSetToTaskExecutors.computeIfAbsent(hostedPartition.getDataSetId(), ignored -> new HashMap<>());\n-\t\t\tfinal Set<ResultPartitionID> previouslyHostedPartitions = taskExecutorHosts.put(taskExecutorId, hostedPartition.getHostedPartitions());\n-\n-\t\t\tfinal boolean noPartitionLost = previouslyHostedPartitions == null || hostedPartition.getHostedPartitions().containsAll(previouslyHostedPartitions);\n-\t\t\tif (noPartitionLost) {\n-\t\t\t\tpotentiallyCorruptedDataSets.remove(hostedPartition.getDataSetId());\n-\t\t\t}\n-\t\t});\n-\n-\t\t// now only contains data sets for which a partition is no longer tracked\n-\t\treturn potentiallyCorruptedDataSets;\n-\t}\n-\n-\tprivate void updateDataSetMetaData(ClusterPartitionReport clusterPartitionReport) {\n-\t\t// add meta info for new data sets\n-\t\tclusterPartitionReport.getEntries().forEach(entry ->\n-\t\t\tdataSetMetaInfo.compute(entry.getDataSetId(), (dataSetID, dataSetMetaInfo) -> {\n-\t\t\t\tif (dataSetMetaInfo == null) {\n-\t\t\t\t\treturn new DataSetMetaInfo(entry.getNumTotalPartitions());\n-\t\t\t\t} else {\n-\t\t\t\t\t// double check that the meta data is consistent\n-\t\t\t\t\tPreconditions.checkState(dataSetMetaInfo.getNumTotalPartitions() == entry.getNumTotalPartitions());\n-\t\t\t\t\treturn dataSetMetaInfo;\n-\t\t\t\t}\n-\t\t\t}));\n-\t}\n-\n-\tprivate void checkForFullyLostDatasets(Set<IntermediateDataSetID> dataSetsWithLostPartitions) {\n-\t\tdataSetsWithLostPartitions.forEach(dataSetId -> {\n-\t\t\tif (getHostingTaskExecutors(dataSetId).isEmpty()) {\n-\t\t\t\tLOG.debug(\"There are no longer partitions being tracked for dataset {}.\", dataSetId);\n-\t\t\t\tdataSetMetaInfo.remove(dataSetId);\n-\t\t\t\tOptional.ofNullable(partitionReleaseCompletionFutures.get(dataSetId)).map(future -> future.complete(null));\n-\t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate Map<ResourceID, Set<IntermediateDataSetID>> prepareReleaseCalls(Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\tfinal Map<ResourceID, Set<IntermediateDataSetID>> releaseCalls = new HashMap<>();\n-\t\tdataSetsToRelease.forEach(dataSetToRelease -> {\n-\t\t\tfinal Set<ResourceID> hostingTaskExecutors = getHostingTaskExecutors(dataSetToRelease);\n-\t\t\thostingTaskExecutors.forEach(hostingTaskExecutor -> insert(hostingTaskExecutor, dataSetToRelease, releaseCalls));\n-\t\t});\n-\t\treturn releaseCalls;\n-\t}\n-\n-\tprivate Set<ResourceID> getHostingTaskExecutors(IntermediateDataSetID dataSetId) {\n-\t\tPreconditions.checkNotNull(dataSetId);\n-\n-\t\tMap<ResourceID, Set<ResultPartitionID>> trackedPartitions = dataSetToTaskExecutors.get(dataSetId);\n-\t\tif (trackedPartitions == null) {\n-\t\t\treturn Collections.emptySet();\n-\t\t} else {\n-\t\t\treturn trackedPartitions.keySet();\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic Map<IntermediateDataSetID, DataSetMetaInfo> listDataSets() {\n-\t\treturn dataSetMetaInfo.entrySet().stream()\n-\t\t\t.filter(entry -> {\n-\t\t\t\tfinal Map<ResourceID, Set<ResultPartitionID>> taskExecutorToPartitions = dataSetToTaskExecutors.get(entry.getKey());\n-\t\t\t\tPreconditions.checkState(taskExecutorToPartitions != null, \"Have metadata entry for dataset %s, but no partition is tracked.\", entry.getKey());\n-\n-\t\t\t\tint numTrackedPartitions = 0;\n-\t\t\t\tfor (Set<ResultPartitionID> hostedPartitions : taskExecutorToPartitions.values()) {\n-\t\t\t\t\tnumTrackedPartitions += hostedPartitions.size();\n-\t\t\t\t}\n-\n-\t\t\t\treturn numTrackedPartitions == entry.getValue().getNumTotalPartitions();\n-\t\t\t})\n-\t\t\t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\t}\n-\n-\tprivate static <K, V> void insert(K key1, V value, Map<K, Set<V>> collection) {\n-\t\tcollection.compute(key1, (key, values) -> {\n-\t\t\tif (values == null) {\n-\t\t\t\tvalues = new HashSet<>();\n-\t\t\t}\n-\t\t\tvalues.add(value);\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-\n-\tprivate static <K1, K2, V> void removeInnerKey(K1 key1, K2 value, Map<K1, Map<K2, V>> collection) {\n-\t\tcollection.computeIfPresent(key1, (key, values) -> {\n-\t\t\tvalues.remove(value);\n-\t\t\tif (values.isEmpty()) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\t\t\treturn values;\n-\t\t});\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM2MjYwNA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398362604", "bodyText": "nit: can be private", "author": "azagrebin", "createdAt": "2020-03-26T07:27:47Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\t// dataset is considered complete since all partitions are being tracked\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\t// dataset is no longer considered complete since partition 2 was lost\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testReleasePartition() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n+\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\n+\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n+\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n+\t}\n+\n+\t@Test\n+\tpublic void testShutdownProcessing() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n+\n+\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n+\t\treturn new ClusterPartitionReport(Collections.singletonList(\n+\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n+\t\t\t\tdataSetId,\n+\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n+\t\t\t\tnumTotalPartitions)));\n+\t}\n+\n+\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n+\n+\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDAzNTU1OQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400035559", "bodyText": "it is a field that is purposefully accessed from the outside and hence shouldn't be private for clarity reasons.", "author": "zentol", "createdAt": "2020-03-30T09:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM2MjYwNA=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\ndeleted file mode 100644\nindex 043d200aa5..0000000000\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\n+++ /dev/null\n\n@@ -1,241 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.Test;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.contains;\n-import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.hasKey;\n-import static org.hamcrest.collection.IsEmptyCollection.empty;\n-import static org.junit.Assert.assertEquals;\n-\n-/**\n- * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n- */\n-public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n-\n-\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n-\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n-\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n-\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n-\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n-\n-\t@Test\n-\tpublic void testProcessEmptyClusterPartitionReport() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n-\t * its partitions is lost.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n-\t * data set is lost on another task executor.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsBasics() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n-\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n-\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// dataset is considered complete since all partitions are being tracked\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\t// dataset is no longer considered complete since partition 2 was lost\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testReleasePartition() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\n-\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n-\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n-\t}\n-\n-\t@Test\n-\tpublic void testShutdownProcessing() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n-\t\treturn new ClusterPartitionReport(Collections.singletonList(\n-\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n-\t\t\t\tdataSetId,\n-\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n-\t\t\t\tnumTotalPartitions)));\n-\t}\n-\n-\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n-\n-\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();\n-\n-\t\t@Override\n-\t\tpublic void releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\t\treleaseCalls.add(Tuple2.of(taskExecutorId, dataSetsToRelease));\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM3MTg5OA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398371898", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tassertEquals(0, tracker.listDataSets().size());\n          \n          \n            \n            \t\tassertThat(tracker.listDataSets().size(), is(0));\n          \n      \n    \n    \n  \n\nminor: looks we usually do it like this for readability", "author": "azagrebin", "createdAt": "2020-03-26T07:49:13Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\ndeleted file mode 100644\nindex 043d200aa5..0000000000\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\n+++ /dev/null\n\n@@ -1,241 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.Test;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.contains;\n-import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.hasKey;\n-import static org.hamcrest.collection.IsEmptyCollection.empty;\n-import static org.junit.Assert.assertEquals;\n-\n-/**\n- * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n- */\n-public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n-\n-\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n-\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n-\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n-\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n-\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n-\n-\t@Test\n-\tpublic void testProcessEmptyClusterPartitionReport() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n-\t * its partitions is lost.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n-\t * data set is lost on another task executor.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsBasics() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n-\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n-\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// dataset is considered complete since all partitions are being tracked\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\t// dataset is no longer considered complete since partition 2 was lost\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testReleasePartition() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\n-\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n-\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n-\t}\n-\n-\t@Test\n-\tpublic void testShutdownProcessing() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n-\t\treturn new ClusterPartitionReport(Collections.singletonList(\n-\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n-\t\t\t\tdataSetId,\n-\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n-\t\t\t\tnumTotalPartitions)));\n-\t}\n-\n-\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n-\n-\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();\n-\n-\t\t@Override\n-\t\tpublic void releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\t\treleaseCalls.add(Tuple2.of(taskExecutorId, dataSetsToRelease));\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MTU1Mw==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398381553", "bodyText": "For this particular component, I would also suggest to test that the freeing of internal maps is checked because their sizes are somewhat side effect of the component in the system. They could be injected in constructor for testing or checked over some other test API. As mentioned in another comment, not sure that e.g. partitionReleaseCompletionFutures is freed properly and we do not accumulate data there over time.\nI was also thinking to use listDataSets for this check but it seems to be tricky.", "author": "azagrebin", "createdAt": "2020-03-26T08:09:40Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA0NTExNA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400045114", "bodyText": "I will add a VisibleForTesting method for checking whether all maps are empty.", "author": "zentol", "createdAt": "2020-03-30T09:21:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MTU1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\ndeleted file mode 100644\nindex 043d200aa5..0000000000\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\n+++ /dev/null\n\n@@ -1,241 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.Test;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.contains;\n-import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.hasKey;\n-import static org.hamcrest.collection.IsEmptyCollection.empty;\n-import static org.junit.Assert.assertEquals;\n-\n-/**\n- * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n- */\n-public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n-\n-\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n-\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n-\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n-\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n-\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n-\n-\t@Test\n-\tpublic void testProcessEmptyClusterPartitionReport() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n-\t * its partitions is lost.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n-\t * data set is lost on another task executor.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsBasics() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n-\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n-\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// dataset is considered complete since all partitions are being tracked\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\t// dataset is no longer considered complete since partition 2 was lost\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testReleasePartition() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\n-\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n-\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n-\t}\n-\n-\t@Test\n-\tpublic void testShutdownProcessing() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n-\t\treturn new ClusterPartitionReport(Collections.singletonList(\n-\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n-\t\t\t\tdataSetId,\n-\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n-\t\t\t\tnumTotalPartitions)));\n-\t}\n-\n-\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n-\n-\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();\n-\n-\t\t@Override\n-\t\tpublic void releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\t\treleaseCalls.add(Tuple2.of(taskExecutorId, dataSetsToRelease));\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398383525", "bodyText": "minor: this test tests 3 transitions:\nincomplete -> incomplete\nincomplete -> complete\ncomplete -> incomplete\nI would prefer 3 test cases\nsome other tests also test multiple things", "author": "azagrebin", "createdAt": "2020-03-26T08:13:42Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.hasKey;\n+import static org.hamcrest.collection.IsEmptyCollection.empty;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n+ */\n+public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n+\n+\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n+\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n+\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n+\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n+\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n+\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n+\n+\t@Test\n+\tpublic void testProcessEmptyClusterPartitionReport() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\t\tassertThat(partitionReleaser.releaseCalls, empty());\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n+\t * its partitions is lost.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t/**\n+\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n+\t * data set is lost on another task executor.\n+\t */\n+\t@Test\n+\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_2,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n+\n+\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsBasics() {\n+\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n+\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n+\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n+\n+\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n+\t\tassertThat(listing, hasKey(DATA_SET_ID));\n+\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n+\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n+\n+\t\ttracker.processTaskExecutorClusterPartitionReport(\n+\t\t\tTASK_EXECUTOR_ID_1,\n+\t\t\tEMPTY_PARTITION_REPORT);\n+\t\tassertEquals(0, tracker.listDataSets().size());\n+\t}\n+\n+\t@Test\n+\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4OTg2OA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398389868", "bodyText": "Just an idea, if we init partitionReleaser/tracker in test setup and factor out\ntracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, createClusterPartitionReport(..\ninto e.g. report(TASK_EXECUTOR_ID_1, DATA_SET_ID, 2, PARTITION_ID_1, ..)\nthis would reduce code size and simplify writing tests with duplicated setup but testing one thing.", "author": "azagrebin", "createdAt": "2020-03-26T08:25:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDcyNzgyNg==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400727826", "bodyText": "I've addressed your second comment (I think).\nAs for the other one, your state transitions aren't quite correct, or rather overly simplified.\nIt goes from unknown -> partially-complete -> complete -> partially-complete.\nTrying to split these up just means we duplicate code we already have. You can't go partially-complete -> complete without first having to go unknown -> partially-complete, so if you were to write separate tests you're running the same code unknown -> partially-complete twice.\nAnd this is true for all state transitions.\nAssertions are usable enough for us to differentiate between failures at different stages, so I don't buy in to the whole \"test exactly one thing\" mentality.", "author": "zentol", "createdAt": "2020-03-31T08:21:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTQyNDAyMQ==", "url": "https://github.com/apache/flink/pull/11443#discussion_r401424021", "bodyText": "Indeed, there is going to be some code duplication. The idea was to reduce its size with the report method (which happened) in favour of isolated checks at the end. I am ok to keep it like this.", "author": "azagrebin", "createdAt": "2020-04-01T07:57:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM4MzUyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\ndeleted file mode 100644\nindex 043d200aa5..0000000000\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResourceManagerPartitionTrackerImplTest.java\n+++ /dev/null\n\n@@ -1,241 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.io.network.partition;\n-\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.Test;\n-\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.contains;\n-import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.hasKey;\n-import static org.hamcrest.collection.IsEmptyCollection.empty;\n-import static org.junit.Assert.assertEquals;\n-\n-/**\n- * Test for the {@link ResourceManagerPartitionTrackerImpl}.\n- */\n-public class ResourceManagerPartitionTrackerImplTest extends TestLogger {\n-\n-\tprivate static final ClusterPartitionReport EMPTY_PARTITION_REPORT = new ClusterPartitionReport(Collections.emptySet());\n-\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_1 = ResourceID.generate();\n-\tprivate static final ResourceID TASK_EXECUTOR_ID_2 = ResourceID.generate();\n-\tprivate static final IntermediateDataSetID DATA_SET_ID = new IntermediateDataSetID();\n-\tprivate static final ResultPartitionID PARTITION_ID_1 = new ResultPartitionID();\n-\tprivate static final ResultPartitionID PARTITION_ID_2 = new ResultPartitionID();\n-\n-\t@Test\n-\tpublic void testProcessEmptyClusterPartitionReport() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting multiple partitions of a data set receives a release call if a subset of\n-\t * its partitions is lost.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnSameTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t/**\n-\t * Verifies that a task executor hosting partitions of a data set receives a release call if a partition of the\n-\t * data set is lost on another task executor.\n-\t */\n-\t@Test\n-\tpublic void testReportProcessingWithPartitionLossOnOtherTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(TASK_EXECUTOR_ID_1, EMPTY_PARTITION_REPORT);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsBasics() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 1, PARTITION_ID_1));\n-\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\t\tDataSetMetaInfo metaInfo = listing.get(DATA_SET_ID);\n-\t\tassertEquals(1, metaInfo.getNumTotalPartitions());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsOnSingleTaskExecutor() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\t// data set consists of 2 partitions but only 1 is being tracked -> incomplete and should not be listed (yet)\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// start tracking another partitions, but we lost partition 1 so the data set is still incomplete\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\n-\t\t// dataset is considered complete since all partitions are being tracked\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\t// dataset is no longer considered complete since partition 2 was lost\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testListDataSetsMultiplePartitionsAcrossTaskExecutors() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\t\tfinal Map<IntermediateDataSetID, DataSetMetaInfo> listing = tracker.listDataSets();\n-\t\tassertThat(listing, hasKey(DATA_SET_ID));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tEMPTY_PARTITION_REPORT);\n-\t\tassertEquals(0, tracker.listDataSets().size());\n-\t}\n-\n-\t@Test\n-\tpublic void testReleasePartition() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_1));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 2, PARTITION_ID_2));\n-\n-\t\ttracker.releaseClusterPartitions(DATA_SET_ID);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, containsInAnyOrder(\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_1, Collections.singleton(DATA_SET_ID)),\n-\t\t\tTuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\n-\t\t// the data set should still be tracked, since the partition release was not confirmed yet by the task executors\n-\t\tassertThat(tracker.listDataSets().keySet(), contains(DATA_SET_ID));\n-\t}\n-\n-\t@Test\n-\tpublic void testShutdownProcessing() {\n-\t\tTestClusterPartitionReleaser partitionReleaser = new TestClusterPartitionReleaser();\n-\t\tfinal ResourceManagerPartitionTracker tracker = new ResourceManagerPartitionTrackerImpl(partitionReleaser);\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\t\tassertThat(partitionReleaser.releaseCalls, empty());\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_1,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, PARTITION_ID_1, PARTITION_ID_2));\n-\n-\t\ttracker.processTaskExecutorClusterPartitionReport(\n-\t\t\tTASK_EXECUTOR_ID_2,\n-\t\t\tcreateClusterPartitionReport(DATA_SET_ID, 3, new ResultPartitionID()));\n-\n-\t\ttracker.processTaskExecutorShutdown(TASK_EXECUTOR_ID_1);\n-\n-\t\tassertThat(partitionReleaser.releaseCalls, contains(Tuple2.of(TASK_EXECUTOR_ID_2, Collections.singleton(DATA_SET_ID))));\n-\t}\n-\n-\tprivate static ClusterPartitionReport createClusterPartitionReport(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionId) {\n-\t\treturn new ClusterPartitionReport(Collections.singletonList(\n-\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(\n-\t\t\t\tdataSetId,\n-\t\t\t\tnew HashSet<>(Arrays.asList(partitionId)),\n-\t\t\t\tnumTotalPartitions)));\n-\t}\n-\n-\tprivate static class TestClusterPartitionReleaser implements ClusterPartitionReleaser {\n-\n-\t\tfinal List<Tuple2<ResourceID, Set<IntermediateDataSetID>>> releaseCalls = new ArrayList<>();\n-\n-\t\t@Override\n-\t\tpublic void releaseClusterPartitions(ResourceID taskExecutorId, Set<IntermediateDataSetID> dataSetsToRelease) {\n-\t\t\treleaseCalls.add(Tuple2.of(taskExecutorId, dataSetsToRelease));\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng==", "url": "https://github.com/apache/flink/pull/11443#discussion_r398397956", "bodyText": "Could we reuse some code from ResourceManagerTest?", "author": "azagrebin", "createdAt": "2020-03-26T08:40:08Z", "path": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.resourcemanager;\n+\n+import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n+import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n+import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n+import org.apache.flink.runtime.instance.HardwareDescription;\n+import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.registration.RegistrationResponse;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n+import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n+import org.apache.flink.runtime.rpc.RpcUtils;\n+import org.apache.flink.runtime.rpc.TestingRpcService;\n+import org.apache.flink.runtime.taskexecutor.SlotReport;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n+import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n+import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n+import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n+import org.apache.flink.util.TestLogger;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n+ */\n+public class ResourceManagerPartitionLifecycleTest extends TestLogger {", "originalCommit": "0240c441a2ea285e49af3172c2e5829207be011b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTA3NDQ1Ng==", "url": "https://github.com/apache/flink/pull/11443#discussion_r399074456", "bodyText": "Possibly, most of the code was in fact copied over. I'm not a huge fan of making methods from other tests accessible just because they happen to now match the same use-case.\nWhat we really want are some flexible utility methods/resources, but this would be quite an effor.", "author": "zentol", "createdAt": "2020-03-27T07:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI2MTMwOA==", "url": "https://github.com/apache/flink/pull/11443#discussion_r400261308", "bodyText": "Indeed, we want good utility methods/resources and it is an effort. It looks like we have accumulated some technical debt in tests. At this point, it may be only feasible to approach the ideal state of code stepwise. I leave it up to you then whether we can do here any steps or not, it is indeed time-consuming.", "author": "azagrebin", "createdAt": "2020-03-30T15:00:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM5Nzk1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "chunk": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java\ndeleted file mode 100644\nindex aa20ff4638..0000000000\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/ResourceManagerPartitionLifecycleTest.java\n+++ /dev/null\n\n@@ -1,247 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.runtime.resourcemanager;\n-\n-import org.apache.flink.api.common.time.Time;\n-import org.apache.flink.runtime.clusterframework.types.ResourceID;\n-import org.apache.flink.runtime.clusterframework.types.ResourceProfile;\n-import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n-import org.apache.flink.runtime.highavailability.TestingHighAvailabilityServices;\n-import org.apache.flink.runtime.instance.HardwareDescription;\n-import org.apache.flink.runtime.io.network.partition.ResourceManagerPartitionTrackerImpl;\n-import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n-import org.apache.flink.runtime.leaderelection.TestingLeaderElectionService;\n-import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n-import org.apache.flink.runtime.registration.RegistrationResponse;\n-import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;\n-import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerBuilder;\n-import org.apache.flink.runtime.rpc.RpcUtils;\n-import org.apache.flink.runtime.rpc.TestingRpcService;\n-import org.apache.flink.runtime.taskexecutor.SlotReport;\n-import org.apache.flink.runtime.taskexecutor.TaskExecutorGateway;\n-import org.apache.flink.runtime.taskexecutor.TaskExecutorHeartbeatPayload;\n-import org.apache.flink.runtime.taskexecutor.TestingTaskExecutorGatewayBuilder;\n-import org.apache.flink.runtime.taskexecutor.partition.ClusterPartitionReport;\n-import org.apache.flink.runtime.testingUtils.TestingUtils;\n-import org.apache.flink.runtime.util.TestingFatalErrorHandler;\n-import org.apache.flink.util.TestLogger;\n-\n-import org.junit.After;\n-import org.junit.AfterClass;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n-\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.UUID;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.hamcrest.Matchers.contains;\n-import static org.hamcrest.Matchers.instanceOf;\n-import static org.junit.Assert.assertThat;\n-\n-/**\n- * Tests for the partition-lifecycle logic in the {@link ResourceManager}.\n- */\n-public class ResourceManagerPartitionLifecycleTest extends TestLogger {\n-\n-\tprivate static final Time TIMEOUT = Time.minutes(2L);\n-\n-\tprivate static TestingRpcService rpcService;\n-\n-\tprivate TestingHighAvailabilityServices highAvailabilityServices;\n-\n-\tprivate TestingLeaderElectionService resourceManagerLeaderElectionService;\n-\n-\tprivate TestingFatalErrorHandler testingFatalErrorHandler;\n-\n-\tprivate TestingResourceManager resourceManager;\n-\n-\t@BeforeClass\n-\tpublic static void setupClass() {\n-\t\trpcService = new TestingRpcService();\n-\t}\n-\n-\t@Before\n-\tpublic void setup() throws Exception {\n-\t\thighAvailabilityServices = new TestingHighAvailabilityServices();\n-\t\tresourceManagerLeaderElectionService = new TestingLeaderElectionService();\n-\t\thighAvailabilityServices.setResourceManagerLeaderElectionService(resourceManagerLeaderElectionService);\n-\t\ttestingFatalErrorHandler = new TestingFatalErrorHandler();\n-\t}\n-\n-\t@After\n-\tpublic void after() throws Exception {\n-\t\tif (resourceManager != null) {\n-\t\t\tRpcUtils.terminateRpcEndpoint(resourceManager, TIMEOUT);\n-\t\t}\n-\n-\t\tif (highAvailabilityServices != null) {\n-\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n-\t\t}\n-\n-\t\tif (testingFatalErrorHandler.hasExceptionOccurred()) {\n-\t\t\ttestingFatalErrorHandler.rethrowError();\n-\t\t}\n-\t}\n-\n-\t@AfterClass\n-\tpublic static void tearDownClass() throws Exception {\n-\t\tif (rpcService != null) {\n-\t\t\tRpcUtils.terminateRpcServices(TIMEOUT, rpcService);\n-\t\t}\n-\t}\n-\n-\t@Test\n-\tpublic void testClusterPartitionReportHandling() throws Exception {\n-\t\tfinal CompletableFuture<Collection<IntermediateDataSetID>> clusterPartitionReleaseFuture = new CompletableFuture<>();\n-\t\trunTest(\n-\t\t\tbuilder -> builder.setReleaseClusterPartitionsConsumer(clusterPartitionReleaseFuture::complete),\n-\t\t\t(resourceManagerGateway, taskManagerId1, ignored) -> {\n-\t\t\t\tIntermediateDataSetID dataSetID = new IntermediateDataSetID();\n-\t\t\t\tResultPartitionID resultPartitionID = new ResultPartitionID();\n-\n-\t\t\t\tresourceManagerGateway.heartbeatFromTaskManager(\n-\t\t\t\t\ttaskManagerId1,\n-\t\t\t\t\tcreateTaskExecutorHeartbeatPayload(dataSetID, 2, resultPartitionID, new ResultPartitionID()));\n-\n-\t\t\t\t// send a heartbeat containing 1 partition less -> partition loss -> should result in partition release\n-\t\t\t\tresourceManagerGateway.heartbeatFromTaskManager(\n-\t\t\t\t\ttaskManagerId1,\n-\t\t\t\t\tcreateTaskExecutorHeartbeatPayload(dataSetID, 2, resultPartitionID));\n-\n-\t\t\t\tCollection<IntermediateDataSetID> intermediateDataSetIDS = clusterPartitionReleaseFuture.get(TIMEOUT.toMilliseconds(), TimeUnit.MILLISECONDS);\n-\t\t\t\tassertThat(intermediateDataSetIDS, contains(dataSetID));\n-\t\t\t});\n-\t}\n-\n-\t@Test\n-\tpublic void testTaskExecutorShutdownHandling() throws Exception {\n-\t\tfinal CompletableFuture<Collection<IntermediateDataSetID>> clusterPartitionReleaseFuture = new CompletableFuture<>();\n-\t\trunTest(\n-\t\t\tbuilder -> builder.setReleaseClusterPartitionsConsumer(clusterPartitionReleaseFuture::complete),\n-\t\t\t(resourceManagerGateway, taskManagerId1, taskManagerId2) -> {\n-\t\t\t\tIntermediateDataSetID dataSetID = new IntermediateDataSetID();\n-\n-\t\t\t\tresourceManagerGateway.heartbeatFromTaskManager(\n-\t\t\t\t\ttaskManagerId1,\n-\t\t\t\t\tcreateTaskExecutorHeartbeatPayload(dataSetID, 2, new ResultPartitionID()));\n-\n-\t\t\t\t// we need a partition on another task executor so that there's something to release when one task executor goes down\n-\t\t\t\tresourceManagerGateway.heartbeatFromTaskManager(\n-\t\t\t\t\ttaskManagerId2,\n-\t\t\t\t\tcreateTaskExecutorHeartbeatPayload(dataSetID, 2, new ResultPartitionID()));\n-\n-\t\t\t\tresourceManagerGateway.disconnectTaskManager(taskManagerId2, new RuntimeException(\"test exception\"));\n-\t\t\t\tCollection<IntermediateDataSetID> intermediateDataSetIDS = clusterPartitionReleaseFuture.get(TIMEOUT.toMilliseconds(), TimeUnit.MILLISECONDS);\n-\t\t\t\tassertThat(intermediateDataSetIDS, contains(dataSetID));\n-\t\t\t});\n-\t}\n-\n-\tprivate void runTest(TaskExecutorSetup taskExecutorBuilderSetup, TestAction testAction) throws Exception {\n-\t\tfinal ResourceManagerGateway resourceManagerGateway = createAndStartResourceManager();\n-\n-\t\tTestingTaskExecutorGatewayBuilder testingTaskExecutorGateway1Builder = new TestingTaskExecutorGatewayBuilder();\n-\t\ttaskExecutorBuilderSetup.accept(testingTaskExecutorGateway1Builder);\n-\t\tfinal TaskExecutorGateway taskExecutorGateway1 = testingTaskExecutorGateway1Builder\n-\t\t\t.setAddress(UUID.randomUUID().toString())\n-\t\t\t.createTestingTaskExecutorGateway();\n-\t\trpcService.registerGateway(taskExecutorGateway1.getAddress(), taskExecutorGateway1);\n-\n-\t\tfinal TaskExecutorGateway taskExecutorGateway2 = new TestingTaskExecutorGatewayBuilder()\n-\t\t\t.setAddress(UUID.randomUUID().toString())\n-\t\t\t.createTestingTaskExecutorGateway();\n-\t\trpcService.registerGateway(taskExecutorGateway2.getAddress(), taskExecutorGateway2);\n-\n-\t\tfinal ResourceID taskManagerId1 = ResourceID.generate();\n-\t\tfinal ResourceID taskManagerId2 = ResourceID.generate();\n-\t\tregisterTaskExecutor(resourceManagerGateway, taskManagerId1, taskExecutorGateway1.getAddress());\n-\t\tregisterTaskExecutor(resourceManagerGateway, taskManagerId2, taskExecutorGateway2.getAddress());\n-\n-\t\ttestAction.accept(resourceManagerGateway, taskManagerId1, taskManagerId2);\n-\t}\n-\n-\tprivate void registerTaskExecutor(ResourceManagerGateway resourceManagerGateway, ResourceID taskExecutorId, String taskExecutorAddress) throws Exception {\n-\t\tfinal TaskExecutorRegistration taskExecutorRegistration = new TaskExecutorRegistration(\n-\t\t\ttaskExecutorAddress,\n-\t\t\ttaskExecutorId,\n-\t\t\t1234,\n-\t\t\tnew HardwareDescription(42, 1337L, 1337L, 0L),\n-\t\t\tResourceProfile.ZERO,\n-\t\t\tResourceProfile.ZERO);\n-\t\tfinal CompletableFuture<RegistrationResponse> registrationFuture = resourceManagerGateway.registerTaskExecutor(\n-\t\t\ttaskExecutorRegistration,\n-\t\t\tTestingUtils.TIMEOUT());\n-\n-\t\tassertThat(registrationFuture.get(), instanceOf(RegistrationResponse.Success.class));\n-\t}\n-\n-\tprivate ResourceManagerGateway createAndStartResourceManager() throws Exception {\n-\t\tfinal SlotManager slotManager = SlotManagerBuilder.newBuilder()\n-\t\t\t.setScheduledExecutor(rpcService.getScheduledExecutor())\n-\t\t\t.build();\n-\t\tfinal JobLeaderIdService jobLeaderIdService = new JobLeaderIdService(\n-\t\t\thighAvailabilityServices,\n-\t\t\trpcService.getScheduledExecutor(),\n-\t\t\tTestingUtils.infiniteTime());\n-\n-\t\tfinal TestingResourceManager resourceManager = new TestingResourceManager(\n-\t\t\trpcService,\n-\t\t\tResourceManager.RESOURCE_MANAGER_NAME + UUID.randomUUID(),\n-\t\t\tResourceID.generate(),\n-\t\t\thighAvailabilityServices,\n-\t\t\tnew HeartbeatServices(100000L, 1000000L),\n-\t\t\tslotManager,\n-\t\t\tResourceManagerPartitionTrackerImpl::new,\n-\t\t\tjobLeaderIdService,\n-\t\t\ttestingFatalErrorHandler,\n-\t\t\tUnregisteredMetricGroups.createUnregisteredResourceManagerMetricGroup());\n-\n-\t\tresourceManager.start();\n-\n-\t\t// first make the ResourceManager the leader\n-\t\tresourceManagerLeaderElectionService.isLeader(ResourceManagerId.generate().toUUID()).get();\n-\n-\t\tthis.resourceManager = resourceManager;\n-\n-\t\treturn resourceManager.getSelfGateway(ResourceManagerGateway.class);\n-\t}\n-\n-\tprivate static TaskExecutorHeartbeatPayload createTaskExecutorHeartbeatPayload(IntermediateDataSetID dataSetId, int numTotalPartitions, ResultPartitionID... partitionIds) {\n-\t\treturn new TaskExecutorHeartbeatPayload(\n-\t\t\tnew SlotReport(),\n-\t\t\tnew ClusterPartitionReport(Collections.singletonList(\n-\t\t\t\tnew ClusterPartitionReport.ClusterPartitionReportEntry(dataSetId, new HashSet<>(Arrays.asList(partitionIds)), numTotalPartitions)\n-\t\t\t)));\n-\t}\n-\n-\t@FunctionalInterface\n-\tprivate interface TaskExecutorSetup {\n-\t\tvoid accept(TestingTaskExecutorGatewayBuilder taskExecutorGatewayBuilder) throws Exception;\n-\t}\n-\n-\t@FunctionalInterface\n-\tprivate interface TestAction {\n-\t\tvoid accept(ResourceManagerGateway resourceManagerGateway, ResourceID taskExecutorId1, ResourceID taskExecutorId2) throws Exception;\n-\t}\n-}\n"}}, {"oid": "8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "url": "https://github.com/apache/flink/commit/8c735174c9681bdd42a78b5e1df838d4cdc0a06d", "message": "[hotfix][coordination] Add sanity checks to ClusterPartitionReportEntry", "committedDate": "2020-03-30T08:07:18Z", "type": "commit"}, {"oid": "df635bda88e745a98993533481f9a343603bf919", "url": "https://github.com/apache/flink/commit/df635bda88e745a98993533481f9a343603bf919", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "committedDate": "2020-03-30T08:07:19Z", "type": "forcePushed"}, {"oid": "14b3f8a73814fa648afc42be527a565821a262a8", "url": "https://github.com/apache/flink/commit/14b3f8a73814fa648afc42be527a565821a262a8", "message": "[TMP] add missing timeout", "committedDate": "2020-03-30T11:11:04Z", "type": "forcePushed"}, {"oid": "8a86aa89a221085749362a508718bd4959ac8190", "url": "https://github.com/apache/flink/commit/8a86aa89a221085749362a508718bd4959ac8190", "message": "simplify reporting", "committedDate": "2020-03-30T11:41:17Z", "type": "forcePushed"}, {"oid": "998a1c6d2107addf7bfafd0114b2f696837134aa", "url": "https://github.com/apache/flink/commit/998a1c6d2107addf7bfafd0114b2f696837134aa", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "committedDate": "2020-04-02T10:05:25Z", "type": "forcePushed"}, {"oid": "c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "url": "https://github.com/apache/flink/commit/c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "committedDate": "2020-04-02T15:13:35Z", "type": "commit"}, {"oid": "c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "url": "https://github.com/apache/flink/commit/c04e7d39c3d7b3ba8e6678d8bd193f7d7ce5405f", "message": "[FLINK-14791][coordination] ResourceManager tracks ClusterPartitions", "committedDate": "2020-04-02T15:13:35Z", "type": "forcePushed"}]}