{"pr_number": 13157, "pr_title": "[FLINK-18900][hive] HiveCatalog should error out when listing partitions with an invalid spec", "pr_createdAt": "2020-08-15T15:07:53Z", "pr_url": "https://github.com/apache/flink/pull/13157", "timeline": [{"oid": "b64a33a3e57e4af2522aeaf274c4711ddb2b00ef", "url": "https://github.com/apache/flink/commit/b64a33a3e57e4af2522aeaf274c4711ddb2b00ef", "message": "[FLINK-18900][hive] HiveCatalog should error out when listing partitions with an invalid spec", "committedDate": "2020-08-15T14:51:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI1MzU3OQ==", "url": "https://github.com/apache/flink/pull/13157#discussion_r471253579", "bodyText": "In case of invalid partition spec, we should throw PartitionSpecInvalidException", "author": "lirui-apache", "createdAt": "2020-08-17T06:20:30Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -765,7 +765,7 @@ public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSp\n \n \t@Override\n \tpublic List<CatalogPartitionSpec> listPartitions(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)\n-\t\t\tthrows TableNotExistException, TableNotPartitionedException, CatalogException {\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException, PartitionNotExistException, CatalogException {", "originalCommit": "b64a33a3e57e4af2522aeaf274c4711ddb2b00ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 0f871e37ae1..012ca3a09e0 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -765,7 +765,7 @@ public class HiveCatalog extends AbstractCatalog {\n \n \t@Override\n \tpublic List<CatalogPartitionSpec> listPartitions(ObjectPath tablePath, CatalogPartitionSpec partitionSpec)\n-\t\t\tthrows TableNotExistException, TableNotPartitionedException, PartitionNotExistException, CatalogException {\n+\t\t\tthrows TableNotExistException, TableNotPartitionedException, PartitionSpecInvalidException, CatalogException {\n \t\tcheckNotNull(tablePath, \"Table path cannot be null\");\n \t\tcheckNotNull(partitionSpec, \"CatalogPartitionSpec cannot be null\");\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI1Mzk3Mg==", "url": "https://github.com/apache/flink/pull/13157#discussion_r471253972", "bodyText": "We can simply try calling getOrderedFullPartitionValues, which will check if the spec is valid", "author": "lirui-apache", "createdAt": "2020-08-17T06:21:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -777,8 +777,15 @@ public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSp\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n-\t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n-\t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n+\t\t\tif (partialVals.size() == 1 && StringUtils.isNullOrWhitespaceOnly(partialVals.get(0))) {", "originalCommit": "b64a33a3e57e4af2522aeaf274c4711ddb2b00ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 0f871e37ae1..012ca3a09e0 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -777,15 +777,11 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n-\t\t\tif (partialVals.size() == 1 && StringUtils.isNullOrWhitespaceOnly(partialVals.get(0))) {\n-\t\t\t\t// listing partitions with an invalid spec\n-\t\t\t\tthrow new PartitionNotExistException(getName(), tablePath, partitionSpec);\n-\t\t\t} else {\n-\t\t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n-\t\t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n-\t\t\t}\n-\t\t} catch (PartitionNotExistException e) {\n-\t\t\tthrow new PartitionNotExistException(getName(), tablePath, partitionSpec, e);\n+\t\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n+\t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n+\t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n+\t\t} catch (PartitionSpecInvalidException e) {\n+\t\t\tthrow new PartitionSpecInvalidException(getName(), getFieldNames(hiveTable.getPartitionKeys()), tablePath, partitionSpec, e);\n \t\t} catch (TException e) {\n \t\t\tthrow new CatalogException(\n \t\t\t\tString.format(\"Failed to list partitions of table %s\", tablePath), e);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTI1NTA0NA==", "url": "https://github.com/apache/flink/pull/13157#discussion_r471255044", "bodyText": "Why do we need a different table schema? I think you can just reuse the table in this test", "author": "lirui-apache", "createdAt": "2020-08-17T06:24:52Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java", "diffHunk": "@@ -467,14 +468,36 @@ public void testAddDropPartitions() throws Exception {\n \n \t@Test\n \tpublic void testShowPartitions() throws Exception {\n-\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date, country string)\");\n-\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n+\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (country string)\");", "originalCommit": "b64a33a3e57e4af2522aeaf274c4711ddb2b00ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java\nindex db7eccc089e..29a9fb1b4e1 100644\n--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java\n+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveDialectITCase.java\n\n@@ -468,36 +468,14 @@ public class HiveDialectITCase {\n \n \t@Test\n \tpublic void testShowPartitions() throws Exception {\n-\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (country string)\");\n-\t\ttableEnv.executeSql(\"alter table tbl add partition (country='china') partition (country='us')\");\n+\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date, country string)\");\n+\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n \n \t\tObjectPath tablePath = new ObjectPath(\"default\", \"tbl\");\n \t\tassertEquals(2, hiveCatalog.listPartitions(tablePath).size());\n \n \t\tList<Row> partitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n \t\tassertEquals(2, partitions.size());\n-\t\tassertTrue(partitions.toString().contains(\"country=china\"));\n-\t\tassertTrue(partitions.toString().contains(\"country=us\"));\n-\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (country='china')\").collect());\n-\t\tassertEquals(1, partitions.size());\n-\t\tassertTrue(partitions.toString().contains(\"country=china\"));\n-\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (country='japan')\").collect());\n-\t\tassertEquals(0, partitions.size());\n-\t\ttry {\n-\t\t\tLists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (city='china')\").collect());\n-\t\t} catch (TableException e) {\n-\t\t\tassertEquals(String.format(\"Could not execute SHOW PARTITIONS %s.%s PARTITION (city=china)\", hiveCatalog.getName(), tablePath), e.getMessage());\n-\t\t}\n-\n-\t\ttableEnv.executeSql(\"alter table tbl drop partition (country='china'),partition (country='us')\");\n-\t\tassertEquals(0, hiveCatalog.listPartitions(tablePath).size());\n-\n-\t\ttableEnv.executeSql(\"drop table tbl\");\n-\t\ttableEnv.executeSql(\"create table tbl (x int,y binary) partitioned by (dt date, country string)\");\n-\t\ttableEnv.executeSql(\"alter table tbl add partition (dt='2020-04-30',country='china') partition (dt='2020-04-30',country='us')\");\n-\n-\t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl\").collect());\n-\t\tassertEquals(2, partitions.size());\n \t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=china\"));\n \t\tassertTrue(partitions.toString().contains(\"dt=2020-04-30/country=us\"));\n \t\tpartitions = Lists.newArrayList(tableEnv.executeSql(\"show partitions tbl partition (dt='2020-04-30')\").collect());\n"}}, {"oid": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "url": "https://github.com/apache/flink/commit/f77481fbdf9900bd2f411ef045cd91985ebb1085", "message": "[FLINK-18900][hive] HiveCatalog should error out when listing partitions with an invalid spec", "committedDate": "2020-08-19T08:41:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzkwNTA1NA==", "url": "https://github.com/apache/flink/pull/13157#discussion_r473905054", "bodyText": "Why do we need to catch the exception and throw another one?", "author": "lirui-apache", "createdAt": "2020-08-20T11:38:49Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -777,8 +777,11 @@ public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSp\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n+\t\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n \t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n \t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n+\t\t} catch (PartitionSpecInvalidException e) {", "originalCommit": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bb82e904f49ed492fe36badf6a44bed3267b687c", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 012ca3a09e0..9a4eb3d373f 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -772,16 +772,14 @@ public class HiveCatalog extends AbstractCatalog {\n \t\tTable hiveTable = getHiveTable(tablePath);\n \n \t\tensurePartitionedTable(tablePath, hiveTable);\n+\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n \n \t\ttry {\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n-\t\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n \t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n \t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n-\t\t} catch (PartitionSpecInvalidException e) {\n-\t\t\tthrow new PartitionSpecInvalidException(getName(), getFieldNames(hiveTable.getPartitionKeys()), tablePath, partitionSpec, e);\n \t\t} catch (TException e) {\n \t\t\tthrow new CatalogException(\n \t\t\t\tString.format(\"Failed to list partitions of table %s\", tablePath), e);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzkwNTU2OA==", "url": "https://github.com/apache/flink/pull/13157#discussion_r473905568", "bodyText": "This should be done before we call HiveReflectionUtils.getPvals", "author": "lirui-apache", "createdAt": "2020-08-20T11:39:47Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -777,8 +777,11 @@ public void dropPartition(ObjectPath tablePath, CatalogPartitionSpec partitionSp\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n+\t\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);", "originalCommit": "f77481fbdf9900bd2f411ef045cd91985ebb1085", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bb82e904f49ed492fe36badf6a44bed3267b687c", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 012ca3a09e0..9a4eb3d373f 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -772,16 +772,14 @@ public class HiveCatalog extends AbstractCatalog {\n \t\tTable hiveTable = getHiveTable(tablePath);\n \n \t\tensurePartitionedTable(tablePath, hiveTable);\n+\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n \n \t\ttry {\n \t\t\t// partition spec can be partial\n \t\t\tList<String> partialVals = HiveReflectionUtils.getPvals(hiveShim, hiveTable.getPartitionKeys(),\n \t\t\t\tpartitionSpec.getPartitionSpec());\n-\t\t\tcheckValidPartitionSpec(partitionSpec, getFieldNames(hiveTable.getPartitionKeys()), tablePath);\n \t\t\treturn client.listPartitionNames(tablePath.getDatabaseName(), tablePath.getObjectName(), partialVals,\n \t\t\t\t(short) -1).stream().map(HiveCatalog::createPartitionSpec).collect(Collectors.toList());\n-\t\t} catch (PartitionSpecInvalidException e) {\n-\t\t\tthrow new PartitionSpecInvalidException(getName(), getFieldNames(hiveTable.getPartitionKeys()), tablePath, partitionSpec, e);\n \t\t} catch (TException e) {\n \t\t\tthrow new CatalogException(\n \t\t\t\tString.format(\"Failed to list partitions of table %s\", tablePath), e);\n"}}, {"oid": "bb82e904f49ed492fe36badf6a44bed3267b687c", "url": "https://github.com/apache/flink/commit/bb82e904f49ed492fe36badf6a44bed3267b687c", "message": "[FLINK-18900][hive] HiveCatalog should error out when listing partitions with an invalid spec", "committedDate": "2020-08-21T07:06:42Z", "type": "commit"}]}