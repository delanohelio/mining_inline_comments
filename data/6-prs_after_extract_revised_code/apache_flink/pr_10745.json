{"pr_number": 10745, "pr_title": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Type\u2026", "pr_createdAt": "2020-01-02T09:24:49Z", "pr_url": "https://github.com/apache/flink/pull/10745", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362411634", "bodyText": "This is wrong fix, you should modify returnType.", "author": "JingsongLi", "createdAt": "2020-01-02T09:31:49Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +113,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {\n+\t\treturn schema.toRowDataType();", "originalCommit": "0c00c793d00c60c7dafc5b4f8209459a0dcc940b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQyMTcyOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362421728", "bodyText": "updated", "author": "docete", "createdAt": "2020-01-02T10:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQxMTYzNA=="}], "type": "inlineReview", "revised_code": {"commit": "24f4f9782b3332d62bfb0933b423ae3a5843318b", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\nindex f3f14602cb..1c63249b41 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n\n@@ -115,7 +123,7 @@ public class JDBCTableSource implements\n \n \t@Override\n \tpublic DataType getProducedDataType() {\n-\t\treturn schema.toRowDataType();\n+\t\treturn producedDataType;\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465196", "bodyText": "Please remove the overrided implementation of getReturnType()", "author": "wuchong", "createdAt": "2020-01-02T13:01:52Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -112,6 +120,11 @@ public boolean isBounded() {\n \t\treturn returnType;\n \t}\n \n+\t@Override\n+\tpublic DataType getProducedDataType() {", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM3MQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691371", "bodyText": "Sure", "author": "docete", "createdAt": "2020-01-03T02:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTE5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f0a7af07324182da2121f873e8e857459f425194", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\nindex bbed02dc4f..cdc056fb9f 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n\n@@ -122,7 +116,7 @@ public class JDBCTableSource implements\n \n \t@Override\n \tpublic DataType getProducedDataType() {\n-\t\treturn producedDataType;\n+\t\treturn schema.toRowDataType();\n \t}\n \n \t@Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362465428", "bodyText": "Use returnType.getFieldNames instead reconstruct the selected field names again?", "author": "wuchong", "createdAt": "2020-01-02T13:02:52Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -180,6 +193,20 @@ public boolean equals(Object o) {\n \t\t}\n \t}\n \n+\t@Override\n+\tpublic String explainSource() {\n+\t\tif (selectFields == null) {\n+\t\t\treturn String.format(\n+\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n+\t\t} else {\n+\t\t\tString[] fields = new String[selectFields.length];\n+\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n+\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n+\t\t\t}\n+\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MTM4NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362691385", "bodyText": "Sure", "author": "docete", "createdAt": "2020-01-03T02:17:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjQ2NTQyOA=="}], "type": "inlineReview", "revised_code": {"commit": "f0a7af07324182da2121f873e8e857459f425194", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\nindex bbed02dc4f..cdc056fb9f 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n\n@@ -193,20 +192,6 @@ public class JDBCTableSource implements\n \t\t}\n \t}\n \n-\t@Override\n-\tpublic String explainSource() {\n-\t\tif (selectFields == null) {\n-\t\t\treturn String.format(\n-\t\t\t\t\"JDBCTableSource(read fields: %s)\", String.join(\", \", schema.getFieldNames()));\n-\t\t} else {\n-\t\t\tString[] fields = new String[selectFields.length];\n-\t\t\tfor (int i = 0; i < selectFields.length; i++) {\n-\t\t\t\tfields[i] = schema.getFieldName(selectFields[i]).get();\n-\t\t\t}\n-\t\t\treturn String.format(\"JDBCTableSource(read fields: %s)\", String.join(\", \", fields));\n-\t\t}\n-\t}\n-\n \t/**\n \t * Builder for a {@link JDBCTableSource}.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r362693737", "bodyText": "Could you add full list of types to have a full coverage? For example, add TIMESTAMP, TIMESTAMP(9), DECIMAL(38, 18), DECIMAL, FLOAT (we have a bug for float before), etc...\nI would also suggest to combine source integrate tests and sink integrate tests, e.g.  read from collections and write into jdbc using SQL, and read from JDBC to verify the result.", "author": "wuchong", "createdAt": "2020-01-03T02:40:52Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.java.io.jdbc;\n+\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.java.StreamTableEnvironment;\n+import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\n+/**\n+ * ITCase for {@link JDBCTableSource}.\n+ */\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIME('15:35:00'), 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIME('15:36:01'), 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +", "originalCommit": "2cc4e7b6e88e9f655235b8c97707c41f45b6b238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDU0ODEwMA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r374548100", "bodyText": "Have add more types for derby. and the combining of source and sink integrate tests will postpone to since the sinks did not support new type system.", "author": "docete", "createdAt": "2020-02-04T09:08:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MjY5MzczNw=="}], "type": "inlineReview", "revised_code": {"commit": "f0a7af07324182da2121f873e8e857459f425194", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\nindex 57e43a1258..7b8277ec2a 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n\n@@ -109,38 +109,4 @@ public class JDBCTableSourceITCase extends AbstractTestBase {\n \t\t\t\t\"2,2020-01-01T15:36:01.123456,15:36:01,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n-\n-\t@Test\n-\tpublic void testProjectableJDBCSource() throws Exception {\n-\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n-\t\t\t\t.useBlinkPlanner()\n-\t\t\t\t.inStreamingMode()\n-\t\t\t\t.build();\n-\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n-\n-\t\ttEnv.sqlUpdate(\n-\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n-\t\t\t\t\"id BIGINT,\" +\n-\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n-\t\t\t\t\"time_col TIME,\" +\n-\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n-\t\t\t\t\") WITH (\" +\n-\t\t\t\t\"  'connector.type'='jdbc',\" +\n-\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n-\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n-\t\t\t\t\")\"\n-\t\t);\n-\n-\t\tStreamITCase.clear();\n-\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n-\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n-\t\tenv.execute();\n-\n-\t\tList<String> expected =\n-\t\t\tArrays.asList(\n-\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n-\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n-\t\tStreamITCase.compareWithList(expected);\n-\t}\n }\n"}}, {"oid": "f0a7af07324182da2121f873e8e857459f425194", "url": "https://github.com/apache/flink/commit/f0a7af07324182da2121f873e8e857459f425194", "message": "[FLINK-15445][connectors/jdbc] JDBC Table Source didn't work for Types with precision (or/and scale)", "committedDate": "2020-02-04T07:56:32Z", "type": "commit"}, {"oid": "4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "url": "https://github.com/apache/flink/commit/4a2e1789331ad3d6b7ed5a34f2a64c7893528a1b", "message": "fixup: adjust dependencies", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "24f4f9782b3332d62bfb0933b423ae3a5843318b", "url": "https://github.com/apache/flink/commit/24f4f9782b3332d62bfb0933b423ae3a5843318b", "message": "fixup: fix projection push-down", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "a22c3c94d4e6323543ca9b23254bcea343625f5d", "url": "https://github.com/apache/flink/commit/a22c3c94d4e6323543ca9b23254bcea343625f5d", "message": "[FLINK-15445][connectors/jdbc] JDBCTableSource should override and change explainSource() API to explain the pushdown applied", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "a3c34fd3dc54726056e63db06136917a566068a4", "url": "https://github.com/apache/flink/commit/a3c34fd3dc54726056e63db06136917a566068a4", "message": "fixup: address Jark's comments", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "5858540ad796f272248b09f6b65ea9f2dd402a6f", "url": "https://github.com/apache/flink/commit/5858540ad796f272248b09f6b65ea9f2dd402a6f", "message": "fixup: test more data types", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "5ed269626192adabb37b5481c21c91ad4c892592", "url": "https://github.com/apache/flink/commit/5ed269626192adabb37b5481c21c91ad4c892592", "message": "[FLINK-15445][connectors/jdbc] validate if the dialect instance support a specific data type", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "e8d921d91449b3c3c8afea064af7ec99daf8d976", "url": "https://github.com/apache/flink/commit/e8d921d91449b3c3c8afea064af7ec99daf8d976", "message": "fixup: derby only support decimal with precision [1,31]", "committedDate": "2020-02-04T07:58:57Z", "type": "commit"}, {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "message": "fixup: rebase to resolve conflicts", "committedDate": "2020-02-04T08:59:05Z", "type": "commit"}, {"oid": "f7456ac7e14681e18199941e0962b207eb99bef9", "url": "https://github.com/apache/flink/commit/f7456ac7e14681e18199941e0962b207eb99bef9", "message": "fixup: rebase to resolve conflicts", "committedDate": "2020-02-04T08:59:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376195245", "bodyText": "Can we remove the returnType member field?\nIt's error-prone to maintain two objects. The returnType is only used in getDataStream and can be derived via TypeConversions.fromDataTypeToLegacyInfo(producedDataType).", "author": "wuchong", "createdAt": "2020-02-07T03:10:06Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java", "diffHunk": "@@ -73,17 +75,23 @@ private JDBCTableSource(\n \t\tthis.selectFields = selectFields;\n \n \t\tfinal TypeInformation<?>[] schemaTypeInfos = schema.getFieldTypes();\n+\t\tfinal DataType[] schemaDataTypes = schema.getFieldDataTypes();\n \t\tfinal String[] schemaFieldNames = schema.getFieldNames();\n \t\tif (selectFields != null) {\n \t\t\tTypeInformation<?>[] typeInfos = new TypeInformation[selectFields.length];\n-\t\t\tString[] typeNames = new String[selectFields.length];\n+\t\t\tDataType[] dataTypes = new DataType[selectFields.length];\n+\t\t\tString[] fieldNames = new String[selectFields.length];\n \t\t\tfor (int i = 0; i < selectFields.length; i++) {\n \t\t\t\ttypeInfos[i] = schemaTypeInfos[selectFields[i]];\n-\t\t\t\ttypeNames[i] = schemaFieldNames[selectFields[i]];\n+\t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n+\t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, typeNames);\n+\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjI2Njc4NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376266784", "bodyText": "Make sense.", "author": "docete", "createdAt": "2020-02-07T08:30:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NTI0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "905972ad292b8657c04b5c4165371f0d8088f824", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\nindex 7b945ea080..52db3da029 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSource.java\n\n@@ -86,11 +86,9 @@ public class JDBCTableSource implements\n \t\t\t\tdataTypes[i] = schemaDataTypes[selectFields[i]];\n \t\t\t\tfieldNames[i] = schemaFieldNames[selectFields[i]];\n \t\t\t}\n-\t\t\tthis.returnType = new RowTypeInfo(typeInfos, fieldNames);\n \t\t\tthis.producedDataType =\n \t\t\t\t\tTableSchema.builder().fields(fieldNames, dataTypes).build().toRowDataType();\n \t\t} else {\n-\t\t\tthis.returnType = new RowTypeInfo(schemaTypeInfos, schemaFieldNames);\n \t\t\tthis.producedDataType = schema.toRowDataType();\n \t\t}\n \t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NjEzMw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376196133", "bodyText": "I think it would be better to move the validation logic into JDBCValidator.\nhttps://github.com/apache/flink/blob/master/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/table/descriptors/JDBCValidator.java#L73", "author": "wuchong", "createdAt": "2020-02-07T03:14:13Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java", "diffHunk": "@@ -130,8 +130,11 @@\n \t\tTableSchema schema = TableSchemaUtils.getPhysicalSchema(\n \t\t\tdescriptorProperties.getTableSchema(SCHEMA));\n \n+\t\tJDBCOptions options = getJDBCOptions(descriptorProperties);\n+\t\toptions.getDialect().validate(schema);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java\nindex d13101de82..5ab621f203 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceSinkFactory.java\n\n@@ -130,11 +130,8 @@ public class JDBCTableSourceSinkFactory implements\n \t\tTableSchema schema = TableSchemaUtils.getPhysicalSchema(\n \t\t\tdescriptorProperties.getTableSchema(SCHEMA));\n \n-\t\tJDBCOptions options = getJDBCOptions(descriptorProperties);\n-\t\toptions.getDialect().validate(schema);\n-\n \t\treturn JDBCTableSource.builder()\n-\t\t\t.setOptions(options)\n+\t\t\t.setOptions(getJDBCOptions(descriptorProperties))\n \t\t\t.setReadOptions(getJDBCReadOptions(descriptorProperties))\n \t\t\t.setLookupOptions(getJDBCLookupOptions(descriptorProperties))\n \t\t\t.setSchema(schema)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376198345", "bodyText": "You can create a unit test JDBCDataTypeTest to verify all the types with different precision with different dialects. This doesn't involve a job submission, and is a lightweight unit test. You can take FlinkDDLDataTypeTest as an example.", "author": "wuchong", "createdAt": "2020-02-07T03:25:04Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDQzNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450437", "bodyText": "I think we don't need this test any more, because is already covered by JDBCDataTypeTest.", "author": "wuchong", "createdAt": "2020-02-11T05:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5ODM0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "6336abbade5c71df3bd26fad40209e89642810d8", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\nindex c41d0a85fa..d0f22f4748 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n\n@@ -78,7 +78,7 @@ public class JDBCTableSourceITCase extends AbstractTestBase {\n \t\ttry (\n \t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n \t\t\tStatement stat = conn.createStatement()) {\n-\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t\tstat.executeUpdate(\"DROP TABLE \" + INPUT_TABLE);\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199069", "bodyText": "throws ValidationException on the method signature. And please add a description about the exception in the javadoc.", "author": "wuchong", "createdAt": "2020-02-07T03:28:03Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDYwNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680607", "bodyText": "ValidationException is a RuntimeException, adding it to method signature won' t force caller the check it. Maybe we just need to add it in java doc like LogicalTypeParser.parse and FieldInfoUtils.validateInputTypeInfo ?", "author": "libenchao", "createdAt": "2020-02-08T02:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\nindex cbe052ae9a..5dba839edd 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n\n@@ -39,11 +40,10 @@ public interface JDBCDialect extends Serializable {\n \n \t/**\n \t * Check if this dialect instance support a specific data type in table schema.\n-\t *\n-\t * @param schema the table schema\n+\t * @param schema the table schema.\n+\t * @exception ValidationException in case of the table schema contains unsupported type.\n \t */\n-\tdefault void validate(TableSchema schema) {\n-\t\treturn;\n+\tdefault void validate(TableSchema schema) throws ValidationException {\n \t}\n \n \t/**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTA5OA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376199098", "bodyText": "remove emtpy line.", "author": "wuchong", "createdAt": "2020-02-07T03:28:10Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\nindex cbe052ae9a..5dba839edd 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n\n@@ -39,11 +40,10 @@ public interface JDBCDialect extends Serializable {\n \n \t/**\n \t * Check if this dialect instance support a specific data type in table schema.\n-\t *\n-\t * @param schema the table schema\n+\t * @param schema the table schema.\n+\t * @exception ValidationException in case of the table schema contains unsupported type.\n \t */\n-\tdefault void validate(TableSchema schema) {\n-\t\treturn;\n+\tdefault void validate(TableSchema schema) throws ValidationException {\n \t}\n \n \t/**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMjUwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376202505", "bodyText": "We don't need to return if the return type is void.", "author": "wuchong", "createdAt": "2020-02-07T03:46:24Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java", "diffHunk": "@@ -35,6 +37,15 @@\n \t */\n \tboolean canHandle(String url);\n \n+\t/**\n+\t * Check if this dialect instance support a specific data type in table schema.\n+\t *\n+\t * @param schema the table schema\n+\t */\n+\tdefault void validate(TableSchema schema) {\n+\t\treturn;", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\nindex cbe052ae9a..5dba839edd 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialect.java\n\n@@ -39,11 +40,10 @@ public interface JDBCDialect extends Serializable {\n \n \t/**\n \t * Check if this dialect instance support a specific data type in table schema.\n-\t *\n-\t * @param schema the table schema\n+\t * @param schema the table schema.\n+\t * @exception ValidationException in case of the table schema contains unsupported type.\n \t */\n-\tdefault void validate(TableSchema schema) {\n-\t\treturn;\n+\tdefault void validate(TableSchema schema) throws ValidationException {\n \t}\n \n \t/**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwMzcwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376203705", "bodyText": "It seems that the validation logic is the same, maybe we can refactor it a bit more to have a AbstractJDBCDialect which implements JDBCDialect.\n\tprivate abstract static class AbstractDialect implements JDBCDialect {\n\n\t\t@Override\n\t\tpublic void validate(TableSchema schema) throws ValidationException {\n\t\t\t// implement the common validation logic here\n\t\t}\n\t\t\n\t\tpublic abstract int maxDecimalPrecision();\n\t\t\n\t\tpublic abstract int minDecimalPrecision();\n\t\t\n\t\tpublic abstract int maxTimestampPrecision();\n\t\t\n\t\tpublic abstract int minTimestampPrecision();\n\t\t\n\t\tpublic abstract List<LogicalTypeRoot> unsupportedTypes();\n\t}", "author": "wuchong", "createdAt": "2020-02-07T03:52:30Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -70,11 +105,33 @@ public String quoteIdentifier(String identifier) {\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_MYSQL_TIMESTAMP_PRECISION = 6;\n+\n+\t\tprivate static final int MIN_MYSQL_TIMESTAMP_PRECISION = 0;\n+\n \t\t@Override\n \t\tpublic boolean canHandle(String url) {\n \t\t\treturn url.startsWith(\"jdbc:mysql:\");\n \t\t}\n \n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\t\t\t\tif (TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > MAX_MYSQL_TIMESTAMP_PRECISION) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tMIN_MYSQL_TIMESTAMP_PRECISION,\n+\t\t\t\t\t\t\t\t\t\tMAX_MYSQL_TIMESTAMP_PRECISION));\n+\t\t\t\t\t}\n+\t\t\t\t}", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 4ee751c3d1..2c3f4db923 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -99,39 +151,72 @@ public final class JDBCDialects {\n \t\tpublic String quoteIdentifier(String identifier) {\n \t\t\treturn identifier;\n \t\t}\n+\n+\t\t@Override\n+\t\tpublic int maxDecimalPrecision() {\n+\t\t\treturn MAX_DECIMAL_PRECISION;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic int minDecimalPrecision() {\n+\t\t\treturn MIN_DECIMAL_PRECISION;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic int maxTimestampPrecision() {\n+\t\t\treturn MAX_TIMESTAMP_PRECISION;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic int minTimestampPrecision() {\n+\t\t\treturn MIN_TIMESTAMP_PRECISION;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<LogicalTypeRoot> unsupportedTypes() {\n+\t\t\t// The data types used in Derby are list at\n+\t\t\t// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html\n+\n+\t\t\t// TODO: We can't convert BINARY data type to\n+\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.\n+\t\t\treturn Arrays.asList(\n+\t\t\t\t\tLogicalTypeRoot.BINARY,\n+\t\t\t\t\tLogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,\n+\t\t\t\t\tLogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,\n+\t\t\t\t\tLogicalTypeRoot.INTERVAL_YEAR_MONTH,\n+\t\t\t\t\tLogicalTypeRoot.INTERVAL_DAY_TIME,\n+\t\t\t\t\tLogicalTypeRoot.ARRAY,\n+\t\t\t\t\tLogicalTypeRoot.MULTISET,\n+\t\t\t\t\tLogicalTypeRoot.MAP,\n+\t\t\t\t\tLogicalTypeRoot.ROW,\n+\t\t\t\t\tLogicalTypeRoot.DISTINCT_TYPE,\n+\t\t\t\t\tLogicalTypeRoot.STRUCTURED_TYPE,\n+\t\t\t\t\tLogicalTypeRoot.NULL,\n+\t\t\t\t\tLogicalTypeRoot.RAW,\n+\t\t\t\t\tLogicalTypeRoot.SYMBOL,\n+\t\t\t\t\tLogicalTypeRoot.UNRESOLVED);\n+\t\t}\n \t}\n \n-\tprivate static class MySQLDialect implements JDBCDialect {\n+\tprivate static class MySQLDialect extends AbstractDialect {\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n-\t\tprivate static final int MAX_MYSQL_TIMESTAMP_PRECISION = 6;\n+\t\t// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:\n+\t\t// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html\n+\t\tprivate static final int MAX_TIMESTAMP_PRECISION = 6;\n+\t\tprivate static final int MIN_TIMESTAMP_PRECISION = 1;\n \n-\t\tprivate static final int MIN_MYSQL_TIMESTAMP_PRECISION = 0;\n+\t\t// Define MAX/MIN precision of DECIMAL type according to Mysql docs:\n+\t\t// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html\n+\t\tprivate static final int MAX_DECIMAL_PRECISION = 65;\n+\t\tprivate static final int MIN_DECIMAL_PRECISION = 1;\n \n \t\t@Override\n \t\tpublic boolean canHandle(String url) {\n \t\t\treturn url.startsWith(\"jdbc:mysql:\");\n \t\t}\n \n-\t\t@Override\n-\t\tpublic void validate(TableSchema schema) {\n-\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n-\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n-\t\t\t\tString fieldName = schema.getFieldName(i).get();\n-\t\t\t\tif (TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n-\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n-\t\t\t\t\tif (precision > MAX_MYSQL_TIMESTAMP_PRECISION) {\n-\t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of range [%d, %d].\",\n-\t\t\t\t\t\t\t\t\t\tfieldName,\n-\t\t\t\t\t\t\t\t\t\tMIN_MYSQL_TIMESTAMP_PRECISION,\n-\t\t\t\t\t\t\t\t\t\tMAX_MYSQL_TIMESTAMP_PRECISION));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n \t\t@Override\n \t\tpublic Optional<String> defaultDriverName() {\n \t\t\treturn Optional.of(\"com.mysql.jdbc.Driver\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNDE1NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376204154", "bodyText": "Could you add a comment above these constants that includes documentation link describes the precision? So that we can have the single truth.", "author": "wuchong", "createdAt": "2020-02-07T03:55:16Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -50,11 +60,36 @@\n \n \t\tprivate static final long serialVersionUID = 1L;\n \n+\t\tprivate static final int MAX_DERBY_DECIMAL_PRECISION = 31;\n+\n+\t\tprivate static final int MIN_DERBY_DECIMAL_PRECISION = 1;", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 4ee751c3d1..2c3f4db923 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -56,40 +59,89 @@ public final class JDBCDialects {\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n-\n-\t\tprivate static final long serialVersionUID = 1L;\n-\n-\t\tprivate static final int MAX_DERBY_DECIMAL_PRECISION = 31;\n-\n-\t\tprivate static final int MIN_DERBY_DECIMAL_PRECISION = 1;\n-\n-\t\t@Override\n-\t\tpublic boolean canHandle(String url) {\n-\t\t\treturn url.startsWith(\"jdbc:derby:\");\n-\t\t}\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n \n \t\t@Override\n-\t\tpublic void validate(TableSchema schema) {\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n \t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n \t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n \t\t\t\tString fieldName = schema.getFieldName(i).get();\n-\t\t\t\t// We only validate precision of DECIMAL type for blink planner.\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n \t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n \t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n \t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n-\t\t\t\t\tif (precision > MAX_DERBY_DECIMAL_PRECISION\n-\t\t\t\t\t\t\t|| precision < MIN_DERBY_DECIMAL_PRECISION) {\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n \t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n-\t\t\t\t\t\t\t\t\t\tMIN_DERBY_DECIMAL_PRECISION,\n-\t\t\t\t\t\t\t\t\t\tMAX_DERBY_DECIMAL_PRECISION));\n+\t\t\t\t\t\t\t\t\t\tminTimestampPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision()));\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n+\t\tpublic abstract int maxDecimalPrecision();\n+\n+\t\tpublic abstract int minDecimalPrecision();\n+\n+\t\tpublic abstract int maxTimestampPrecision();\n+\n+\t\tpublic abstract int minTimestampPrecision();\n+\n+\t\t/**\n+\t\t * Defines the unsupported types for the dialect.\n+\t\t * @return a list of logical type roots.\n+\t\t */\n+\t\tpublic abstract List<LogicalTypeRoot> unsupportedTypes();\n+\t}\n+\n+\tprivate static class DerbyDialect extends AbstractDialect {\n+\n+\t\tprivate static final long serialVersionUID = 1L;\n+\n+\t\t// Define MAX/MIN precision of TIMESTAMP type according to derby docs:\n+\t\t// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html\n+\t\tprivate static final int MAX_TIMESTAMP_PRECISION = 9;\n+\t\tprivate static final int MIN_TIMESTAMP_PRECISION = 1;\n+\n+\t\t// Define MAX/MIN precision of DECIMAL type according to derby docs:\n+\t\t// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html\n+\t\tprivate static final int MAX_DECIMAL_PRECISION = 31;\n+\t\tprivate static final int MIN_DECIMAL_PRECISION = 1;\n+\n+\t\t@Override\n+\t\tpublic boolean canHandle(String url) {\n+\t\t\treturn url.startsWith(\"jdbc:derby:\");\n+\t\t}\n+\n \t\t@Override\n \t\tpublic Optional<String> defaultDriverName() {\n \t\t\treturn Optional.of(\"org.apache.derby.jdbc.EmbeddedDriver\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376205505", "bodyText": "Should we use stat.executeUdpate?", "author": "wuchong", "createdAt": "2020-02-07T04:03:04Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5MzA5OA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376893098", "bodyText": "execute can be used for any SQL statement, but executeUpdate is more clearly here. I will fix this.", "author": "docete", "createdAt": "2020-02-10T07:02:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNTUwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "6336abbade5c71df3bd26fad40209e89642810d8", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\nindex c41d0a85fa..d0f22f4748 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n\n@@ -78,7 +78,7 @@ public class JDBCTableSourceITCase extends AbstractTestBase {\n \t\ttry (\n \t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n \t\t\tStatement stat = conn.createStatement()) {\n-\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t\tstat.executeUpdate(\"DROP TABLE \" + INPUT_TABLE);\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376206438", "bodyText": "Do we really need this? Is there any error messages thrown when run these tests?", "author": "wuchong", "createdAt": "2020-02-07T04:08:47Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NDE2OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376894169", "bodyText": "We add this to get rid of derby.log or the UT will end up with the derby.log file in the root of the project (flink-jdbc).", "author": "docete", "createdAt": "2020-02-10T07:07:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIwNjQzOA=="}], "type": "inlineReview", "revised_code": {"commit": "6336abbade5c71df3bd26fad40209e89642810d8", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\nindex c41d0a85fa..d0f22f4748 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n\n@@ -78,7 +78,7 @@ public class JDBCTableSourceITCase extends AbstractTestBase {\n \t\ttry (\n \t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n \t\t\tStatement stat = conn.createStatement()) {\n-\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t\tstat.executeUpdate(\"DROP TABLE \" + INPUT_TABLE);\n \t\t}\n \t}\n \n"}}, {"oid": "905972ad292b8657c04b5c4165371f0d8088f824", "url": "https://github.com/apache/flink/commit/905972ad292b8657c04b5c4165371f0d8088f824", "message": "fixup: remove the old style RowTypeInfo", "committedDate": "2020-02-07T08:21:35Z", "type": "commit"}, {"oid": "1aa14851215234acf47801d002c436bfd51588c2", "url": "https://github.com/apache/flink/commit/1aa14851215234acf47801d002c436bfd51588c2", "message": "fixup: remove useless code", "committedDate": "2020-02-07T08:28:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376680959", "bodyText": "seems indent is not correct.", "author": "libenchao", "createdAt": "2020-02-08T02:39:41Z", "path": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java", "diffHunk": "@@ -18,93 +18,167 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n-import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableException;\n import org.apache.flink.table.api.java.StreamTableEnvironment;\n import org.apache.flink.table.runtime.utils.StreamITCase;\n+import org.apache.flink.test.util.AbstractTestBase;\n import org.apache.flink.types.Row;\n \n-import org.junit.BeforeClass;\n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.ArrayList;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.Arrays;\n import java.util.List;\n \n+\n /**\n- * IT case for {@link JDBCTableSource}.\n+ * ITCase for {@link JDBCTableSource}.\n  */\n-public class JDBCTableSourceITCase extends JDBCTestBase {\n-\n-\tprivate static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-\tprivate static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();\n-\tprivate static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);\n-\n-\tstatic final String TABLE_SOURCE_SQL = \"CREATE TABLE books (\" +\n-\t\t\" id int, \" +\n-\t\t\" title varchar, \" +\n-\t\t\" author varchar, \" +\n-\t\t\" price double, \" +\n-\t\t\" qty int \" +\n-\t\t\") with (\" +\n-\t\t\" 'connector.type' = 'jdbc', \" +\n-\t\t\" 'connector.url' = 'jdbc:derby:memory:ebookshop', \" +\n-\t\t\" 'connector.table' = 'books', \" +\n-\t\t\" 'connector.driver' = 'org.apache.derby.jdbc.EmbeddedDriver' \" +\n-\t\t\")\";\n-\n-\t@BeforeClass\n-\tpublic static void createTable() {\n-\t\ttEnv.sqlUpdate(TABLE_SOURCE_SQL);\n+public class JDBCTableSourceITCase extends AbstractTestBase {\n+\n+\tpublic static final String DRIVER_CLASS = \"org.apache.derby.jdbc.EmbeddedDriver\";\n+\tpublic static final String DB_URL = \"jdbc:derby:memory:test\";\n+\tpublic static final String INPUT_TABLE = \"jdbcSource\";\n+\n+\t@Before\n+\tpublic void before() throws ClassNotFoundException, SQLException {\n+\t\tSystem.setProperty(\"derby.stream.error.field\", JDBCTestBase.class.getCanonicalName() + \".DEV_NULL\");\n+\t\tClass.forName(DRIVER_CLASS);\n+\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL + \";create=true\");\n+\t\t\tStatement statement = conn.createStatement()) {\n+\t\t\tstatement.executeUpdate(\"CREATE TABLE \" + INPUT_TABLE + \" (\" +\n+\t\t\t\t\t\"id BIGINT NOT NULL,\" +\n+\t\t\t\t\t\"timestamp6_col TIMESTAMP, \" +\n+\t\t\t\t\t\"timestamp9_col TIMESTAMP, \" +\n+\t\t\t\t\t\"time_col TIME, \" +\n+\t\t\t\t\t\"real_col FLOAT(23), \" +    // A precision of 23 or less makes FLOAT equivalent to REAL.\n+\t\t\t\t\t\"double_col FLOAT(24),\" +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.\n+\t\t\t\t\t\"decimal_col DECIMAL(10, 4))\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)\");\n+\t\t\tstatement.executeUpdate(\"INSERT INTO \" + INPUT_TABLE + \" VALUES (\" +\n+\t\t\t\t\t\"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), \" +\n+\t\t\t\t\t\"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)\");\n+\t\t}\n+\t}\n+\n+\t@After\n+\tpublic void clearOutputTable() throws Exception {\n+\t\tClass.forName(DRIVER_CLASS);\n+\t\ttry (\n+\t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n+\t\t\tStatement stat = conn.createStatement()) {\n+\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t}\n \t}\n \n \t@Test\n-\tpublic void testFieldsProjection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t.useBlinkPlanner()\n+\t\t\t.inStreamingMode()\n+\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"double_col DOUBLE,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ID_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT * FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001\");\n-\t\texpected.add(\"1002\");\n-\t\texpected.add(\"1003\");\n-\t\texpected.add(\"1004\");\n-\t\texpected.add(\"1005\");\n-\t\texpected.add(\"1006\");\n-\t\texpected.add(\"1007\");\n-\t\texpected.add(\"1008\");\n-\t\texpected.add(\"1009\");\n-\t\texpected.add(\"1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234\",\n+\t\t\t\t\"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n \t@Test\n-\tpublic void testAllFieldsSelection() throws Exception {\n-\t\tStreamITCase.clear();\n+\tpublic void testProjectableJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +\n+\t\t\t\t\"id BIGINT,\" +\n+\t\t\t\t\"timestamp6_col TIMESTAMP(6),\" +\n+\t\t\t\t\"timestamp9_col TIMESTAMP(9),\" +\n+\t\t\t\t\"time_col TIME,\" +\n+\t\t\t\t\"real_col FLOAT,\" +\n+\t\t\t\t\"decimal_col DECIMAL(10, 4)\" +\n+\t\t\t\t\") WITH (\" +\n+\t\t\t\t\"  'connector.type'='jdbc',\" +\n+\t\t\t\t\"  'connector.url'='\" + DB_URL + \"',\" +\n+\t\t\t\t\"  'connector.table'='\" + INPUT_TABLE + \"'\" +\n+\t\t\t\t\")\"\n+\t\t);\n \n-\t\tTable result = tEnv.sqlQuery(SELECT_ALL_BOOKS);\n-\t\tDataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);\n-\t\tresultSet.addSink(new StreamITCase.StringSink<>());\n+\t\tStreamITCase.clear();\n+\t\ttEnv.toAppendStream(tEnv.sqlQuery(\"SELECT timestamp6_col, decimal_col FROM \" + INPUT_TABLE), Row.class)\n+\t\t\t\t.addSink(new StreamITCase.StringSink<>());\n \t\tenv.execute();\n \n-\t\tList<String> expected = new ArrayList<>();\n-\t\texpected.add(\"1001,Java public for dummies,Tan Ah Teck,11.11,11\");\n-\t\texpected.add(\"1002,More Java for dummies,Tan Ah Teck,22.22,22\");\n-\t\texpected.add(\"1003,More Java for more dummies,Mohammad Ali,33.33,33\");\n-\t\texpected.add(\"1004,A Cup of Java,Kumar,44.44,44\");\n-\t\texpected.add(\"1005,A Teaspoon of Java,Kevin Jones,55.55,55\");\n-\t\texpected.add(\"1006,A Teaspoon of Java 1.4,Kevin Jones,66.66,66\");\n-\t\texpected.add(\"1007,A Teaspoon of Java 1.5,Kevin Jones,77.77,77\");\n-\t\texpected.add(\"1008,A Teaspoon of Java 1.6,Kevin Jones,88.88,88\");\n-\t\texpected.add(\"1009,A Teaspoon of Java 1.7,Kevin Jones,99.99,99\");\n-\t\texpected.add(\"1010,A Teaspoon of Java 1.8,Kevin Jones,null,1010\");\n-\n+\t\tList<String> expected =\n+\t\t\tArrays.asList(\n+\t\t\t\t\"2020-01-01T15:35:00.123456,100.1234\",\n+\t\t\t\t\"2020-01-01T15:36:01.123456,101.1234\");\n \t\tStreamITCase.compareWithList(expected);\n \t}\n \n+\t@Test(expected = TableException.class)\n+\tpublic void testInvalidPrecisionOfJDBCSource() throws Exception {\n+\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tEnvironmentSettings envSettings = EnvironmentSettings.newInstance()\n+\t\t\t\t.useBlinkPlanner()\n+\t\t\t\t.inStreamingMode()\n+\t\t\t\t.build();\n+\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);\n+\n+\t\ttEnv.sqlUpdate(\n+\t\t\"CREATE TABLE \" + INPUT_TABLE + \"(\" +", "originalCommit": "f7456ac7e14681e18199941e0962b207eb99bef9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg5NzcwOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r376897708", "bodyText": "nice catch", "author": "docete", "createdAt": "2020-02-10T07:20:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4MDk1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "6336abbade5c71df3bd26fad40209e89642810d8", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\nindex c41d0a85fa..d0f22f4748 100644\n--- a/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n+++ b/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCTableSourceITCase.java\n\n@@ -78,7 +78,7 @@ public class JDBCTableSourceITCase extends AbstractTestBase {\n \t\ttry (\n \t\t\tConnection conn = DriverManager.getConnection(DB_URL);\n \t\t\tStatement stat = conn.createStatement()) {\n-\t\t\tstat.execute(\"DROP TABLE \" + INPUT_TABLE);\n+\t\t\tstat.executeUpdate(\"DROP TABLE \" + INPUT_TABLE);\n \t\t}\n \t}\n \n"}}, {"oid": "a12d3d5cee04047c93c01119ad07332a23b7deb0", "url": "https://github.com/apache/flink/commit/a12d3d5cee04047c93c01119ad07332a23b7deb0", "message": "[FLINK-15445][connectors/jdbc] Refactor the validation of data types for dialects", "committedDate": "2020-02-10T06:48:07Z", "type": "commit"}, {"oid": "6336abbade5c71df3bd26fad40209e89642810d8", "url": "https://github.com/apache/flink/commit/6336abbade5c71df3bd26fad40209e89642810d8", "message": "fixup: address Jark&benchao's comments", "committedDate": "2020-02-10T07:23:18Z", "type": "commit"}, {"oid": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "url": "https://github.com/apache/flink/commit/330adf7ac3582db6b47a093e6f342351a3ba7e43", "message": "fixup: checkstyle", "committedDate": "2020-02-10T08:50:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440051", "bodyText": "Could we just simply dt.getLogicalType() instanceof VarBinaryType  to match it is a VarBinaryType? I think currently there isn't a LegacyTypeInformationType which is VARBINARY.  The same to the below if branches.", "author": "wuchong", "createdAt": "2020-02-11T04:13:56Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU4NTIzNw==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377585237", "bodyText": "Yes, I think so. Will update soon.", "author": "docete", "createdAt": "2020-02-11T11:44:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDA1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "5e538c4875e6241d494585c6e2a8f586ac078131", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 2c3f4db923..770d39eed4 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -71,43 +71,48 @@ public final class JDBCDialects {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",\n+\t\t\t\t\t\t\t\t\tdialectName(),\n+\t\t\t\t\t\t\t\t\tdt.toString()));\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof DecimalType) {\n \t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxDecimalPrecision()\n \t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the DECIMAL \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof TimestampType) {\n \t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxTimestampPrecision()\n \t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the TIMESTAMP \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminTimestampPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n+\t\tpublic abstract String dialectName();\n+\n \t\tpublic abstract int maxDecimalPrecision();\n \n \t\tpublic abstract int minDecimalPrecision();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440632", "bodyText": "Improve the error message a bit more:\nString.format(\"The precision of filed '%s' is out of the TIMESTAMP precision range [%d, %d] supported by the %s dialect.\",\nfieldName,\nminTimestampPrecision(),\nmaxTimestampPrecision(),\ndialectName);", "author": "wuchong", "createdAt": "2020-02-11T04:18:24Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxDecimalPrecision()\n+\t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\t\t\tfieldName,\n+\t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// only validate precision of DECIMAL type for blink planner\n+\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n+\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n+\t\t\t\t\tif (precision > maxTimestampPrecision()\n+\t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n+\t\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDcwOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377440708", "bodyText": "The same to the DECIMAL type.", "author": "wuchong", "createdAt": "2020-02-11T04:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0MDYzMg=="}], "type": "inlineReview", "revised_code": {"commit": "5e538c4875e6241d494585c6e2a8f586ac078131", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 2c3f4db923..770d39eed4 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -71,43 +71,48 @@ public final class JDBCDialects {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",\n+\t\t\t\t\t\t\t\t\tdialectName(),\n+\t\t\t\t\t\t\t\t\tdt.toString()));\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof DecimalType) {\n \t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxDecimalPrecision()\n \t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the DECIMAL \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof TimestampType) {\n \t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxTimestampPrecision()\n \t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the TIMESTAMP \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminTimestampPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n+\t\tpublic abstract String dialectName();\n+\n \t\tpublic abstract int maxDecimalPrecision();\n \n \t\tpublic abstract int minDecimalPrecision();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ1MDUzOA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377450538", "bodyText": "String.format(\"The %s dialect doesn't support type: %s.\", dialectName, dt.toString())", "author": "wuchong", "createdAt": "2020-02-11T05:25:55Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -46,10 +59,84 @@\n \t\treturn Optional.empty();\n \t}\n \n-\tprivate static class DerbyDialect implements JDBCDialect {\n+\tprivate abstract static class AbstractDialect implements JDBCDialect {\n+\n+\t\t@Override\n+\t\tpublic void validate(TableSchema schema) throws ValidationException {\n+\t\t\tfor (int i = 0; i < schema.getFieldCount(); i++) {\n+\t\t\t\tDataType dt = schema.getFieldDataType(i).get();\n+\t\t\t\tString fieldName = schema.getFieldName(i).get();\n+\n+\t\t\t\t// TODO: We can't convert VARBINARY(n) data type to\n+\t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n+\t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n+\t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n+\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n+\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));", "originalCommit": "330adf7ac3582db6b47a093e6f342351a3ba7e43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5e538c4875e6241d494585c6e2a8f586ac078131", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 2c3f4db923..770d39eed4 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -71,43 +71,48 @@ public final class JDBCDialects {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",\n+\t\t\t\t\t\t\t\t\tdialectName(),\n+\t\t\t\t\t\t\t\t\tdt.toString()));\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& DECIMAL == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof DecimalType) {\n \t\t\t\t\tint precision = ((DecimalType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxDecimalPrecision()\n \t\t\t\t\t\t\t|| precision < minDecimalPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the DECIMAL \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminDecimalPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxDecimalPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \n \t\t\t\t// only validate precision of DECIMAL type for blink planner\n-\t\t\t\tif (!(dt.getLogicalType() instanceof LegacyTypeInformationType)\n-\t\t\t\t\t\t&& TIMESTAMP_WITHOUT_TIME_ZONE == dt.getLogicalType().getTypeRoot()) {\n+\t\t\t\tif (dt.getLogicalType() instanceof TimestampType) {\n \t\t\t\t\tint precision = ((TimestampType) dt.getLogicalType()).getPrecision();\n \t\t\t\t\tif (precision > maxTimestampPrecision()\n \t\t\t\t\t\t\t|| precision < minTimestampPrecision()) {\n \t\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\t\tString.format(\"The precision of %s is out of the range [%d, %d].\",\n+\t\t\t\t\t\t\t\tString.format(\"The precision of field '%s' is out of the TIMESTAMP \" +\n+\t\t\t\t\t\t\t\t\t\t\t\t\"precision range [%d, %d] supported by %s dialect.\",\n \t\t\t\t\t\t\t\t\t\tfieldName,\n \t\t\t\t\t\t\t\t\t\tminTimestampPrecision(),\n-\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision()));\n+\t\t\t\t\t\t\t\t\t\tmaxTimestampPrecision(),\n+\t\t\t\t\t\t\t\t\t\tdialectName()));\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n \n+\t\tpublic abstract String dialectName();\n+\n \t\tpublic abstract int maxDecimalPrecision();\n \n \t\tpublic abstract int minDecimalPrecision();\n"}}, {"oid": "5e538c4875e6241d494585c6e2a8f586ac078131", "url": "https://github.com/apache/flink/commit/5e538c4875e6241d494585c6e2a8f586ac078131", "message": "fixup: address jark's comments about improving error msg", "committedDate": "2020-02-11T12:30:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYxOTE1NA==", "url": "https://github.com/apache/flink/pull/10745#discussion_r377619154", "bodyText": "Nit: don't -> doesn't", "author": "wuchong", "createdAt": "2020-02-11T13:01:23Z", "path": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java", "diffHunk": "@@ -71,43 +71,48 @@ public void validate(TableSchema schema) throws ValidationException {\n \t\t\t\t//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter\n \t\t\t\t//  when n is smaller than Integer.MAX_VALUE\n \t\t\t\tif (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||\n-\t\t\t\t\t\t(!(dt.getLogicalType() instanceof LegacyTypeInformationType) &&\n-\t\t\t\t\t\t(VARBINARY == dt.getLogicalType().getTypeRoot()\n-\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength()))) {\n+\t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n+\t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The dialect don't support type: %s.\", dt.toString()));\n+\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",", "originalCommit": "5e538c4875e6241d494585c6e2a8f586ac078131", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "chunk": "diff --git a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\nindex 770d39eed4..ec97416d18 100644\n--- a/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n+++ b/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/dialect/JDBCDialects.java\n\n@@ -74,7 +69,7 @@ public final class JDBCDialects {\n \t\t\t\t\t\t(dt.getLogicalType() instanceof VarBinaryType\n \t\t\t\t\t\t\t&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {\n \t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"The %s dialect don't support type: %s.\",\n+\t\t\t\t\t\t\tString.format(\"The %s dialect doesn't support type: %s.\",\n \t\t\t\t\t\t\t\t\tdialectName(),\n \t\t\t\t\t\t\t\t\tdt.toString()));\n \t\t\t\t}\n"}}, {"oid": "2f27266467ee029955ac26e1acc0e64628d8f2dd", "url": "https://github.com/apache/flink/commit/2f27266467ee029955ac26e1acc0e64628d8f2dd", "message": "fixup: checkstyle", "committedDate": "2020-02-12T02:54:08Z", "type": "commit"}, {"oid": "5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "url": "https://github.com/apache/flink/commit/5f30317b81fbc0dcf2d518adb8a0ddde1847bfe6", "message": "fixup: Nit don't -> doesn't", "committedDate": "2020-02-12T03:14:58Z", "type": "commit"}, {"oid": "7790797e5e90af02bc94b48f5326077573600f25", "url": "https://github.com/apache/flink/commit/7790797e5e90af02bc94b48f5326077573600f25", "message": "fixup: checkstyle", "committedDate": "2020-02-12T03:17:38Z", "type": "commit"}]}