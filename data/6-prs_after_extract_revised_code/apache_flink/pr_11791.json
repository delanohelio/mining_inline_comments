{"pr_number": 11791, "pr_title": "[FLINK-17210][sql-parser][hive] Implement database DDLs for Hive dialect", "pr_createdAt": "2020-04-17T09:41:32Z", "pr_url": "https://github.com/apache/flink/pull/11791", "timeline": [{"oid": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "url": "https://github.com/apache/flink/commit/6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "message": "rebase", "committedDate": "2020-04-24T03:07:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414269563", "bodyText": "We usually put the properties key in the connector validator instead of the SqlNode, i think.", "author": "danny0405", "createdAt": "2020-04-24T03:42:35Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -247,7 +253,10 @@ public CatalogDatabase getDatabase(String databaseName) throws DatabaseNotExistE\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tproperties.put(HiveCatalogConfig.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tif (!isGeneric) {\n+\t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n+\t\t}", "originalCommit": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxOTU0Ng==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414319546", "bodyText": "I put the property keys in SqlNode because the SqlNode has to add these extra properties for extended Hive semantics.", "author": "lirui-apache", "createdAt": "2020-04-24T06:10:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NjY2MA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956660", "bodyText": "But generally, the Catalog code should not see the SqlNodes, because the former belongs to the planner, the later belongs to parser.", "author": "danny0405", "createdAt": "2020-04-25T02:30:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTAyODg1OQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415028859", "bodyText": "Both SqlNode and HiveCatalog need to access these properties. I can't put them in the hive connector because hive connector depends on sql-parser and not the other way around. So any suggestions where they should go?", "author": "lirui-apache", "createdAt": "2020-04-25T10:00:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0NzEzNg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415247136", "bodyText": "After another think i think it is okey, because we passes around the Hive specific properties through the SqlNode which is the root cause i think.\nWe can have a refactor when we really have a new planner.", "author": "danny0405", "createdAt": "2020-04-26T07:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDI2OTU2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 9abe679fab2..42a6e606386 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -253,7 +250,7 @@ public class HiveCatalog extends AbstractCatalog {\n \n \t\tMap<String, String> properties = hiveDatabase.getParameters();\n \n-\t\tboolean isGeneric = getObjectIsGeneric(properties);\n+\t\tboolean isGeneric = isGenericForGet(properties);\n \t\tif (!isGeneric) {\n \t\t\tproperties.put(SqlCreateHiveDatabase.DATABASE_LOCATION_URI, hiveDatabase.getLocationUri());\n \t\t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414305289", "bodyText": "How about name it getObjectIsGenericDefaultTrue", "author": "danny0405", "createdAt": "2020-04-24T05:33:22Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false", "originalCommit": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMyMDk0OQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414320949", "bodyText": "It's not just about the default value. For example, when creating an object and the is_generic key is missing, we'll add is_generic=false to the properties.\nSince the logic is different based on whether we're creating or retrieving an object, I think it's better to reflect that in the method names.", "author": "lirui-apache", "createdAt": "2020-04-24T06:14:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDQ1NDAzMQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414454031", "bodyText": "The implementation is not really create the object but only append a property there.", "author": "danny0405", "createdAt": "2020-04-24T10:03:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDU3MDIzOA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414570238", "bodyText": "Sorry to hit the \"resolve\" button by mistake...\nYou're right but my point is we should differentiate whether we're creating or getting an object here. E.g. createObjectIsGeneric should be used in \"create\" methods like createTable or createDatabase, while getObjectIsGeneric should be used in \"get\" methods like getTable or getDatabase. It's clearer if the method names carry such information.", "author": "lirui-apache", "createdAt": "2020-04-24T13:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1Njk2OA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956968", "bodyText": "I think each method should take care itself, if it only appends a property item, how about name it \"appendIsGenericIfNecessary\" or something like that ?", "author": "danny0405", "createdAt": "2020-04-25T02:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTAzMDI4MQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415030281", "bodyText": "Having \"create\" or \"get\" in the method names are more friendly to the method caller. The caller doesn't need to know whether the default value should be true or false, or whether a default value should be appended if missing. Instead, the caller just need to choose one method according to whether it's creating or getting an object.\nPerhaps we can name them as isGenericForCreate and isGenericForGet? So that it's clear the method doesn't really create an object, but just to decide whether an object is generic in different scenarios. Does that sound better to you?", "author": "lirui-apache", "createdAt": "2020-04-25T10:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMTExMg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415201112", "bodyText": "I am with isGenericForCreate and isGenericForGet. -1 for createObjectIsGeneric and getObjectIsGeneric", "author": "JingsongLi", "createdAt": "2020-04-26T03:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI0NzM1NQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415247355", "bodyText": "I'm also fine with name isGenericForCreate and isGenericForGet.", "author": "danny0405", "createdAt": "2020-04-26T08:00:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTI4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 9abe679fab2..42a6e606386 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -1446,7 +1389,7 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t}\n \t}\n \n-\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\tstatic boolean isGenericForCreate(Map<String, String> properties) {\n \t\t// When creating an object, a hive object needs explicitly have a key is_generic = false\n \t\t// otherwise, this is a generic object if 1) the key is missing 2) is_generic = true\n \t\t// this is opposite to reading an object. See getObjectIsGeneric().\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTM3OQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414305379", "bodyText": "How about name it getObjectIsGenericDefaultFalse", "author": "danny0405", "createdAt": "2020-04-24T05:33:38Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1403,4 +1446,29 @@ public CatalogColumnStatistics getPartitionColumnStatistics(ObjectPath tablePath\n \t\t}\n \t}\n \n+\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When creating an object, a hive object needs explicitly have a key is_generic = false\n+\t\t// otherwise, this is a generic object if 1) the key is missing 2) is_generic = true\n+\t\t// this is opposite to reading an object. See getObjectIsGeneric().\n+\t\tif (properties == null) {\n+\t\t\treturn true;\n+\t\t}\n+\t\tboolean isGeneric;\n+\t\tif (!properties.containsKey(CatalogConfig.IS_GENERIC)) {\n+\t\t\t// must be a generic object\n+\t\t\tisGeneric = true;\n+\t\t\tproperties.put(CatalogConfig.IS_GENERIC, String.valueOf(true));\n+\t\t} else {\n+\t\t\tisGeneric = Boolean.parseBoolean(properties.get(CatalogConfig.IS_GENERIC));\n+\t\t}\n+\t\treturn isGeneric;\n+\t}\n+\n+\tprivate static boolean getObjectIsGeneric(Map<String, String> properties) {\n+\t\t// When retrieving an object, a generic object needs explicitly have a key is_generic = true", "originalCommit": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwMDg2NA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415200864", "bodyText": "+1\nThere is two default value between getXX and createXX in catalog.", "author": "JingsongLi", "createdAt": "2020-04-26T03:15:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMwNTM3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 9abe679fab2..42a6e606386 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -1446,7 +1389,7 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t}\n \t}\n \n-\tprivate static boolean createObjectIsGeneric(Map<String, String> properties) {\n+\tstatic boolean isGenericForCreate(Map<String, String> properties) {\n \t\t// When creating an object, a hive object needs explicitly have a key is_generic = false\n \t\t// otherwise, this is a generic object if 1) the key is missing 2) is_generic = true\n \t\t// this is opposite to reading an object. See getObjectIsGeneric().\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414313942", "bodyText": "It seems hacky we put these properties internal through the table options, i think these properties should be kept in each SqlNode but not the property list.", "author": "danny0405", "createdAt": "2020-04-24T05:56:43Z", "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.ddl.hive;\n+\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.sql.parser.impl.ParseException;\n+import org.apache.flink.table.catalog.config.CatalogConfig;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+import static org.apache.flink.sql.parser.ddl.hive.SqlAlterHiveDatabase.ALTER_DATABASE_OP;\n+import static org.apache.flink.sql.parser.ddl.hive.SqlCreateHiveDatabase.DATABASE_LOCATION_URI;\n+\n+/**\n+ * Util methods for Hive DDL Sql nodes.\n+ */\n+public class HiveDDLUtils {\n+\n+\tprivate static final Set<String> RESERVED_DB_PROPERTIES = new HashSet<>();\n+", "originalCommit": "6359340c0b5b01923d7ac2824b9eb1071e0f2f37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDM0MDIwMQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414340201", "bodyText": "I agree it seems hacky. But the problem is some semantics are specific to Hive and may never be adopted in Flink, e.g. the locations of table and database. If we keep them in the SqlNode, the planner needs to be able handle them, and the Catalog object (e.g. CatalogDatabase) needs extra fields for the semantics. To avoid this, we have to encode the Hive-specific semantics as properties, which has been mentioned and discussed in FLIP-123.", "author": "lirui-apache", "createdAt": "2020-04-24T06:57:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NzIxNg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414957216", "bodyText": "As discussed offline, this is by-design intentionally, previously we design the Flink databases DDLs to have a extensible properties for such \"hacky\" extention way to make the planner code clean.\nWhile for the long term, if we also want to make a Hive dialect DML, we may need another planner which is another big story.", "author": "danny0405", "createdAt": "2020-04-25T02:33:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDMxMzk0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java b/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/HiveDDLUtils.java\nsimilarity index 93%\nrename from flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java\nrename to flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/HiveDDLUtils.java\nindex cdbb53281e5..a367e1ea9df 100644\n--- a/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/ddl/hive/HiveDDLUtils.java\n+++ b/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/ddl/HiveDDLUtils.java\n\n@@ -16,10 +16,10 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.sql.parser.ddl.hive;\n+package org.apache.flink.sql.parser.hive.ddl;\n \n import org.apache.flink.sql.parser.ddl.SqlTableOption;\n-import org.apache.flink.sql.parser.impl.ParseException;\n+import org.apache.flink.sql.parser.hive.impl.ParseException;\n import org.apache.flink.table.catalog.config.CatalogConfig;\n \n import org.apache.calcite.sql.SqlLiteral;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDk1NjIzMQ==", "url": "https://github.com/apache/flink/pull/11791#discussion_r414956231", "bodyText": "The comment is not correct now, we actually do not switch parser dialect with the setConformance interface.", "author": "danny0405", "createdAt": "2020-04-25T02:27:48Z", "path": "flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/**\n+ * Flink sql parser for hive dialect.\n+ *\n+ * <p>This module contains the DDLs and some custom DMLs for Apache Flink.\n+ *\n+ * <p>Most of the sql grammars belong to sql standard or Flink's dialect. To support\n+ * a new sql dialect, add a new sql conformance to\n+ * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance},\n+ * then use this sql conformance to make context aware decisions in parse block. See the usage of\n+ * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance#HIVE} in {@code parserimpls.ftl}.\n+ *\n+ * <p>To use a specific sql dialect for the parser, config the parser to the specific sql conformance\n+ * with a code snippet like below:\n+ * <blockquote><pre>\n+ *   SqlParser.create(source,\n+ *   \t\tSqlParser.configBuilder()\n+ *   \t\t\t.setParserFactory(parserImplFactory())\n+ * \t\t\t\t.setQuoting(Quoting.DOUBLE_QUOTE)\n+ * \t\t\t\t.setUnquotedCasing(Casing.TO_UPPER)\n+ * \t\t\t\t.setQuotedCasing(Casing.UNCHANGED)\n+ * \t\t\t\t.setConformance(conformance0) // the sql conformance you want use.\n+ * \t\t\t\t.build());\n+ * </pre></blockquote>", "originalCommit": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java b/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java\nindex d58895bb7f6..c9dcf2f44ac 100644\n--- a/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java\n+++ b/flink-table/flink-sql-parser-hive/src/main/java/org/apache/flink/sql/parser/hive/package-info.java\n\n@@ -18,20 +18,14 @@\n /**\n  * Flink sql parser for hive dialect.\n  *\n- * <p>This module contains the DDLs and some custom DMLs for Apache Flink.\n+ * <p>This module contains the DDLs and some custom DMLs for the Hive dialect.\n  *\n- * <p>Most of the sql grammars belong to sql standard or Flink's dialect. To support\n- * a new sql dialect, add a new sql conformance to\n- * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance},\n- * then use this sql conformance to make context aware decisions in parse block. See the usage of\n- * {@link org.apache.flink.sql.parser.validate.FlinkSqlConformance#HIVE} in {@code parserimpls.ftl}.\n- *\n- * <p>To use a specific sql dialect for the parser, config the parser to the specific sql conformance\n- * with a code snippet like below:\n+ * <p>To use a specific sql dialect for the parser, get the corresponding sql conformance and use it\n+ * with FlinkSqlParserImplFactory to create the parser like below:\n  * <blockquote><pre>\n  *   SqlParser.create(source,\n  *   \t\tSqlParser.configBuilder()\n- *   \t\t\t.setParserFactory(parserImplFactory())\n+ *   \t\t\t.setParserFactory(new FlinkSqlParserImplFactory(conformance0))\n  * \t\t\t\t.setQuoting(Quoting.DOUBLE_QUOTE)\n  * \t\t\t\t.setUnquotedCasing(Casing.TO_UPPER)\n  * \t\t\t\t.setQuotedCasing(Casing.UNCHANGED)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTIwNDI2Mg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415204262", "bodyText": "I can see HiveCatalog becomes big and big, can we split database related codes to a separate class?", "author": "JingsongLi", "createdAt": "2020-04-26T03:37:33Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -292,18 +303,62 @@ public void alterDatabase(String databaseName, CatalogDatabase newDatabase, bool\n \t\tcheckNotNull(newDatabase, \"newDatabase cannot be null\");\n \n \t\t// client.alterDatabase doesn't throw any exception if there is no existing database\n-\t\tif (!databaseExists(databaseName)) {\n+\t\tDatabase hiveDB;\n+\t\ttry {\n+\t\t\thiveDB = getHiveDatabase(databaseName);\n+\t\t} catch (DatabaseNotExistException e) {\n \t\t\tif (!ignoreIfNotExists) {\n \t\t\t\tthrow new DatabaseNotExistException(getName(), databaseName);\n \t\t\t}\n \n \t\t\treturn;\n \t\t}\n-\n-\t\tDatabase newHiveDatabase = instantiateHiveDatabase(databaseName, newDatabase);\n+\t\tMap<String, String> params = hiveDB.getParameters();", "originalCommit": "3a861a4778cf6b4347788fd7a6cfc6e6c5bd970a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 457f3997e2f..42a6e606386 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -280,22 +277,6 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t}\n \t}\n \n-\tprivate static Database instantiateHiveDatabase(String databaseName, CatalogDatabase database) {\n-\n-\t\tMap<String, String> properties = database.getProperties();\n-\n-\t\tboolean isGeneric = createObjectIsGeneric(properties);\n-\n-\t\tString dbLocationUri = isGeneric ? null : properties.remove(SqlCreateHiveDatabase.DATABASE_LOCATION_URI);\n-\n-\t\treturn new Database(\n-\t\t\t\tdatabaseName,\n-\t\t\t\tdatabase.getComment(),\n-\t\t\t\tdbLocationUri,\n-\t\t\t\tproperties\n-\t\t);\n-\t}\n-\n \t@Override\n \tpublic void alterDatabase(String databaseName, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n \t\t\tthrows DatabaseNotExistException, CatalogException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415274850", "bodyText": "Why the unparse SQL is different ?", "author": "danny0405", "createdAt": "2020-04-26T10:21:43Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +", "originalCommit": "f5a365a9ea932cb758f2888c119943173f809f57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3OTU1Mg==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415279552", "bodyText": "Creating a hive database requires explicit is_generic=false in the properties. But hive users won't write DDL like that, so SqlCreateHiveDatabase will automatically add it.", "author": "lirui-apache", "createdAt": "2020-04-26T10:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4NDU0Mw==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415284543", "bodyText": "I mean the dbproperties keyword.", "author": "danny0405", "createdAt": "2020-04-26T11:13:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDg1MA=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java b/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\nindex 13bfbdf0147..420003081b8 100644\n--- a/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\n+++ b/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\n\n@@ -61,47 +61,30 @@ public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n \t@Test\n \tpublic void testCreateDatabase() {\n \t\tsql(\"create database db1\")\n-\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\");\n \t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n \t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n-\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n-\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t\t\t\"COMMENT 'comment db1'\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/path/to/db1'\");\n \t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n-\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH DBPROPERTIES (\\n\" +\n \t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n-\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2'\\n\" +\n \t\t\t\t\t\t\")\");\n \t}\n \n \t@Test\n \tpublic void testAlterDatabase() {\n \t\tsql(\"alter database db1 set dbproperties('k1'='v1')\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_PROPS'\\n\" +\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET DBPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1'\\n\" +\n \t\t\t\t\t\t\")\");\n \t\tsql(\"alter database db1 set location '/new/path'\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_LOCATION',\\n\" +\n-\t\t\t\t\t\t\"  'database.location_uri' = '/new/path'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET LOCATION '/new/path'\");\n \t\tsql(\"alter database db1 set owner user user1\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_OWNER',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.type' = 'user',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.name' = 'USER1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET OWNER USER `USER1`\");\n \t\tsql(\"alter database db1 set owner role role1\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_OWNER',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.type' = 'role',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.name' = 'ROLE1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET OWNER ROLE `ROLE1`\");\n \t}\n \n \t@Test\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDkxMA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415274910", "bodyText": "The unparse SQL is different.", "author": "danny0405", "createdAt": "2020-04-26T10:22:13Z", "path": "flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser.hive;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.parser.SqlParserTest;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link FlinkHiveSqlParserImpl}.\n+ */\n+public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n+\n+\t@Override\n+\tprotected SqlParserImplFactory parserImplFactory() {\n+\t\treturn FlinkHiveSqlParserImpl.FACTORY;\n+\t}\n+\n+\t// overrides test methods that we don't support\n+\t@Override\n+\tpublic void testDescribeStatement() {\n+\t}\n+\n+\t@Override\n+\tpublic void testTableHintsInInsert() {\n+\t}\n+\n+\t@Override\n+\tpublic void testDescribeSchema() {\n+\t}\n+\n+\t@Test\n+\tpublic void testShowDatabases() {\n+\t\tsql(\"show databases\").ok(\"SHOW DATABASES\");\n+\t}\n+\n+\t@Test\n+\tpublic void testUseDatabase() {\n+\t\t// use database\n+\t\tsql(\"use db1\").ok(\"USE `DB1`\");\n+\t}\n+\n+\t@Test\n+\tpublic void testCreateDatabase() {\n+\t\tsql(\"create database db1\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n+\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n+\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n+\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\")\");\n+\t}\n+\n+\t@Test\n+\tpublic void testAlterDatabase() {\n+\t\tsql(\"alter database db1 set dbproperties('k1'='v1')\")\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n+\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_PROPS'\\n\" +", "originalCommit": "f5a365a9ea932cb758f2888c119943173f809f57", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MDE2MA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415280160", "bodyText": "This is similar to creating a DB -- the hive-specific semantics have to be encoded as properties. Flink only allows altering DB properties. But Hive supports altering DB location and owner. So we add alter.database.op in the properties to indicate which alter operation we want to perform.", "author": "lirui-apache", "createdAt": "2020-04-26T10:51:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI3NDkxMA=="}], "type": "inlineReview", "revised_code": {"commit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "chunk": "diff --git a/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java b/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\nindex 13bfbdf0147..420003081b8 100644\n--- a/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\n+++ b/flink-table/flink-sql-parser-hive/src/test/java/org/apache/flink/sql/parser/hive/FlinkHiveSqlParserImplTest.java\n\n@@ -61,47 +61,30 @@ public class FlinkHiveSqlParserImplTest extends SqlParserTest {\n \t@Test\n \tpublic void testCreateDatabase() {\n \t\tsql(\"create database db1\")\n-\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1`\");\n \t\tsql(\"create database db1 comment 'comment db1' location '/path/to/db1'\")\n \t\t\t\t.ok(\"CREATE DATABASE `DB1`\\n\" +\n-\t\t\t\t\t\t\"COMMENT 'comment db1' WITH (\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false',\\n\" +\n-\t\t\t\t\t\t\"  'database.location_uri' = '/path/to/db1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t\t\t\"COMMENT 'comment db1'\\n\" +\n+\t\t\t\t\t\t\"LOCATION '/path/to/db1'\");\n \t\tsql(\"create database db1 with dbproperties ('k1'='v1','k2'='v2')\")\n-\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH (\\n\" +\n+\t\t\t\t.ok(\"CREATE DATABASE `DB1` WITH DBPROPERTIES (\\n\" +\n \t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n-\t\t\t\t\t\t\"  'k2' = 'v2',\\n\" +\n-\t\t\t\t\t\t\"  'is_generic' = 'false'\\n\" +\n+\t\t\t\t\t\t\"  'k2' = 'v2'\\n\" +\n \t\t\t\t\t\t\")\");\n \t}\n \n \t@Test\n \tpublic void testAlterDatabase() {\n \t\tsql(\"alter database db1 set dbproperties('k1'='v1')\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'k1' = 'v1',\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_PROPS'\\n\" +\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET DBPROPERTIES (\\n\" +\n+\t\t\t\t\t\t\"  'k1' = 'v1'\\n\" +\n \t\t\t\t\t\t\")\");\n \t\tsql(\"alter database db1 set location '/new/path'\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_LOCATION',\\n\" +\n-\t\t\t\t\t\t\"  'database.location_uri' = '/new/path'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET LOCATION '/new/path'\");\n \t\tsql(\"alter database db1 set owner user user1\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_OWNER',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.type' = 'user',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.name' = 'USER1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET OWNER USER `USER1`\");\n \t\tsql(\"alter database db1 set owner role role1\")\n-\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET (\\n\" +\n-\t\t\t\t\t\t\"  'alter.database.op' = 'CHANGE_OWNER',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.type' = 'role',\\n\" +\n-\t\t\t\t\t\t\"  'database.owner.name' = 'ROLE1'\\n\" +\n-\t\t\t\t\t\t\")\");\n+\t\t\t\t.ok(\"ALTER DATABASE `DB1` SET OWNER ROLE `ROLE1`\");\n \t}\n \n \t@Test\n"}}, {"oid": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "url": "https://github.com/apache/flink/commit/a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "message": "fix unparse sql", "committedDate": "2020-04-26T13:08:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NTY0NA==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415475644", "bodyText": "We do not need to extend SqlParserImplFactory , how about just a tool class named FlinkSqlParserFactories and there is a method to return the factory by conformance FlinkSqlParserFactories#createFactory(SqlConformance) ?", "author": "danny0405", "createdAt": "2020-04-27T02:56:12Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.delegation;\n+\n+import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+import org.apache.flink.sql.parser.impl.FlinkSqlParserImpl;\n+import org.apache.flink.sql.parser.validate.FlinkSqlConformance;\n+\n+import org.apache.calcite.sql.parser.SqlAbstractParserImpl;\n+import org.apache.calcite.sql.parser.SqlParserImplFactory;\n+import org.apache.calcite.sql.validate.SqlConformance;\n+\n+import java.io.Reader;\n+\n+/**\n+ * A SqlParserImplFactory that creates the parser according to SqlConformance.\n+ */\n+public class FlinkSqlParserImplFactory implements SqlParserImplFactory {\n+\n+\tprivate final SqlConformance conformance;\n+\n+\tpublic FlinkSqlParserImplFactory(SqlConformance conformance) {\n+\t\tthis.conformance = conformance;\n+\t}\n+\n+\t@Override\n+\tpublic SqlAbstractParserImpl getParser(Reader stream) {\n+\t\tif (conformance == FlinkSqlConformance.HIVE) {\n+\t\t\treturn FlinkHiveSqlParserImpl.FACTORY.getParser(stream);\n+\t\t} else {\n+\t\t\treturn FlinkSqlParserImpl.FACTORY.getParser(stream);\n+\t\t}\n+\t}", "originalCommit": "a6a3a9e51fd9ba484d1ab4ccd0646dcacc0692ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ4Nzg1Mw==", "url": "https://github.com/apache/flink/pull/11791#discussion_r415487853", "bodyText": "OK, sounds good.", "author": "lirui-apache", "createdAt": "2020-04-27T03:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NTY0NA=="}], "type": "inlineReview", "revised_code": {"commit": "0fe9131d725743f48cc12d7c31f2904da2329b7b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java b/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/FlinkSqlParserImplFactory.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java\nrename to flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/FlinkSqlParserImplFactory.java\nindex 6d21e9bd04c..72ba1b1ea29 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/delegation/FlinkSqlParserImplFactory.java\n+++ b/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/FlinkSqlParserImplFactory.java\n\n@@ -16,9 +16,9 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.delegation;\n+package org.apache.flink.sql.parser;\n \n-import org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl;\n+import org.apache.flink.sql.parser.impl.FlinkHiveSqlParserImpl;\n import org.apache.flink.sql.parser.impl.FlinkSqlParserImpl;\n import org.apache.flink.sql.parser.validate.FlinkSqlConformance;\n \n"}}, {"oid": "0fe9131d725743f48cc12d7c31f2904da2329b7b", "url": "https://github.com/apache/flink/commit/0fe9131d725743f48cc12d7c31f2904da2329b7b", "message": "[FLINK-17210][sql-parser][hive] Implement database DDLs for Hive dialect", "committedDate": "2020-04-27T03:43:23Z", "type": "commit"}, {"oid": "5d8dd83cb42ee85a5bf268eaf9da194e57c2a29b", "url": "https://github.com/apache/flink/commit/5d8dd83cb42ee85a5bf268eaf9da194e57c2a29b", "message": "rebase", "committedDate": "2020-04-27T03:43:23Z", "type": "commit"}, {"oid": "559d777c9d6c72d75da299dfee565eebf16ae192", "url": "https://github.com/apache/flink/commit/559d777c9d6c72d75da299dfee565eebf16ae192", "message": "move hive parser to a separate module", "committedDate": "2020-04-27T03:43:23Z", "type": "commit"}, {"oid": "5bca4b1125bd4523102c6c222fff0ba2ae8817a2", "url": "https://github.com/apache/flink/commit/5bca4b1125bd4523102c6c222fff0ba2ae8817a2", "message": "address comments", "committedDate": "2020-04-27T03:43:23Z", "type": "commit"}, {"oid": "325f3813b762de41e5e248703b645644978f47d8", "url": "https://github.com/apache/flink/commit/325f3813b762de41e5e248703b645644978f47d8", "message": "fix unparse sql", "committedDate": "2020-04-27T03:43:23Z", "type": "commit"}, {"oid": "5ce52e3cda62c629df16d95ccb345d830b3a3531", "url": "https://github.com/apache/flink/commit/5ce52e3cda62c629df16d95ccb345d830b3a3531", "message": "add util class to create SqlParserImplFactory", "committedDate": "2020-04-27T03:53:57Z", "type": "commit"}, {"oid": "5ce52e3cda62c629df16d95ccb345d830b3a3531", "url": "https://github.com/apache/flink/commit/5ce52e3cda62c629df16d95ccb345d830b3a3531", "message": "add util class to create SqlParserImplFactory", "committedDate": "2020-04-27T03:53:57Z", "type": "forcePushed"}, {"oid": "93939641c80b0361c6e84a01588e631633b299eb", "url": "https://github.com/apache/flink/commit/93939641c80b0361c6e84a01588e631633b299eb", "message": "include flink-sql-parser-hive into blink planner jar", "committedDate": "2020-04-28T03:43:00Z", "type": "commit"}]}