{"pr_number": 12649, "pr_title": "[FLINK-18286] Implement type inference for GET/FLATTEN ", "pr_createdAt": "2020-06-15T06:57:23Z", "pr_url": "https://github.com/apache/flink/pull/12649", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MDk0OQ==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440250949", "bodyText": "remove false? the nullability is already excluded by LITERAL", "author": "twalthr", "createdAt": "2020-06-15T15:15:38Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java", "diffHunk": "@@ -972,13 +972,28 @@\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"flatten\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(sequence(InputTypeStrategies.COMPOSITE))\n+\t\t\t.outputTypeStrategy(callContext -> {\n+\t\t\t\tthrow new UnsupportedOperationException(\"FLATTEN should be resolved to GET expressions\");\n+\t\t\t})\n \t\t\t.build();\n \tpublic static final BuiltInFunctionDefinition GET =\n \t\tnew BuiltInFunctionDefinition.Builder()\n \t\t\t.name(\"get\")\n \t\t\t.kind(OTHER)\n-\t\t\t.outputTypeStrategy(TypeStrategies.MISSING)\n+\t\t\t.inputTypeStrategy(\n+\t\t\t\tsequence(\n+\t\t\t\t\tInputTypeStrategies.COMPOSITE,\n+\t\t\t\t\tand(\n+\t\t\t\t\t\tInputTypeStrategies.LITERAL,\n+\t\t\t\t\t\tor(\n+\t\t\t\t\t\t\tlogical(LogicalTypeRoot.INTEGER),\n+\t\t\t\t\t\t\tlogical(LogicalTypeFamily.CHARACTER_STRING, false)", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java\nindex e93ba05e62..1dba8caee4 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/functions/BuiltInFunctionDefinitions.java\n\n@@ -988,7 +1098,7 @@ public final class BuiltInFunctionDefinitions {\n \t\t\t\t\t\tInputTypeStrategies.LITERAL,\n \t\t\t\t\t\tor(\n \t\t\t\t\t\t\tlogical(LogicalTypeRoot.INTEGER),\n-\t\t\t\t\t\t\tlogical(LogicalTypeFamily.CHARACTER_STRING, false)\n+\t\t\t\t\t\t\tlogical(LogicalTypeFamily.CHARACTER_STRING)\n \t\t\t\t\t\t)\n \t\t\t\t\t)\n \t\t\t\t)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MjM2Mg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440252362", "bodyText": "nit: move to separate class like all other argument strategies", "author": "twalthr", "createdAt": "2020-06-15T15:17:36Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\nindex 8d855ef767..2437fab376 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\n\n@@ -204,32 +205,7 @@ public final class InputTypeStrategies {\n \t/**\n \t * Strategy that checks that the argument has a composite type.\n \t */\n-\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {\n-\t\t@Override\n-\t\tpublic Optional<DataType> inferArgumentType(\n-\t\t\t\tCallContext callContext,\n-\t\t\t\tint argumentPos,\n-\t\t\t\tboolean throwOnFailure) {\n-\t\t\t\tDataType dataType = callContext.getArgumentDataTypes().get(argumentPos);\n-\t\t\tif (!LogicalTypeChecks.isCompositeType(dataType.getLogicalType())) {\n-\t\t\t\tif (throwOnFailure) {\n-\t\t\t\t\tthrow callContext.newValidationError(\n-\t\t\t\t\t\t\"A composite type expected. Got: %s\",\n-\t\t\t\t\t\tdataType);\n-\t\t\t\t}\n-\t\t\t\treturn Optional.empty();\n-\t\t\t}\n-\n-\t\t\treturn Optional.of(dataType);\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Signature.Argument getExpectedArgument(\n-\t\t\t\tFunctionDefinition functionDefinition,\n-\t\t\t\tint argumentPos) {\n-\t\t\treturn Signature.Argument.of(\"<COMPOSITE>\");\n-\t\t}\n-\t};\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new CompositeArgumentTypeStrategy();\n \n \t/**\n \t * Strategy for an argument that corresponds to an explicitly defined type casting.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1MzQ5MA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440253490", "bodyText": "nit: wrong indention", "author": "twalthr", "createdAt": "2020-06-15T15:19:11Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java", "diffHunk": "@@ -198,6 +201,36 @@ public static InputTypeStrategy comparable(\n \t */\n \tpublic static final LiteralArgumentTypeStrategy LITERAL_OR_NULL = new LiteralArgumentTypeStrategy(true);\n \n+\t/**\n+\t * Strategy that checks that the argument has a composite type.\n+\t */\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {\n+\t\t@Override\n+\t\tpublic Optional<DataType> inferArgumentType(\n+\t\t\t\tCallContext callContext,\n+\t\t\t\tint argumentPos,\n+\t\t\t\tboolean throwOnFailure) {\n+\t\t\t\tDataType dataType = callContext.getArgumentDataTypes().get(argumentPos);", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\nindex 8d855ef767..2437fab376 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/InputTypeStrategies.java\n\n@@ -204,32 +205,7 @@ public final class InputTypeStrategies {\n \t/**\n \t * Strategy that checks that the argument has a composite type.\n \t */\n-\tpublic static final ArgumentTypeStrategy COMPOSITE = new ArgumentTypeStrategy() {\n-\t\t@Override\n-\t\tpublic Optional<DataType> inferArgumentType(\n-\t\t\t\tCallContext callContext,\n-\t\t\t\tint argumentPos,\n-\t\t\t\tboolean throwOnFailure) {\n-\t\t\t\tDataType dataType = callContext.getArgumentDataTypes().get(argumentPos);\n-\t\t\tif (!LogicalTypeChecks.isCompositeType(dataType.getLogicalType())) {\n-\t\t\t\tif (throwOnFailure) {\n-\t\t\t\t\tthrow callContext.newValidationError(\n-\t\t\t\t\t\t\"A composite type expected. Got: %s\",\n-\t\t\t\t\t\tdataType);\n-\t\t\t\t}\n-\t\t\t\treturn Optional.empty();\n-\t\t\t}\n-\n-\t\t\treturn Optional.of(dataType);\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Signature.Argument getExpectedArgument(\n-\t\t\t\tFunctionDefinition functionDefinition,\n-\t\t\t\tint argumentPos) {\n-\t\t\treturn Signature.Argument.of(\"<COMPOSITE>\");\n-\t\t}\n-\t};\n+\tpublic static final ArgumentTypeStrategy COMPOSITE = new CompositeArgumentTypeStrategy();\n \n \t/**\n \t * Strategy for an argument that corresponds to an explicitly defined type casting.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1OTA5Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440259097", "bodyText": "I don't like that we use TableSchema here. This class doesn't belong here. Furthermore, it does not support distinct type yet. Maybe we can just introduce a DataTypeUtils.getFieldType(int) and DataTypeUtils.getFieldType(name). Can we make it support distinct type (an AtomicDataType)?", "author": "twalthr", "createdAt": "2020-06-15T15:27:02Z", "path": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java", "diffHunk": "@@ -315,6 +317,36 @@ public static TypeStrategy nullable(TypeStrategy initialStrategy) {\n \t\treturn Optional.of(fromLogicalToDataType(inferredType));\n \t};\n \n+\t/**\n+\t * Type strategy that returns a type of a field nested inside a composite type that is described by the second argument.\n+\t * The second argument must be a literal that describes either the nested field name or index.\n+\t */\n+\tpublic static final TypeStrategy GET = callContext -> {\n+\t\tList<DataType> argumentDataTypes = callContext.getArgumentDataTypes();\n+\t\tDataType rowDataType = argumentDataTypes.get(0);\n+\t\tTableSchema nestedSchema = DataTypeUtils.expandCompositeTypeToSchema(rowDataType);", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDg5NDEzOQ==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440894139", "bodyText": "I think it does already support the DISTINCT type. Moreover AtomicDataType can not hold a composite type. AtomicDataType can not hold the bridging classes of nested fields. IMO Distinct type can be any of FieldsDataType, AtomicDataType, 'KeyValueDataType` etc.\nThe way we support DISTINCT type is that we do not check the LogicalTypeRoot in expandCompositeTypeToSchema, but we check only for the FieldsDataType.", "author": "dawidwys", "createdAt": "2020-06-16T14:27:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI1OTA5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java\nindex a4af9c6ec2..feea0375a8 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeStrategies.java\n\n@@ -317,6 +331,33 @@ public final class TypeStrategies {\n \t\treturn Optional.of(fromLogicalToDataType(inferredType));\n \t};\n \n+\t/**\n+\t * Type strategy that returns the type of a string concatenation. It assumes that the first two\n+\t * arguments are of the same family of either {@link LogicalTypeFamily#BINARY_STRING} or\n+\t * {@link LogicalTypeFamily#CHARACTER_STRING}.\n+\t */\n+\tpublic static final TypeStrategy STRING_CONCAT = callContext -> {\n+\t\tfinal List<DataType> argumentDataTypes = callContext.getArgumentDataTypes();\n+\t\tfinal LogicalType type1 = argumentDataTypes.get(0).getLogicalType();\n+\t\tfinal LogicalType type2 = argumentDataTypes.get(1).getLogicalType();\n+\t\tint length = getLength(type1) + getLength(type2);\n+\t\t// handle overflow\n+\t\tif (length < 0) {\n+\t\t\tlength = CharType.MAX_LENGTH;\n+\t\t}\n+\t\tfinal LogicalType minimumType;\n+\t\tif (hasFamily(type1, LogicalTypeFamily.CHARACTER_STRING) || hasFamily(type2, LogicalTypeFamily.CHARACTER_STRING)) {\n+\t\t\tminimumType = new CharType(false, length);\n+\t\t} else if (hasFamily(type1, LogicalTypeFamily.BINARY_STRING) || hasFamily(type2, LogicalTypeFamily.BINARY_STRING)) {\n+\t\t\tminimumType = new BinaryType(false, length);\n+\t\t} else {\n+\t\t\treturn Optional.empty();\n+\t\t}\n+\t\t// deal with nullability handling and varying semantics\n+\t\treturn findCommonType(Arrays.asList(type1, type2, minimumType))\n+\t\t\t.map(TypeConversions::fromLogicalToDataType);\n+\t};\n+\n \t/**\n \t * Type strategy that returns a type of a field nested inside a composite type that is described by the second argument.\n \t * The second argument must be a literal that describes either the nested field name or index.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI2MjI0Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440262247", "bodyText": "shall we upgrade the test base to accept AbstractDataType similar to DataStructureConverterTest? it would make the code more readable.", "author": "twalthr", "createdAt": "2020-06-15T15:31:30Z", "path": "flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java", "diffHunk": "@@ -582,7 +587,43 @@\n \t\t\t\t\t\t\t\t\"My constraint says %s must be nullable.\",\n \t\t\t\t\t\t\t\targs -> args.get(0).getLogicalType().isNullable()))))\n \t\t\t\t.calledWithArgumentTypes(DataTypes.BOOLEAN().notNull())\n-\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\")\n+\t\t\t\t.expectErrorMessage(\"My constraint says BOOLEAN NOT NULL must be nullable.\"),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with ROW\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT())))\n+\t\t\t\t.expectSignature(\"f(<COMPOSITE>)\")\n+\t\t\t\t.expectArgumentTypes(DataTypes.ROW(DataTypes.FIELD(\"f0\", DataTypes.BIGINT()))),\n+\n+\t\t\tTestSpec\n+\t\t\t\t.forStrategy(\n+\t\t\t\t\t\"Composite type strategy with STRUCTURED type\",\n+\t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n+\t\t\t\t)\n+\t\t\t\t.calledWithArgumentTypes(new FieldsDataType(", "originalCommit": "ba5bc62416b82d8e6f1be2c848242de2d3c546e5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java\nindex ab47602ba4..57a48af954 100644\n--- a/flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java\n+++ b/flink-table/flink-table-common/src/test/java/org/apache/flink/table/types/inference/InputTypeStrategiesTest.java\n\n@@ -603,27 +598,16 @@ public class InputTypeStrategiesTest extends InputTypeStrategiesTestBase {\n \t\t\t\t\t\"Composite type strategy with STRUCTURED type\",\n \t\t\t\t\tsequence(InputTypeStrategies.COMPOSITE)\n \t\t\t\t)\n-\t\t\t\t.calledWithArgumentTypes(new FieldsDataType(\n-\t\t\t\t\tStructuredType.newBuilder(ObjectIdentifier.of(\"cat\", \"db\", \"type\"))\n-\t\t\t\t\t\t.attributes(Collections.singletonList(\n-\t\t\t\t\t\t\tnew StructuredType.StructuredAttribute(\"f0\", new BigIntType(false))\n-\t\t\t\t\t\t))\n-\t\t\t\t\t\t.build(),\n-\t\t\t\t\tCollections.singletonList(\n-\t\t\t\t\t\tDataTypes.BIGINT().notNull()\n-\t\t\t\t\t)\n-\t\t\t\t).notNull())\n+\t\t\t\t.calledWithArgumentTypes(DataTypes.of(SimpleStructuredType.class).notNull())\n \t\t\t\t.expectSignature(\"f(<COMPOSITE>)\")\n-\t\t\t\t.expectArgumentTypes(new FieldsDataType(\n-\t\t\t\t\tStructuredType.newBuilder(ObjectIdentifier.of(\"cat\", \"db\", \"type\"))\n-\t\t\t\t\t\t.attributes(Collections.singletonList(\n-\t\t\t\t\t\t\tnew StructuredType.StructuredAttribute(\"f0\", new BigIntType(false))\n-\t\t\t\t\t\t))\n-\t\t\t\t\t\t.build(),\n-\t\t\t\t\tCollections.singletonList(\n-\t\t\t\t\t\tDataTypes.BIGINT().notNull()\n-\t\t\t\t\t)\n-\t\t\t\t).notNull())\n+\t\t\t\t.expectArgumentTypes(DataTypes.of(SimpleStructuredType.class).notNull())\n \t\t);\n \t}\n+\n+\t/**\n+\t * Simple pojo that should be converted to a Structured type.\n+\t */\n+\tpublic static class SimpleStructuredType {\n+\t\tpublic long f0;\n+\t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MTcwMA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440771700", "bodyText": "use the newly introduced testResult such that a result must be defined only once", "author": "twalthr", "createdAt": "2020-06-16T11:12:43Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\nindex ef4bf81e80..380ffa0811 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n\n@@ -62,17 +62,17 @@ import static org.junit.Assert.assertThat;\n @RunWith(Suite.class)\n @Suite.SuiteClasses(\n \t{\n-\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n-\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessFromTable.class,\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessAfterCall.class\n \t}\n )\n public class CompositeTypeAccessExpressionITCase {\n \n \t/**\n-\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * Regular tests. See also {@link FieldAccessAfterCall} for tests that access a nested field of an expression or\n \t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n \t */\n-\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\tpublic static class FieldAccessFromTable extends BuiltInFunctionTestBase {\n \t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n \t\tpublic static List<TestSpec> testData() {\n \t\t\treturn Arrays.asList(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MjgyNg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440772826", "bodyText": "should we port AT as well or in a separate PR?", "author": "twalthr", "createdAt": "2020-06-16T11:15:04Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc5NDc3Nw==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440794777", "bodyText": "I have it in a different branch. I found this PR already quite big.", "author": "dawidwys", "createdAt": "2020-06-16T11:59:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3MjgyNg=="}], "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\nindex ef4bf81e80..380ffa0811 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n\n@@ -62,17 +62,17 @@ import static org.junit.Assert.assertThat;\n @RunWith(Suite.class)\n @Suite.SuiteClasses(\n \t{\n-\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n-\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessFromTable.class,\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessAfterCall.class\n \t}\n )\n public class CompositeTypeAccessExpressionITCase {\n \n \t/**\n-\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * Regular tests. See also {@link FieldAccessAfterCall} for tests that access a nested field of an expression or\n \t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n \t */\n-\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\tpublic static class FieldAccessFromTable extends BuiltInFunctionTestBase {\n \t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n \t\tpublic static List<TestSpec> testData() {\n \t\t\treturn Arrays.asList(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc3NjQ4Mg==", "url": "https://github.com/apache/flink/pull/12649#discussion_r440776482", "bodyText": "nit: very good tests in general but can we rename the two suits to FieldAccessAfterCall and FieldAccessFromTable or similar to highlight the difference?", "author": "twalthr", "createdAt": "2020-06-16T11:22:32Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.expressions;\n+\n+import org.apache.flink.table.annotation.DataTypeHint;\n+import org.apache.flink.table.annotation.FunctionHint;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n+import org.apache.flink.table.functions.ScalarFunction;\n+import org.apache.flink.table.functions.TableFunction;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Suite;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.flink.table.api.DataTypes.ARRAY;\n+import static org.apache.flink.table.api.DataTypes.BIGINT;\n+import static org.apache.flink.table.api.DataTypes.FIELD;\n+import static org.apache.flink.table.api.DataTypes.MAP;\n+import static org.apache.flink.table.api.DataTypes.ROW;\n+import static org.apache.flink.table.api.DataTypes.STRING;\n+import static org.apache.flink.table.api.Expressions.$;\n+import static org.apache.flink.table.api.Expressions.call;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Tests for functions that access nested fields/elements of composite/collection types.\n+ */\n+@RunWith(Suite.class)\n+@Suite.SuiteClasses(\n+\t{\n+\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n+\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t}\n+)\n+public class CompositeTypeAccessExpressionITCase {\n+\n+\t/**\n+\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n+\t */\n+\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n+\t\tpublic static List<TestSpec> testData() {\n+\t\t\treturn Arrays.asList(\n+\n+\t\t\t\t// Actually in case of SQL it does not use the GET method, but\n+\t\t\t\t// a custom logic for accessing nested fields of a Table.\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.GET)\n+\t\t\t\t\t.onFieldsWithData(null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t.testTableApiResult($(\"f0\").get(\"nested\"), null, BIGINT().nullable())\n+\t\t\t\t\t.testTableApiResult($(\"f1\").get(\"nested\"), 1L, BIGINT().notNull())\n+\t\t\t\t\t.testSqlResult(\"f0.nested\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1.nested\", 1L, BIGINT().notNull()),\n+\n+\t\t\t\t// In Calcite it maps to FlinkSqlOperatorTable.ITEM\n+\t\t\t\tTestSpec.forFunction(BuiltInFunctionDefinitions.AT)\n+\t\t\t\t\t.onFieldsWithData(null, new int[] {1}, null, singletonMap(\"nested\", 1), null, Row.of(1))\n+\t\t\t\t\t.andDataTypes(\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tARRAY(BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).nullable(),\n+\t\t\t\t\t\tMAP(STRING(), BIGINT().notNull()).notNull(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).nullable(),\n+\t\t\t\t\t\tROW(FIELD(\"nested\", BIGINT().notNull())).notNull()\n+\t\t\t\t\t)\n+\t\t\t\t\t// accessing elements of MAP or ARRAY is a runtime operations,\n+\t\t\t\t\t// we do not know about the size or contents during the inference\n+\t\t\t\t\t// therefore the results are always nullable\n+\t\t\t\t\t.testSqlResult(\"f0[1]\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f1[1]\", 1L, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f2['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f3['nested']\", 1L, BIGINT().nullable())\n+\n+\t\t\t\t\t// we know all the fields of a type up front, therefore we can\n+\t\t\t\t\t// derive more accurate types during the inference\n+\t\t\t\t\t.testSqlResult(\"f4['nested']\", null, BIGINT().nullable())\n+\t\t\t\t\t.testSqlResult(\"f5['nested']\", 1L, BIGINT().notNull())\n+\t\t\t);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * A class for customized tests.\n+\t */\n+\tpublic static class CallFieldAccess {", "originalCommit": "b3dd92d0c41cee2472bb521eec89741c42bc05cd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "326222e8eb85270297876eba22ffda57ebe592c3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\nindex ef4bf81e80..380ffa0811 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/expressions/CompositeTypeAccessExpressionITCase.java\n\n@@ -62,17 +62,17 @@ import static org.junit.Assert.assertThat;\n @RunWith(Suite.class)\n @Suite.SuiteClasses(\n \t{\n-\t\tCompositeTypeAccessExpressionITCase.TableFieldAccess.class,\n-\t\tCompositeTypeAccessExpressionITCase.CallFieldAccess.class\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessFromTable.class,\n+\t\tCompositeTypeAccessExpressionITCase.FieldAccessAfterCall.class\n \t}\n )\n public class CompositeTypeAccessExpressionITCase {\n \n \t/**\n-\t * Regular tests. See also {@link CallFieldAccess} for tests that access a nested field of an expression or\n+\t * Regular tests. See also {@link FieldAccessAfterCall} for tests that access a nested field of an expression or\n \t * for {@link BuiltInFunctionDefinitions#FLATTEN} which produces multiple columns from a single one.\n \t */\n-\tpublic static class TableFieldAccess extends BuiltInFunctionTestBase {\n+\tpublic static class FieldAccessFromTable extends BuiltInFunctionTestBase {\n \t\t@Parameterized.Parameters(name = \"{index}: {0}\")\n \t\tpublic static List<TestSpec> testData() {\n \t\t\treturn Arrays.asList(\n"}}, {"oid": "326222e8eb85270297876eba22ffda57ebe592c3", "url": "https://github.com/apache/flink/commit/326222e8eb85270297876eba22ffda57ebe592c3", "message": "Comments addressed", "committedDate": "2020-06-16T14:32:47Z", "type": "forcePushed"}, {"oid": "67905cf92819be26e49afbcf38794e340cca8e9c", "url": "https://github.com/apache/flink/commit/67905cf92819be26e49afbcf38794e340cca8e9c", "message": "Comments addressed", "committedDate": "2020-06-17T07:16:46Z", "type": "forcePushed"}, {"oid": "4588093f97836f4f04c7c2c9992ea5049bf71c4c", "url": "https://github.com/apache/flink/commit/4588093f97836f4f04c7c2c9992ea5049bf71c4c", "message": "Comments addressed", "committedDate": "2020-06-17T08:59:37Z", "type": "forcePushed"}, {"oid": "43596996d023eb47c9589f95d8baf8a3cee83016", "url": "https://github.com/apache/flink/commit/43596996d023eb47c9589f95d8baf8a3cee83016", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-06-18T09:16:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyODI4MA==", "url": "https://github.com/apache/flink/pull/12649#discussion_r443528280", "bodyText": "In Calcite, when the record type is nullable, the attributes are always nullable, so, this fix is not that necessary from the Calcite side.", "author": "danny0405", "createdAt": "2020-06-22T12:39:36Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.calcite.sql.fun;\n+\n+import org.apache.calcite.rel.type.RelDataType;\n+import org.apache.calcite.rel.type.RelDataTypeFactory;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.sql.SqlCall;\n+import org.apache.calcite.sql.SqlCallBinding;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlOperandCountRange;\n+import org.apache.calcite.sql.SqlOperatorBinding;\n+import org.apache.calcite.sql.SqlSpecialOperator;\n+import org.apache.calcite.sql.SqlUtil;\n+import org.apache.calcite.sql.SqlWriter;\n+import org.apache.calcite.sql.parser.SqlParserPos;\n+import org.apache.calcite.sql.type.OperandTypes;\n+import org.apache.calcite.sql.type.SqlOperandCountRanges;\n+import org.apache.calcite.sql.type.SqlSingleOperandTypeChecker;\n+import org.apache.calcite.sql.type.SqlTypeFamily;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.calcite.sql.util.SqlBasicVisitor;\n+import org.apache.calcite.sql.util.SqlVisitor;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+import org.apache.calcite.sql.validate.SqlValidatorScope;\n+import org.apache.calcite.sql.validate.SqlValidatorUtil;\n+import org.apache.calcite.util.Litmus;\n+import org.apache.calcite.util.Static;\n+\n+import java.util.Arrays;\n+\n+/**\n+ * The dot operator {@code .}, used to access a field of a\n+ * record. For example, {@code a.b}.\n+ *\n+ * <p>This class was copied over from Calcite to fix the derived type.\n+ * If the ROW is nullable force the accessed field to be nullable as well.\n+ */\n+public class SqlDotOperator extends SqlSpecialOperator {\n+  SqlDotOperator() {\n+    super(\"DOT\", SqlKind.DOT, 100, true, null, null, null);\n+  }\n+\n+  @Override public ReduceResult reduceExpr(int ordinal, TokenSequence list) {\n+    SqlNode left = list.node(ordinal - 1);\n+    SqlNode right = list.node(ordinal + 1);\n+    return new ReduceResult(ordinal - 1,\n+        ordinal + 2,\n+        createCall(\n+            SqlParserPos.sum(\n+                Arrays.asList(left.getParserPosition(),\n+                    right.getParserPosition(),\n+                    list.pos(ordinal))),\n+            left,\n+            right));\n+  }\n+\n+  @Override public void unparse(SqlWriter writer, SqlCall call, int leftPrec,\n+      int rightPrec) {\n+    final SqlWriter.Frame frame =\n+        writer.startList(SqlWriter.FrameTypeEnum.IDENTIFIER);\n+    call.operand(0).unparse(writer, leftPrec, 0);\n+    writer.sep(\".\");\n+    call.operand(1).unparse(writer, 0, 0);\n+    writer.endList(frame);\n+  }\n+\n+  @Override public SqlOperandCountRange getOperandCountRange() {\n+    return SqlOperandCountRanges.of(2);\n+  }\n+\n+  @Override public <R> void acceptCall(SqlVisitor<R> visitor, SqlCall call,\n+      boolean onlyExpressions, SqlBasicVisitor.ArgHandler<R> argHandler) {\n+    if (onlyExpressions) {\n+      // Do not visit operands[1] -- it is not an expression.\n+      argHandler.visitChild(visitor, call, 0, call.operand(0));\n+    } else {\n+      super.acceptCall(visitor, call, onlyExpressions, argHandler);\n+    }\n+  }\n+\n+  @Override public RelDataType deriveType(SqlValidator validator,\n+      SqlValidatorScope scope, SqlCall call) {\n+    final SqlNode operand = call.getOperandList().get(0);\n+    final RelDataType nodeType =\n+        validator.deriveType(scope, operand);\n+    assert nodeType != null;\n+    if (!nodeType.isStruct()) {\n+      throw SqlUtil.newContextException(operand.getParserPosition(),\n+          Static.RESOURCE.incompatibleTypes());\n+    }\n+\n+    final SqlNode fieldId = call.operand(1);\n+    final String fieldName = fieldId.toString();\n+    final RelDataTypeField field =\n+        nodeType.getField(fieldName, false, false);\n+    if (field == null) {\n+      throw SqlUtil.newContextException(fieldId.getParserPosition(),\n+          Static.RESOURCE.unknownField(fieldName));\n+    }\n+    RelDataType type = field.getType();\n+    if (nodeType.isNullable() && !type.isNullable()) {\n+        type = validator.getTypeFactory().createTypeWithNullability(type, true);\n+    }", "originalCommit": "43596996d023eb47c9589f95d8baf8a3cee83016", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cdbf1943f1d1d221a527d8b1701213a25870c710", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java\ndeleted file mode 100644\nindex b7aaf61443..0000000000\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/calcite/sql/fun/SqlDotOperator.java\n+++ /dev/null\n\n@@ -1,194 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to you under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.calcite.sql.fun;\n-\n-import org.apache.calcite.rel.type.RelDataType;\n-import org.apache.calcite.rel.type.RelDataTypeFactory;\n-import org.apache.calcite.rel.type.RelDataTypeField;\n-import org.apache.calcite.sql.SqlCall;\n-import org.apache.calcite.sql.SqlCallBinding;\n-import org.apache.calcite.sql.SqlKind;\n-import org.apache.calcite.sql.SqlNode;\n-import org.apache.calcite.sql.SqlOperandCountRange;\n-import org.apache.calcite.sql.SqlOperatorBinding;\n-import org.apache.calcite.sql.SqlSpecialOperator;\n-import org.apache.calcite.sql.SqlUtil;\n-import org.apache.calcite.sql.SqlWriter;\n-import org.apache.calcite.sql.parser.SqlParserPos;\n-import org.apache.calcite.sql.type.OperandTypes;\n-import org.apache.calcite.sql.type.SqlOperandCountRanges;\n-import org.apache.calcite.sql.type.SqlSingleOperandTypeChecker;\n-import org.apache.calcite.sql.type.SqlTypeFamily;\n-import org.apache.calcite.sql.type.SqlTypeName;\n-import org.apache.calcite.sql.util.SqlBasicVisitor;\n-import org.apache.calcite.sql.util.SqlVisitor;\n-import org.apache.calcite.sql.validate.SqlValidator;\n-import org.apache.calcite.sql.validate.SqlValidatorScope;\n-import org.apache.calcite.sql.validate.SqlValidatorUtil;\n-import org.apache.calcite.util.Litmus;\n-import org.apache.calcite.util.Static;\n-\n-import java.util.Arrays;\n-\n-/**\n- * The dot operator {@code .}, used to access a field of a\n- * record. For example, {@code a.b}.\n- *\n- * <p>This class was copied over from Calcite to fix the derived type.\n- * If the ROW is nullable force the accessed field to be nullable as well.\n- */\n-public class SqlDotOperator extends SqlSpecialOperator {\n-  SqlDotOperator() {\n-    super(\"DOT\", SqlKind.DOT, 100, true, null, null, null);\n-  }\n-\n-  @Override public ReduceResult reduceExpr(int ordinal, TokenSequence list) {\n-    SqlNode left = list.node(ordinal - 1);\n-    SqlNode right = list.node(ordinal + 1);\n-    return new ReduceResult(ordinal - 1,\n-        ordinal + 2,\n-        createCall(\n-            SqlParserPos.sum(\n-                Arrays.asList(left.getParserPosition(),\n-                    right.getParserPosition(),\n-                    list.pos(ordinal))),\n-            left,\n-            right));\n-  }\n-\n-  @Override public void unparse(SqlWriter writer, SqlCall call, int leftPrec,\n-      int rightPrec) {\n-    final SqlWriter.Frame frame =\n-        writer.startList(SqlWriter.FrameTypeEnum.IDENTIFIER);\n-    call.operand(0).unparse(writer, leftPrec, 0);\n-    writer.sep(\".\");\n-    call.operand(1).unparse(writer, 0, 0);\n-    writer.endList(frame);\n-  }\n-\n-  @Override public SqlOperandCountRange getOperandCountRange() {\n-    return SqlOperandCountRanges.of(2);\n-  }\n-\n-  @Override public <R> void acceptCall(SqlVisitor<R> visitor, SqlCall call,\n-      boolean onlyExpressions, SqlBasicVisitor.ArgHandler<R> argHandler) {\n-    if (onlyExpressions) {\n-      // Do not visit operands[1] -- it is not an expression.\n-      argHandler.visitChild(visitor, call, 0, call.operand(0));\n-    } else {\n-      super.acceptCall(visitor, call, onlyExpressions, argHandler);\n-    }\n-  }\n-\n-  @Override public RelDataType deriveType(SqlValidator validator,\n-      SqlValidatorScope scope, SqlCall call) {\n-    final SqlNode operand = call.getOperandList().get(0);\n-    final RelDataType nodeType =\n-        validator.deriveType(scope, operand);\n-    assert nodeType != null;\n-    if (!nodeType.isStruct()) {\n-      throw SqlUtil.newContextException(operand.getParserPosition(),\n-          Static.RESOURCE.incompatibleTypes());\n-    }\n-\n-    final SqlNode fieldId = call.operand(1);\n-    final String fieldName = fieldId.toString();\n-    final RelDataTypeField field =\n-        nodeType.getField(fieldName, false, false);\n-    if (field == null) {\n-      throw SqlUtil.newContextException(fieldId.getParserPosition(),\n-          Static.RESOURCE.unknownField(fieldName));\n-    }\n-    RelDataType type = field.getType();\n-    if (nodeType.isNullable() && !type.isNullable()) {\n-        type = validator.getTypeFactory().createTypeWithNullability(type, true);\n-    }\n-\n-    // Validate and determine coercibility and resulting collation\n-    // name of binary operator if needed.\n-    type = adjustType(validator, call, type);\n-    SqlValidatorUtil.checkCharsetAndCollateConsistentIfCharType(type);\n-    return type;\n-  }\n-\n-  public void validateCall(\n-      SqlCall call,\n-      SqlValidator validator,\n-      SqlValidatorScope scope,\n-      SqlValidatorScope operandScope) {\n-    assert call.getOperator() == this;\n-    // Do not visit call.getOperandList().get(1) here.\n-    // call.getOperandList().get(1) will be validated when deriveType() is called.\n-    call.getOperandList().get(0).validateExpr(validator, operandScope);\n-  }\n-\n-  @Override public boolean checkOperandTypes(SqlCallBinding callBinding,\n-      boolean throwOnFailure) {\n-    final SqlNode left = callBinding.operand(0);\n-    final SqlNode right = callBinding.operand(1);\n-    final RelDataType type =\n-        callBinding.getValidator().deriveType(callBinding.getScope(), left);\n-    if (type.getSqlTypeName() != SqlTypeName.ROW) {\n-      return false;\n-    } else if (type.getSqlIdentifier().isStar()) {\n-      return false;\n-    }\n-    final RelDataType operandType = callBinding.getOperandType(0);\n-    final SqlSingleOperandTypeChecker checker = getChecker(operandType);\n-    // Actually operand0 always comes from parsing the SqlIdentifier, so there\n-    // is no need to make implicit type coercion.\n-    return checker.checkSingleOperandType(callBinding, right, 0,\n-        throwOnFailure);\n-  }\n-\n-  private SqlSingleOperandTypeChecker getChecker(RelDataType operandType) {\n-    switch (operandType.getSqlTypeName()) {\n-    case ROW:\n-      return OperandTypes.family(SqlTypeFamily.STRING);\n-    default:\n-      throw new AssertionError(operandType.getSqlTypeName());\n-    }\n-  }\n-\n-  @Override public boolean validRexOperands(final int count, final Litmus litmus) {\n-    return litmus.fail(\"DOT is valid only for SqlCall not for RexCall\");\n-  }\n-\n-  @Override public String getAllowedSignatures(String name) {\n-    return \"<A>.<B>\";\n-  }\n-\n-  @Override public RelDataType inferReturnType(SqlOperatorBinding opBinding) {\n-    final RelDataTypeFactory typeFactory = opBinding.getTypeFactory();\n-    final RelDataType recordType = opBinding.getOperandType(0);\n-    switch (recordType.getSqlTypeName()) {\n-    case ROW:\n-      final String fieldName =\n-          opBinding.getOperandLiteralValue(1, String.class);\n-      final RelDataType type = opBinding.getOperandType(0)\n-          .getField(fieldName, false, false)\n-          .getType();\n-      if (recordType.isNullable()) {\n-        return typeFactory.createTypeWithNullability(type, true);\n-      } else {\n-        return type;\n-      }\n-    default:\n-      throw new AssertionError();\n-    }\n-  }\n-}\n"}}, {"oid": "cdbf1943f1d1d221a527d8b1701213a25870c710", "url": "https://github.com/apache/flink/commit/cdbf1943f1d1d221a527d8b1701213a25870c710", "message": "[hotfix] Suppport ITEM for ROW types.", "committedDate": "2020-07-16T12:03:54Z", "type": "commit"}, {"oid": "4dd633a9c0621b97d075c386d77b69abfaf408fe", "url": "https://github.com/apache/flink/commit/4dd633a9c0621b97d075c386d77b69abfaf408fe", "message": "[FLINK-18286] Fix type inference for GET & AT Calcite functions\n\nThis commit fixes how Calcite infers/derives types for accessing nested columns of a ROW type.", "committedDate": "2020-07-16T12:08:19Z", "type": "commit"}, {"oid": "6ba85a29c018561157e8591f4c4713ea12ae5c5b", "url": "https://github.com/apache/flink/commit/6ba85a29c018561157e8591f4c4713ea12ae5c5b", "message": "[hotfix] Reuse a Flink cluster for expressions tests.", "committedDate": "2020-07-16T12:08:22Z", "type": "commit"}, {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-07-16T12:12:31Z", "type": "commit"}, {"oid": "93f58e49ad79331331d4d85bbe7720ecfe39f968", "url": "https://github.com/apache/flink/commit/93f58e49ad79331331d4d85bbe7720ecfe39f968", "message": "[FLINK-18286] Implement type inference for GET/FLATTEN", "committedDate": "2020-07-16T12:12:31Z", "type": "forcePushed"}]}