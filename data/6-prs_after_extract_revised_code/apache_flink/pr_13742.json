{"pr_number": 13742, "pr_title": "[FLINK-19626][table-planner-blink] Introduce multi-input operator construction algorithm", "pr_createdAt": "2020-10-22T07:19:13Z", "pr_url": "https://github.com/apache/flink/pull/13742", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUzODA2MQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511538061", "bodyText": "nodes -> operators", "author": "godfreyhe", "createdAt": "2020-10-25T02:39:16Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java", "diffHunk": "@@ -99,4 +99,12 @@\n \t\tkey(\"table.optimizer.join-reorder-enabled\")\n \t\t\t.defaultValue(false)\n \t\t\t.withDescription(\"Enables join reorder in optimizer. Default is disabled.\");\n+\n+\t@Documentation.TableOption(execMode = Documentation.ExecMode.BATCH_STREAMING)\n+\tpublic static final ConfigOption<Boolean> TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED =\n+\t\tkey(\"table.optimizer.multiple-input-enabled\")\n+\t\t\t.defaultValue(false)\n+\t\t\t.withDescription(\"Enables creating multiple input nodes to reduce shuffling. \" +", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "02e0db125833bf12ab5856b08a82a03fc5454958", "chunk": "diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java\nindex 510ba2154a..dfeda995a6 100644\n--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java\n+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/config/OptimizerConfigOptions.java\n\n@@ -104,7 +104,7 @@ public class OptimizerConfigOptions {\n \tpublic static final ConfigOption<Boolean> TABLE_OPTIMIZER_MULTIPLE_INPUT_ENABLED =\n \t\tkey(\"table.optimizer.multiple-input-enabled\")\n \t\t\t.defaultValue(false)\n-\t\t\t.withDescription(\"Enables creating multiple input nodes to reduce shuffling. \" +\n+\t\t\t.withDescription(\"Enables creating multiple input operators to reduce shuffling. \" +\n \t\t\t\t\"Default is disabled as currently its physical operators are not implemented. \" +\n \t\t\t\t\"This option is currently only used for plan test cases.\");\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MDE4OA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511540188", "bodyText": "add some comments to explain the fields", "author": "godfreyhe", "createdAt": "2020-10-25T03:09:20Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "02e0db125833bf12ab5856b08a82a03fc5454958", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\ndeleted file mode 100644\nindex d14d8df68c..0000000000\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\n+++ /dev/null\n\n@@ -1,455 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.table.planner.plan.reuse;\n-\n-import org.apache.flink.annotation.Internal;\n-import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.transformations.ShuffleMode;\n-import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n-import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n-import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.apache.calcite.rel.RelNode;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Queue;\n-import java.util.Set;\n-import java.util.TreeMap;\n-\n-/**\n- * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n- *\n- * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n- * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n- *\n- * <p>For example, consider the SQL query:\n- * <pre>\n- * WITH\n- *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n- *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n- * SELECT * FROM\n- *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n- *   UNION ALL\n- *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n- * </pre>\n- *\n- * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n- * <pre>\n- * Union(all=[true], union=[cnt1, cnt2])\n- * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n- * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n- * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n- * :     :  +- Exchange(distribution=[hash[a]])\n- * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n- * :     :        +- Calc(select=[a])\n- * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n- * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n- * :        +- Exchange(distribution=[hash[d]])\n- * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n- * :              +- Calc(select=[d])\n- * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n- * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n- *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n- *       :- Reused(reference_id=[1])\n- *       +- Reused(reference_id=[2])\n- * </pre>\n- *\n- * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n- * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n- * the opposite. This physical plan will thus cause a deadlock.\n- *\n- * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n- * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n- *\n- * <p>For a detailed explanation of the algorithm, see appendix of the\n- * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n- */\n-@Internal\n-public class InputPriorityConflictResolver {\n-\n-\tprivate final List<ExecNode<?, ?>> roots;\n-\tprivate final Set<ExecNode<?, ?>> boundaries;\n-\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n-\tprivate final ShuffleMode shuffleMode;\n-\n-\tprivate TopologyGraph graph;\n-\n-\tpublic InputPriorityConflictResolver(\n-\t\t\tList<ExecNode<?, ?>> roots,\n-\t\t\tSet<ExecNode<?, ?>> boundaries,\n-\t\t\tExecEdge.DamBehavior safeDamBehavior,\n-\t\t\tShuffleMode shuffleMode) {\n-\t\tPreconditions.checkArgument(\n-\t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n-\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n-\t\tthis.roots = roots;\n-\t\tthis.boundaries = boundaries;\n-\t\tthis.safeDamBehavior = safeDamBehavior;\n-\t\tthis.shuffleMode = shuffleMode;\n-\t}\n-\n-\tpublic void detectAndResolve() {\n-\t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots, boundaries);\n-\n-\t\t// check and resolve conflicts about input priorities\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t\tcheckInputPriorities(node);\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\t}\n-\n-\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {\n-\t\t// we first calculate the topological order of all nodes in the graph\n-\t\tdetectAndResolve();\n-\t\t// check that no exchange is contained in the multiple input node\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\treturn;\n-\t\t\t\t}\n-\t\t\t\tvisitInputs(node);\n-\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t!(node instanceof BatchExecExchange),\n-\t\t\t\t\t\"There is exchange in a multiple input node. This is a bug.\");\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\n-\t\tMap<ExecNode<?, ?>, Integer> orders = graph.calculateOrder();\n-\n-\t\t// now extract only the orders of the boundaries and renumbering the orders\n-\t\t// so that the smallest order starts from 0\n-\t\tSet<Integer> boundaryOrderSet = new HashSet<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tboundaryOrderSet.add(orders.getOrDefault(boundary, 0));\n-\t\t}\n-\t\tList<Integer> boundaryOrderList = new ArrayList<>(boundaryOrderSet);\n-\t\tCollections.sort(boundaryOrderList);\n-\n-\t\tMap<ExecNode<?, ?>, Integer> results = new HashMap<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tresults.put(boundary, boundaryOrderList.indexOf(orders.get(boundary)));\n-\t\t}\n-\t\treturn results;\n-\t}\n-\n-\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n-\t\t// group inputs by input priorities\n-\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n-\t\tPreconditions.checkState(\n-\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n-\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n-\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n-\t\tfor (int i = 0; i < node.getInputEdges().size(); i++) {\n-\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n-\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n-\t\t}\n-\n-\t\t// add edges between neighboring priority groups\n-\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n-\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n-\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n-\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n-\n-\t\t\tfor (int higher : higherGroup) {\n-\t\t\t\tfor (int lower : lowerGroup) {\n-\t\t\t\t\taddTopologyEdges(node, higher, lower);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n-\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n-\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n-\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n-\n-\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n-\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n-\t\t\tif (graph.link(higherNode, ancestor)) {\n-\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n-\t\t\t} else {\n-\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n-\t\t\t\t// and revert all linked edges\n-\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n-\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n-\t\t\t\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\t\t\t} else {\n-\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n-\t\t\t\t}\n-\n-\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n-\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n-\t\t\t\t}\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Find the ancestors by going through PIPELINED edges.\n-\t */\n-\t@VisibleForTesting\n-\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n-\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n-\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tboolean hasAncestor = false;\n-\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n-\t\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n-\t\t\t\t\t\t// we only go through PIPELINED edges\n-\t\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(safeDamBehavior)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\thasAncestor = true;\n-\t\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif (!hasAncestor) {\n-\t\t\t\t\tret.add(node);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t};\n-\t\tnode.accept(ancestorVisitor);\n-\t\treturn ret;\n-\t}\n-\n-\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n-\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n-\n-\t\tFlinkRelDistribution distribution;\n-\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n-\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n-\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n-\t\t\t// should not occur\n-\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n-\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n-\t\t} else {\n-\t\t\tdistribution = FlinkRelDistribution.ANY();\n-\t\t}\n-\n-\t\tBatchExecExchange exchange = new BatchExecExchange(\n-\t\t\tinputRel.getCluster(),\n-\t\t\tinputRel.getTraitSet().replace(distribution),\n-\t\t\tinputRel,\n-\t\t\tdistribution);\n-\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\treturn exchange;\n-\t}\n-\n-\t/**\n-\t * A data structure storing the topological information of an {@link ExecNode} graph.\n-\t */\n-\t@VisibleForTesting\n-\tstatic class TopologyGraph {\n-\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n-\t\t\tthis(roots, Collections.emptySet());\n-\t\t}\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n-\t\t\tthis.nodes = new HashMap<>();\n-\n-\t\t\t// we first link all edges in the original exec node graph\n-\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t\t@Override\n-\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n-\t\t\t\t\t\tlink(input, node);\n-\t\t\t\t\t}\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t};\n-\t\t\troots.forEach(n -> n.accept(visitor));\n-\t\t}\n-\n-\t\t/**\n-\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n-\t\t * Returns if this edge is successfully added.\n-\t\t */\n-\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tif (canReach(toNode, fromNode)) {\n-\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n-\t\t\t\treturn false;\n-\t\t\t} else {\n-\t\t\t\t// link `from` and `to`\n-\t\t\t\tfromNode.outputs.add(toNode);\n-\t\t\t\ttoNode.inputs.add(fromNode);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t}\n-\n-\t\t/**\n-\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n-\t\t */\n-\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tfromNode.outputs.remove(toNode);\n-\t\t\ttoNode.inputs.remove(fromNode);\n-\t\t}\n-\n-\t\t/**\n-\t\t * Calculate the topological order of the currently added nodes.\n-\t\t * The smallest order is 0 and two equal integers indicate that\n-\t\t * they're not comparable on a topology graph.\n-\t\t */\n-\t\tMap<ExecNode<?, ?>, Integer> calculateOrder() {\n-\t\t\tMap<ExecNode<?, ?>, Integer> result = new HashMap<>();\n-\t\t\tMap<TopologyNode, Integer> inputsVisitedMap = new HashMap<>();\n-\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tfor (TopologyNode node : nodes.values()) {\n-\t\t\t\tif (node.inputs.size() == 0) {\n-\t\t\t\t\tqueue.offer(node);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tint order = -1;\n-\t\t\t\tfor (TopologyNode input : node.inputs) {\n-\t\t\t\t\torder = Math.max(\n-\t\t\t\t\t\torder,\n-\t\t\t\t\t\tPreconditions.checkNotNull(\n-\t\t\t\t\t\t\tresult.get(input.execNode),\n-\t\t\t\t\t\t\t\"The topological order of an input node is not calculated. This is a bug.\"));\n-\t\t\t\t}\n-\t\t\t\torder++;\n-\t\t\t\tresult.put(node.execNode, order);\n-\n-\t\t\t\tfor (TopologyNode output : node.outputs) {\n-\t\t\t\t\tint inputsVisited = inputsVisitedMap.compute(output, (k, v) -> v == null ? 1 : v + 1);\n-\t\t\t\t\tif (inputsVisited == output.inputs.size()) {\n-\t\t\t\t\t\tqueue.offer(output);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn result;\n-\t\t}\n-\n-\t\t@VisibleForTesting\n-\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\t\t\treturn canReach(fromNode, toNode);\n-\t\t}\n-\n-\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n-\t\t\tSet<TopologyNode> visited = new HashSet<>();\n-\t\t\tvisited.add(from);\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tqueue.offer(from);\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tif (to.equals(node)) {\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\n-\t\t\t\tfor (TopologyNode next : node.outputs) {\n-\t\t\t\t\tif (visited.contains(next)) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t\tvisited.add(next);\n-\t\t\t\t\tqueue.offer(next);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n-\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\tDataStream<?> currentStream =\n-\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n-\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n-\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n-\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\t\t\tDataStream<?> existingStream =\n-\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n-\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n-\t\t\t\t\t\t\treturn entry.getValue();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tTopologyNode result = new TopologyNode(execNode);\n-\t\t\t\tnodes.put(execNode, result);\n-\t\t\t\treturn result;\n-\t\t\t} else {\n-\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode(execNode));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A node in the {@link TopologyGraph}.\n-\t */\n-\tprivate static class TopologyNode {\n-\t\tprivate final ExecNode<?, ?> execNode;\n-\t\tprivate final Set<TopologyNode> inputs;\n-\t\tprivate final Set<TopologyNode> outputs;\n-\n-\t\tprivate TopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\tthis.execNode = execNode;\n-\t\t\tthis.inputs = new HashSet<>();\n-\t\t\tthis.outputs = new HashSet<>();\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MDUyMw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511540523", "bodyText": "We should introduce another class to calculate the input orders, named: InputOrderDerivation ? maybe we also need a Base class of InputOrderDerivation and InputPriorityConflictResolver", "author": "godfreyhe", "createdAt": "2020-10-25T03:14:42Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;\n \n \tprivate TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\tpublic InputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior,\n+\t\t\tShuffleMode shuffleMode) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n+\t\tthis.shuffleMode = shuffleMode;\n \t}\n \n \tpublic void detectAndResolve() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n \t\t\t\tcheckInputPriorities(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n+\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "02e0db125833bf12ab5856b08a82a03fc5454958", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\ndeleted file mode 100644\nindex d14d8df68c..0000000000\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\n+++ /dev/null\n\n@@ -1,455 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.table.planner.plan.reuse;\n-\n-import org.apache.flink.annotation.Internal;\n-import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.transformations.ShuffleMode;\n-import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n-import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n-import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.apache.calcite.rel.RelNode;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Queue;\n-import java.util.Set;\n-import java.util.TreeMap;\n-\n-/**\n- * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n- *\n- * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n- * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n- *\n- * <p>For example, consider the SQL query:\n- * <pre>\n- * WITH\n- *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n- *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n- * SELECT * FROM\n- *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n- *   UNION ALL\n- *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n- * </pre>\n- *\n- * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n- * <pre>\n- * Union(all=[true], union=[cnt1, cnt2])\n- * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n- * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n- * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n- * :     :  +- Exchange(distribution=[hash[a]])\n- * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n- * :     :        +- Calc(select=[a])\n- * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n- * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n- * :        +- Exchange(distribution=[hash[d]])\n- * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n- * :              +- Calc(select=[d])\n- * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n- * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n- *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n- *       :- Reused(reference_id=[1])\n- *       +- Reused(reference_id=[2])\n- * </pre>\n- *\n- * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n- * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n- * the opposite. This physical plan will thus cause a deadlock.\n- *\n- * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n- * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n- *\n- * <p>For a detailed explanation of the algorithm, see appendix of the\n- * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n- */\n-@Internal\n-public class InputPriorityConflictResolver {\n-\n-\tprivate final List<ExecNode<?, ?>> roots;\n-\tprivate final Set<ExecNode<?, ?>> boundaries;\n-\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n-\tprivate final ShuffleMode shuffleMode;\n-\n-\tprivate TopologyGraph graph;\n-\n-\tpublic InputPriorityConflictResolver(\n-\t\t\tList<ExecNode<?, ?>> roots,\n-\t\t\tSet<ExecNode<?, ?>> boundaries,\n-\t\t\tExecEdge.DamBehavior safeDamBehavior,\n-\t\t\tShuffleMode shuffleMode) {\n-\t\tPreconditions.checkArgument(\n-\t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n-\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n-\t\tthis.roots = roots;\n-\t\tthis.boundaries = boundaries;\n-\t\tthis.safeDamBehavior = safeDamBehavior;\n-\t\tthis.shuffleMode = shuffleMode;\n-\t}\n-\n-\tpublic void detectAndResolve() {\n-\t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots, boundaries);\n-\n-\t\t// check and resolve conflicts about input priorities\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t\tcheckInputPriorities(node);\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\t}\n-\n-\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {\n-\t\t// we first calculate the topological order of all nodes in the graph\n-\t\tdetectAndResolve();\n-\t\t// check that no exchange is contained in the multiple input node\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\treturn;\n-\t\t\t\t}\n-\t\t\t\tvisitInputs(node);\n-\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t!(node instanceof BatchExecExchange),\n-\t\t\t\t\t\"There is exchange in a multiple input node. This is a bug.\");\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\n-\t\tMap<ExecNode<?, ?>, Integer> orders = graph.calculateOrder();\n-\n-\t\t// now extract only the orders of the boundaries and renumbering the orders\n-\t\t// so that the smallest order starts from 0\n-\t\tSet<Integer> boundaryOrderSet = new HashSet<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tboundaryOrderSet.add(orders.getOrDefault(boundary, 0));\n-\t\t}\n-\t\tList<Integer> boundaryOrderList = new ArrayList<>(boundaryOrderSet);\n-\t\tCollections.sort(boundaryOrderList);\n-\n-\t\tMap<ExecNode<?, ?>, Integer> results = new HashMap<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tresults.put(boundary, boundaryOrderList.indexOf(orders.get(boundary)));\n-\t\t}\n-\t\treturn results;\n-\t}\n-\n-\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n-\t\t// group inputs by input priorities\n-\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n-\t\tPreconditions.checkState(\n-\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n-\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n-\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n-\t\tfor (int i = 0; i < node.getInputEdges().size(); i++) {\n-\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n-\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n-\t\t}\n-\n-\t\t// add edges between neighboring priority groups\n-\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n-\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n-\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n-\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n-\n-\t\t\tfor (int higher : higherGroup) {\n-\t\t\t\tfor (int lower : lowerGroup) {\n-\t\t\t\t\taddTopologyEdges(node, higher, lower);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n-\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n-\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n-\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n-\n-\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n-\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n-\t\t\tif (graph.link(higherNode, ancestor)) {\n-\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n-\t\t\t} else {\n-\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n-\t\t\t\t// and revert all linked edges\n-\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n-\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n-\t\t\t\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\t\t\t} else {\n-\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n-\t\t\t\t}\n-\n-\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n-\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n-\t\t\t\t}\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Find the ancestors by going through PIPELINED edges.\n-\t */\n-\t@VisibleForTesting\n-\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n-\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n-\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tboolean hasAncestor = false;\n-\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n-\t\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n-\t\t\t\t\t\t// we only go through PIPELINED edges\n-\t\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(safeDamBehavior)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\thasAncestor = true;\n-\t\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif (!hasAncestor) {\n-\t\t\t\t\tret.add(node);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t};\n-\t\tnode.accept(ancestorVisitor);\n-\t\treturn ret;\n-\t}\n-\n-\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n-\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n-\n-\t\tFlinkRelDistribution distribution;\n-\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n-\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n-\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n-\t\t\t// should not occur\n-\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n-\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n-\t\t} else {\n-\t\t\tdistribution = FlinkRelDistribution.ANY();\n-\t\t}\n-\n-\t\tBatchExecExchange exchange = new BatchExecExchange(\n-\t\t\tinputRel.getCluster(),\n-\t\t\tinputRel.getTraitSet().replace(distribution),\n-\t\t\tinputRel,\n-\t\t\tdistribution);\n-\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\treturn exchange;\n-\t}\n-\n-\t/**\n-\t * A data structure storing the topological information of an {@link ExecNode} graph.\n-\t */\n-\t@VisibleForTesting\n-\tstatic class TopologyGraph {\n-\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n-\t\t\tthis(roots, Collections.emptySet());\n-\t\t}\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n-\t\t\tthis.nodes = new HashMap<>();\n-\n-\t\t\t// we first link all edges in the original exec node graph\n-\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t\t@Override\n-\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n-\t\t\t\t\t\tlink(input, node);\n-\t\t\t\t\t}\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t};\n-\t\t\troots.forEach(n -> n.accept(visitor));\n-\t\t}\n-\n-\t\t/**\n-\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n-\t\t * Returns if this edge is successfully added.\n-\t\t */\n-\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tif (canReach(toNode, fromNode)) {\n-\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n-\t\t\t\treturn false;\n-\t\t\t} else {\n-\t\t\t\t// link `from` and `to`\n-\t\t\t\tfromNode.outputs.add(toNode);\n-\t\t\t\ttoNode.inputs.add(fromNode);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t}\n-\n-\t\t/**\n-\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n-\t\t */\n-\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tfromNode.outputs.remove(toNode);\n-\t\t\ttoNode.inputs.remove(fromNode);\n-\t\t}\n-\n-\t\t/**\n-\t\t * Calculate the topological order of the currently added nodes.\n-\t\t * The smallest order is 0 and two equal integers indicate that\n-\t\t * they're not comparable on a topology graph.\n-\t\t */\n-\t\tMap<ExecNode<?, ?>, Integer> calculateOrder() {\n-\t\t\tMap<ExecNode<?, ?>, Integer> result = new HashMap<>();\n-\t\t\tMap<TopologyNode, Integer> inputsVisitedMap = new HashMap<>();\n-\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tfor (TopologyNode node : nodes.values()) {\n-\t\t\t\tif (node.inputs.size() == 0) {\n-\t\t\t\t\tqueue.offer(node);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tint order = -1;\n-\t\t\t\tfor (TopologyNode input : node.inputs) {\n-\t\t\t\t\torder = Math.max(\n-\t\t\t\t\t\torder,\n-\t\t\t\t\t\tPreconditions.checkNotNull(\n-\t\t\t\t\t\t\tresult.get(input.execNode),\n-\t\t\t\t\t\t\t\"The topological order of an input node is not calculated. This is a bug.\"));\n-\t\t\t\t}\n-\t\t\t\torder++;\n-\t\t\t\tresult.put(node.execNode, order);\n-\n-\t\t\t\tfor (TopologyNode output : node.outputs) {\n-\t\t\t\t\tint inputsVisited = inputsVisitedMap.compute(output, (k, v) -> v == null ? 1 : v + 1);\n-\t\t\t\t\tif (inputsVisited == output.inputs.size()) {\n-\t\t\t\t\t\tqueue.offer(output);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn result;\n-\t\t}\n-\n-\t\t@VisibleForTesting\n-\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\t\t\treturn canReach(fromNode, toNode);\n-\t\t}\n-\n-\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n-\t\t\tSet<TopologyNode> visited = new HashSet<>();\n-\t\t\tvisited.add(from);\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tqueue.offer(from);\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tif (to.equals(node)) {\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\n-\t\t\t\tfor (TopologyNode next : node.outputs) {\n-\t\t\t\t\tif (visited.contains(next)) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t\tvisited.add(next);\n-\t\t\t\t\tqueue.offer(next);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n-\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\tDataStream<?> currentStream =\n-\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n-\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n-\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n-\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\t\t\tDataStream<?> existingStream =\n-\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n-\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n-\t\t\t\t\t\t\treturn entry.getValue();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tTopologyNode result = new TopologyNode(execNode);\n-\t\t\t\tnodes.put(execNode, result);\n-\t\t\t\treturn result;\n-\t\t\t} else {\n-\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode(execNode));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A node in the {@link TopologyGraph}.\n-\t */\n-\tprivate static class TopologyNode {\n-\t\tprivate final ExecNode<?, ?> execNode;\n-\t\tprivate final Set<TopologyNode> inputs;\n-\t\tprivate final Set<TopologyNode> outputs;\n-\n-\t\tprivate TopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\tthis.execNode = execNode;\n-\t\t\tthis.inputs = new HashSet<>();\n-\t\t\tthis.outputs = new HashSet<>();\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU0MTMxMg==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511541312", "bodyText": "please check the number of roots should always be 1. If we separate this part of logic into another class, we can give ExecNode<?, ?> root instead of List<ExecNode<?, ?>> roots as a part of the constructor parameters", "author": "godfreyhe", "createdAt": "2020-10-25T03:25:15Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java", "diffHunk": "@@ -97,31 +98,79 @@\n public class InputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n+\tprivate final ShuffleMode shuffleMode;\n \n \tprivate TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\tpublic InputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior,\n+\t\t\tShuffleMode shuffleMode) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n+\t\tthis.shuffleMode = shuffleMode;\n \t}\n \n \tpublic void detectAndResolve() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n \t\t\t\tcheckInputPriorities(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n+\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {\n+\t\t// we first calculate the topological order of all nodes in the graph\n+\t\tdetectAndResolve();\n+\t\t// check that no exchange is contained in the multiple input node\n+\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\t!(node instanceof BatchExecExchange),\n+\t\t\t\t\t\"There is exchange in a multiple input node. This is a bug.\");\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(inputPriorityVisitor));", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "02e0db125833bf12ab5856b08a82a03fc5454958", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\ndeleted file mode 100644\nindex d14d8df68c..0000000000\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolver.java\n+++ /dev/null\n\n@@ -1,455 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.table.planner.plan.reuse;\n-\n-import org.apache.flink.annotation.Internal;\n-import org.apache.flink.annotation.VisibleForTesting;\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.transformations.ShuffleMode;\n-import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n-import org.apache.flink.table.planner.plan.nodes.exec.BatchExecNode;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n-import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n-import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n-import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n-import org.apache.flink.util.Preconditions;\n-\n-import org.apache.calcite.rel.RelNode;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Queue;\n-import java.util.Set;\n-import java.util.TreeMap;\n-\n-/**\n- * This class contains algorithm to detect and resolve input priority conflict in an {@link ExecNode} graph.\n- *\n- * <p>Some batch operators (for example, hash join and nested loop join) have different priorities for their inputs.\n- * When some operators are reused, a deadlock may occur due to the conflict in these priorities.\n- *\n- * <p>For example, consider the SQL query:\n- * <pre>\n- * WITH\n- *   T1 AS (SELECT a, COUNT(*) AS cnt1 FROM x GROUP BY a),\n- *   T2 AS (SELECT d, COUNT(*) AS cnt2 FROM y GROUP BY d)\n- * SELECT * FROM\n- *   (SELECT cnt1, cnt2 FROM T1 LEFT JOIN T2 ON a = d)\n- *   UNION ALL\n- *   (SELECT cnt1, cnt2 FROM T2 LEFT JOIN T1 ON d = a)\n- * </pre>\n- *\n- * <p>When sub-plan reuse are enabled, we'll get the following physical plan:\n- * <pre>\n- * Union(all=[true], union=[cnt1, cnt2])\n- * :- Calc(select=[CAST(cnt1) AS cnt1, cnt2])\n- * :  +- HashJoin(joinType=[LeftOuterJoin], where=[=(a, d)], select=[a, cnt1, d, cnt2], build=[right])\n- * :     :- HashAggregate(isMerge=[true], groupBy=[a], select=[a, Final_COUNT(count1$0) AS cnt1], reuse_id=[2])\n- * :     :  +- Exchange(distribution=[hash[a]])\n- * :     :     +- LocalHashAggregate(groupBy=[a], select=[a, Partial_COUNT(*) AS count1$0])\n- * :     :        +- Calc(select=[a])\n- * :     :           +- LegacyTableSourceScan(table=[[default_catalog, default_database, x, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])\n- * :     +- HashAggregate(isMerge=[true], groupBy=[d], select=[d, Final_COUNT(count1$0) AS cnt2], reuse_id=[1])\n- * :        +- Exchange(distribution=[hash[d]])\n- * :           +- LocalHashAggregate(groupBy=[d], select=[d, Partial_COUNT(*) AS count1$0])\n- * :              +- Calc(select=[d])\n- * :                 +- LegacyTableSourceScan(table=[[default_catalog, default_database, y, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])\n- * +- Calc(select=[cnt1, CAST(cnt2) AS cnt2])\n- *    +- HashJoin(joinType=[LeftOuterJoin], where=[=(d, a)], select=[d, cnt2, a, cnt1], build=[right])\n- *       :- Reused(reference_id=[1])\n- *       +- Reused(reference_id=[2])\n- * </pre>\n- *\n- * <p>Note that the first hash join needs to read all results from the hash aggregate whose reuse id is 1\n- * before reading the results from the hash aggregate whose reuse id is 2, while the second hash join requires\n- * the opposite. This physical plan will thus cause a deadlock.\n- *\n- * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n- * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n- *\n- * <p>For a detailed explanation of the algorithm, see appendix of the\n- * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n- */\n-@Internal\n-public class InputPriorityConflictResolver {\n-\n-\tprivate final List<ExecNode<?, ?>> roots;\n-\tprivate final Set<ExecNode<?, ?>> boundaries;\n-\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n-\tprivate final ShuffleMode shuffleMode;\n-\n-\tprivate TopologyGraph graph;\n-\n-\tpublic InputPriorityConflictResolver(\n-\t\t\tList<ExecNode<?, ?>> roots,\n-\t\t\tSet<ExecNode<?, ?>> boundaries,\n-\t\t\tExecEdge.DamBehavior safeDamBehavior,\n-\t\t\tShuffleMode shuffleMode) {\n-\t\tPreconditions.checkArgument(\n-\t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n-\t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n-\t\tthis.roots = roots;\n-\t\tthis.boundaries = boundaries;\n-\t\tthis.safeDamBehavior = safeDamBehavior;\n-\t\tthis.shuffleMode = shuffleMode;\n-\t}\n-\n-\tpublic void detectAndResolve() {\n-\t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots, boundaries);\n-\n-\t\t// check and resolve conflicts about input priorities\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t\tcheckInputPriorities(node);\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\t}\n-\n-\tpublic Map<ExecNode<?, ?>, Integer> calculateInputOrder() {\n-\t\t// we first calculate the topological order of all nodes in the graph\n-\t\tdetectAndResolve();\n-\t\t// check that no exchange is contained in the multiple input node\n-\t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\treturn;\n-\t\t\t\t}\n-\t\t\t\tvisitInputs(node);\n-\t\t\t\tPreconditions.checkState(\n-\t\t\t\t\t!(node instanceof BatchExecExchange),\n-\t\t\t\t\t\"There is exchange in a multiple input node. This is a bug.\");\n-\t\t\t}\n-\t\t};\n-\t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n-\n-\t\tMap<ExecNode<?, ?>, Integer> orders = graph.calculateOrder();\n-\n-\t\t// now extract only the orders of the boundaries and renumbering the orders\n-\t\t// so that the smallest order starts from 0\n-\t\tSet<Integer> boundaryOrderSet = new HashSet<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tboundaryOrderSet.add(orders.getOrDefault(boundary, 0));\n-\t\t}\n-\t\tList<Integer> boundaryOrderList = new ArrayList<>(boundaryOrderSet);\n-\t\tCollections.sort(boundaryOrderList);\n-\n-\t\tMap<ExecNode<?, ?>, Integer> results = new HashMap<>();\n-\t\tfor (ExecNode<?, ?> boundary : boundaries) {\n-\t\t\tresults.put(boundary, boundaryOrderList.indexOf(orders.get(boundary)));\n-\t\t}\n-\t\treturn results;\n-\t}\n-\n-\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n-\t\t// group inputs by input priorities\n-\t\tTreeMap<Integer, List<Integer>> inputPriorityGroupMap = new TreeMap<>();\n-\t\tPreconditions.checkState(\n-\t\t\tnode.getInputNodes().size() == node.getInputEdges().size(),\n-\t\t\t\"Number of inputs nodes does not equal to number of input edges for node \" +\n-\t\t\t\tnode.getClass().getName() + \". This is a bug.\");\n-\t\tfor (int i = 0; i < node.getInputEdges().size(); i++) {\n-\t\t\tint priority = node.getInputEdges().get(i).getPriority();\n-\t\t\tinputPriorityGroupMap.computeIfAbsent(priority, k -> new ArrayList<>()).add(i);\n-\t\t}\n-\n-\t\t// add edges between neighboring priority groups\n-\t\tList<List<Integer>> inputPriorityGroups = new ArrayList<>(inputPriorityGroupMap.values());\n-\t\tfor (int i = 0; i + 1 < inputPriorityGroups.size(); i++) {\n-\t\t\tList<Integer> higherGroup = inputPriorityGroups.get(i);\n-\t\t\tList<Integer> lowerGroup = inputPriorityGroups.get(i + 1);\n-\n-\t\t\tfor (int higher : higherGroup) {\n-\t\t\t\tfor (int lower : lowerGroup) {\n-\t\t\t\t\taddTopologyEdges(node, higher, lower);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate void addTopologyEdges(ExecNode<?, ?> node, int higherInput, int lowerInput) {\n-\t\tExecNode<?, ?> higherNode = node.getInputNodes().get(higherInput);\n-\t\tExecNode<?, ?> lowerNode = node.getInputNodes().get(lowerInput);\n-\t\tList<ExecNode<?, ?>> lowerAncestors = calculateAncestors(lowerNode);\n-\n-\t\tList<Tuple2<ExecNode<?, ?>, ExecNode<?, ?>>> linkedEdges = new ArrayList<>();\n-\t\tfor (ExecNode<?, ?> ancestor : lowerAncestors) {\n-\t\t\tif (graph.link(higherNode, ancestor)) {\n-\t\t\t\tlinkedEdges.add(Tuple2.of(higherNode, ancestor));\n-\t\t\t} else {\n-\t\t\t\t// a conflict occurs, resolve it by adding a batch exchange\n-\t\t\t\t// and revert all linked edges\n-\t\t\t\tif (lowerNode instanceof BatchExecExchange) {\n-\t\t\t\t\tBatchExecExchange exchange = (BatchExecExchange) lowerNode;\n-\t\t\t\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\t\t\t} else {\n-\t\t\t\t\tnode.replaceInputNode(lowerInput, (ExecNode) createExchange(node, lowerInput));\n-\t\t\t\t}\n-\n-\t\t\t\tfor (Tuple2<ExecNode<?, ?>, ExecNode<?, ?>> linkedEdge : linkedEdges) {\n-\t\t\t\t\tgraph.unlink(linkedEdge.f0, linkedEdge.f1);\n-\t\t\t\t}\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Find the ancestors by going through PIPELINED edges.\n-\t */\n-\t@VisibleForTesting\n-\tList<ExecNode<?, ?>> calculateAncestors(ExecNode<?, ?> node) {\n-\t\tList<ExecNode<?, ?>> ret = new ArrayList<>();\n-\t\tAbstractExecNodeExactlyOnceVisitor ancestorVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t@Override\n-\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tboolean hasAncestor = false;\n-\n-\t\t\t\tif (!boundaries.contains(node)) {\n-\t\t\t\t\tList<ExecEdge> inputEdges = node.getInputEdges();\n-\t\t\t\t\tfor (int i = 0; i < inputEdges.size(); i++) {\n-\t\t\t\t\t\t// we only go through PIPELINED edges\n-\t\t\t\t\t\tif (inputEdges.get(i).getDamBehavior().stricterOrEqual(safeDamBehavior)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\thasAncestor = true;\n-\t\t\t\t\t\tnode.getInputNodes().get(i).accept(this);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif (!hasAncestor) {\n-\t\t\t\t\tret.add(node);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t};\n-\t\tnode.accept(ancestorVisitor);\n-\t\treturn ret;\n-\t}\n-\n-\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n-\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n-\n-\t\tFlinkRelDistribution distribution;\n-\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n-\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n-\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n-\t\t\t// should not occur\n-\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n-\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n-\t\t} else {\n-\t\t\tdistribution = FlinkRelDistribution.ANY();\n-\t\t}\n-\n-\t\tBatchExecExchange exchange = new BatchExecExchange(\n-\t\t\tinputRel.getCluster(),\n-\t\t\tinputRel.getTraitSet().replace(distribution),\n-\t\t\tinputRel,\n-\t\t\tdistribution);\n-\t\texchange.setRequiredShuffleMode(shuffleMode);\n-\t\treturn exchange;\n-\t}\n-\n-\t/**\n-\t * A data structure storing the topological information of an {@link ExecNode} graph.\n-\t */\n-\t@VisibleForTesting\n-\tstatic class TopologyGraph {\n-\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n-\t\t\tthis(roots, Collections.emptySet());\n-\t\t}\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n-\t\t\tthis.nodes = new HashMap<>();\n-\n-\t\t\t// we first link all edges in the original exec node graph\n-\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t\t@Override\n-\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\t\tif (boundaries.contains(node)) {\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n-\t\t\t\t\t\tlink(input, node);\n-\t\t\t\t\t}\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t};\n-\t\t\troots.forEach(n -> n.accept(visitor));\n-\t\t}\n-\n-\t\t/**\n-\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n-\t\t * Returns if this edge is successfully added.\n-\t\t */\n-\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tif (canReach(toNode, fromNode)) {\n-\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n-\t\t\t\treturn false;\n-\t\t\t} else {\n-\t\t\t\t// link `from` and `to`\n-\t\t\t\tfromNode.outputs.add(toNode);\n-\t\t\t\ttoNode.inputs.add(fromNode);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t}\n-\n-\t\t/**\n-\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n-\t\t */\n-\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tfromNode.outputs.remove(toNode);\n-\t\t\ttoNode.inputs.remove(fromNode);\n-\t\t}\n-\n-\t\t/**\n-\t\t * Calculate the topological order of the currently added nodes.\n-\t\t * The smallest order is 0 and two equal integers indicate that\n-\t\t * they're not comparable on a topology graph.\n-\t\t */\n-\t\tMap<ExecNode<?, ?>, Integer> calculateOrder() {\n-\t\t\tMap<ExecNode<?, ?>, Integer> result = new HashMap<>();\n-\t\t\tMap<TopologyNode, Integer> inputsVisitedMap = new HashMap<>();\n-\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tfor (TopologyNode node : nodes.values()) {\n-\t\t\t\tif (node.inputs.size() == 0) {\n-\t\t\t\t\tqueue.offer(node);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tint order = -1;\n-\t\t\t\tfor (TopologyNode input : node.inputs) {\n-\t\t\t\t\torder = Math.max(\n-\t\t\t\t\t\torder,\n-\t\t\t\t\t\tPreconditions.checkNotNull(\n-\t\t\t\t\t\t\tresult.get(input.execNode),\n-\t\t\t\t\t\t\t\"The topological order of an input node is not calculated. This is a bug.\"));\n-\t\t\t\t}\n-\t\t\t\torder++;\n-\t\t\t\tresult.put(node.execNode, order);\n-\n-\t\t\t\tfor (TopologyNode output : node.outputs) {\n-\t\t\t\t\tint inputsVisited = inputsVisitedMap.compute(output, (k, v) -> v == null ? 1 : v + 1);\n-\t\t\t\t\tif (inputsVisited == output.inputs.size()) {\n-\t\t\t\t\t\tqueue.offer(output);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn result;\n-\t\t}\n-\n-\t\t@VisibleForTesting\n-\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\t\t\treturn canReach(fromNode, toNode);\n-\t\t}\n-\n-\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n-\t\t\tSet<TopologyNode> visited = new HashSet<>();\n-\t\t\tvisited.add(from);\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tqueue.offer(from);\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tif (to.equals(node)) {\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\n-\t\t\t\tfor (TopologyNode next : node.outputs) {\n-\t\t\t\t\tif (visited.contains(next)) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t\tvisited.add(next);\n-\t\t\t\t\tqueue.offer(next);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n-\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\tDataStream<?> currentStream =\n-\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n-\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n-\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n-\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\t\t\tDataStream<?> existingStream =\n-\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n-\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n-\t\t\t\t\t\t\treturn entry.getValue();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tTopologyNode result = new TopologyNode(execNode);\n-\t\t\t\tnodes.put(execNode, result);\n-\t\t\t\treturn result;\n-\t\t\t} else {\n-\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode(execNode));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A node in the {@link TopologyGraph}.\n-\t */\n-\tprivate static class TopologyNode {\n-\t\tprivate final ExecNode<?, ?> execNode;\n-\t\tprivate final Set<TopologyNode> inputs;\n-\t\tprivate final Set<TopologyNode> outputs;\n-\n-\t\tprivate TopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\tthis.execNode = execNode;\n-\t\t\tthis.inputs = new HashSet<>();\n-\t\t\tthis.outputs = new HashSet<>();\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NDgzNg==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511594836", "bodyText": "wrapper.execNode instanceof Exchange", "author": "godfreyhe", "createdAt": "2020-10-25T13:06:02Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTI2Nw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511595267", "bodyText": "rename to replaceGroup ?", "author": "godfreyhe", "createdAt": "2020-10-25T13:09:42Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {\n+\t\tif (node instanceof BatchExecBoundedStreamScan) {\n+\t\t\tBatchExecBoundedStreamScan scan = (BatchExecBoundedStreamScan) node;\n+\t\t\treturn scan.boundedStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t} else if (node instanceof StreamExecDataStreamScan) {\n+\t\t\tStreamExecDataStreamScan scan = (StreamExecDataStreamScan) node;\n+\t\t\treturn scan.dataStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Nodes Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNode<?, ?>> createMultipleInputNodes(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNode<?, ?>> result = new ArrayList<>();\n+\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap = new HashMap<>();\n+\t\tfor (ExecNodeWrapper sinkWrapper : sinkWrappers) {\n+\t\t\tresult.add(getMultipleInputNode(sinkWrapper, visitMap));\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\tprivate ExecNode<?, ?> getMultipleInputNode(\n+\t\t\tExecNodeWrapper wrapper,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\tif (visitMap.containsKey(wrapper)) {\n+\t\t\treturn visitMap.get(wrapper);\n+\t\t}\n+\n+\t\tfor (int i = 0; i < wrapper.inputs.size(); i++) {\n+\t\t\twrapper.execNode.replaceInputNode(i, (ExecNode) getMultipleInputNode(wrapper.inputs.get(i), visitMap));\n+\t\t}\n+\n+\t\tExecNode<?, ?> ret;\n+\t\tif (wrapper.group != null && wrapper == wrapper.group.root) {\n+\t\t\tret = createMultipleInputNode(wrapper.group, visitMap);\n+\t\t} else {\n+\t\t\tret = wrapper.execNode;\n+\t\t}\n+\t\tvisitMap.put(wrapper, ret);\n+\t\treturn ret;\n+\t}\n+\n+\tprivate ExecNode<?, ?> createMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap) {\n+\t\t// calculate the inputs of the multiple input node\n+\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs = new ArrayList<>();\n+\t\tfor (ExecNodeWrapper member : group.members) {\n+\t\t\tfor (int i = 0; i < member.inputs.size(); i++) {\n+\t\t\t\tExecNodeWrapper memberInput = member.inputs.get(i);\n+\t\t\t\tif (group.members.contains(memberInput)) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\tvisitMap.containsKey(memberInput),\n+\t\t\t\t\t\"Input of a multiple input member is not visited. This is a bug.\");\n+\n+\t\t\t\tExecNode<?, ?> inputNode = visitMap.get(memberInput);\n+\t\t\t\tExecEdge inputEdge = member.execNode.getInputEdges().get(i);\n+\t\t\t\tinputs.add(Tuple2.of(inputNode, inputEdge));\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (isStreaming) {\n+\t\t\treturn createStreamMultipleInputNode(group, inputs);\n+\t\t} else {\n+\t\t\treturn createBatchMultipleInputNode(group, inputs);\n+\t\t}\n+\t}\n+\n+\tprivate StreamExecMultipleInputNode createStreamMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tinputRels[i] = (RelNode) inputs.get(i).f0;\n+\t\t}\n+\n+\t\treturn new StreamExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel);\n+\t}\n+\n+\tprivate BatchExecMultipleInputNode createBatchMultipleInputNode(\n+\t\t\tMultipleInputGroup group,\n+\t\t\tList<Tuple2<ExecNode<?, ?>, ExecEdge>> inputs) {\n+\t\t// first calculate the input orders using InputPriorityConflictResolver\n+\t\tSet<ExecNode<?, ?>> inputSet = new HashSet<>();\n+\t\tfor (Tuple2<ExecNode<?, ?>, ExecEdge> t : inputs) {\n+\t\t\tinputSet.add(t.f0);\n+\t\t}\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\tCollections.singletonList(group.root.execNode),\n+\t\t\tinputSet,\n+\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\tShuffleMode.PIPELINED);\n+\t\tMap<ExecNode<?, ?>, Integer> inputOrderMap = resolver.calculateInputOrder();\n+\n+\t\t// then create input rels and edges with the input orders\n+\t\tRelNode outputRel = (RelNode) group.root.execNode;\n+\t\tRelNode[] inputRels = new RelNode[inputs.size()];\n+\t\tExecEdge[] inputEdges = new ExecEdge[inputs.size()];\n+\t\tfor (int i = 0; i < inputs.size(); i++) {\n+\t\t\tExecNode<?, ?> inputNode = inputs.get(i).f0;\n+\t\t\tExecEdge originalInputEdge = inputs.get(i).f1;\n+\t\t\tinputRels[i] = (RelNode) inputNode;\n+\t\t\tinputEdges[i] = ExecEdge.builder()\n+\t\t\t\t.requiredShuffle(originalInputEdge.getRequiredShuffle())\n+\t\t\t\t.damBehavior(originalInputEdge.getDamBehavior())\n+\t\t\t\t.priority(inputOrderMap.get(inputNode))\n+\t\t\t\t.build();\n+\t\t}\n+\n+\t\treturn new BatchExecMultipleInputNode(\n+\t\t\toutputRel.getCluster(),\n+\t\t\toutputRel.getTraitSet(),\n+\t\t\tinputRels,\n+\t\t\toutputRel,\n+\t\t\tinputEdges);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Helper Classes\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate static class ExecNodeWrapper {\n+\t\tprivate final ExecNode<?, ?> execNode;\n+\t\tprivate final List<ExecNodeWrapper> inputs;\n+\t\tprivate final List<ExecNodeWrapper> outputs;\n+\t\tprivate MultipleInputGroup group;\n+\n+\t\tprivate ExecNodeWrapper(ExecNode<?, ?> execNode) {\n+\t\t\tthis.execNode = execNode;\n+\t\t\tthis.inputs = new ArrayList<>();\n+\t\t\tthis.outputs = new ArrayList<>();\n+\t\t\tthis.group = null;\n+\t\t}\n+\n+\t\tprivate void createGroup() {\n+\t\t\tthis.group = new MultipleInputGroup(this);\n+\t\t}\n+\n+\t\tprivate void addToGroup(MultipleInputGroup group) {", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5OTExOQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511699119", "bodyText": "We expect this wrapper not to be in any group.", "author": "tsreaper", "createdAt": "2020-10-26T03:09:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTI2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NTY0MA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511595640", "bodyText": "wrapper.execNode instanceof Union", "author": "godfreyhe", "createdAt": "2020-10-25T13:13:03Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzEzNw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597137", "bodyText": "Do these optimizations necessary, or can we delete any optimization, but the result is correct", "author": "godfreyhe", "createdAt": "2020-10-25T13:26:21Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5OTI4MQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511699281", "bodyText": "They're best-to-have, not a must.", "author": "tsreaper", "createdAt": "2020-10-26T03:09:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzEzNw=="}], "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzIxNA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597214", "bodyText": "sink -> root", "author": "godfreyhe", "createdAt": "2020-10-25T13:27:07Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzMyOQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511597329", "bodyText": "visitMap -> visitedMap", "author": "godfreyhe", "createdAt": "2020-10-25T13:28:16Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {\n+\t\tif (node instanceof BatchExecBoundedStreamScan) {\n+\t\t\tBatchExecBoundedStreamScan scan = (BatchExecBoundedStreamScan) node;\n+\t\t\treturn scan.boundedStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t} else if (node instanceof StreamExecDataStreamScan) {\n+\t\t\tStreamExecDataStreamScan scan = (StreamExecDataStreamScan) node;\n+\t\t\treturn scan.dataStreamTable().dataStream().getTransformation() instanceof SourceTransformation;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Nodes Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNode<?, ?>> createMultipleInputNodes(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNode<?, ?>> result = new ArrayList<>();\n+\t\tMap<ExecNodeWrapper, ExecNode<?, ?>> visitMap = new HashMap<>();", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMjk1MA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511602950", "bodyText": "SourceProvider could also provide new Source", "author": "godfreyhe", "createdAt": "2020-10-25T14:18:52Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\tPreconditions.checkNotNull(\n+\t\t\twrapper.group,\n+\t\t\t\"Exec node wrapper does not have a multiple input group. This is a bug.\");\n+\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\tif (inputWrapper.group == wrapper.group) {\n+\t\t\t\t// one of the input is in the same group, so this node is not the tail of the group\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\t\treturn true;\n+\t}\n+\n+\tprivate boolean isNewSource(ExecNode<?, ?> node) {", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwMDE5Mg==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511700192", "bodyText": "FLIP-146 is not ready when this PR is submitted. I'll rebase the master branch.", "author": "tsreaper", "createdAt": "2020-10-26T03:14:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMjk1MA=="}], "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMzUwNw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511603507", "bodyText": "Tail => Header ?", "author": "godfreyhe", "createdAt": "2020-10-25T14:23:23Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecUnion;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> sinkNodes, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\tsinkNodes,\n+\t\t\t\tCollections.emptySet(),\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = wrapExecNodes(sinkNodes);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(sinkWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(sinkWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> sinkNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\tsinkNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> sinkWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> sink : sinkNodes) {\n+\t\t\tExecNodeWrapper sinkWrapper = wrapperMap.get(sink);\n+\t\t\tPreconditions.checkNotNull(sinkWrapper, \"Sink node is not wrapped. This is a bug.\");\n+\t\t\tsinkWrappers.add(sinkWrapper);\n+\t\t}\n+\t\treturn sinkWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> sinkWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(sinkWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof BatchExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof StreamExecExchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion =\n+\t\t\t\twrapper.execNode instanceof BatchExecUnion || wrapper.execNode instanceof StreamExecUnion;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isTailOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(inputWrapper -> isNewSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof BatchExecExchange) &&\n+\t\t\t\t\t!(input instanceof StreamExecExchange) &&\n+\t\t\t\t\t!isNewSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&\n+\t\t\t\t\t((BatchExecExchange) inputWrapper.execNode)\n+\t\t\t\t\t\t.distribution.getType() == RelDistribution.Type.SINGLETON);\n+\n+\t\t\tif (shouldRemove) {\n+\t\t\t\twrapper.removeFromGroup();\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate boolean isTailOfMultipleInputGroup(ExecNodeWrapper wrapper) {", "originalCommit": "3c50597400946ab21e7dba24673a0b5df30eaf2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwMDUwMA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511700500", "bodyText": "I actually don't like the idea of head and tail in the multiple input operator as they're ambiguous. Why don't we use phrases like input, entrance and root?", "author": "tsreaper", "createdAt": "2020-10-26T03:15:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMzUwNw=="}], "type": "inlineReview", "revised_code": {"commit": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\nindex 186586025b..a52aa66383 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/MultipleInputNodeCreationProcessor.java\n\n@@ -18,6 +18,9 @@\n \n package org.apache.flink.table.planner.plan.reuse;\n \n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.streaming.api.transformations.SourceTransformation;\n"}}, {"oid": "bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "url": "https://github.com/apache/flink/commit/bd35ec8e23b97addfbf796e11221b94b85f2b0a3", "message": "[FLINK-19626][table-planner-blink] Introduce multi-input operator construction algorithm", "committedDate": "2020-10-26T05:54:09Z", "type": "commit"}, {"oid": "d96520855ab1759593c9970c02ad5dc57b8dc4ec", "url": "https://github.com/apache/flink/commit/d96520855ab1759593c9970c02ad5dc57b8dc4ec", "message": "[fix] Fix checkstyle", "committedDate": "2020-10-26T05:54:09Z", "type": "commit"}, {"oid": "06e3e00eb815a7e6777491ef2b9a645a330bb2d0", "url": "https://github.com/apache/flink/commit/06e3e00eb815a7e6777491ef2b9a645a330bb2d0", "message": "[fix] Fix scalastyle", "committedDate": "2020-10-26T05:54:10Z", "type": "commit"}, {"oid": "02e0db125833bf12ab5856b08a82a03fc5454958", "url": "https://github.com/apache/flink/commit/02e0db125833bf12ab5856b08a82a03fc5454958", "message": "[fix] Fix comments", "committedDate": "2020-10-26T06:57:02Z", "type": "commit"}, {"oid": "1121f5b1525294821397a62519e33f21c01a097a", "url": "https://github.com/apache/flink/commit/1121f5b1525294821397a62519e33f21c01a097a", "message": "[fix] Fix docs", "committedDate": "2020-10-26T09:43:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkxMTU5Mw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r511911593", "bodyText": "order -> distance", "author": "godfreyhe", "createdAt": "2020-10-26T12:08:05Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest order is 0 (which are exactly the nodes without inputs) and the distances of", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\nsimilarity index 97%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\nindex 37651da42f..2f51585a45 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/TopologyGraph.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.reuse;\n+package org.apache.flink.table.planner.plan.processor.utils;\n \n import org.apache.flink.annotation.Internal;\n import org.apache.flink.annotation.VisibleForTesting;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1OTMxNg==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512059316", "bodyText": "InputPriorityBasedTopologyGraphGenerator ?  even we can simplified it as TopologyGraphGenerator", "author": "godfreyhe", "createdAt": "2020-10-26T15:39:34Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nindex 6fb05df31d..904a9c78f7 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\n\n@@ -82,7 +82,7 @@ import java.util.TreeMap;\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public abstract class AbstractInputPriorityConflictResolver {\n+public abstract class InputPriorityGraphGenerator {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n \tprivate final Set<ExecNode<?, ?>> boundaries;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MjU0MQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512062541", "bodyText": "createTopologyGraph?", "author": "godfreyhe", "createdAt": "2020-10-26T15:43:44Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n \n-\tprivate TopologyGraph graph;\n+\tprotected TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\t/**\n+\t * Create an {@link AbstractInputPriorityConflictResolver} for the given {@link ExecNode} sub-graph.\n+\t *\n+\t * @param roots the first layer of nodes on the output side of the sub-graph\n+\t * @param boundaries the first layer of nodes on the input side of the sub-graph\n+\t * @param safeDamBehavior when checking for conflicts we'll ignore the edges with\n+\t *                        {@link ExecEdge.DamBehavior} stricter or equal than this\n+\t */\n+\tpublic AbstractInputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n \t}\n \n-\tpublic void detectAndResolve() {\n+\tprotected void createTopologyGraphAndResolveConflict() {", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nindex 6fb05df31d..904a9c78f7 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\n\n@@ -82,7 +82,7 @@ import java.util.TreeMap;\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public abstract class AbstractInputPriorityConflictResolver {\n+public abstract class InputPriorityGraphGenerator {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n \tprivate final Set<ExecNode<?, ?>> boundaries;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MzYzNQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512063635", "bodyText": "resolveInputPriorityConflict", "author": "godfreyhe", "createdAt": "2020-10-26T15:45:06Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -205,146 +205,5 @@ protected void visitNode(ExecNode<?, ?> node) {\n \t\treturn ret;\n \t}\n \n-\tprivate BatchExecExchange createExchange(ExecNode<?, ?> node, int idx) {\n-\t\tRelNode inputRel = (RelNode) node.getInputNodes().get(idx);\n-\n-\t\tFlinkRelDistribution distribution;\n-\t\tExecEdge.RequiredShuffle requiredShuffle = node.getInputEdges().get(idx).getRequiredShuffle();\n-\t\tif (requiredShuffle.getType() == ExecEdge.ShuffleType.HASH) {\n-\t\t\tdistribution = FlinkRelDistribution.hash(requiredShuffle.getKeys(), true);\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.BROADCAST) {\n-\t\t\t// should not occur\n-\t\t\tthrow new IllegalStateException(\n-\t\t\t\t\"Trying to resolve input priority conflict on broadcast side. This is not expected.\");\n-\t\t} else if (requiredShuffle.getType() == ExecEdge.ShuffleType.SINGLETON) {\n-\t\t\tdistribution = FlinkRelDistribution.SINGLETON();\n-\t\t} else {\n-\t\t\tdistribution = FlinkRelDistribution.ANY();\n-\t\t}\n-\n-\t\tBatchExecExchange exchange = new BatchExecExchange(\n-\t\t\tinputRel.getCluster(),\n-\t\t\tinputRel.getTraitSet().replace(distribution),\n-\t\t\tinputRel,\n-\t\t\tdistribution);\n-\t\texchange.setRequiredShuffleMode(ShuffleMode.BATCH);\n-\t\treturn exchange;\n-\t}\n-\n-\t/**\n-\t * A data structure storing the topological information of an {@link ExecNode} graph.\n-\t */\n-\t@VisibleForTesting\n-\tstatic class TopologyGraph {\n-\t\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n-\n-\t\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n-\t\t\tthis.nodes = new HashMap<>();\n-\n-\t\t\t// we first link all edges in the original exec node graph\n-\t\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n-\t\t\t\t@Override\n-\t\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n-\t\t\t\t\t\tlink(input, node);\n-\t\t\t\t\t}\n-\t\t\t\t\tvisitInputs(node);\n-\t\t\t\t}\n-\t\t\t};\n-\t\t\troots.forEach(n -> n.accept(visitor));\n-\t\t}\n-\n-\t\t/**\n-\t\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n-\t\t * Returns if this edge is successfully added.\n-\t\t */\n-\t\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tif (canReach(toNode, fromNode)) {\n-\t\t\t\t// invalid edge, as `to` is the predecessor of `from`\n-\t\t\t\treturn false;\n-\t\t\t} else {\n-\t\t\t\t// link `from` and `to`\n-\t\t\t\tfromNode.outputs.add(toNode);\n-\t\t\t\ttoNode.inputs.add(fromNode);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t}\n-\n-\t\t/**\n-\t\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n-\t\t */\n-\t\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\n-\t\t\tfromNode.outputs.remove(toNode);\n-\t\t\ttoNode.inputs.remove(fromNode);\n-\t\t}\n-\n-\t\t@VisibleForTesting\n-\t\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n-\t\t\tTopologyNode fromNode = getTopologyNode(from);\n-\t\t\tTopologyNode toNode = getTopologyNode(to);\n-\t\t\treturn canReach(fromNode, toNode);\n-\t\t}\n-\n-\t\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n-\t\t\tSet<TopologyNode> visited = new HashSet<>();\n-\t\t\tvisited.add(from);\n-\t\t\tQueue<TopologyNode> queue = new LinkedList<>();\n-\t\t\tqueue.offer(from);\n-\n-\t\t\twhile (!queue.isEmpty()) {\n-\t\t\t\tTopologyNode node = queue.poll();\n-\t\t\t\tif (to.equals(node)) {\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\n-\t\t\t\tfor (TopologyNode next : node.outputs) {\n-\t\t\t\t\tif (visited.contains(next)) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t\tvisited.add(next);\n-\t\t\t\t\tqueue.offer(next);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {\n-\t\t\t// NOTE: We treat different `BatchExecBoundedStreamScan`s with same `DataStream` object as the same\n-\t\t\tif (execNode instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\tDataStream<?> currentStream =\n-\t\t\t\t\t((BatchExecBoundedStreamScan) execNode).boundedStreamTable().dataStream();\n-\t\t\t\tfor (Map.Entry<ExecNode<?, ?>, TopologyNode> entry : nodes.entrySet()) {\n-\t\t\t\t\tExecNode<?, ?> key = entry.getKey();\n-\t\t\t\t\tif (key instanceof BatchExecBoundedStreamScan) {\n-\t\t\t\t\t\tDataStream<?> existingStream =\n-\t\t\t\t\t\t\t((BatchExecBoundedStreamScan) key).boundedStreamTable().dataStream();\n-\t\t\t\t\t\tif (existingStream.equals(currentStream)) {\n-\t\t\t\t\t\t\treturn entry.getValue();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tTopologyNode result = new TopologyNode();\n-\t\t\t\tnodes.put(execNode, result);\n-\t\t\t\treturn result;\n-\t\t\t} else {\n-\t\t\t\treturn nodes.computeIfAbsent(execNode, k -> new TopologyNode());\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A node in the {@link TopologyGraph}.\n-\t */\n-\tprivate static class TopologyNode {\n-\t\tprivate final Set<TopologyNode> inputs = new HashSet<>();\n-\t\tprivate final Set<TopologyNode> outputs = new HashSet<>();\n-\t}\n+\tprotected abstract void resolveConflict(ExecNode<?, ?> node, int conflictInput);", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nindex 6fb05df31d..904a9c78f7 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\n\n@@ -205,5 +205,5 @@ public abstract class AbstractInputPriorityConflictResolver {\n \t\treturn ret;\n \t}\n \n-\tprotected abstract void resolveConflict(ExecNode<?, ?> node, int conflictInput);\n+\tprotected abstract void resolveInputPriorityConflict(ExecNode<?, ?> node, int conflictInput);\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NDIyNg==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512064226", "bodyText": "updateTopologyGraph", "author": "godfreyhe", "createdAt": "2020-10-26T15:45:51Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java", "diffHunk": "@@ -88,41 +76,58 @@\n  *\n  * <p>This class maintains a topological graph in which an edge pointing from vertex A to vertex B indicates\n  * that the results from vertex A need to be read before those from vertex B. A loop in the graph indicates\n- * a deadlock, and we resolve such deadlock by inserting a {@link BatchExecExchange} with batch shuffle mode.\n+ * a deadlock, and different subclasses of this class resolve the conflict in different ways.\n  *\n  * <p>For a detailed explanation of the algorithm, see appendix of the\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public class InputPriorityConflictResolver {\n+public abstract class AbstractInputPriorityConflictResolver {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n+\tprivate final Set<ExecNode<?, ?>> boundaries;\n+\tprivate final ExecEdge.DamBehavior safeDamBehavior;\n \n-\tprivate TopologyGraph graph;\n+\tprotected TopologyGraph graph;\n \n-\tpublic InputPriorityConflictResolver(List<ExecNode<?, ?>> roots) {\n+\t/**\n+\t * Create an {@link AbstractInputPriorityConflictResolver} for the given {@link ExecNode} sub-graph.\n+\t *\n+\t * @param roots the first layer of nodes on the output side of the sub-graph\n+\t * @param boundaries the first layer of nodes on the input side of the sub-graph\n+\t * @param safeDamBehavior when checking for conflicts we'll ignore the edges with\n+\t *                        {@link ExecEdge.DamBehavior} stricter or equal than this\n+\t */\n+\tpublic AbstractInputPriorityConflictResolver(\n+\t\t\tList<ExecNode<?, ?>> roots,\n+\t\t\tSet<ExecNode<?, ?>> boundaries,\n+\t\t\tExecEdge.DamBehavior safeDamBehavior) {\n \t\tPreconditions.checkArgument(\n \t\t\troots.stream().allMatch(root -> root instanceof BatchExecNode),\n \t\t\t\"InputPriorityConflictResolver can only be used for batch jobs.\");\n \t\tthis.roots = roots;\n+\t\tthis.boundaries = boundaries;\n+\t\tthis.safeDamBehavior = safeDamBehavior;\n \t}\n \n-\tpublic void detectAndResolve() {\n+\tprotected void createTopologyGraphAndResolveConflict() {\n \t\t// build an initial topology graph\n-\t\tgraph = new TopologyGraph(roots);\n+\t\tgraph = new TopologyGraph(roots, boundaries);\n \n \t\t// check and resolve conflicts about input priorities\n \t\tAbstractExecNodeExactlyOnceVisitor inputPriorityVisitor = new AbstractExecNodeExactlyOnceVisitor() {\n \t\t\t@Override\n \t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n-\t\t\t\tvisitInputs(node);\n-\t\t\t\tcheckInputPriorities(node);\n+\t\t\t\tif (!boundaries.contains(node)) {\n+\t\t\t\t\tvisitInputs(node);\n+\t\t\t\t}\n+\t\t\t\tupdateTopologyGraphAndResolveConflict(node);\n \t\t\t}\n \t\t};\n \t\troots.forEach(n -> n.accept(inputPriorityVisitor));\n \t}\n \n-\tprivate void checkInputPriorities(ExecNode<?, ?> node) {\n+\tprivate void updateTopologyGraphAndResolveConflict(ExecNode<?, ?> node) {", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\nindex 6fb05df31d..904a9c78f7 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/AbstractInputPriorityConflictResolver.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityGraphGenerator.java\n\n@@ -82,7 +82,7 @@ import java.util.TreeMap;\n  * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n  */\n @Internal\n-public abstract class AbstractInputPriorityConflictResolver {\n+public abstract class InputPriorityGraphGenerator {\n \n \tprivate final List<ExecNode<?, ?>> roots;\n \tprivate final Set<ExecNode<?, ?>> boundaries;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NDQ2Ng==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512064466", "bodyText": "InputPriorityConflictResolver", "author": "godfreyhe", "createdAt": "2020-10-26T15:46:09Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.trait.FlinkRelDistribution;\n+\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Subclass of the {@link AbstractInputPriorityConflictResolver}.\n+ *\n+ * <p>This class resolve conflicts by inserting a {@link BatchExecExchange} into the conflicting input.\n+ */\n+@Internal\n+public class InputPriorityConflictResolverWithExchange extends AbstractInputPriorityConflictResolver {", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolver.java\nsimilarity index 87%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolver.java\nindex 25f2400beb..d524da4edc 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputPriorityConflictResolverWithExchange.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolver.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.reuse;\n+package org.apache.flink.table.planner.plan.processor.utils;\n \n import org.apache.flink.annotation.Internal;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2NzM2Mw==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512067363", "bodyText": "the package name is not correct", "author": "godfreyhe", "createdAt": "2020-10-26T15:49:46Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.reuse;", "originalCommit": "1121f5b1525294821397a62519e33f21c01a097a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputOrderCalculator.java\nsimilarity index 90%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputOrderCalculator.java\nindex e189578fdb..1a148d19b7 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/reuse/InputOrderCalculator.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/InputOrderCalculator.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.reuse;\n+package org.apache.flink.table.planner.plan.processor.utils;\n \n import org.apache.flink.annotation.Internal;\n import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n"}}, {"oid": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "url": "https://github.com/apache/flink/commit/006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "message": "[fix] Fix comments", "committedDate": "2020-10-27T03:24:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwNzMwNQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512407305", "bodyText": "move it to org.apache.flink.table.planner.plan.nodes.process", "author": "godfreyhe", "createdAt": "2020-10-27T04:17:26Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor;", "originalCommit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/DeadlockBreakupProcessor.java\nsimilarity index 93%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/DeadlockBreakupProcessor.java\nindex 1b07b659d6..7a23fc3e4e 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/DeadlockBreakupProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/DeadlockBreakupProcessor.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.processor;\n+package org.apache.flink.table.planner.plan.processors;\n \n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.table.api.TableException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQwOTMxNA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512409314", "bodyText": "also check the shuffle mode ?", "author": "godfreyhe", "createdAt": "2020-10-27T04:26:11Z", "path": "flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+\n+/**\n+ * Tests for {@link InputPriorityConflictResolver}.\n+ */\n+public class InputPriorityConflictResolverTest {\n+\n+\t@Test\n+\tpublic void testDetectAndResolve() {\n+\t\t// P = ExecEdge.DamBehavior.PIPELINED, E = ExecEdge.DamBehavior.END_INPUT\n+\t\t// P100 = PIPELINED + priority 100\n+\t\t//\n+\t\t// 0 --------(P0)----> 1 --(P0)-----------> 7\n+\t\t//  \\                    \\-(P0)-> 2 -(P0)--/\n+\t\t//   \\-------(P0)----> 3 --(P1)-----------/\n+\t\t//    \\------(P0)----> 4 --(P10)---------/\n+\t\t//     \\              /                 /\n+\t\t//      \\    8 -(P0)-<                 /\n+\t\t//       \\            \\               /\n+\t\t//        \\--(E0)----> 5 --(P10)-----/\n+\t\t// 6 ---------(P100)----------------/\n+\t\tTestingBatchExecNode[] nodes = new TestingBatchExecNode[9];\n+\t\tfor (int i = 0; i < nodes.length; i++) {\n+\t\t\tnodes[i] = new TestingBatchExecNode();\n+\t\t}\n+\t\tnodes[1].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[2].addInput(nodes[1], ExecEdge.builder().priority(0).build());\n+\t\tnodes[3].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[4].addInput(nodes[8], ExecEdge.builder().priority(0).build());\n+\t\tnodes[4].addInput(nodes[0], ExecEdge.builder().priority(0).build());\n+\t\tnodes[5].addInput(nodes[8], ExecEdge.builder().priority(0).build());\n+\t\tnodes[5].addInput(nodes[0], ExecEdge.builder().damBehavior(ExecEdge.DamBehavior.END_INPUT).priority(0).build());\n+\t\tnodes[7].addInput(nodes[1], ExecEdge.builder().priority(0).build());\n+\t\tnodes[7].addInput(nodes[2], ExecEdge.builder().priority(0).build());\n+\t\tnodes[7].addInput(nodes[3], ExecEdge.builder().priority(1).build());\n+\t\tnodes[7].addInput(nodes[4], ExecEdge.builder().priority(10).build());\n+\t\tnodes[7].addInput(nodes[5], ExecEdge.builder().priority(10).build());\n+\t\tnodes[7].addInput(nodes[6], ExecEdge.builder().priority(100).build());\n+\n+\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\tCollections.singletonList(nodes[7]),\n+\t\t\tExecEdge.DamBehavior.END_INPUT,\n+\t\t\tShuffleMode.BATCH);\n+\t\tresolver.detectAndResolve();\n+\t\tAssert.assertEquals(nodes[1], nodes[7].getInputNodes().get(0));\n+\t\tAssert.assertEquals(nodes[2], nodes[7].getInputNodes().get(1));\n+\t\tAssert.assertTrue(nodes[7].getInputNodes().get(2) instanceof BatchExecExchange);", "originalCommit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processors/utils/InputPriorityConflictResolverTest.java\nsimilarity index 90%\nrename from flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java\nrename to flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processors/utils/InputPriorityConflictResolverTest.java\nindex 4bb03e4b47..eab251a14b 100644\n--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processor/utils/InputPriorityConflictResolverTest.java\n+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/plan/processors/utils/InputPriorityConflictResolverTest.java\n\n@@ -16,8 +16,9 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.processor.utils;\n+package org.apache.flink.table.planner.plan.processors.utils;\n \n+import org.apache.flink.configuration.Configuration;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n import org.apache.flink.table.planner.plan.nodes.exec.TestingBatchExecNode;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQxMjI0NA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512412244", "bodyText": "should also consider StreamExecExchange here? BatchExecExchange -> Exchange", "author": "godfreyhe", "createdAt": "2020-10-27T04:39:00Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor;\n+\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.transformations.ShuffleMode;\n+import org.apache.flink.streaming.api.transformations.SourceTransformation;\n+import org.apache.flink.table.connector.source.SourceProvider;\n+import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecEdge;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecExchange;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDataStreamScan;\n+import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecMultipleInputNode;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessContext;\n+import org.apache.flink.table.planner.plan.nodes.process.DAGProcessor;\n+import org.apache.flink.table.planner.plan.processor.utils.InputOrderCalculator;\n+import org.apache.flink.table.planner.plan.processor.utils.InputPriorityConflictResolver;\n+import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;\n+import org.apache.flink.util.Preconditions;\n+\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Exchange;\n+import org.apache.calcite.rel.core.Union;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A {@link DAGProcessor} which organize {@link ExecNode}s into multiple input nodes.\n+ *\n+ * <p>For a detailed explanation of the algorithm, see appendix of the\n+ * <a href=\"https://docs.google.com/document/d/1qKVohV12qn-bM51cBZ8Hcgp31ntwClxjoiNBUOqVHsI\">design doc</a>.\n+ */\n+public class MultipleInputNodeCreationProcessor implements DAGProcessor {\n+\n+\tprivate final boolean isStreaming;\n+\n+\tpublic MultipleInputNodeCreationProcessor(boolean isStreaming) {\n+\t\tthis.isStreaming = isStreaming;\n+\t}\n+\n+\t@Override\n+\tpublic List<ExecNode<?, ?>> process(List<ExecNode<?, ?>> roots, DAGProcessContext context) {\n+\t\tif (!isStreaming) {\n+\t\t\t// As multiple input nodes use function call to deliver records between sub-operators,\n+\t\t\t// we cannot rely on network buffers to buffer records not yet ready to be read,\n+\t\t\t// so only BLOCKING dam behavior is safe here.\n+\t\t\t// If conflict is detected under this stricter constraint,\n+\t\t\t// we add a PIPELINED exchange to mark that its input and output node cannot be merged\n+\t\t\t// into the same multiple input node\n+\t\t\tInputPriorityConflictResolver resolver = new InputPriorityConflictResolver(\n+\t\t\t\troots,\n+\t\t\t\tExecEdge.DamBehavior.BLOCKING,\n+\t\t\t\tShuffleMode.PIPELINED);\n+\t\t\tresolver.detectAndResolve();\n+\t\t}\n+\n+\t\tList<ExecNodeWrapper> rootWrappers = wrapExecNodes(roots);\n+\t\t// sort all nodes in topological order, sinks come first and sources come last\n+\t\tList<ExecNodeWrapper> orderedWrappers = topologicalSort(rootWrappers);\n+\t\t// group nodes into multiple input groups\n+\t\tcreateMultipleInputGroups(orderedWrappers);\n+\t\t// apply optimizations to remove unnecessary nodes out of multiple input groups\n+\t\toptimizeMultipleInputGroups(orderedWrappers);\n+\n+\t\t// create the real multiple input nodes\n+\t\treturn createMultipleInputNodes(rootWrappers);\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Wrapping and Sorting\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate List<ExecNodeWrapper> wrapExecNodes(List<ExecNode<?, ?>> rootNodes) {\n+\t\tMap<ExecNode<?, ?>, ExecNodeWrapper> wrapperMap = new HashMap<>();\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tExecNodeWrapper wrapper = wrapperMap.computeIfAbsent(node, k -> new ExecNodeWrapper(node));\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tExecNodeWrapper inputWrapper = wrapperMap.computeIfAbsent(input, k -> new ExecNodeWrapper(input));\n+\t\t\t\t\twrapper.inputs.add(inputWrapper);\n+\t\t\t\t\tinputWrapper.outputs.add(wrapper);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\trootNodes.forEach(s -> s.accept(visitor));\n+\n+\t\tList<ExecNodeWrapper> rootWrappers = new ArrayList<>();\n+\t\tfor (ExecNode<?, ?> root : rootNodes) {\n+\t\t\tExecNodeWrapper rootWrapper = wrapperMap.get(root);\n+\t\t\tPreconditions.checkNotNull(rootWrapper, \"Root node is not wrapped. This is a bug.\");\n+\t\t\trootWrappers.add(rootWrapper);\n+\t\t}\n+\t\treturn rootWrappers;\n+\t}\n+\n+\tprivate List<ExecNodeWrapper> topologicalSort(List<ExecNodeWrapper> rootWrappers) {\n+\t\tList<ExecNodeWrapper> result = new ArrayList<>();\n+\t\tQueue<ExecNodeWrapper> queue = new LinkedList<>(rootWrappers);\n+\t\tMap<ExecNodeWrapper, Integer> visitCountMap = new HashMap<>();\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tExecNodeWrapper wrapper = queue.poll();\n+\t\t\tresult.add(wrapper);\n+\t\t\tfor (ExecNodeWrapper inputWrapper : wrapper.inputs) {\n+\t\t\t\tint visitCount = visitCountMap.compute(inputWrapper, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (visitCount == inputWrapper.outputs.size()) {\n+\t\t\t\t\tqueue.offer(inputWrapper);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Creating\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void createMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sinks to sources\n+\t\tfor (ExecNodeWrapper wrapper : orderedWrappers) {\n+\t\t\t// we skip nodes which cannot be a member of a multiple input node\n+\t\t\tif (!canBeMultipleInputNodeMember(wrapper)) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we first try to assign this wrapper into the same group with its outputs\n+\t\t\tMultipleInputGroup outputGroup = canBeInSameGroupWithOutputs(wrapper);\n+\t\t\tif (outputGroup != null) {\n+\t\t\t\twrapper.addToGroup(outputGroup);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\t// we then try to create a new multiple input group with this node as the root\n+\t\t\tif (canBeRootOfMultipleInputGroup(wrapper)) {\n+\t\t\t\twrapper.createGroup();\n+\t\t\t}\n+\n+\t\t\t// all our attempts failed, this node will not be in a multiple input node\n+\t\t}\n+\t}\n+\n+\tprivate boolean canBeMultipleInputNodeMember(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.inputs.isEmpty()) {\n+\t\t\t// sources cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\t\tif (wrapper.execNode instanceof Exchange) {\n+\t\t\t// exchange cannot be a member of multiple input node\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * A node can only be assigned into the same multiple input group of its outputs\n+\t * if all outputs have a group and are the same.\n+\t *\n+\t * @return the {@link MultipleInputGroup} of the outputs if all outputs have a\n+\t *         group and are the same, null otherwise\n+\t */\n+\tprivate MultipleInputGroup canBeInSameGroupWithOutputs(ExecNodeWrapper wrapper) {\n+\t\tif (wrapper.outputs.isEmpty()) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tMultipleInputGroup outputGroup = wrapper.outputs.get(0).group;\n+\t\tif (outputGroup == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\tfor (ExecNodeWrapper outputWrapper : wrapper.outputs) {\n+\t\t\tif (outputWrapper.group != outputGroup) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn outputGroup;\n+\t}\n+\n+\tprivate boolean canBeRootOfMultipleInputGroup(ExecNodeWrapper wrapper) {\n+\t\t// only a node with more than one input can be the root,\n+\t\t// as one-input operator chaining are handled by operator chains\n+\t\treturn wrapper.inputs.size() >= 2;\n+\t}\n+\n+\t// --------------------------------------------------------------------------------\n+\t// Multiple Input Groups Optimizing\n+\t// --------------------------------------------------------------------------------\n+\n+\tprivate void optimizeMultipleInputGroups(List<ExecNodeWrapper> orderedWrappers) {\n+\t\t// wrappers are checked in topological order from sources to sinks\n+\t\tfor (int i = orderedWrappers.size() - 1; i >= 0; i--) {\n+\t\t\tExecNodeWrapper wrapper = orderedWrappers.get(i);\n+\t\t\tMultipleInputGroup group = wrapper.group;\n+\t\t\tif (group == null) {\n+\t\t\t\t// we only consider nodes currently in a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean isUnion = wrapper.execNode instanceof Union;\n+\n+\t\t\tif (group.members.size() == 1) {\n+\t\t\t\tPreconditions.checkState(\n+\t\t\t\t\twrapper == group.root,\n+\t\t\t\t\t\"The only member of a multiple input group is not its root. This is a bug.\");\n+\t\t\t\t// optimization 1. we clean up multiple input groups with only 1 member,\n+\t\t\t\t// unless one of its input is a FLIP-27 source (for maximizing source chaining),\n+\t\t\t\t// however unions do not apply to this optimization because they're not real operators\n+\t\t\t\tif (isUnion || wrapper.inputs.stream().noneMatch(\n+\t\t\t\t\t\tinputWrapper -> isChainableSource(inputWrapper.execNode))) {\n+\t\t\t\t\twrapper.removeFromGroup();\n+\t\t\t\t}\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif (!isEntranceOfMultipleInputGroup(wrapper)) {\n+\t\t\t\t// we're not removing a node from the middle of a multiple input group\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tboolean shouldRemove = false;\n+\t\t\tif (isUnion) {\n+\t\t\t\t// optimization 2. we do not allow union to be the tail of a multiple input\n+\t\t\t\t// as we're paying extra function calls for this, unless one of the united\n+\t\t\t\t// input is a FLIP-27 source\n+\t\t\t\tshouldRemove = wrapper.inputs.stream().noneMatch(\n+\t\t\t\t\tinputWrapper -> isChainableSource(inputWrapper.execNode));\n+\t\t\t} else if (wrapper.inputs.size() == 1) {\n+\t\t\t\t// optimization 3. for one-input operators we'll remove it unless its input\n+\t\t\t\t// is an exchange or a FLIP-27 source, this is mainly to avoid the following\n+\t\t\t\t// pattern:\n+\t\t\t\t// non-chainable source -> calc --\\\n+\t\t\t\t//                                 join ->\n+\t\t\t\t// non-chainable source -> calc --/\n+\t\t\t\t// if we move two calcs into the multiple input group rooted at the join, we're\n+\t\t\t\t// directly shuffling large amount of records from the source without filtering\n+\t\t\t\t// by the calc\n+\t\t\t\tExecNode<?, ?> input = wrapper.inputs.get(0).execNode;\n+\t\t\t\tshouldRemove = !(input instanceof Exchange) && !isChainableSource(input);\n+\t\t\t}\n+\n+\t\t\t// optimization 4. for singleton operations (for example singleton global agg)\n+\t\t\t// we're not including it into the multiple input node as we have to ensure that\n+\t\t\t// the whole multiple input can only have 1 parallelism.\n+\t\t\t// continuous singleton operations connected by forwarding shuffle will be dealt\n+\t\t\t// together with optimization 3\n+\t\t\tshouldRemove |= wrapper.inputs.stream().anyMatch(inputWrapper ->\n+\t\t\t\tinputWrapper.execNode instanceof BatchExecExchange &&", "originalCommit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/MultipleInputNodeCreationProcessor.java\nsimilarity index 97%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/MultipleInputNodeCreationProcessor.java\nindex 9870f926ea..3b787ff0d4 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/MultipleInputNodeCreationProcessor.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/MultipleInputNodeCreationProcessor.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.processor;\n+package org.apache.flink.table.planner.plan.processors;\n \n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.streaming.api.transformations.ShuffleMode;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMTkzNQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512431935", "bodyText": "give distance a definition ?", "author": "godfreyhe", "createdAt": "2020-10-27T05:52:46Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n+\t * other nodes are the largest distances in their inputs plus 1.\n+\t */\n+\tMap<ExecNode<?, ?>, Integer> calculateDistance() {", "originalCommit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\nsimilarity index 90%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\nindex 2f51585a45..c52aaaf271 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.processor.utils;\n+package org.apache.flink.table.planner.plan.processors.utils;\n \n import org.apache.flink.annotation.Internal;\n import org.apache.flink.annotation.VisibleForTesting;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQzMjQ4MQ==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512432481", "bodyText": "getOrCreateTopologyNode ?", "author": "godfreyhe", "createdAt": "2020-10-27T05:54:36Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.plan.processor.utils;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.annotation.VisibleForTesting;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.table.planner.plan.nodes.exec.AbstractExecNodeExactlyOnceVisitor;\n+import org.apache.flink.table.planner.plan.nodes.exec.ExecNode;\n+import org.apache.flink.table.planner.plan.nodes.physical.batch.BatchExecBoundedStreamScan;\n+import org.apache.flink.util.Preconditions;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/**\n+ * A data structure storing the topological and input priority information of an {@link ExecNode} graph.\n+ */\n+@Internal\n+class TopologyGraph {\n+\n+\tprivate final Map<ExecNode<?, ?>, TopologyNode> nodes;\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots) {\n+\t\tthis(roots, Collections.emptySet());\n+\t}\n+\n+\tTopologyGraph(List<ExecNode<?, ?>> roots, Set<ExecNode<?, ?>> boundaries) {\n+\t\tthis.nodes = new HashMap<>();\n+\n+\t\t// we first link all edges in the original exec node graph\n+\t\tAbstractExecNodeExactlyOnceVisitor visitor = new AbstractExecNodeExactlyOnceVisitor() {\n+\t\t\t@Override\n+\t\t\tprotected void visitNode(ExecNode<?, ?> node) {\n+\t\t\t\tif (boundaries.contains(node)) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\t\t\t\tfor (ExecNode<?, ?> input : node.getInputNodes()) {\n+\t\t\t\t\tlink(input, node);\n+\t\t\t\t}\n+\t\t\t\tvisitInputs(node);\n+\t\t\t}\n+\t\t};\n+\t\troots.forEach(n -> n.accept(visitor));\n+\t}\n+\n+\t/**\n+\t * Link an edge from `from` node to `to` node if no loop will occur after adding this edge.\n+\t * Returns if this edge is successfully added.\n+\t */\n+\tboolean link(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tif (canReach(toNode, fromNode)) {\n+\t\t\t// invalid edge, as `to` is the predecessor of `from`\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\t// link `from` and `to`\n+\t\t\tfromNode.outputs.add(toNode);\n+\t\t\ttoNode.inputs.add(fromNode);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Remove the edge from `from` node to `to` node. If there is no edge between them then do nothing.\n+\t */\n+\tvoid unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\n+\t\tfromNode.outputs.remove(toNode);\n+\t\ttoNode.inputs.remove(fromNode);\n+\t}\n+\n+\t/**\n+\t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n+\t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n+\t * other nodes are the largest distances in their inputs plus 1.\n+\t */\n+\tMap<ExecNode<?, ?>, Integer> calculateDistance() {\n+\t\tMap<ExecNode<?, ?>, Integer> result = new HashMap<>();\n+\t\tMap<TopologyNode, Integer> inputsVisitedMap = new HashMap<>();\n+\n+\t\tQueue<TopologyNode> queue = new LinkedList<>();\n+\t\tfor (TopologyNode node : nodes.values()) {\n+\t\t\tif (node.inputs.size() == 0) {\n+\t\t\t\tqueue.offer(node);\n+\t\t\t}\n+\t\t}\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tTopologyNode node = queue.poll();\n+\t\t\tint dist = -1;\n+\t\t\tfor (TopologyNode input : node.inputs) {\n+\t\t\t\tdist = Math.max(\n+\t\t\t\t\t\tdist,\n+\t\t\t\t\tPreconditions.checkNotNull(\n+\t\t\t\t\t\tresult.get(input.execNode),\n+\t\t\t\t\t\t\"The distance of an input node is not calculated. This is a bug.\"));\n+\t\t\t}\n+\t\t\tdist++;\n+\t\t\tresult.put(node.execNode, dist);\n+\n+\t\t\tfor (TopologyNode output : node.outputs) {\n+\t\t\t\tint inputsVisited = inputsVisitedMap.compute(output, (k, v) -> v == null ? 1 : v + 1);\n+\t\t\t\tif (inputsVisited == output.inputs.size()) {\n+\t\t\t\t\tqueue.offer(output);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n+\t@VisibleForTesting\n+\tboolean canReach(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n+\t\tTopologyNode fromNode = getTopologyNode(from);\n+\t\tTopologyNode toNode = getTopologyNode(to);\n+\t\treturn canReach(fromNode, toNode);\n+\t}\n+\n+\tprivate boolean canReach(TopologyNode from, TopologyNode to) {\n+\t\tSet<TopologyNode> visited = new HashSet<>();\n+\t\tvisited.add(from);\n+\t\tQueue<TopologyNode> queue = new LinkedList<>();\n+\t\tqueue.offer(from);\n+\n+\t\twhile (!queue.isEmpty()) {\n+\t\t\tTopologyNode node = queue.poll();\n+\t\t\tif (to.equals(node)) {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\n+\t\t\tfor (TopologyNode next : node.outputs) {\n+\t\t\t\tif (visited.contains(next)) {\n+\t\t\t\t\tcontinue;\n+\t\t\t\t}\n+\t\t\t\tvisited.add(next);\n+\t\t\t\tqueue.offer(next);\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn false;\n+\t}\n+\n+\tprivate TopologyNode getTopologyNode(ExecNode<?, ?> execNode) {", "originalCommit": "006756c81fa5c333b6fdc4bcbfc8820492a18e9b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\nsimilarity index 90%\nrename from flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\nrename to flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\nindex 2f51585a45..c52aaaf271 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processor/utils/TopologyGraph.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.flink.table.planner.plan.processor.utils;\n+package org.apache.flink.table.planner.plan.processors.utils;\n \n import org.apache.flink.annotation.Internal;\n import org.apache.flink.annotation.VisibleForTesting;\n"}}, {"oid": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "url": "https://github.com/apache/flink/commit/d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "message": "[fix] Fix comments", "committedDate": "2020-10-27T12:33:14Z", "type": "commit"}, {"oid": "9d6abe331e7dd4ef610dbc826aee44216ce14894", "url": "https://github.com/apache/flink/commit/9d6abe331e7dd4ef610dbc826aee44216ce14894", "message": "[fix] Change method name", "committedDate": "2020-10-27T12:38:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY1NzgxMA==", "url": "https://github.com/apache/flink/pull/13742#discussion_r512657810", "bodyText": "rename to calculateMaxDistance", "author": "godfreyhe", "createdAt": "2020-10-27T12:40:01Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/processors/utils/TopologyGraph.java", "diffHunk": "@@ -100,6 +100,9 @@ void unlink(ExecNode<?, ?> from, ExecNode<?, ?> to) {\n \t * Calculate the maximum distance of the currently added nodes from the nodes without inputs.\n \t * The smallest distance is 0 (which are exactly the nodes without inputs) and the distances of\n \t * other nodes are the largest distances in their inputs plus 1.\n+\t *\n+\t * <p>Distance of a node is defined as the number of edges one needs to go through from the\n+\t * nodes without inputs to this node.\n \t */\n \tMap<ExecNode<?, ?>, Integer> calculateDistance() {", "originalCommit": "d56e9dc09351c5465a26c4c3a047ea11bbe0515d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}