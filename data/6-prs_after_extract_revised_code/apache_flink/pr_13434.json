{"pr_number": 13434, "pr_title": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "pr_createdAt": "2020-09-21T03:59:14Z", "pr_url": "https://github.com/apache/flink/pull/13434", "timeline": [{"oid": "03e7a1d140dacb7667db4601272c599dff775370", "url": "https://github.com/apache/flink/commit/03e7a1d140dacb7667db4601272c599dff775370", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-21T12:36:31Z", "type": "forcePushed"}, {"oid": "dacd4ae1b261fa2dca65aa02b69142c34da121be", "url": "https://github.com/apache/flink/commit/dacd4ae1b261fa2dca65aa02b69142c34da121be", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-22T05:45:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcxNjk0OA==", "url": "https://github.com/apache/flink/pull/13434#discussion_r492716948", "bodyText": "Let's not modify HadoopUtils. Instead, if hadoopConfDir is not null, set ConfigConstants.PATH_HADOOP_CONFIG in the Configuration instance.", "author": "lirui-apache", "createdAt": "2020-09-22T13:06:04Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -199,7 +207,7 @@ private static HiveConf createHiveConf(@Nullable String hiveConfDir) {\n \t\tConfiguration hadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());\n \n \t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n-\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(new org.apache.flink.configuration.Configuration())) {\n+\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(new org.apache.flink.configuration.Configuration(), hadoopConfDir)) {", "originalCommit": "dacd4ae1b261fa2dca65aa02b69142c34da121be", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b79b3d92803dc7478830ac866e68936d1d372f58", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 57494099bc5..ec7be285051 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -203,11 +204,16 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t\t\tString.format(\"Failed to get hive-site.xml from %s\", hiveConfDir), e);\n \t\t}\n \n+\t\torg.apache.flink.configuration.Configuration flinkConfiguration = new org.apache.flink.configuration.Configuration();\n+\t\tif (!isNullOrWhitespaceOnly(hadoopConfDir)) {\n+\t\t\tflinkConfiguration.setString(ConfigConstants.PATH_HADOOP_CONFIG, hadoopConfDir);\n+\t\t}\n+\n \t\t// create HiveConf from hadoop configuration\n-\t\tConfiguration hadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());\n+\t\tConfiguration hadoopConf = HadoopUtils.getHadoopConfiguration(flinkConfiguration);\n \n \t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n-\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(new org.apache.flink.configuration.Configuration(), hadoopConfDir)) {\n+\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(flinkConfiguration)) {\n \t\t\tFile mapredSite = new File(new File(possibleHadoopConfPath), \"mapred-site.xml\");\n \t\t\tif (mapredSite.exists()) {\n \t\t\t\thadoopConf.addResource(new Path(mapredSite.getAbsolutePath()));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcxOTU1Mg==", "url": "https://github.com/apache/flink/pull/13434#discussion_r492719552", "bodyText": "Create a new test case for this", "author": "lirui-apache", "createdAt": "2020-09-22T13:09:57Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java", "diffHunk": "@@ -57,10 +59,10 @@\n \tpublic ExpectedException expectedException = ExpectedException.none();\n \n \t@Test\n-\tpublic void test() {\n+\tpublic void test() throws IOException {", "originalCommit": "dacd4ae1b261fa2dca65aa02b69142c34da121be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTkzMTYzMA==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495931630", "bodyText": "test is too generic, please give a meaningful name for this test", "author": "godfreyhe", "createdAt": "2020-09-28T13:18:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcxOTU1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "b79b3d92803dc7478830ac866e68936d1d372f58", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\nindex 686c524c59d..c16ffbeed06 100644\n--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\n+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\n\n@@ -59,10 +58,10 @@ public class HiveCatalogFactoryTest extends TestLogger {\n \tpublic ExpectedException expectedException = ExpectedException.none();\n \n \t@Test\n-\tpublic void test() throws IOException {\n+\tpublic void test() {\n \t\tfinal String catalogName = \"mycatalog\";\n \n-\t\tHiveCatalog expectedCatalog = HiveTestUtils.createHiveCatalog(catalogName, null);\n+\t\tfinal HiveCatalog expectedCatalog = HiveTestUtils.createHiveCatalog(catalogName, null);\n \n \t\tfinal HiveCatalogDescriptor catalogDescriptor = new HiveCatalogDescriptor();\n \t\tcatalogDescriptor.hiveSitePath(CONF_DIR.getPath());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjcyMDMyOQ==", "url": "https://github.com/apache/flink/pull/13434#discussion_r492720329", "bodyText": "Besides, we need to add a test case for the hadoop conf dir configuration.", "author": "lirui-apache", "createdAt": "2020-09-22T13:11:04Z", "path": "flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java", "diffHunk": "@@ -57,10 +59,10 @@\n \tpublic ExpectedException expectedException = ExpectedException.none();\n \n \t@Test\n-\tpublic void test() {\n+\tpublic void test() throws IOException {", "originalCommit": "dacd4ae1b261fa2dca65aa02b69142c34da121be", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b79b3d92803dc7478830ac866e68936d1d372f58", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\nindex 686c524c59d..c16ffbeed06 100644\n--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\n+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/catalog/hive/factories/HiveCatalogFactoryTest.java\n\n@@ -59,10 +58,10 @@ public class HiveCatalogFactoryTest extends TestLogger {\n \tpublic ExpectedException expectedException = ExpectedException.none();\n \n \t@Test\n-\tpublic void test() throws IOException {\n+\tpublic void test() {\n \t\tfinal String catalogName = \"mycatalog\";\n \n-\t\tHiveCatalog expectedCatalog = HiveTestUtils.createHiveCatalog(catalogName, null);\n+\t\tfinal HiveCatalog expectedCatalog = HiveTestUtils.createHiveCatalog(catalogName, null);\n \n \t\tfinal HiveCatalogDescriptor catalogDescriptor = new HiveCatalogDescriptor();\n \t\tcatalogDescriptor.hiveSitePath(CONF_DIR.getPath());\n"}}, {"oid": "b79b3d92803dc7478830ac866e68936d1d372f58", "url": "https://github.com/apache/flink/commit/b79b3d92803dc7478830ac866e68936d1d372f58", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-23T07:21:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUxOTc5Mw==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495519793", "bodyText": "Why do we need hdfs-default.xml?", "author": "lirui-apache", "createdAt": "2020-09-27T02:52:25Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java", "diffHunk": "@@ -421,6 +424,31 @@ public static void checkAcidTable(CatalogTable catalogTable, ObjectPath tablePat\n \t\t}\n \t}\n \n+\t/**\n+\t * Returns a new Hadoop Configuration object using the path to the hadoop conf configured.\n+\t *\n+\t * @param hadoopConfDir Hadoop conf directory path.\n+\t * @return A Hadoop configuration instance.\n+\t */\n+\tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n+\t\tConfiguration hadoopConfiguration = new Configuration();\n+\t\tif (new File(hadoopConfDir).exists()) {\n+\t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/core-site.xml\"));\n+\t\t\t}\n+\t\t\tif (new File(hadoopConfDir + \"/hdfs-default.xml\").exists()) {", "originalCommit": "f4033f16e326039a0f33958d096ba0c30ef26a6f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "07fb09f8651e79ab795f1d888d794b018a213a4f", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\nindex 938afdb1b85..37b581bd848 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n\n@@ -431,22 +431,28 @@ public class HiveTableUtil {\n \t * @return A Hadoop configuration instance.\n \t */\n \tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n-\t\tConfiguration hadoopConfiguration = new Configuration();\n \t\tif (new File(hadoopConfDir).exists()) {\n-\t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/core-site.xml\"));\n+\t\t\tConfiguration hadoopConfiguration = new Configuration();\n+\t\t\tFile coreSite = new File(hadoopConfDir, \"core-site.xml\");\n+\t\t\tif (coreSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(coreSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/hdfs-default.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-default.xml\"));\n+\t\t\tFile hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\n+\t\t\tif (hdfsSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hdfsSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/hdfs-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-site.xml\"));\n+\t\t\tFile yarnSite = new File(hadoopConfDir, \"yarn-site.xml\");\n+\t\t\tif (yarnSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(yarnSite.getAbsolutePath());\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/mapred-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/mapred-site.xml\"));\n+\t\t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n+\t\t\tFile mapredSite = new File(hadoopConfDir, \"mapred-site.xml\");\n+\t\t\tif (mapredSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(mapredSite.getAbsolutePath());\n \t\t\t}\n+\t\t\treturn hadoopConfiguration;\n \t\t}\n-\t\treturn hadoopConfiguration;\n+\t\treturn null;\n \t}\n \n \tprivate static class ExpressionExtractor implements ExpressionVisitor<String> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUxOTg2OQ==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495519869", "bodyText": "We also need yarn-site.xml, which is needed to generate Parquet splits in a kerberized environment.", "author": "lirui-apache", "createdAt": "2020-09-27T02:53:33Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java", "diffHunk": "@@ -421,6 +424,31 @@ public static void checkAcidTable(CatalogTable catalogTable, ObjectPath tablePat\n \t\t}\n \t}\n \n+\t/**\n+\t * Returns a new Hadoop Configuration object using the path to the hadoop conf configured.\n+\t *\n+\t * @param hadoopConfDir Hadoop conf directory path.\n+\t * @return A Hadoop configuration instance.\n+\t */\n+\tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n+\t\tConfiguration hadoopConfiguration = new Configuration();\n+\t\tif (new File(hadoopConfDir).exists()) {\n+\t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/core-site.xml\"));\n+\t\t\t}\n+\t\t\tif (new File(hadoopConfDir + \"/hdfs-default.xml\").exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-default.xml\"));\n+\t\t\t}\n+\t\t\tif (new File(hadoopConfDir + \"/hdfs-site.xml\").exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-site.xml\"));\n+\t\t\t}\n+\t\t\tif (new File(hadoopConfDir + \"/mapred-site.xml\").exists()) {", "originalCommit": "f4033f16e326039a0f33958d096ba0c30ef26a6f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "07fb09f8651e79ab795f1d888d794b018a213a4f", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\nindex 938afdb1b85..37b581bd848 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n\n@@ -431,22 +431,28 @@ public class HiveTableUtil {\n \t * @return A Hadoop configuration instance.\n \t */\n \tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n-\t\tConfiguration hadoopConfiguration = new Configuration();\n \t\tif (new File(hadoopConfDir).exists()) {\n-\t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/core-site.xml\"));\n+\t\t\tConfiguration hadoopConfiguration = new Configuration();\n+\t\t\tFile coreSite = new File(hadoopConfDir, \"core-site.xml\");\n+\t\t\tif (coreSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(coreSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/hdfs-default.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-default.xml\"));\n+\t\t\tFile hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\n+\t\t\tif (hdfsSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hdfsSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/hdfs-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-site.xml\"));\n+\t\t\tFile yarnSite = new File(hadoopConfDir, \"yarn-site.xml\");\n+\t\t\tif (yarnSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(yarnSite.getAbsolutePath());\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/mapred-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/mapred-site.xml\"));\n+\t\t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n+\t\t\tFile mapredSite = new File(hadoopConfDir, \"mapred-site.xml\");\n+\t\t\tif (mapredSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(mapredSite.getAbsolutePath());\n \t\t\t}\n+\t\t\treturn hadoopConfiguration;\n \t\t}\n-\t\treturn hadoopConfiguration;\n+\t\treturn null;\n \t}\n \n \tprivate static class ExpressionExtractor implements ExpressionVisitor<String> {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUyMDIxNw==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495520217", "bodyText": "Let's not rely on HadoopUtils.getHadoopConfiguration to get the configuration. We can just call HadoopUtils.possibleHadoopConfPaths to get the paths and load the files by ourselves. And the loading logic should be consistent with HiveTableUtil.getHadoopConfiguration.", "author": "lirui-apache", "createdAt": "2020-09-27T02:58:03Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -196,15 +204,21 @@ private static HiveConf createHiveConf(@Nullable String hiveConfDir) {\n \t\t}\n \n \t\t// create HiveConf from hadoop configuration\n-\t\tConfiguration hadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());\n+\t\tConfiguration hadoopConf;\n \n-\t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n-\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(new org.apache.flink.configuration.Configuration())) {\n-\t\t\tFile mapredSite = new File(new File(possibleHadoopConfPath), \"mapred-site.xml\");\n-\t\t\tif (mapredSite.exists()) {\n-\t\t\t\thadoopConf.addResource(new Path(mapredSite.getAbsolutePath()));\n-\t\t\t\tbreak;\n+\t\tif (isNullOrWhitespaceOnly(hadoopConfDir)) {\n+\t\t\thadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());", "originalCommit": "f4033f16e326039a0f33958d096ba0c30ef26a6f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "07fb09f8651e79ab795f1d888d794b018a213a4f", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 51d5b8cf39c..768b28b230b 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -203,24 +202,19 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t\t\tString.format(\"Failed to get hive-site.xml from %s\", hiveConfDir), e);\n \t\t}\n \n-\t\t// create HiveConf from hadoop configuration\n-\t\tConfiguration hadoopConf;\n-\n+\t\t// create HiveConf from hadoop configuration with hadoop conf directory configured.\n+\t\tConfiguration hadoopConf = null;\n \t\tif (isNullOrWhitespaceOnly(hadoopConfDir)) {\n-\t\t\thadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());\n-\n-\t\t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n \t\t\tfor (String possibleHadoopConfPath : HadoopUtils.possibleHadoopConfPaths(new org.apache.flink.configuration.Configuration())) {\n-\t\t\t\tFile mapredSite = new File(new File(possibleHadoopConfPath), \"mapred-site.xml\");\n-\t\t\t\tif (mapredSite.exists()) {\n-\t\t\t\t\thadoopConf.addResource(new Path(mapredSite.getAbsolutePath()));\n+\t\t\t\thadoopConf = getHadoopConfiguration(possibleHadoopConfPath);\n+\t\t\t\tif (hadoopConf != null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t}\n \t\t} else {\n-\t\t\thadoopConf = HiveTableUtil.getHadoopConfiguration(hadoopConfDir);\n+\t\t\thadoopConf = getHadoopConfiguration(hadoopConfDir);\n \t\t}\n-\t\treturn new HiveConf(hadoopConf, HiveConf.class);\n+\t\treturn new HiveConf(hadoopConf == null ? new Configuration() : hadoopConf, HiveConf.class);\n \t}\n \n \t@VisibleForTesting\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUzNDU5MA==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495534590", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n          \n          \n            \n            \t\t\tif (new File(hadoopConfDir, \"core-site.xml\").exists()) {", "author": "lirui-apache", "createdAt": "2020-09-27T06:15:53Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java", "diffHunk": "@@ -431,22 +431,23 @@ public static void checkAcidTable(CatalogTable catalogTable, ObjectPath tablePat\n \t * @return A Hadoop configuration instance.\n \t */\n \tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n-\t\tConfiguration hadoopConfiguration = new Configuration();\n \t\tif (new File(hadoopConfDir).exists()) {\n+\t\t\tConfiguration hadoopConfiguration = new Configuration();\n \t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {", "originalCommit": "2c08acf8c251407d39f2628b970d5b7eb437af99", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "07fb09f8651e79ab795f1d888d794b018a213a4f", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\nindex 8918b07e87a..37b581bd848 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/util/HiveTableUtil.java\n\n@@ -433,17 +433,22 @@ public class HiveTableUtil {\n \tpublic static Configuration getHadoopConfiguration(String hadoopConfDir) {\n \t\tif (new File(hadoopConfDir).exists()) {\n \t\t\tConfiguration hadoopConfiguration = new Configuration();\n-\t\t\tif (new File(hadoopConfDir + \"/core-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/core-site.xml\"));\n+\t\t\tFile coreSite = new File(hadoopConfDir, \"core-site.xml\");\n+\t\t\tif (coreSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(coreSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/hdfs-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/hdfs-site.xml\"));\n+\t\t\tFile hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\n+\t\t\tif (hdfsSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(new Path(hdfsSite.getAbsolutePath()));\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/yarn-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/yarn-site.xml\"));\n+\t\t\tFile yarnSite = new File(hadoopConfDir, \"yarn-site.xml\");\n+\t\t\tif (yarnSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(yarnSite.getAbsolutePath());\n \t\t\t}\n-\t\t\tif (new File(hadoopConfDir + \"/mapred-site.xml\").exists()) {\n-\t\t\t\thadoopConfiguration.addResource(new Path(hadoopConfDir + \"/mapred-site.xml\"));\n+\t\t\t// Add mapred-site.xml. We need to read configurations like compression codec.\n+\t\t\tFile mapredSite = new File(hadoopConfDir, \"mapred-site.xml\");\n+\t\t\tif (mapredSite.exists()) {\n+\t\t\t\thadoopConfiguration.addResource(mapredSite.getAbsolutePath());\n \t\t\t}\n \t\t\treturn hadoopConfiguration;\n \t\t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTUzNDg4NQ==", "url": "https://github.com/apache/flink/pull/13434#discussion_r495534885", "bodyText": "This comment should be migrated to HiveTableUtil::getHadoopConfiguration", "author": "lirui-apache", "createdAt": "2020-09-27T06:20:10Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -195,18 +202,19 @@ private static HiveConf createHiveConf(@Nullable String hiveConfDir) {\n \t\t\t\tString.format(\"Failed to get hive-site.xml from %s\", hiveConfDir), e);\n \t\t}\n \n-\t\t// create HiveConf from hadoop configuration\n-\t\tConfiguration hadoopConf = HadoopUtils.getHadoopConfiguration(new org.apache.flink.configuration.Configuration());\n-\n-\t\t// Add mapred-site.xml. We need to read configurations like compression codec.", "originalCommit": "2c08acf8c251407d39f2628b970d5b7eb437af99", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "07fb09f8651e79ab795f1d888d794b018a213a4f", "url": "https://github.com/apache/flink/commit/07fb09f8651e79ab795f1d888d794b018a213a4f", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-27T07:27:19Z", "type": "forcePushed"}, {"oid": "b5ac11907cbbe41f2b13e2b738458a3315d61663", "url": "https://github.com/apache/flink/commit/b5ac11907cbbe41f2b13e2b738458a3315d61663", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-27T07:30:41Z", "type": "forcePushed"}, {"oid": "7a2be06f74b2abc1d40f345fe995634e0f246413", "url": "https://github.com/apache/flink/commit/7a2be06f74b2abc1d40f345fe995634e0f246413", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-29T02:04:43Z", "type": "forcePushed"}, {"oid": "d7638e715ec7af65f8a45e276204ce86bce4369c", "url": "https://github.com/apache/flink/commit/d7638e715ec7af65f8a45e276204ce86bce4369c", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-29T03:30:37Z", "type": "forcePushed"}, {"oid": "cda95f3a0ee6988a3852d5e731ad360484bad53a", "url": "https://github.com/apache/flink/commit/cda95f3a0ee6988a3852d5e731ad360484bad53a", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-29T03:49:13Z", "type": "commit"}, {"oid": "cda95f3a0ee6988a3852d5e731ad360484bad53a", "url": "https://github.com/apache/flink/commit/cda95f3a0ee6988a3852d5e731ad360484bad53a", "message": "[FLINK-19292][hive] HiveCatalog should support specifying Hadoop conf dir with configuration", "committedDate": "2020-09-29T03:49:13Z", "type": "forcePushed"}]}