{"pr_number": 13763, "pr_title": "[FLINK-19779][avro] Remove the record_ field name prefix for Confluen\u2026", "pr_createdAt": "2020-10-23T07:56:31Z", "pr_url": "https://github.com/apache/flink/pull/13763", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDc3NDA0NA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r510774044", "bodyText": "I think there is still existed a bug. see: https://issues.apache.org/jira/browse/FLINK-19779?focusedCommentId=17219588&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17219588", "author": "V1ncentzzZ", "createdAt": "2020-10-23T09:59:21Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -362,7 +369,11 @@ public static Schema convertToSchema(LogicalType logicalType, String rowName) {\n \t\t\t\t\t.record(rowName)\n \t\t\t\t\t.fields();\n \t\t\t\tfor (int i = 0; i < rowType.getFieldCount(); i++) {\n-\t\t\t\t\tString fieldName = rowName + \"_\" + fieldNames.get(i);\n+\t\t\t\t\tString fieldName = fieldNames.get(i);\n+\t\t\t\t\tif (rowName.equals(fieldName)) {\n+\t\t\t\t\t\t// Can not build schema when the record and field have the same name\n+\t\t\t\t\t\tfieldName = rowName + \"_\" + fieldName;\n+\t\t\t\t\t}", "originalCommit": "b92a32d37a8eaa446f24f44e467e588cdabcd9f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc1Nzc0NQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r511757745", "bodyText": "Yes, the avro schema builder does not allow same name field names, even if they are in different scope (different layer).", "author": "danny0405", "createdAt": "2020-10-26T07:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDc3NDA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTkzNTYwNw==", "url": "https://github.com/apache/flink/pull/13763#discussion_r511935607", "bodyText": "That is not correct. The builder does support same names for fields in different nested levels.\nAvro in general does not support same record types with different schemas. And it does it rightly so. Therefore a schema like:\n{\n    \"type\": \"record\", \n    \"name\": \"top\", \n    \"fields\": [ \n\t\t  {\n             \"name\": \"top\", \n             \"type\": { \n                 \"type\": \"record\", \n                 \"name\": \"nested\", \n                 \"fields\": [ \n                     {\"type\": \"string\", \"name\": \"top\"} \n                 ]\n             }\n          }\n    ] \n}\n\nis valid and supported. However if we change the name of the nested record to top it will be invalid:\n{\n    \"type\": \"record\", \n    \"name\": \"top\", \n    \"fields\": [ \n\t\t  {\n             \"name\": \"top\", \n             \"type\": { \n                 \"type\": \"record\", \n                 \"name\": \"top\", \n                 \"fields\": [ \n                     {\"type\": \"string\", \"name\": \"top\"} \n                 ]\n             }\n          }\n    ] \n}\n\nI think the core problem lays in how the rowName is generated. I think we should never adjust the fieldName, but we should append the fieldName to the rowName.\nBTW another shortcoming that I see is that we are losing the record name when converting from Schema to DataType. I think it is not a real issue though.", "author": "dawidwys", "createdAt": "2020-10-26T12:52:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDc3NDA0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1NDM0NQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r511954345", "bodyText": "I think the correct solution will be:\n\t\t\t\tRowType rowType = (RowType) logicalType;\n\t\t\t\tList<String> fieldNames = rowType.getFieldNames();\n\t\t\t\t// we have to make sure the record name is different in a Schema\n\t\t\t\tSchemaBuilder.FieldAssembler<Schema> builder =\n\t\t\t\t\t\tgetNullableBuilder(logicalType)\n\t\t\t\t\t\t\t\t.record(rowName)\n\t\t\t\t\t\t\t\t.fields();\n\t\t\t\tfor (int i = 0; i < rowType.getFieldCount(); i++) {\n\t\t\t\t\tString fieldName = fieldNames.get(i);\n\t\t\t\t\tbuilder = builder\n\t\t\t\t\t\t.name(fieldName)\n\t\t\t\t\t\t.type(convertToSchema(rowType.getTypeAt(i), rowName + \"_\" + fieldName))\n\t\t\t\t\t\t.noDefault();\n\t\t\t\t}\n\t\t\t\treturn builder.endRecord();", "author": "dawidwys", "createdAt": "2020-10-26T13:22:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDc3NDA0NA=="}], "type": "inlineReview", "revised_code": {"commit": "6ef138838f1a4a0b45a42eff39b75847c9b39767", "chunk": "diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\nindex 32fae9df95a..1ee3a2d225a 100644\n--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n\n@@ -353,21 +357,24 @@ public class AvroSchemaConverter {\n \t\t\t\t\t\t\", it only supports precision less than 3.\");\n \t\t\t\t}\n \t\t\t\t// use int to represents Time, we only support millisecond when deserialization\n-\t\t\t\treturn LogicalTypes.timeMillis().addToSchema(SchemaBuilder.builder().intType());\n+\t\t\t\tSchema time = LogicalTypes.timeMillis()\n+\t\t\t\t\t\t.addToSchema(SchemaBuilder.builder().intType());\n+\t\t\t\treturn logicalType.isNullable() ? nullableSchema(time) : time;\n \t\t\tcase DECIMAL:\n \t\t\t\tDecimalType decimalType = (DecimalType) logicalType;\n \t\t\t\t// store BigDecimal as byte[]\n-\t\t\t\treturn LogicalTypes\n-\t\t\t\t\t.decimal(decimalType.getPrecision(), decimalType.getScale())\n-\t\t\t\t\t.addToSchema(SchemaBuilder.builder().bytesType());\n+\t\t\t\tSchema decimal = LogicalTypes\n+\t\t\t\t\t\t.decimal(decimalType.getPrecision(), decimalType.getScale())\n+\t\t\t\t\t\t.addToSchema(SchemaBuilder.builder().bytesType());\n+\t\t\t\treturn logicalType.isNullable() ? nullableSchema(decimal) : decimal;\n \t\t\tcase ROW:\n \t\t\t\tRowType rowType = (RowType) logicalType;\n \t\t\t\tList<String> fieldNames = rowType.getFieldNames();\n \t\t\t\t// we have to make sure the record name is different in a Schema\n-\t\t\t\tSchemaBuilder.FieldAssembler<Schema> builder = SchemaBuilder\n-\t\t\t\t\t.builder()\n-\t\t\t\t\t.record(rowName)\n-\t\t\t\t\t.fields();\n+\t\t\t\tSchemaBuilder.FieldAssembler<Schema> builder =\n+\t\t\t\t\t\tgetNullableBuilder(logicalType)\n+\t\t\t\t\t\t\t\t.record(rowName)\n+\t\t\t\t\t\t\t\t.fields();\n \t\t\t\tfor (int i = 0; i < rowType.getFieldCount(); i++) {\n \t\t\t\t\tString fieldName = fieldNames.get(i);\n \t\t\t\t\tif (rowName.equals(fieldName)) {\n"}}, {"oid": "6ef138838f1a4a0b45a42eff39b75847c9b39767", "url": "https://github.com/apache/flink/commit/6ef138838f1a4a0b45a42eff39b75847c9b39767", "message": "[FLINK-19779][avro] Remove the record_ field name prefix for Confluent Avro format deserialization\n\n* Add prefix for the field name only when the record and field have the\n  same name\n* Fix the nullability and precision during data type and Avro schema\n  conversion", "committedDate": "2020-10-26T07:48:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0MjIwNA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r511842204", "bodyText": "Could we stick to a single way of declaring Schema nullable? With this PR we have two methods for the same purpose:\n\nnullableSchema\ngetNullableBuilder\n\nEither use the nullableSchema everywhere or use getNullableBuilder(...).type(...).", "author": "dawidwys", "createdAt": "2020-10-26T10:01:56Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -417,4 +431,11 @@ public static LogicalType extractValueTypeToAvroMap(LogicalType type) {\n \t\t}\n \t\treturn builder;\n \t}\n+\n+\t/** Returns schema with nullable true. */\n+\tprivate static Schema nullableSchema(Schema schema) {", "originalCommit": "6ef138838f1a4a0b45a42eff39b75847c9b39767", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM2NzYyNw==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512367627", "bodyText": "They are for different purpose but i think we can use nullableSchema altogether.", "author": "danny0405", "createdAt": "2020-10-27T01:46:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0MjIwNA=="}], "type": "inlineReview", "revised_code": {"commit": "11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "chunk": "diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\nindex 1ee3a2d225a..d79edf4329b 100644\n--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n\n@@ -424,14 +447,6 @@ public class AvroSchemaConverter {\n \t\treturn valueType;\n \t}\n \n-\tprivate static SchemaBuilder.BaseTypeBuilder<Schema> getNullableBuilder(LogicalType logicalType) {\n-\t\tSchemaBuilder.TypeBuilder<Schema> builder = SchemaBuilder.builder();\n-\t\tif (logicalType.isNullable()) {\n-\t\t\treturn builder.nullable();\n-\t\t}\n-\t\treturn builder;\n-\t}\n-\n \t/** Returns schema with nullable true. */\n \tprivate static Schema nullableSchema(Schema schema) {\n \t\treturn schema.isNullable()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1NTM0MA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r511955340", "bodyText": "I think it would be nice to add a test that we can convert back and forth between DataType and Schema in respect to the field names.", "author": "dawidwys", "createdAt": "2020-10-26T13:24:18Z", "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java", "diffHunk": "@@ -104,48 +104,115 @@ public void testRowTypeAvroSchemaConversion() {\n \t\t\t\tDataTypes.FIELD(\"row3\", DataTypes.ROW(DataTypes.FIELD(\"c\", DataTypes.STRING())))))\n \t\t\t.build().toRowDataType().getLogicalType();\n \t\tSchema schema = AvroSchemaConverter.convertToSchema(rowType);\n-\t\tassertEquals(\"{\\n\" +\n+\t\tassertEquals(\"[ {\\n\" +", "originalCommit": "6ef138838f1a4a0b45a42eff39b75847c9b39767", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3NDIxMA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512374210", "bodyText": "Already added, see AvroSchemaConverterTest.testConversionIntegralityNullable.", "author": "danny0405", "createdAt": "2020-10-27T02:11:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1NTM0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU4NjMyNA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512586324", "bodyText": "True, you test though only the DataType -> Schema and back. We could also add a test for Schema -> DataType (you start with a Schema), which is important in case of a schema registry.", "author": "dawidwys", "createdAt": "2020-10-27T10:48:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1NTM0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjYxODc4OQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512618789", "bodyText": "Thanks, i would add schema -> DataType -> schema test.", "author": "danny0405", "createdAt": "2020-10-27T11:38:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1NTM0MA=="}], "type": "inlineReview", "revised_code": {"commit": "11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "chunk": "diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java\nindex c0b19493ce4..af792e74cd3 100644\n--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java\n+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java\n\n@@ -104,48 +104,48 @@ public class AvroSchemaConverterTest {\n \t\t\t\tDataTypes.FIELD(\"row3\", DataTypes.ROW(DataTypes.FIELD(\"c\", DataTypes.STRING())))))\n \t\t\t.build().toRowDataType().getLogicalType();\n \t\tSchema schema = AvroSchemaConverter.convertToSchema(rowType);\n-\t\tassertEquals(\"[ {\\n\" +\n-\t\t\t\"  \\\"type\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"  \\\"name\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"  \\\"fields\\\" : [ {\\n\" +\n-\t\t\t\"    \\\"name\\\" : \\\"row1\\\",\\n\" +\n-\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n-\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"      \\\"name\\\" : \\\"row1\\\",\\n\" +\n-\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n-\t\t\t\"        \\\"name\\\" : \\\"a\\\",\\n\" +\n-\t\t\t\"        \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n-\t\t\t\"      } ]\\n\" +\n-\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n-\t\t\t\"  }, {\\n\" +\n-\t\t\t\"    \\\"name\\\" : \\\"row2\\\",\\n\" +\n-\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n-\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"      \\\"name\\\" : \\\"row2\\\",\\n\" +\n-\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n-\t\t\t\"        \\\"name\\\" : \\\"b\\\",\\n\" +\n-\t\t\t\"        \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n-\t\t\t\"      } ]\\n\" +\n-\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n-\t\t\t\"  }, {\\n\" +\n-\t\t\t\"    \\\"name\\\" : \\\"row3\\\",\\n\" +\n-\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n-\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"      \\\"name\\\" : \\\"row3\\\",\\n\" +\n-\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n-\t\t\t\"        \\\"name\\\" : \\\"row3_row3\\\",\\n\" +\n-\t\t\t\"        \\\"type\\\" : [ {\\n\" +\n-\t\t\t\"          \\\"type\\\" : \\\"record\\\",\\n\" +\n-\t\t\t\"          \\\"name\\\" : \\\"row3_row3\\\",\\n\" +\n-\t\t\t\"          \\\"fields\\\" : [ {\\n\" +\n-\t\t\t\"            \\\"name\\\" : \\\"c\\\",\\n\" +\n-\t\t\t\"            \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n-\t\t\t\"          } ]\\n\" +\n-\t\t\t\"        }, \\\"null\\\" ]\\n\" +\n-\t\t\t\"      } ]\\n\" +\n-\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n-\t\t\t\"  } ]\\n\" +\n-\t\t\t\"}, \\\"null\\\" ]\", schema.toString(true));\n+\t\tassertEquals(\"{\\n\" +\n+\t\t\t\t\"  \\\"type\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"  \\\"name\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"  \\\"fields\\\" : [ {\\n\" +\n+\t\t\t\t\"    \\\"name\\\" : \\\"row1\\\",\\n\" +\n+\t\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n+\t\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"      \\\"name\\\" : \\\"record_row1\\\",\\n\" +\n+\t\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n+\t\t\t\t\"        \\\"name\\\" : \\\"a\\\",\\n\" +\n+\t\t\t\t\"        \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"      } ]\\n\" +\n+\t\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"  }, {\\n\" +\n+\t\t\t\t\"    \\\"name\\\" : \\\"row2\\\",\\n\" +\n+\t\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n+\t\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"      \\\"name\\\" : \\\"record_row2\\\",\\n\" +\n+\t\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n+\t\t\t\t\"        \\\"name\\\" : \\\"b\\\",\\n\" +\n+\t\t\t\t\"        \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"      } ]\\n\" +\n+\t\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"  }, {\\n\" +\n+\t\t\t\t\"    \\\"name\\\" : \\\"row3\\\",\\n\" +\n+\t\t\t\t\"    \\\"type\\\" : [ {\\n\" +\n+\t\t\t\t\"      \\\"type\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"      \\\"name\\\" : \\\"record_row3\\\",\\n\" +\n+\t\t\t\t\"      \\\"fields\\\" : [ {\\n\" +\n+\t\t\t\t\"        \\\"name\\\" : \\\"row3\\\",\\n\" +\n+\t\t\t\t\"        \\\"type\\\" : [ {\\n\" +\n+\t\t\t\t\"          \\\"type\\\" : \\\"record\\\",\\n\" +\n+\t\t\t\t\"          \\\"name\\\" : \\\"record_row3_row3\\\",\\n\" +\n+\t\t\t\t\"          \\\"fields\\\" : [ {\\n\" +\n+\t\t\t\t\"            \\\"name\\\" : \\\"c\\\",\\n\" +\n+\t\t\t\t\"            \\\"type\\\" : [ \\\"string\\\", \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"          } ]\\n\" +\n+\t\t\t\t\"        }, \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"      } ]\\n\" +\n+\t\t\t\t\"    }, \\\"null\\\" ]\\n\" +\n+\t\t\t\t\"  } ]\\n\" +\n+\t\t\t\t\"}\", schema.toString(true));\n \t}\n \n \t/**\n"}}, {"oid": "11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "url": "https://github.com/apache/flink/commit/11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "message": "[FLINK-19779][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-27T06:47:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MzUyMg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512593522", "bodyText": "That comment is misleading. There is no limitation in the actual handling of nulls with SpecificRecord/GenericRecord.\nIt's just that the LogicalTimeRecord has those fields declared as notNull. The previous dataType was just wrong and did not describe the LogicalTimeRecord correctly.", "author": "dawidwys", "createdAt": "2020-10-27T10:57:25Z", "path": "flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java", "diffHunk": "@@ -181,10 +181,11 @@ public void testSpecificType() throws Exception {\n \t\tencoder.flush();\n \t\tbyte[] input = byteArrayOutputStream.toByteArray();\n \n+\t\t// SE/DE SpecificRecord using the GenericRecord way only supports non-nullable data type.", "originalCommit": "11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjYxNjYxNg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512616616", "bodyText": "Thanks, would remove it.", "author": "danny0405", "createdAt": "2020-10-27T11:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MzUyMg=="}], "type": "inlineReview", "revised_code": {"commit": "e57049c1254d265b0cf68e424a3b8d8bd53f729c", "chunk": "diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java\nindex bb7379ba461..ea2d68bff0f 100644\n--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java\n+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java\n\n@@ -181,7 +181,6 @@ public class AvroRowDataDeSerializationSchemaTest {\n \t\tencoder.flush();\n \t\tbyte[] input = byteArrayOutputStream.toByteArray();\n \n-\t\t// SE/DE SpecificRecord using the GenericRecord way only supports non-nullable data type.\n \t\tDataType dataType = ROW(\n \t\t\t\tFIELD(\"type_timestamp_millis\", TIMESTAMP(3).notNull()),\n \t\t\t\tFIELD(\"type_date\", DATE().notNull()),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512596226", "bodyText": "Is it necessary? Shouldn't the top be handled in the planner and just passed with a correct setting in the logicalType? I think it is a bit too deep to fix it here.", "author": "dawidwys", "createdAt": "2020-10-27T11:00:48Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -297,17 +299,22 @@ private static DataType convertToDataType(Schema schema) {\n \t * @return Avro's {@link Schema} matching this logical type.\n \t */\n \tpublic static Schema convertToSchema(LogicalType logicalType) {\n-\t\treturn convertToSchema(logicalType, \"record\");\n+\t\treturn convertToSchema(logicalType, \"record\", true);\n \t}\n \n \t/**\n \t * Converts Flink SQL {@link LogicalType} (can be nested) into an Avro schema.\n \t *\n \t * @param logicalType logical type\n \t * @param rowName     the record name\n+\t * @param top         whether it is parsing the root record,\n+\t *                    if it is, the logical type nullability would be ignored\n \t * @return Avro's {@link Schema} matching this logical type.\n \t */\n-\tpublic static Schema convertToSchema(LogicalType logicalType, String rowName) {\n+\tpublic static Schema convertToSchema(\n+\t\t\tLogicalType logicalType,\n+\t\t\tString rowName,\n+\t\t\tboolean top) {", "originalCommit": "11a1af7e69935ec9bbbcc4aeea5dcb5a8b0c68ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjYxODE0Mg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512618142", "bodyText": "It is meaningless to have a nullable top row type, and in the CREATE TABLE DDL there is no way to specify that the table row is not nullable, actually it should always be not null(even if all the fields are null).", "author": "danny0405", "createdAt": "2020-10-27T11:37:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjYyODYxMw==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512628613", "bodyText": "I agree that the top row should not be nullable. It is a property of a SQL Table though, not of the format. If I am not wrong there you can not specify a ROW nullable in CREATE TABLE because it is always NOT NULL. Therefore it is meaningless to have additional parameter in the convertToSchema.\nWhat I am saying is that the format should not be bother with this. It is purely a matter of the planner.", "author": "dawidwys", "createdAt": "2020-10-27T11:50:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjYzMjQzNg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512632436", "bodyText": "I have the same feeling with @dawidwys .", "author": "wuchong", "createdAt": "2020-10-27T11:56:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY0MDgzNQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512640835", "bodyText": "Currently, only avro format cares about the outer row nullability, we can switch to change the planner if we found more user cases.\nTechnically to say, it is impossible to infer the outer row nullability only from the DDL.", "author": "danny0405", "createdAt": "2020-10-27T12:11:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY3MzkzMQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r512673931", "bodyText": "Sorry, but I don't understand your arguments.\nWhat do you mean by \"inferring\" the outer row nullability? There is no inference. In SQL the outer row is always NOT NULL or otherwise the row simply does not exist.\nIn the current shape you have two arguments that contradict each other. Take this example:\nDataType type = DataTypes.ROW(\n    DataTypes.FIELD(\"f0\", DataTypes.ROW(\n        DataTypes.FIELD(\"f1\", DataTypes.BIGINT()\n    ).nullable()\n).nullable();\n\nSchema schema = convertToSchema(type, \"record\", true) // <- you're overriding the property of type\n// you can achieve the same with\n// Schema schema = convertToSchema(type.notNull(), \"record\")\n\nIt's the Planners task to produce a notNull type for a Tables type.", "author": "dawidwys", "createdAt": "2020-10-27T13:05:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEzNTc4OQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513135789", "bodyText": "In SQL the outer row is always NOT NULL\n\nIt is not true, a row (null, null) is nullable true. And i don't think it makes sense to change the planner behavior in general in order to fix a specific use case.\n\n// you can achieve the same with\n// Schema schema = convertToSchema(type.notNull(), \"record\")\n\nI don't think we should let each invoker to decide whether to make the data type not null, because in current codebase, we should always do that, make the decision everyone is error-prone and hard to maintain.\nI have merge the top into a row type nullability switch, see AvroSchemaConverter.convertToSchema(LogicalType logicalType).", "author": "danny0405", "createdAt": "2020-10-28T02:04:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzI1MjcyNA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513252724", "bodyText": "It is not true, a row (null, null) is nullable true. And i don't think it makes sense to change the planner behavior in general in order to fix a specific use case.\n\nExcuse me, but I wholeheartedly disagree with your statement. null =/= (null, null). (null, null) is still NOT NULL. A whole row in a Table can not be null. Only particular columns can be null. Therefore the top level row of a Table is always NOT NULL.\nI am not suggesting changing planner behaviour for a particular use case. The planner should always produce NOT NULL type for a top level row of a Table. If it doesn't, it is a bug.", "author": "dawidwys", "createdAt": "2020-10-28T08:19:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzI1MzY4Mg==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513253682", "bodyText": "I don't think we should let each invoker to decide whether to make the data type not null, because in current codebase, we should always do that, make the decision everyone is error-prone and hard to maintain.\n\nI agree making the same decision over and over again at multiple location is error prone and hard to maintain and that's what I want to avoid.", "author": "dawidwys", "createdAt": "2020-10-28T08:20:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzI2NDk4Nw==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513264987", "bodyText": "Excuse me, but I wholeheartedly disagree with your statement. null =/= (null, null). (null, null) is still NOT NULL. A whole row in a Table can not be null. Only particular columns can be null. Therefore the top level row of a Table is always NOT NULL.\n\nYou can test it in PostgreSQL with the following SQL:\ncreate type my_type as (a int, b varchar(20));\n\ncreate table t1(\n  f0 my_type,\n  f1 varchar(20)\n);\n\ninsert into t1 values((null, null), 'def');\n\nselect f0 is null from t1; -- it returns true", "author": "danny0405", "createdAt": "2020-10-28T08:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzI2OTYyNA==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513269624", "bodyText": "But the my_type is not a top level row in your example. It is a type of a column. It has nothing to do with the case we're discussing.", "author": "dawidwys", "createdAt": "2020-10-28T08:48:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzkyNzM5MQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513927391", "bodyText": "@dawidwys  I have changed the schema row type to be always nullable false, please take a look again if you have time, thanks so much.", "author": "danny0405", "createdAt": "2020-10-29T03:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5NjIyNg=="}], "type": "inlineReview", "revised_code": {"commit": "ba752274c9926115f65f5ecbef55b71b0b71cfa2", "chunk": "diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\nindex d79edf4329b..414370ed56c 100644\n--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n\n@@ -299,7 +300,13 @@ public class AvroSchemaConverter {\n \t * @return Avro's {@link Schema} matching this logical type.\n \t */\n \tpublic static Schema convertToSchema(LogicalType logicalType) {\n-\t\treturn convertToSchema(logicalType, \"record\", true);\n+\t\t// If it is parsing the root row type, switches from nullable true to false\n+\t\t// because a nullable row type is meaningless and would generate wrong schema.\n+\t\tif (logicalType.getTypeRoot() == LogicalTypeRoot.ROW\n+\t\t\t\t&& logicalType.isNullable()) {\n+\t\t\tlogicalType = logicalType.copy(false);\n+\t\t}\n+\t\treturn convertToSchema(logicalType, \"record\");\n \t}\n \n \t/**\n"}}, {"oid": "e57049c1254d265b0cf68e424a3b8d8bd53f729c", "url": "https://github.com/apache/flink/commit/e57049c1254d265b0cf68e424a3b8d8bd53f729c", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-27T12:07:03Z", "type": "forcePushed"}, {"oid": "a28669bca9c36dac74f89da177825d73bea0ece0", "url": "https://github.com/apache/flink/commit/a28669bca9c36dac74f89da177825d73bea0ece0", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-27T12:12:35Z", "type": "forcePushed"}, {"oid": "ba752274c9926115f65f5ecbef55b71b0b71cfa2", "url": "https://github.com/apache/flink/commit/ba752274c9926115f65f5ecbef55b71b0b71cfa2", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-28T01:53:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE0MDIxNQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513140215", "bodyText": "I think we can make the parameter to be RowType, that would make sense to use it as the top-level row type and not generate nullable for it. Besides, would be better to add comments in the Javadoc. Currently, this method has the same Javadoc with convertToSchema(LogicalType logicalType, String rowName).", "author": "wuchong", "createdAt": "2020-10-28T02:21:06Z", "path": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java", "diffHunk": "@@ -297,32 +300,53 @@ private static DataType convertToDataType(Schema schema) {\n \t * @return Avro's {@link Schema} matching this logical type.\n \t */\n \tpublic static Schema convertToSchema(LogicalType logicalType) {\n+\t\t// If it is parsing the root row type, switches from nullable true to false\n+\t\t// because a nullable row type is meaningless and would generate wrong schema.\n+\t\tif (logicalType.getTypeRoot() == LogicalTypeRoot.ROW", "originalCommit": "ba752274c9926115f65f5ecbef55b71b0b71cfa2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2MjIyOQ==", "url": "https://github.com/apache/flink/pull/13763#discussion_r513162229", "bodyText": "Although the AvroSchemaConverter is a tool class, it is still used as a public API, so i'm inclined to keep the signature unchanged. Another reason is that only logical type is enough for the conversion.\nHave added more documents to the methods.", "author": "danny0405", "createdAt": "2020-10-28T03:43:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE0MDIxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "f004220668e20dcd9860026b69566868d473db33", "chunk": "diff --git a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\nindex 414370ed56c..6436905c65c 100644\n--- a/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n+++ b/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverter.java\n\n@@ -296,6 +296,9 @@ public class AvroSchemaConverter {\n \t/**\n \t * Converts Flink SQL {@link LogicalType} (can be nested) into an Avro schema.\n \t *\n+\t * <p>If {@code logicalType} is {@link RowType}, use \"record\" as the type name and makes it\n+\t * nullable false in order to generate the right schema.\n+\t *\n \t * @param logicalType logical type\n \t * @return Avro's {@link Schema} matching this logical type.\n \t */\n"}}, {"oid": "f004220668e20dcd9860026b69566868d473db33", "url": "https://github.com/apache/flink/commit/f004220668e20dcd9860026b69566868d473db33", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-28T03:32:00Z", "type": "forcePushed"}, {"oid": "4c5a06b1ca2833fe7f63c25503660ed7acf9b77d", "url": "https://github.com/apache/flink/commit/4c5a06b1ca2833fe7f63c25503660ed7acf9b77d", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY", "committedDate": "2020-10-28T11:26:19Z", "type": "forcePushed"}, {"oid": "13712c13da124c38a430d4cc50ad28aec4318764", "url": "https://github.com/apache/flink/commit/13712c13da124c38a430d4cc50ad28aec4318764", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY\n* The table schema row type should be always non-nullable", "committedDate": "2020-10-29T03:38:10Z", "type": "forcePushed"}, {"oid": "170355bcb112d902e78ee4fc1644b3e6aee5eb29", "url": "https://github.com/apache/flink/commit/170355bcb112d902e78ee4fc1644b3e6aee5eb29", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY\n* The table schema row type should be always non-nullable", "committedDate": "2020-10-29T08:45:08Z", "type": "forcePushed"}, {"oid": "6282778015269bf5e4a1a036540fc6f5fc441cb3", "url": "https://github.com/apache/flink/commit/6282778015269bf5e4a1a036540fc6f5fc441cb3", "message": "[FLINK-19779][avro] Remove the \"record_\" field name prefix for Avro format deserialization\n\nNever modify and prefix the field name, instead, we now use the {rowName}_{fieldName}\nas the nested row type name because Avro schema does not allow same name row type\nwith different schema.", "committedDate": "2020-10-29T10:12:05Z", "type": "commit"}, {"oid": "e0a53361318b9ee5ceae9a1d6bc50f77a2d28c8a", "url": "https://github.com/apache/flink/commit/e0a53361318b9ee5ceae9a1d6bc50f77a2d28c8a", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY\n* The table schema row type should be always non-nullable", "committedDate": "2020-10-29T10:12:05Z", "type": "commit"}, {"oid": "e0a53361318b9ee5ceae9a1d6bc50f77a2d28c8a", "url": "https://github.com/apache/flink/commit/e0a53361318b9ee5ceae9a1d6bc50f77a2d28c8a", "message": "[FLINK-19786][avro] Fix the nullability and precision for Avro format deserialization\n\n* Fix the TIME schema precision as 3\n* Fix the nullability of type: TIMESTAMP_WITHOUT_TIME_ZONE, DATE, TIME_WITHOUT_TIME_ZONE,\n  DECIMAL, MAP, ARRAY\n* The table schema row type should be always non-nullable", "committedDate": "2020-10-29T10:12:05Z", "type": "forcePushed"}]}