{"pr_number": 12108, "pr_title": "[FLINK-17448][sql-parser][table-api-java][table-planner-blink][hive] Implement table DDLs for Hive dialect part2", "pr_createdAt": "2020-05-13T07:08:39Z", "pr_url": "https://github.com/apache/flink/pull/12108", "timeline": [{"oid": "66bbeb2e19f810744287740e7a691da17ebb8b0c", "url": "https://github.com/apache/flink/commit/66bbeb2e19f810744287740e7a691da17ebb8b0c", "message": "fix hive parser", "committedDate": "2020-05-15T05:27:07Z", "type": "forcePushed"}, {"oid": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "url": "https://github.com/apache/flink/commit/ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "message": "fix hive parser", "committedDate": "2020-05-15T08:20:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjM5Mg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812392", "bodyText": "Move them to a type parser class.", "author": "JingsongLi", "createdAt": "2020-05-15T13:45:54Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -138,6 +158,14 @@\n \tprivate static final String FLINK_FUNCTION_PREFIX = \"flink:\";\n \tprivate static final String FLINK_PYTHON_FUNCTION_PREFIX = FLINK_FUNCTION_PREFIX + \"python:\";\n \n+\tprivate static final Pattern DECIMAL_PATTERN = Pattern.compile(\"^decimal(\\\\((\\\\d+),\\\\s*(\\\\d+)\\\\))?$\");", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 7ea7eb8623f..7b83fe58d7c 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -158,14 +143,6 @@ public class HiveCatalog extends AbstractCatalog {\n \tprivate static final String FLINK_FUNCTION_PREFIX = \"flink:\";\n \tprivate static final String FLINK_PYTHON_FUNCTION_PREFIX = FLINK_FUNCTION_PREFIX + \"python:\";\n \n-\tprivate static final Pattern DECIMAL_PATTERN = Pattern.compile(\"^decimal(\\\\((\\\\d+),\\\\s*(\\\\d+)\\\\))?$\");\n-\tprivate static final Pattern CHAR_PATTERN = Pattern.compile(\"^char\\\\((\\\\d+)\\\\)$\");\n-\tprivate static final Pattern VARCHAR_PATTERN = Pattern.compile(\"^varchar\\\\((\\\\d+)\\\\)$\");\n-\tprivate static final Pattern TIMESTAMP_PATTERN = Pattern.compile(\"^timestamp(\\\\((\\\\d+)\\\\))?$\");\n-\tprivate static final Pattern ARRAY_PATTERN = Pattern.compile(\"^array<(.+)>$\");\n-\tprivate static final Pattern MAP_PATTERN = Pattern.compile(\"^map<(.+)>$\");\n-\tprivate static final Pattern STRUCT_PATTERN = Pattern.compile(\"^row<(.+)>$\");\n-\n \tprivate final HiveConf hiveConf;\n \tprivate final String hiveVersion;\n \tprivate final HiveShim hiveShim;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjQ2NA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812464", "bodyText": "Move them to a type parser class.", "author": "JingsongLi", "createdAt": "2020-05-15T13:46:01Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 7ea7eb8623f..7b83fe58d7c 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -1511,8 +1500,13 @@ public class HiveCatalog extends AbstractCatalog {\n \t\treturn newHiveTable;\n \t}\n \n-\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n-\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\tprivate void alterTableViaProperties(\n+\t\t\tAlterTableOp alterOp,\n+\t\t\tTable hiveTable,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tMap<String, String> oldProps,\n+\t\t\tMap<String, String> newProps,\n+\t\t\tStorageDescriptor sd) {\n \t\tswitch (alterOp) {\n \t\t\tcase CHANGE_TBL_PROPS:\n \t\t\t\toldProps.putAll(newProps);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxMjcxNw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425812717", "bodyText": "Is there no util from hive or flink-sql-parser-hive?", "author": "JingsongLi", "createdAt": "2020-05-15T13:46:25Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 7ea7eb8623f..7b83fe58d7c 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -1511,8 +1500,13 @@ public class HiveCatalog extends AbstractCatalog {\n \t\treturn newHiveTable;\n \t}\n \n-\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n-\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\tprivate void alterTableViaProperties(\n+\t\t\tAlterTableOp alterOp,\n+\t\t\tTable hiveTable,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tMap<String, String> oldProps,\n+\t\t\tMap<String, String> newProps,\n+\t\t\tStorageDescriptor sd) {\n \t\tswitch (alterOp) {\n \t\t\tcase CHANGE_TBL_PROPS:\n \t\t\t\toldProps.putAll(newProps);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxNDEwOA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r425814108", "bodyText": "Move them to an alter util?\nOr maybe a AlterTable, and have two implementation: ObjectAlterTable and PropertiesAlterTable.", "author": "JingsongLi", "createdAt": "2020-05-15T13:48:41Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -484,18 +512,24 @@ public void alterTable(ObjectPath tablePath, CatalogBaseTable newCatalogTable, b\n \t\t\t\t\texistingTable.getClass().getName(), newCatalogTable.getClass().getName()));\n \t\t}\n \n-\t\tTable newTable = instantiateHiveTable(tablePath, newCatalogTable, hiveConf);\n-\n-\t\t// client.alter_table() requires a valid location\n-\t\t// thus, if new table doesn't have that, it reuses location of the old table\n-\t\tif (!newTable.getSd().isSetLocation()) {\n-\t\t\tnewTable.getSd().setLocation(hiveTable.getSd().getLocation());\n+\t\tboolean isGeneric = isGenericForGet(hiveTable.getParameters());\n+\t\tif (isGeneric) {\n+\t\t\thiveTable = alterTableViaCatalogBaseTable(tablePath, newCatalogTable, hiveTable);\n+\t\t} else {\n+\t\t\tAlterTableOp op = extractAlterTableOp(newCatalogTable.getOptions());", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NTc3NQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426145775", "bodyText": "I can extract some methods to the util class, but it's difficulty to move all of them. The alterTableViaProperties needs to access the HMS client to do the cascade ALTER COLUMNS. So I don't think we should make it static.", "author": "lirui-apache", "createdAt": "2020-05-16T11:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgxNDEwOA=="}], "type": "inlineReview", "revised_code": {"commit": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 7ea7eb8623f..7b83fe58d7c 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -521,7 +498,13 @@ public class HiveCatalog extends AbstractCatalog {\n \t\t\t\t// the alter operation isn't encoded as properties\n \t\t\t\thiveTable = alterTableViaCatalogBaseTable(tablePath, newCatalogTable, hiveTable);\n \t\t\t} else {\n-\t\t\t\talterTableViaProperties(op, hiveTable, hiveTable.getParameters(), newCatalogTable.getProperties(), hiveTable.getSd());\n+\t\t\t\talterTableViaProperties(\n+\t\t\t\t\t\top,\n+\t\t\t\t\t\thiveTable,\n+\t\t\t\t\t\t(CatalogTable) newCatalogTable,\n+\t\t\t\t\t\thiveTable.getParameters(),\n+\t\t\t\t\t\tnewCatalogTable.getProperties(),\n+\t\t\t\t\t\thiveTable.getSd());\n \t\t\t}\n \t\t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEwOTMxNA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426109314", "bodyText": "Add a AlterTableColumnTypeOperation?", "author": "JingsongLi", "createdAt": "2020-05-16T02:34:42Z", "path": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java", "diffHunk": "@@ -1450,4 +1489,253 @@ static boolean isGenericForGet(Map<String, String> properties) {\n \t\treturn properties != null && Boolean.parseBoolean(properties.getOrDefault(CatalogConfig.IS_GENERIC, \"false\"));\n \t}\n \n+\tpublic static void disallowChangeIsGeneric(boolean oldIsGeneric, boolean newIsGeneric) {\n+\t\tcheckArgument(oldIsGeneric == newIsGeneric, \"Changing whether a metadata object is generic is not allowed\");\n+\t}\n+\n+\tprivate static AlterTableOp extractAlterTableOp(Map<String, String> props) {\n+\t\tString opStr = props.remove(ALTER_TABLE_OP);\n+\t\tif (opStr != null) {\n+\t\t\treturn AlterTableOp.valueOf(opStr);\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tprivate Table alterTableViaCatalogBaseTable(ObjectPath tablePath, CatalogBaseTable baseTable, Table oldHiveTable) {\n+\t\tTable newHiveTable = instantiateHiveTable(tablePath, baseTable, hiveConf);\n+\t\t// client.alter_table() requires a valid location\n+\t\t// thus, if new table doesn't have that, it reuses location of the old table\n+\t\tif (!newHiveTable.getSd().isSetLocation()) {\n+\t\t\tnewHiveTable.getSd().setLocation(oldHiveTable.getSd().getLocation());\n+\t\t}\n+\t\treturn newHiveTable;\n+\t}\n+\n+\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n+\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\t\tswitch (alterOp) {\n+\t\t\tcase CHANGE_TBL_PROPS:\n+\t\t\t\toldProps.putAll(newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_LOCATION:\n+\t\t\t\tHiveTableUtil.extractLocation(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_FILE_FORMAT:\n+\t\t\t\tString newFileFormat = newProps.remove(STORED_AS_FILE_FORMAT);\n+\t\t\t\tHiveTableUtil.setStorageFormat(sd, newFileFormat, hiveConf);\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_SERDE_PROPS:\n+\t\t\t\tHiveTableUtil.extractRowFormat(sd, newProps);\n+\t\t\t\tbreak;\n+\t\t\tcase ADD_COLUMNS:\n+\t\t\tcase REPLACE_COLUMNS:\n+\t\t\t\tboolean cascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString[] names = newProps.remove(ADD_REPLACE_COL_NAMES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tString[] types = newProps.remove(ADD_REPLACE_COL_TYPES).split(HiveDDLUtils.COL_DELIMITER);\n+\t\t\t\tcheckArgument(names.length == types.length);\n+\t\t\t\tList<FieldSchema> columns = new ArrayList<>(names.length);\n+\t\t\t\tfor (int i = 0; i < names.length; i++) {\n+\t\t\t\t\tString comment = newProps.remove(String.format(ADD_REPLACE_COL_COMMENT_FORMAT, i));\n+\t\t\t\t\tcolumns.add(new FieldSchema(names[i], calTypeStrToHiveTypeInfo(types[i].toLowerCase()).getTypeName(), comment));\n+\t\t\t\t}\n+\t\t\t\taddReplaceColumns(sd, columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\taddReplaceColumns(partition.getSd(), columns, alterOp == REPLACE_COLUMNS);\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade add/replace columns to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHANGE_COLUMN:\n+\t\t\t\tcascade = Boolean.parseBoolean(newProps.remove(ALTER_COL_CASCADE));\n+\t\t\t\tensureCascadeOnPartitionedTable(hiveTable, cascade);\n+\t\t\t\tString oldName = newProps.remove(CHANGE_COL_OLD_NAME).toLowerCase();\n+\t\t\t\tString newName = newProps.remove(CHANGE_COL_NEW_NAME).toLowerCase();\n+\t\t\t\tString newType = newProps.remove(CHANGE_COL_NEW_TYPE).toLowerCase();\n+\t\t\t\tString colComment = newProps.remove(CHANGE_COL_COMMENT);\n+\t\t\t\tboolean first = Boolean.parseBoolean(newProps.remove(CHANGE_COL_FIRST));\n+\t\t\t\tString after = newProps.remove(CHANGE_COL_AFTER);\n+\t\t\t\tif (after != null) {\n+\t\t\t\t\tafter = after.toLowerCase();\n+\t\t\t\t}\n+\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, sd);\n+\t\t\t\tif (cascade) {\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tfor (CatalogPartitionSpec spec : listPartitions(new ObjectPath(hiveTable.getDbName(), hiveTable.getTableName()))) {\n+\t\t\t\t\t\t\tPartition partition = getHivePartition(hiveTable, spec);\n+\t\t\t\t\t\t\tchangeColumn(oldName, newName, newType, colComment, first, after, partition.getSd());\n+\t\t\t\t\t\t\tclient.alter_partition(hiveTable.getDbName(), hiveTable.getTableName(), partition);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tthrow new CatalogException(\"Failed to cascade change column to partitions\", e);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\tthrow new CatalogException(\"Unsupported alter table operation \" + alterOp);\n+\t\t}\n+\t}\n+\n+\tprivate static void ensureCascadeOnPartitionedTable(Table hiveTable, boolean cascade) {\n+\t\tif (cascade) {\n+\t\t\tif (hiveTable == null) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for a partition\");\n+\t\t\t}\n+\t\t\tif (!isTablePartitioned(hiveTable)) {\n+\t\t\t\tthrow new CatalogException(\"Alter columns cascade for non-partitioned table\");\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void changeColumn(String oldName, String newName, String newType, String comment, boolean first,\n+\t\t\tString after, StorageDescriptor sd) {\n+\t\tif (first && after != null) {\n+\t\t\tthrow new CatalogException(\"Both first and after specified for CHANGE COLUMN\");\n+\t\t}\n+\t\tTypeInfo newTypeInfo = calTypeStrToHiveTypeInfo(newType);\n+\t\tList<String> oldNames = getFieldNames(sd.getCols());\n+\t\tint oldIndex = oldNames.indexOf(oldName);\n+\t\tif (oldIndex < 0) {\n+\t\t\tthrow new CatalogException(String.format(\"Old column %s not found for CHANGE COLUMN\", oldName));\n+\t\t}\n+\t\tFieldSchema newField = new FieldSchema(newName, newTypeInfo.getTypeName(), comment);\n+\t\tif ((!first && after == null) || oldName.equals(after)) {\n+\t\t\tsd.getCols().set(oldIndex, newField);\n+\t\t} else {\n+\t\t\t// need change column position\n+\t\t\tsd.getCols().remove(oldIndex);\n+\t\t\tif (first) {\n+\t\t\t\tsd.getCols().add(0, newField);\n+\t\t\t} else {\n+\t\t\t\tint newIndex = oldNames.indexOf(after);\n+\t\t\t\tif (newIndex < 0) {\n+\t\t\t\t\tthrow new CatalogException(String.format(\"After column %s not found for CHANGE COLUMN\", after));\n+\t\t\t\t}\n+\t\t\t\tsd.getCols().add(++newIndex, newField);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void addReplaceColumns(StorageDescriptor sd, List<FieldSchema> columns, boolean replace) {\n+\t\tif (replace) {\n+\t\t\tsd.setCols(columns);\n+\t\t} else {\n+\t\t\tsd.getCols().addAll(columns);\n+\t\t}\n+\t}\n+\n+\tprivate static TypeInfo calTypeStrToHiveTypeInfo(String typeStr) {", "originalCommit": "ba8a1595c785a2fa6d4a8886f216bc7b69b3c813", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "chunk": "diff --git a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\nindex 7ea7eb8623f..7b83fe58d7c 100644\n--- a/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n+++ b/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java\n\n@@ -1511,8 +1500,13 @@ public class HiveCatalog extends AbstractCatalog {\n \t\treturn newHiveTable;\n \t}\n \n-\tprivate void alterTableViaProperties(AlterTableOp alterOp, Table hiveTable, Map<String, String> oldProps,\n-\t\t\tMap<String, String> newProps, StorageDescriptor sd) {\n+\tprivate void alterTableViaProperties(\n+\t\t\tAlterTableOp alterOp,\n+\t\t\tTable hiveTable,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tMap<String, String> oldProps,\n+\t\t\tMap<String, String> newProps,\n+\t\t\tStorageDescriptor sd) {\n \t\tswitch (alterOp) {\n \t\t\tcase CHANGE_TBL_PROPS:\n \t\t\t\toldProps.putAll(newProps);\n"}}, {"oid": "ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "url": "https://github.com/apache/flink/commit/ba3e0e5ab6fff290c83bcbcb1c2dc94e14a5f4f9", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:46:30Z", "type": "forcePushed"}, {"oid": "d263c2535994e33dedb8bf0548e533f7e6049af9", "url": "https://github.com/apache/flink/commit/d263c2535994e33dedb8bf0548e533f7e6049af9", "message": "[FLINK-17448][sql-parser][table-api-java][table-planner-blink][hive] Implement table DDLs for Hive dialect part2", "committedDate": "2020-05-16T09:54:49Z", "type": "commit"}, {"oid": "91df094c8c723370106dac5606db3689de0953e8", "url": "https://github.com/apache/flink/commit/91df094c8c723370106dac5606db3689de0953e8", "message": "remove unused change", "committedDate": "2020-05-16T09:54:49Z", "type": "commit"}, {"oid": "86a531ee0e322ddab3a3a5e23a1098b62081e1d5", "url": "https://github.com/apache/flink/commit/86a531ee0e322ddab3a3a5e23a1098b62081e1d5", "message": "fix unparse", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "8d938603ec47386fcf16e1e509751948bc44b8dc", "url": "https://github.com/apache/flink/commit/8d938603ec47386fcf16e1e509751948bc44b8dc", "message": "fix hive parser", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "5d12339609095aaa590b5168239934468a2002ac", "url": "https://github.com/apache/flink/commit/5d12339609095aaa590b5168239934468a2002ac", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:54:50Z", "type": "commit"}, {"oid": "5d12339609095aaa590b5168239934468a2002ac", "url": "https://github.com/apache/flink/commit/5d12339609095aaa590b5168239934468a2002ac", "message": "add operations for ALTER COLUMNS", "committedDate": "2020-05-16T09:54:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0MTUzMw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426141533", "bodyText": "Add comments to public method.", "author": "JingsongLi", "createdAt": "2020-05-16T10:34:40Z", "path": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java", "diffHunk": "@@ -328,6 +329,17 @@ public CatalogBaseTable getTable() {\n \t\treturn Optional.empty();\n \t}\n \n+\tpublic Optional<CatalogPartition> getPartition(ObjectIdentifier tableIdentifier, CatalogPartitionSpec partitionSpec) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\nindex 2fdc70bcf40..c3c0e7aad4a 100644\n--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\n+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\n\n@@ -329,6 +329,14 @@ public final class CatalogManager {\n \t\treturn Optional.empty();\n \t}\n \n+\t/**\n+\t * Retrieves a partition with a fully qualified table path and partition spec.\n+\t * If the path is not yet fully qualified use{@link #qualifyIdentifier(UnresolvedIdentifier)} first.\n+\t *\n+\t * @param tableIdentifier full path of the table to retrieve\n+\t * @param partitionSpec full partition spec\n+\t * @return partition in the table.\n+\t */\n \tpublic Optional<CatalogPartition> getPartition(ObjectIdentifier tableIdentifier, CatalogPartitionSpec partitionSpec) {\n \t\tCatalog catalog = catalogs.get(tableIdentifier.getCatalogName());\n \t\tif (catalog != null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzcxMA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147710", "bodyText": "one parameter one line.", "author": "JingsongLi", "createdAt": "2020-05-16T12:04:30Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns;\n+import org.apache.flink.sql.parser.ddl.SqlChangeColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.operations.Operation;\n+import org.apache.flink.table.operations.ddl.AlterTableSchemaOperation;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.calcite.sql.SqlDataTypeSpec;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utils methods for converting sql to operations.\n+ */\n+public class OperationConverterUtils {\n+\n+\tprivate OperationConverterUtils() {\n+\t}\n+\n+\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\nindex f83506eacd8..d36bd6f7431 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\n\n@@ -57,9 +57,15 @@ public class OperationConverterUtils {\n \tprivate OperationConverterUtils() {\n \t}\n \n-\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n-\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {\n-\t\t// verify partitions columns appear last in the schema\n+\tpublic static Operation convertAddReplaceColumns(\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tSqlValidator sqlValidator) {\n+\t\t// This is only used by the Hive dialect at the moment. In Hive, only non-partition columns can be\n+\t\t// added/replaced and users will only define non-partition columns in the new column list. Therefore, we require\n+\t\t// that partitions columns must appear last in the schema (which is inline with Hive). Otherwise, we won't be\n+\t\t// able to determine the column positions after the non-partition columns are replaced.\n \t\tTableSchema oldSchema = catalogTable.getSchema();\n \t\tint numPartCol = catalogTable.getPartitionKeys().size();\n \t\tSet<String> lastCols = oldSchema.getTableColumns()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzc4NQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147785", "bodyText": "This is not flink partition style, this is hive partition style", "author": "JingsongLi", "createdAt": "2020-05-16T12:05:16Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.planner.utils;\n+\n+import org.apache.flink.sql.parser.ddl.SqlAddReplaceColumns;\n+import org.apache.flink.sql.parser.ddl.SqlChangeColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableColumn;\n+import org.apache.flink.sql.parser.ddl.SqlTableOption;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.catalog.CatalogTable;\n+import org.apache.flink.table.catalog.CatalogTableImpl;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.operations.Operation;\n+import org.apache.flink.table.operations.ddl.AlterTableSchemaOperation;\n+import org.apache.flink.table.planner.calcite.FlinkTypeFactory;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+\n+import org.apache.calcite.sql.SqlDataTypeSpec;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.sql.validate.SqlValidator;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utils methods for converting sql to operations.\n+ */\n+public class OperationConverterUtils {\n+\n+\tprivate OperationConverterUtils() {\n+\t}\n+\n+\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {\n+\t\t// verify partitions columns appear last in the schema\n+\t\tTableSchema oldSchema = catalogTable.getSchema();\n+\t\tint numPartCol = catalogTable.getPartitionKeys().size();", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE1NTEyMg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426155122", "bodyText": "I have added some explanations in the comments.", "author": "lirui-apache", "createdAt": "2020-05-16T13:47:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzc4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\nindex f83506eacd8..d36bd6f7431 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/utils/OperationConverterUtils.java\n\n@@ -57,9 +57,15 @@ public class OperationConverterUtils {\n \tprivate OperationConverterUtils() {\n \t}\n \n-\tpublic static Operation convertAddReplaceColumns(ObjectIdentifier tableIdentifier,\n-\t\t\tSqlAddReplaceColumns addReplaceColumns, CatalogTable catalogTable, SqlValidator sqlValidator) {\n-\t\t// verify partitions columns appear last in the schema\n+\tpublic static Operation convertAddReplaceColumns(\n+\t\t\tObjectIdentifier tableIdentifier,\n+\t\t\tSqlAddReplaceColumns addReplaceColumns,\n+\t\t\tCatalogTable catalogTable,\n+\t\t\tSqlValidator sqlValidator) {\n+\t\t// This is only used by the Hive dialect at the moment. In Hive, only non-partition columns can be\n+\t\t// added/replaced and users will only define non-partition columns in the new column list. Therefore, we require\n+\t\t// that partitions columns must appear last in the schema (which is inline with Hive). Otherwise, we won't be\n+\t\t// able to determine the column positions after the non-partition columns are replaced.\n \t\tTableSchema oldSchema = catalogTable.getSchema();\n \t\tint numPartCol = catalogTable.getPartitionKeys().size();\n \t\tSet<String> lastCols = oldSchema.getTableColumns()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzg1Nw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147857", "bodyText": "getPartitionSpec?", "author": "JingsongLi", "createdAt": "2020-05-16T12:06:14Z", "path": "flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/SqlPartitionUtils.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.sql.parser;\n+\n+import org.apache.calcite.sql.SqlLiteral;\n+import org.apache.calcite.sql.SqlNode;\n+import org.apache.calcite.sql.SqlNodeList;\n+import org.apache.calcite.util.NlsString;\n+\n+import java.util.LinkedHashMap;\n+\n+/**\n+ * Utils methods for partition DDLs.\n+ */\n+public class SqlPartitionUtils {\n+\n+\tprivate SqlPartitionUtils() {\n+\t}\n+\n+\t/** Get static partition key value pair as strings.\n+\t *\n+\t * <p>For character literals we return the unquoted and unescaped values.\n+\t * For other types we use {@link SqlLiteral#toString()} to get\n+\t * the string format of the value literal.\n+\t *\n+\t * @return the mapping of column names to values of partition specifications,\n+\t * returns an empty map if there is no partition specifications.\n+\t */\n+\tpublic static LinkedHashMap<String, String> getPartitionKVs(SqlNodeList partitionSpec) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE1MzY2MQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426153661", "bodyText": "In our name convention, a partition spec is usually a SqlNodeList. And this is following the example of RichSqlInsert, whose getStaticPartitionKVs returns a map.", "author": "lirui-apache", "createdAt": "2020-05-16T13:28:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0Nzg1Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzkyMA==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147920", "bodyText": "else if", "author": "JingsongLi", "createdAt": "2020-05-16T12:07:06Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\nindex 218908990aa..3dcdb397d5d 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n\n@@ -253,7 +253,7 @@ public class SqlToOperationConverter {\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n \t\t\treturn convertAlterTableProperties(\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n \t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n \t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0NzkyNQ==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426147925", "bodyText": "else", "author": "JingsongLi", "createdAt": "2020-05-16T12:07:11Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {\n+\t\t\tCatalogTable oldTable = (CatalogTable) baseTable;\n+\t\t\tMap<String, String> newProperties = new HashMap<>(oldTable.getProperties());\n+\t\t\tnewProperties.putAll(OperationConverterUtils.extractProperties(alterTableProperties.getPropertyList()));\n+\t\t\tCatalogTable newTable = new CatalogTableImpl(\n+\t\t\t\t\toldTable.getSchema(),\n+\t\t\t\t\toldTable.getPartitionKeys(),\n+\t\t\t\t\tnewProperties,\n+\t\t\t\t\toldTable.getComment());\n+\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, newTable);\n+\t\t}\n+\t\tthrow new ValidationException(\"Unsupported CatalogBaseTable type: \" + baseTable.getClass().getName());", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\nindex 218908990aa..3dcdb397d5d 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n\n@@ -253,7 +253,7 @@ public class SqlToOperationConverter {\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n \t\t\treturn convertAlterTableProperties(\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n \t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n \t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0ODAyMg==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426148022", "bodyText": "oldTable.copy", "author": "JingsongLi", "createdAt": "2020-05-16T12:08:02Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();\n+\t\t\talterTableProperties.getPropertyList().getList().forEach(p ->\n+\t\t\t\t\tprops.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n+\t\t\treturn new AlterPartitionPropertiesOperation(tableIdentifier, partitionSpec, catalogPartition);\n+\t\t}\n+\t\t// it's altering a table\n+\t\tif (baseTable instanceof CatalogTable) {\n+\t\t\tCatalogTable oldTable = (CatalogTable) baseTable;\n+\t\t\tMap<String, String> newProperties = new HashMap<>(oldTable.getProperties());\n+\t\t\tnewProperties.putAll(OperationConverterUtils.extractProperties(alterTableProperties.getPropertyList()));\n+\t\t\tCatalogTable newTable = new CatalogTableImpl(", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\nindex 218908990aa..3dcdb397d5d 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n\n@@ -253,7 +253,7 @@ public class SqlToOperationConverter {\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n \t\t\treturn convertAlterTableProperties(\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n \t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n \t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE0ODE1Nw==", "url": "https://github.com/apache/flink/pull/12108#discussion_r426148157", "bodyText": "Should create a new CatalogPartition with a new Map.", "author": "JingsongLi", "createdAt": "2020-05-16T12:10:13Z", "path": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java", "diffHunk": "@@ -231,82 +239,102 @@ private Operation convertDropTable(SqlDropTable sqlDropTable) {\n \tprivate Operation convertAlterTable(SqlAlterTable sqlAlterTable) {\n \t\tUnresolvedIdentifier unresolvedIdentifier = UnresolvedIdentifier.of(sqlAlterTable.fullTableName());\n \t\tObjectIdentifier tableIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);\n+\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n+\t\tif (!optionalCatalogTable.isPresent() || optionalCatalogTable.get().isTemporary()) {\n+\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n+\t\t\t\t\ttableIdentifier.toString()));\n+\t\t}\n+\t\tCatalogBaseTable baseTable = optionalCatalogTable.get().getTable();\n \t\tif (sqlAlterTable instanceof SqlAlterTableRename) {\n \t\t\tUnresolvedIdentifier newUnresolvedIdentifier =\n \t\t\t\tUnresolvedIdentifier.of(((SqlAlterTableRename) sqlAlterTable).fullNewTableName());\n \t\t\tObjectIdentifier newTableIdentifier = catalogManager.qualifyIdentifier(newUnresolvedIdentifier);\n \t\t\treturn new AlterTableRenameOperation(tableIdentifier, newTableIdentifier);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable = catalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tCatalogTable originalCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tMap<String, String> properties = new HashMap<>(originalCatalogTable.getOptions());\n-\t\t\t\t((SqlAlterTableProperties) sqlAlterTable).getPropertyList().getList().forEach(p ->\n-\t\t\t\t\tproperties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));\n-\t\t\t\tCatalogTable catalogTable = new CatalogTableImpl(\n-\t\t\t\t\toriginalCatalogTable.getSchema(),\n-\t\t\t\t\toriginalCatalogTable.getPartitionKeys(),\n-\t\t\t\t\tproperties,\n-\t\t\t\t\toriginalCatalogTable.getComment());\n-\t\t\t\treturn new AlterTablePropertiesOperation(tableIdentifier, catalogTable);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\ttableIdentifier.toString()));\n-\t\t\t}\n+\t\t\treturn convertAlterTableProperties(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n-\t\t\t\t\t\t.getConstraint();\n-\t\t\t\tvalidateTableConstraint(constraint);\n-\t\t\t\tTableSchema oriSchema = optionalCatalogTable.get().getTable().getSchema();\n-\t\t\t\t// Sanity check for constraint.\n-\t\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n-\t\t\t\tif (constraint.getConstraintName().isPresent()) {\n-\t\t\t\t\tbuilder.primaryKey(\n-\t\t\t\t\t\t\tconstraint.getConstraintName().get(),\n-\t\t\t\t\t\t\tconstraint.getColumnNames());\n-\t\t\t\t} else {\n-\t\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n-\t\t\t\t}\n-\t\t\t\tbuilder.build();\n-\t\t\t\treturn new AlterTableAddConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n+\t\t\t\t\t.getConstraint();\n+\t\t\tvalidateTableConstraint(constraint);\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\t// Sanity check for constraint.\n+\t\t\tTableSchema.Builder builder = TableSchemaUtils.builderWithGivenSchema(oriSchema);\n+\t\t\tif (constraint.getConstraintName().isPresent()) {\n+\t\t\t\tbuilder.primaryKey(\n+\t\t\t\t\t\tconstraint.getConstraintName().get(),\n \t\t\t\t\t\tconstraint.getColumnNames());\n \t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\t\tbuilder.primaryKey(constraint.getColumnNames());\n \t\t\t}\n+\t\t\tbuilder.build();\n+\t\t\treturn new AlterTableAddConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraint.getConstraintName().orElse(null),\n+\t\t\t\t\tconstraint.getColumnNames());\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableDropConstraint) {\n-\t\t\tOptional<CatalogManager.TableLookupResult> optionalCatalogTable =\n-\t\t\t\t\tcatalogManager.getTable(tableIdentifier);\n-\t\t\tif (optionalCatalogTable.isPresent() && !optionalCatalogTable.get().isTemporary()) {\n-\t\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n-\t\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n-\t\t\t\tCatalogTable oriCatalogTable = (CatalogTable) optionalCatalogTable.get().getTable();\n-\t\t\t\tTableSchema oriSchema = oriCatalogTable.getSchema();\n-\t\t\t\tif (!oriSchema.getPrimaryKey()\n-\t\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n-\t\t\t\t\t\t.isPresent()) {\n-\t\t\t\t\tthrow new ValidationException(\n-\t\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n-\t\t\t\t}\n-\t\t\t\treturn new AlterTableDropConstraintOperation(\n-\t\t\t\t\t\ttableIdentifier,\n-\t\t\t\t\t\tconstraintName);\n-\t\t\t} else {\n-\t\t\t\tthrow new ValidationException(String.format(\"Table %s doesn't exist or is a temporary table.\",\n-\t\t\t\t\t\ttableIdentifier.toString()));\n+\t\t\tSqlAlterTableDropConstraint dropConstraint = ((SqlAlterTableDropConstraint) sqlAlterTable);\n+\t\t\tString constraintName = dropConstraint.getConstraintName().getSimple();\n+\t\t\tTableSchema oriSchema = baseTable.getSchema();\n+\t\t\tif (!oriSchema.getPrimaryKey()\n+\t\t\t\t\t.filter(pk -> pk.getName().equals(constraintName))\n+\t\t\t\t\t.isPresent()) {\n+\t\t\t\tthrow new ValidationException(\n+\t\t\t\t\t\tString.format(\"CONSTRAINT [%s] does not exist\", constraintName));\n \t\t\t}\n+\t\t\treturn new AlterTableDropConstraintOperation(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\tconstraintName);\n+\t\t} else if (sqlAlterTable instanceof SqlAddReplaceColumns) {\n+\t\t\treturn OperationConverterUtils.convertAddReplaceColumns(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlAddReplaceColumns) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n+\t\t} else if (sqlAlterTable instanceof SqlChangeColumn) {\n+\t\t\treturn OperationConverterUtils.convertChangeColumn(\n+\t\t\t\t\ttableIdentifier,\n+\t\t\t\t\t(SqlChangeColumn) sqlAlterTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n+\t\t\t\t\tflinkPlanner.getOrCreateSqlValidator());\n \t\t} else {\n \t\t\tthrow new ValidationException(\n \t\t\t\t\tString.format(\"[%s] needs to implement\",\n \t\t\t\t\t\t\tsqlAlterTable.toSqlString(CalciteSqlDialect.DEFAULT)));\n \t\t}\n \t}\n \n+\tprivate Operation convertAlterTableProperties(ObjectIdentifier tableIdentifier, CatalogBaseTable baseTable,\n+\t\t\tSqlAlterTableProperties alterTableProperties) {\n+\t\tLinkedHashMap<String, String> partitionKVs = alterTableProperties.getPartitionKVs();\n+\t\t// it's altering partitions\n+\t\tif (partitionKVs != null) {\n+\t\t\tCatalogPartitionSpec partitionSpec = new CatalogPartitionSpec(partitionKVs);\n+\t\t\tCatalogPartition catalogPartition = catalogManager.getPartition(tableIdentifier, partitionSpec)\n+\t\t\t\t\t.orElseThrow(() -> new ValidationException(String.format(\"Partition %s of table %s doesn't exist\",\n+\t\t\t\t\t\t\tpartitionSpec.getPartitionSpec(), tableIdentifier)));\n+\t\t\tMap<String, String> props = catalogPartition.getProperties();", "originalCommit": "5d12339609095aaa590b5168239934468a2002ac", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "05368729e02dc0fad72c783a85cf951418ef901a", "chunk": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\nindex 218908990aa..3dcdb397d5d 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/operations/SqlToOperationConverter.java\n\n@@ -253,7 +253,7 @@ public class SqlToOperationConverter {\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableProperties) {\n \t\t\treturn convertAlterTableProperties(\n \t\t\t\t\ttableIdentifier,\n-\t\t\t\t\tbaseTable,\n+\t\t\t\t\t(CatalogTable) baseTable,\n \t\t\t\t\t(SqlAlterTableProperties) sqlAlterTable);\n \t\t} else if (sqlAlterTable instanceof SqlAlterTableAddConstraint) {\n \t\t\tSqlTableConstraint constraint = ((SqlAlterTableAddConstraint) sqlAlterTable)\n"}}, {"oid": "21988e7157e176b1d75a16dbfd8016075ad51968", "url": "https://github.com/apache/flink/commit/21988e7157e176b1d75a16dbfd8016075ad51968", "message": "address comments", "committedDate": "2020-05-16T12:14:04Z", "type": "commit"}, {"oid": "05368729e02dc0fad72c783a85cf951418ef901a", "url": "https://github.com/apache/flink/commit/05368729e02dc0fad72c783a85cf951418ef901a", "message": "address comments", "committedDate": "2020-05-16T13:45:55Z", "type": "commit"}]}