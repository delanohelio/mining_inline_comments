{"pr_number": 10726, "pr_title": "Fix race conditions in KafkaStreams startup", "pr_createdAt": "2020-07-14T17:29:35Z", "pr_url": "https://github.com/quarkusio/quarkus/pull/10726", "timeline": [{"oid": "6b662865a15f9d3102f78aba308e0af7dd6778a6", "url": "https://github.com/quarkusio/quarkus/commit/6b662865a15f9d3102f78aba308e0af7dd6778a6", "message": "#10724 Start up KafkaStreams during library and not app start up", "committedDate": "2020-07-14T17:31:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454889653", "bodyText": "I have no idea what this code is doing but this observer will be notified before similar observers with no priority declared (the default priority is 2500, Priority.LIBRARY_BEFORE + 500 = 1500 and observers with smaller priority values are called first).\nIf that's the intention I have nothing to add ;-).", "author": "mkouba", "createdAt": "2020-07-15T08:42:02Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java", "diffHunk": "@@ -189,7 +191,7 @@ private static String asString(List<InetSocketAddress> addresses) {\n                 .collect(Collectors.joining(\",\"));\n     }\n \n-    void onStart(@Observes StartupEvent ev) {\n+    void onStart(@Observes @Priority(Interceptor.Priority.LIBRARY_BEFORE + 500) StartupEvent ev) {", "originalCommit": "6b662865a15f9d3102f78aba308e0af7dd6778a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5NTA3OA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454895078", "bodyText": "Yeah, I should have been clearer :).\nI'm not totally sure this pattern of doing things with startup events and then returning what's initialized here is a good thing. I'm pretty sure this could lead to weird startup issues.\nBut I'm not a Kafka Streams expect, maybe it's how they do things to avoid waiting for things to be fully started.", "author": "gsmet", "createdAt": "2020-07-15T08:50:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwODg0OQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454908849", "bodyText": "Is this issue specific to Kafka Streams even, or is it something more generic? I'd have expected that the dependency resolver of CDI takes care of this: i.e. if there's an injection point for KafkaStreams (or any type, really), it would make sure the corresponding producer is started up before that?\nAlso it's interesting that an NPE happens, I'd rather have expected some sort of \"UnresolvedDependencyInjection\"?", "author": "gunnarmorling", "createdAt": "2020-07-15T09:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkyNTA2MA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454925060", "bodyText": "@gunnarmorling, dependency injection and CDI events are two different concepts. If there is an injection point of type KafkaStreams then CDI ensures that the corresponding bean is created before the injection point is injected.\n\nAlso it's interesting that an NPE happens, I'd rather have expected some sort of \"UnresolvedDependencyInjection\"?\n\nUnsatisfiedResolutionException is thrown if there is an injection point that cannot be satisfied. Which is not the case here  - the producer method would satisfy such an injection point. However, the KafkaStreamsTopologyManager#streams could be uninitialized and thus the producer method returns null.\nSo the real problem is - when/how to initalize the streams so that a bean may not inject an uninitialized producer method.\nThis morning I sent a PR that describes this \"late init\" problem with extensions:\nhttps://github.com/quarkusio/quarkus/blob/f12c24e4be88fd3d7d077ed9087fe8ffecc83f56/docs/src/main/asciidoc/writing-extensions.adoc#user-content-bean_init\nUnfortunately, I'm no kafka streams expert either so I can't answer your question @gsmet...", "author": "mkouba", "createdAt": "2020-07-15T09:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzNzkxNA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454937914", "bodyText": "the producer method would satisfy such an injection point. However, the KafkaStreamsTopologyManager#streams could be uninitialized and thus the producer method returns null.\n\nI see. I think this code is based on the assumpt that if there's a producer method like in this case here, the @observes StartupEvent method of the bean declaring that method is guaranteed to have been run before that producer method is invoked. Seems like that assumption doesn't hold as per what your say.\nIn that light I'm wondering why we don't start the KafkaStreams object simply in the constructor instead of using the start-up observer method.", "author": "gunnarmorling", "createdAt": "2020-07-15T10:02:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk0MDQzMQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r454940431", "bodyText": "StartupEvent method of the bean declaring that method is guaranteed to have been run before that producer method is invoked. Seems like that assumption doesn't hold as per what your say.\n\nYes, completely unfounded assumption.\n\nIn that light I'm wondering why we don't start the KafkaStreams object simply in the constructor instead of using the start-up observer method.\n\nMaybe it has to be started at runtime and some other bean needs to access KafkaStreamsTopologyManager during static init?", "author": "mkouba", "createdAt": "2020-07-15T10:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzNTk0OA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455035948", "bodyText": "I think the problem is KafkaStreamsTopologyManager#setRuntimeConfig called by KafkaStreamsRecorder has to happen before KStream can be started (because once a KStreams instance is started it can't be re-configured afterward). And I guess KafkaStreamsRuntimeConfig is not a bean that could be directly injected in the constructor either ?", "author": "rquinio", "createdAt": "2020-07-15T13:06:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA0MTI3Nw==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455041277", "bodyText": "KafkaStreamsRuntimeConfig can be injected in a bean. However, it is a RUN_TIME config root so it may not be available before the app starts. E.g. if the consuming bean is created during STATIC_INIT you will get CreationException.", "author": "mkouba", "createdAt": "2020-07-15T13:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA0MjM5NA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455042394", "bodyText": "Yes, completely unfounded assumption.\n\nWell, I'd say it's not so far-fetched to assume a bean would be fully initialized and ready to use before any of its methods are invoked by the runtime. But ok, things are as they are.\n\nMaybe it has to be started at runtime\n\nYes, it absolutely must be started at runtime. KafkaStreamsTopologyManager shouldn't be needed at static init IIRC. So can we have it being instantiated at runtime only? Or maybe the suggested priority fix is just fine really :)", "author": "gunnarmorling", "createdAt": "2020-07-15T13:16:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA0OTcxMA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455049710", "bodyText": "Yes, completely unfounded assumption.\n\n\n\nWell, I'd say it's not so far-fetched to assume a bean would be fully initialized and ready to use before any of its methods are invoked by the runtime. But ok, things are as they are.\n\n@gunnarmorling The bean itself is fully initialized but its state is not initialized. In this particular case there is a bean that declares a producer method and an obsever. The producer method is backed by a state that is explicitly initialized after the bean is created and has no connection to the observer except that they both are invoked upon the same bean instance. In other words, it is only guaranteed that the KafkaStreamsTopologyManager is created before the producer or the observer is called.", "author": "mkouba", "createdAt": "2020-07-15T13:26:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM2MTg2Mw==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455361863", "bodyText": "Just to add. I did debug the code and it is showing non deterministic behavior. Sometimes KafkaStreamsTopologyManager would be initialized before the bean expecting the KafkaStreams and other times not.\n    @Inject\n    KafkaStreams kafkaStreams;\n\n    void startUp(@Observes StartupEvent ev) {\n        LOG.info(\"Initializing metrics exporter\");\n    }\n\n    @PostConstruct\n    void init() {\n\n        LOG.info(\"My stream as a String: \" + kafkaStreams);\n        // Sometimes it gives a NPE\n        var metrics = kafkaStreams.metrics();\n\n    }\n\nA quick work around was to change the priority to\n    void startUp(@Observes @Priority(Interceptor.Priority.APPLICATION + 600) StartupEvent ev) {\n        LOG.info(\"Initializing metrics exporter\");\n    }\n\nWhich is what brought me to the code in this PR.", "author": "pcasaes", "createdAt": "2020-07-15T21:26:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTU0ODE1MA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455548150", "bodyText": "I did debug the code and it is showing non deterministic behavior.\n\nYes, that's expected. Your StartupEvent observer has also the default priority - if multiple observers define the same priority the ordering is undefined (random).", "author": "mkouba", "createdAt": "2020-07-16T06:51:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY0MDgxNg==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455640816", "bodyText": "@mkouba can't we just move whatever is in the startup event in the @Produces method?", "author": "gsmet", "createdAt": "2020-07-16T09:10:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY1NjIyMw==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455656223", "bodyText": "Well, that could be a solution. However, it could still break things if someone attempts to access the produced bean during STATIC_INIT....\nBut again I have no idea about the \"business logic\" of this bean...", "author": "mkouba", "createdAt": "2020-07-16T09:35:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY2MzQxMA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455663410", "bodyText": "I think it won't work in all cases, because it's not necessary that an application injects explicitly a KafkaStreams instance into another bean, more over if it is injected in an @applicationScoped bean, it will be created after the application has started, not sure if it's the expected behavior.\nMaybe the best would be to change the scope of the Kafkastreams method to @applicationScoped (it's currently a Singleton) and create a holder class to be sure to access the kafkastreams at runtime.", "author": "vietk", "createdAt": "2020-07-16T09:47:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY3MDExNQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455670115", "bodyText": "However, it could still break things if someone attempts to access the produced bean during STATIC_INIT...\n\nThe KafkaStreams object is only meant to be used at runtime. It processes data from Kafka topics. You could compare it to creating a database connection which also doesn't make sense at static init.", "author": "gunnarmorling", "createdAt": "2020-07-16T09:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY5OTkxMQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455699911", "bodyText": "The KafkaStreams object is only meant to be used at runtime.\n\nIn that case, we should probably throw an exception if someone attempts to use the bean during STATIC_INIT...", "author": "mkouba", "createdAt": "2020-07-16T10:53:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwMzgwNA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455703804", "bodyText": "In that case, we should probably throw an exception if someone attempts to use the bean during STATIC_INIT...\n\nWell, we don't really do that for all the other beans that shouldn't be accessed at static init, do we? At least I never took care of this.\nIf moving things to the @Produces method is OK, that would have my preference. (And I would like to include it in 1.6.1)", "author": "gsmet", "createdAt": "2020-07-16T11:01:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTg5MDU5OA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r455890598", "bodyText": "Just pushed the change. Moved the instantiation to the producer method. As a consequence it no longer will be eager loaded though. Also will throw an exception if the Topology is not defined, not sure if this is the right approach.", "author": "pcasaes", "createdAt": "2020-07-16T15:51:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg4OTY1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ef4ee7656955449f1273657892d4c8ca1963f7df", "chunk": "diff --git a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\nindex b01b6a50b1..7257272ba7 100644\n--- a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n+++ b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n\n@@ -191,56 +188,56 @@ public class KafkaStreamsTopologyManager {\n                 .collect(Collectors.joining(\",\"));\n     }\n \n-    void onStart(@Observes @Priority(Interceptor.Priority.LIBRARY_BEFORE + 500) StartupEvent ev) {\n-        if (executor == null) {\n-            return;\n+    void onStop(@Observes ShutdownEvent ev) {\n+        if (streams != null) {\n+            LOGGER.debug(\"Stopping Kafka Streams pipeline\");\n+            streams.close();\n         }\n+    }\n \n-        String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n-\n-        Properties streamsProperties = getStreamsProperties(properties, bootstrapServersConfig, runtimeConfig);\n-\n-        if (kafkaClientSupplier.isUnsatisfied()) {\n-            streams = new KafkaStreams(topology.get(), streamsProperties);\n-        } else {\n-            streams = new KafkaStreams(topology.get(), streamsProperties, kafkaClientSupplier.get());\n+    @Produces\n+    @Singleton\n+    public KafkaStreams getStreams() {\n+        if (topology.isUnsatisfied()) {\n+            throw new IllegalStateException(\"No Topology producer has been defined\");\n         }\n \n-        if (!stateListener.isUnsatisfied()) {\n-            streams.setStateListener(stateListener.get());\n-        }\n-        if (!globalStateRestoreListener.isUnsatisfied()) {\n-            streams.setGlobalStateRestoreListener(globalStateRestoreListener.get());\n-        }\n+        if (streams == null) {\n+            String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n \n-        adminClientConfig = getAdminClientConfig(streamsProperties);\n+            Properties streamsProperties = getStreamsProperties(properties, bootstrapServersConfig, runtimeConfig);\n \n-        executor.execute(new Runnable() {\n+            if (kafkaClientSupplier.isUnsatisfied()) {\n+                streams = new KafkaStreams(topology.get(), streamsProperties);\n+            } else {\n+                streams = new KafkaStreams(topology.get(), streamsProperties, kafkaClientSupplier.get());\n+            }\n \n-            @Override\n-            public void run() {\n-                try {\n-                    waitForTopicsToBeCreated(runtimeConfig.getTrimmedTopics());\n-                } catch (InterruptedException e) {\n-                    Thread.currentThread().interrupt();\n-                    return;\n-                }\n-                LOGGER.debug(\"Starting Kafka Streams pipeline\");\n-                streams.start();\n+            if (!stateListener.isUnsatisfied()) {\n+                streams.setStateListener(stateListener.get());\n+            }\n+            if (!globalStateRestoreListener.isUnsatisfied()) {\n+                streams.setGlobalStateRestoreListener(globalStateRestoreListener.get());\n             }\n-        });\n-    }\n \n-    void onStop(@Observes @Priority(Interceptor.Priority.LIBRARY_BEFORE + 500) ShutdownEvent ev) {\n-        if (streams != null) {\n-            LOGGER.debug(\"Stopping Kafka Streams pipeline\");\n-            streams.close();\n+            adminClientConfig = getAdminClientConfig(streamsProperties);\n+\n+            executor.execute(new Runnable() {\n+\n+                @Override\n+                public void run() {\n+                    try {\n+                        waitForTopicsToBeCreated(runtimeConfig.getTrimmedTopics());\n+                    } catch (InterruptedException e) {\n+                        Thread.currentThread().interrupt();\n+                        return;\n+                    }\n+                    LOGGER.debug(\"Starting Kafka Streams pipeline\");\n+                    streams.start();\n+                }\n+            });\n         }\n-    }\n \n-    @Produces\n-    @Singleton\n-    public KafkaStreams getStreams() {\n         return streams;\n     }\n \n"}}, {"oid": "ef4ee7656955449f1273657892d4c8ca1963f7df", "url": "https://github.com/quarkusio/quarkus/commit/ef4ee7656955449f1273657892d4c8ca1963f7df", "message": "Instantiate KafkaStreams in producer method", "committedDate": "2020-07-16T15:49:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI4NDk2Mw==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r456284963", "bodyText": "I would use a @PreDestroy here instead.", "author": "gsmet", "createdAt": "2020-07-17T08:03:33Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java", "diffHunk": "@@ -189,56 +188,56 @@ private static String asString(List<InetSocketAddress> addresses) {\n                 .collect(Collectors.joining(\",\"));\n     }\n \n-    void onStart(@Observes StartupEvent ev) {\n-        if (executor == null) {\n-            return;\n+    void onStop(@Observes ShutdownEvent ev) {", "originalCommit": "ef4ee7656955449f1273657892d4c8ca1963f7df", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "chunk": "diff --git a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\nindex 7257272ba7..95f7a4101c 100644\n--- a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n+++ b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n\n@@ -1,281 +1,29 @@\n package io.quarkus.kafka.streams.runtime;\n \n-import java.net.InetSocketAddress;\n-import java.time.Duration;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashSet;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n \n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.event.Observes;\n-import javax.enterprise.inject.Instance;\n-import javax.enterprise.inject.Produces;\n-import javax.inject.Inject;\n-import javax.inject.Singleton;\n-\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.AdminClientConfig;\n import org.apache.kafka.clients.admin.ListTopicsResult;\n-import org.apache.kafka.common.config.SaslConfigs;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.streams.KafkaClientSupplier;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreams.StateListener;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n import org.jboss.logging.Logger;\n \n-import io.quarkus.runtime.ShutdownEvent;\n-\n-/**\n- * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n- * method returning a KS {@link Topology}, then this topology will be configured\n- * and started. Optionally, before starting the pipeline, this manager will wait\n- * for a given set of topics to be created, as KS itself will fail without all\n- * input topics being created upfront.\n- */\n-@ApplicationScoped\n public class KafkaStreamsTopologyManager {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaStreamsTopologyManager.class.getName());\n \n-    private final ExecutorService executor;\n-    private volatile KafkaStreams streams;\n-    private volatile KafkaStreamsRuntimeConfig runtimeConfig;\n-    private volatile Instance<Topology> topology;\n-    private volatile Properties properties;\n     private volatile Properties adminClientConfig;\n \n-    private volatile Instance<KafkaClientSupplier> kafkaClientSupplier;\n-    private volatile Instance<StateListener> stateListener;\n-    private volatile Instance<StateRestoreListener> globalStateRestoreListener;\n-\n-    KafkaStreamsTopologyManager() {\n-        executor = null;\n-    }\n-\n-    @Inject\n-    public KafkaStreamsTopologyManager(Instance<Topology> topology, Instance<KafkaClientSupplier> kafkaClientSupplier,\n-            Instance<StateListener> stateListener, Instance<StateRestoreListener> globalStateRestoreListener) {\n-        // No producer for Topology -> nothing to do\n-        if (topology.isUnsatisfied()) {\n-            LOGGER.debug(\"No Topology producer; Kafka Streams will not be started\");\n-            this.executor = null;\n-            return;\n-        }\n-\n-        this.executor = Executors.newSingleThreadExecutor();\n-        this.topology = topology;\n-        this.kafkaClientSupplier = kafkaClientSupplier;\n-        this.stateListener = stateListener;\n-        this.globalStateRestoreListener = globalStateRestoreListener;\n-    }\n-\n-    /**\n-     * Returns all properties to be passed to Kafka Streams.\n-     */\n-    private static Properties getStreamsProperties(Properties properties, String bootstrapServersConfig,\n-            KafkaStreamsRuntimeConfig runtimeConfig) {\n-        Properties streamsProperties = new Properties();\n-\n-        // build-time options\n-        streamsProperties.putAll(properties);\n-\n-        // dynamic add -- back-compatibility\n-        streamsProperties.putAll(KafkaStreamsPropertiesUtil.quarkusKafkaStreamsProperties());\n-        streamsProperties.putAll(KafkaStreamsPropertiesUtil.appKafkaStreamsProperties());\n-\n-        // add runtime options\n-        streamsProperties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServersConfig);\n-        streamsProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, runtimeConfig.applicationId);\n-\n-        // app id\n-        if (runtimeConfig.applicationServer.isPresent()) {\n-            streamsProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, runtimeConfig.applicationServer.get());\n-        }\n-\n-        // schema registry\n-        if (runtimeConfig.schemaRegistryUrl.isPresent()) {\n-            streamsProperties.put(runtimeConfig.schemaRegistryKey, runtimeConfig.schemaRegistryUrl.get());\n-        }\n-\n-        // set the security protocol (in case we are doing PLAIN_TEXT)\n-        setProperty(runtimeConfig.securityProtocol, streamsProperties, CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);\n-\n-        // sasl\n-        SaslConfig sc = runtimeConfig.sasl;\n-        if (sc != null) {\n-            setProperty(sc.jaasConfig, streamsProperties, SaslConfigs.SASL_JAAS_CONFIG);\n-\n-            setProperty(sc.clientCallbackHandlerClass, streamsProperties, SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS);\n-\n-            setProperty(sc.loginCallbackHandlerClass, streamsProperties, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS);\n-            setProperty(sc.loginClass, streamsProperties, SaslConfigs.SASL_LOGIN_CLASS);\n-\n-            setProperty(sc.kerberosServiceName, streamsProperties, SaslConfigs.SASL_KERBEROS_SERVICE_NAME);\n-            setProperty(sc.kerberosKinitCmd, streamsProperties, SaslConfigs.SASL_KERBEROS_KINIT_CMD);\n-            setProperty(sc.kerberosTicketRenewWindowFactor, streamsProperties,\n-                    SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR);\n-            setProperty(sc.kerberosTicketRenewJitter, streamsProperties, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER);\n-            setProperty(sc.kerberosMinTimeBeforeRelogin, streamsProperties, SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN);\n-\n-            setProperty(sc.loginRefreshWindowFactor, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR);\n-            setProperty(sc.loginRefreshWindowJitter, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER);\n-\n-            setProperty(sc.loginRefreshMinPeriod, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS,\n-                    DurationToSecondsFunction.INSTANCE);\n-            setProperty(sc.loginRefreshBuffer, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS,\n-                    DurationToSecondsFunction.INSTANCE);\n-        }\n-\n-        // ssl\n-        SslConfig ssl = runtimeConfig.ssl;\n-        if (ssl != null) {\n-            setProperty(ssl.protocol, streamsProperties, SslConfigs.SSL_PROTOCOL_CONFIG);\n-            setProperty(ssl.provider, streamsProperties, SslConfigs.SSL_PROVIDER_CONFIG);\n-            setProperty(ssl.cipherSuites, streamsProperties, SslConfigs.SSL_CIPHER_SUITES_CONFIG);\n-            setProperty(ssl.enabledProtocols, streamsProperties, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG);\n-\n-            setStoreConfig(ssl.truststore, streamsProperties, \"ssl.truststore\");\n-            setStoreConfig(ssl.keystore, streamsProperties, \"ssl.keystore\");\n-            setStoreConfig(ssl.key, streamsProperties, \"ssl.key\");\n-\n-            setProperty(ssl.keymanagerAlgorithm, streamsProperties, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG);\n-            setProperty(ssl.trustmanagerAlgorithm, streamsProperties, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG);\n-            Optional<String> eia = Optional.of(ssl.endpointIdentificationAlgorithm.orElse(\"\"));\n-            setProperty(eia, streamsProperties, SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG);\n-            setProperty(ssl.secureRandomImplementation, streamsProperties, SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG);\n-        }\n-\n-        return streamsProperties;\n-    }\n-\n-    private static void setStoreConfig(StoreConfig sc, Properties properties, String key) {\n-        if (sc != null) {\n-            setProperty(sc.type, properties, key + \".type\");\n-            setProperty(sc.location, properties, key + \".location\");\n-            setProperty(sc.password, properties, key + \".password\");\n-        }\n-    }\n-\n-    private static <T> void setProperty(Optional<T> property, Properties properties, String key) {\n-        setProperty(property, properties, key, Objects::toString);\n-    }\n-\n-    private static <T> void setProperty(Optional<T> property, Properties properties, String key, Function<T, String> fn) {\n-        if (property.isPresent()) {\n-            properties.put(key, fn.apply(property.get()));\n-        }\n+    public KafkaStreamsTopologyManager(Properties adminClientConfig) {\n+        this.adminClientConfig = adminClientConfig;\n     }\n \n-    private static String asString(List<InetSocketAddress> addresses) {\n-        return addresses.stream()\n-                .map(InetSocketAddress::toString)\n-                .collect(Collectors.joining(\",\"));\n-    }\n-\n-    void onStop(@Observes ShutdownEvent ev) {\n-        if (streams != null) {\n-            LOGGER.debug(\"Stopping Kafka Streams pipeline\");\n-            streams.close();\n-        }\n-    }\n-\n-    @Produces\n-    @Singleton\n-    public KafkaStreams getStreams() {\n-        if (topology.isUnsatisfied()) {\n-            throw new IllegalStateException(\"No Topology producer has been defined\");\n-        }\n-\n-        if (streams == null) {\n-            String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n-\n-            Properties streamsProperties = getStreamsProperties(properties, bootstrapServersConfig, runtimeConfig);\n-\n-            if (kafkaClientSupplier.isUnsatisfied()) {\n-                streams = new KafkaStreams(topology.get(), streamsProperties);\n-            } else {\n-                streams = new KafkaStreams(topology.get(), streamsProperties, kafkaClientSupplier.get());\n-            }\n-\n-            if (!stateListener.isUnsatisfied()) {\n-                streams.setStateListener(stateListener.get());\n-            }\n-            if (!globalStateRestoreListener.isUnsatisfied()) {\n-                streams.setGlobalStateRestoreListener(globalStateRestoreListener.get());\n-            }\n-\n-            adminClientConfig = getAdminClientConfig(streamsProperties);\n-\n-            executor.execute(new Runnable() {\n-\n-                @Override\n-                public void run() {\n-                    try {\n-                        waitForTopicsToBeCreated(runtimeConfig.getTrimmedTopics());\n-                    } catch (InterruptedException e) {\n-                        Thread.currentThread().interrupt();\n-                        return;\n-                    }\n-                    LOGGER.debug(\"Starting Kafka Streams pipeline\");\n-                    streams.start();\n-                }\n-            });\n-        }\n-\n-        return streams;\n-    }\n-\n-    private void waitForTopicsToBeCreated(Collection<String> topicsToAwait)\n-            throws InterruptedException {\n-        try (AdminClient adminClient = AdminClient.create(adminClientConfig)) {\n-            Set<String> lastMissingTopics = null;\n-            while (true) {\n-                try {\n-                    ListTopicsResult topics = adminClient.listTopics();\n-                    Set<String> existingTopics = topics.names().get(10, TimeUnit.SECONDS);\n-\n-                    if (existingTopics.containsAll(topicsToAwait)) {\n-                        LOGGER.debug(\"All expected topics created: \" + topicsToAwait);\n-                        return;\n-                    } else {\n-                        Set<String> missingTopics = new HashSet<>(topicsToAwait);\n-                        missingTopics.removeAll(existingTopics);\n-\n-                        // Do not spam warnings - topics may take time to be created by an operator like Strimzi\n-                        if (missingTopics.equals(lastMissingTopics)) {\n-                            LOGGER.debug(\"Waiting for topic(s) to be created: \" + missingTopics);\n-                        } else {\n-                            LOGGER.warn(\"Waiting for topic(s) to be created: \" + missingTopics);\n-                            lastMissingTopics = missingTopics;\n-                        }\n-                    }\n-\n-                    Thread.sleep(1_000);\n-                } catch (ExecutionException | TimeoutException e) {\n-                    LOGGER.error(\"Failed to get topic names from broker\", e);\n-                }\n-            }\n-        }\n-    }\n-\n-    public Set<String> getMissingTopics(Collection<String> topicsToCheck)\n-            throws InterruptedException {\n+    public Set<String> getMissingTopics(Collection<String> topicsToCheck) throws InterruptedException {\n         HashSet<String> missing = new HashSet<>(topicsToCheck);\n \n         try (AdminClient adminClient = AdminClient.create(adminClientConfig)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI5MTgyOA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r456291828", "bodyText": "I checked on a small test, a Singleton bean seems to be not created if the bean is not injected anywhere.\nCan someone confirms this behavior, I searched in CDI/Arc I did not find anything.\nIn our application we are not injecting (yet) a KafkaStreams instance, because we don't need it, so this is not gonna work.\nCan you make sure it's working (or not) in that case : not injecting the bean somewhere ?", "author": "vietk", "createdAt": "2020-07-17T08:17:06Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java", "diffHunk": "@@ -189,56 +188,56 @@ private static String asString(List<InetSocketAddress> addresses) {\n                 .collect(Collectors.joining(\",\"));\n     }\n \n-    void onStart(@Observes StartupEvent ev) {\n-        if (executor == null) {\n-            return;\n+    void onStop(@Observes ShutdownEvent ev) {\n+        if (streams != null) {\n+            LOGGER.debug(\"Stopping Kafka Streams pipeline\");\n+            streams.close();\n         }\n+    }\n \n-        String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n-\n-        Properties streamsProperties = getStreamsProperties(properties, bootstrapServersConfig, runtimeConfig);\n-\n-        if (kafkaClientSupplier.isUnsatisfied()) {\n-            streams = new KafkaStreams(topology.get(), streamsProperties);\n-        } else {\n-            streams = new KafkaStreams(topology.get(), streamsProperties, kafkaClientSupplier.get());\n+    @Produces\n+    @Singleton", "originalCommit": "ef4ee7656955449f1273657892d4c8ca1963f7df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMxMzA2NQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r456313065", "bodyText": "I found the doc : https://quarkus.io/guides/cdi-reference#lazy_by_default\n By default, CDI beans are created lazily, when needed.\nA bean with a pseudo-scope (@Dependent and @Singleton ) is created when injected.", "author": "vietk", "createdAt": "2020-07-17T08:56:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI5MTgyOA=="}], "type": "inlineReview", "revised_code": {"commit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "chunk": "diff --git a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\nindex 7257272ba7..95f7a4101c 100644\n--- a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n+++ b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n\n@@ -1,281 +1,29 @@\n package io.quarkus.kafka.streams.runtime;\n \n-import java.net.InetSocketAddress;\n-import java.time.Duration;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashSet;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n \n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.event.Observes;\n-import javax.enterprise.inject.Instance;\n-import javax.enterprise.inject.Produces;\n-import javax.inject.Inject;\n-import javax.inject.Singleton;\n-\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.AdminClientConfig;\n import org.apache.kafka.clients.admin.ListTopicsResult;\n-import org.apache.kafka.common.config.SaslConfigs;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.streams.KafkaClientSupplier;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreams.StateListener;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n import org.jboss.logging.Logger;\n \n-import io.quarkus.runtime.ShutdownEvent;\n-\n-/**\n- * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n- * method returning a KS {@link Topology}, then this topology will be configured\n- * and started. Optionally, before starting the pipeline, this manager will wait\n- * for a given set of topics to be created, as KS itself will fail without all\n- * input topics being created upfront.\n- */\n-@ApplicationScoped\n public class KafkaStreamsTopologyManager {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaStreamsTopologyManager.class.getName());\n \n-    private final ExecutorService executor;\n-    private volatile KafkaStreams streams;\n-    private volatile KafkaStreamsRuntimeConfig runtimeConfig;\n-    private volatile Instance<Topology> topology;\n-    private volatile Properties properties;\n     private volatile Properties adminClientConfig;\n \n-    private volatile Instance<KafkaClientSupplier> kafkaClientSupplier;\n-    private volatile Instance<StateListener> stateListener;\n-    private volatile Instance<StateRestoreListener> globalStateRestoreListener;\n-\n-    KafkaStreamsTopologyManager() {\n-        executor = null;\n-    }\n-\n-    @Inject\n-    public KafkaStreamsTopologyManager(Instance<Topology> topology, Instance<KafkaClientSupplier> kafkaClientSupplier,\n-            Instance<StateListener> stateListener, Instance<StateRestoreListener> globalStateRestoreListener) {\n-        // No producer for Topology -> nothing to do\n-        if (topology.isUnsatisfied()) {\n-            LOGGER.debug(\"No Topology producer; Kafka Streams will not be started\");\n-            this.executor = null;\n-            return;\n-        }\n-\n-        this.executor = Executors.newSingleThreadExecutor();\n-        this.topology = topology;\n-        this.kafkaClientSupplier = kafkaClientSupplier;\n-        this.stateListener = stateListener;\n-        this.globalStateRestoreListener = globalStateRestoreListener;\n-    }\n-\n-    /**\n-     * Returns all properties to be passed to Kafka Streams.\n-     */\n-    private static Properties getStreamsProperties(Properties properties, String bootstrapServersConfig,\n-            KafkaStreamsRuntimeConfig runtimeConfig) {\n-        Properties streamsProperties = new Properties();\n-\n-        // build-time options\n-        streamsProperties.putAll(properties);\n-\n-        // dynamic add -- back-compatibility\n-        streamsProperties.putAll(KafkaStreamsPropertiesUtil.quarkusKafkaStreamsProperties());\n-        streamsProperties.putAll(KafkaStreamsPropertiesUtil.appKafkaStreamsProperties());\n-\n-        // add runtime options\n-        streamsProperties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServersConfig);\n-        streamsProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, runtimeConfig.applicationId);\n-\n-        // app id\n-        if (runtimeConfig.applicationServer.isPresent()) {\n-            streamsProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, runtimeConfig.applicationServer.get());\n-        }\n-\n-        // schema registry\n-        if (runtimeConfig.schemaRegistryUrl.isPresent()) {\n-            streamsProperties.put(runtimeConfig.schemaRegistryKey, runtimeConfig.schemaRegistryUrl.get());\n-        }\n-\n-        // set the security protocol (in case we are doing PLAIN_TEXT)\n-        setProperty(runtimeConfig.securityProtocol, streamsProperties, CommonClientConfigs.SECURITY_PROTOCOL_CONFIG);\n-\n-        // sasl\n-        SaslConfig sc = runtimeConfig.sasl;\n-        if (sc != null) {\n-            setProperty(sc.jaasConfig, streamsProperties, SaslConfigs.SASL_JAAS_CONFIG);\n-\n-            setProperty(sc.clientCallbackHandlerClass, streamsProperties, SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS);\n-\n-            setProperty(sc.loginCallbackHandlerClass, streamsProperties, SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS);\n-            setProperty(sc.loginClass, streamsProperties, SaslConfigs.SASL_LOGIN_CLASS);\n-\n-            setProperty(sc.kerberosServiceName, streamsProperties, SaslConfigs.SASL_KERBEROS_SERVICE_NAME);\n-            setProperty(sc.kerberosKinitCmd, streamsProperties, SaslConfigs.SASL_KERBEROS_KINIT_CMD);\n-            setProperty(sc.kerberosTicketRenewWindowFactor, streamsProperties,\n-                    SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR);\n-            setProperty(sc.kerberosTicketRenewJitter, streamsProperties, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER);\n-            setProperty(sc.kerberosMinTimeBeforeRelogin, streamsProperties, SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN);\n-\n-            setProperty(sc.loginRefreshWindowFactor, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR);\n-            setProperty(sc.loginRefreshWindowJitter, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER);\n-\n-            setProperty(sc.loginRefreshMinPeriod, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS,\n-                    DurationToSecondsFunction.INSTANCE);\n-            setProperty(sc.loginRefreshBuffer, streamsProperties, SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS,\n-                    DurationToSecondsFunction.INSTANCE);\n-        }\n-\n-        // ssl\n-        SslConfig ssl = runtimeConfig.ssl;\n-        if (ssl != null) {\n-            setProperty(ssl.protocol, streamsProperties, SslConfigs.SSL_PROTOCOL_CONFIG);\n-            setProperty(ssl.provider, streamsProperties, SslConfigs.SSL_PROVIDER_CONFIG);\n-            setProperty(ssl.cipherSuites, streamsProperties, SslConfigs.SSL_CIPHER_SUITES_CONFIG);\n-            setProperty(ssl.enabledProtocols, streamsProperties, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG);\n-\n-            setStoreConfig(ssl.truststore, streamsProperties, \"ssl.truststore\");\n-            setStoreConfig(ssl.keystore, streamsProperties, \"ssl.keystore\");\n-            setStoreConfig(ssl.key, streamsProperties, \"ssl.key\");\n-\n-            setProperty(ssl.keymanagerAlgorithm, streamsProperties, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG);\n-            setProperty(ssl.trustmanagerAlgorithm, streamsProperties, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG);\n-            Optional<String> eia = Optional.of(ssl.endpointIdentificationAlgorithm.orElse(\"\"));\n-            setProperty(eia, streamsProperties, SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG);\n-            setProperty(ssl.secureRandomImplementation, streamsProperties, SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG);\n-        }\n-\n-        return streamsProperties;\n-    }\n-\n-    private static void setStoreConfig(StoreConfig sc, Properties properties, String key) {\n-        if (sc != null) {\n-            setProperty(sc.type, properties, key + \".type\");\n-            setProperty(sc.location, properties, key + \".location\");\n-            setProperty(sc.password, properties, key + \".password\");\n-        }\n-    }\n-\n-    private static <T> void setProperty(Optional<T> property, Properties properties, String key) {\n-        setProperty(property, properties, key, Objects::toString);\n-    }\n-\n-    private static <T> void setProperty(Optional<T> property, Properties properties, String key, Function<T, String> fn) {\n-        if (property.isPresent()) {\n-            properties.put(key, fn.apply(property.get()));\n-        }\n+    public KafkaStreamsTopologyManager(Properties adminClientConfig) {\n+        this.adminClientConfig = adminClientConfig;\n     }\n \n-    private static String asString(List<InetSocketAddress> addresses) {\n-        return addresses.stream()\n-                .map(InetSocketAddress::toString)\n-                .collect(Collectors.joining(\",\"));\n-    }\n-\n-    void onStop(@Observes ShutdownEvent ev) {\n-        if (streams != null) {\n-            LOGGER.debug(\"Stopping Kafka Streams pipeline\");\n-            streams.close();\n-        }\n-    }\n-\n-    @Produces\n-    @Singleton\n-    public KafkaStreams getStreams() {\n-        if (topology.isUnsatisfied()) {\n-            throw new IllegalStateException(\"No Topology producer has been defined\");\n-        }\n-\n-        if (streams == null) {\n-            String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n-\n-            Properties streamsProperties = getStreamsProperties(properties, bootstrapServersConfig, runtimeConfig);\n-\n-            if (kafkaClientSupplier.isUnsatisfied()) {\n-                streams = new KafkaStreams(topology.get(), streamsProperties);\n-            } else {\n-                streams = new KafkaStreams(topology.get(), streamsProperties, kafkaClientSupplier.get());\n-            }\n-\n-            if (!stateListener.isUnsatisfied()) {\n-                streams.setStateListener(stateListener.get());\n-            }\n-            if (!globalStateRestoreListener.isUnsatisfied()) {\n-                streams.setGlobalStateRestoreListener(globalStateRestoreListener.get());\n-            }\n-\n-            adminClientConfig = getAdminClientConfig(streamsProperties);\n-\n-            executor.execute(new Runnable() {\n-\n-                @Override\n-                public void run() {\n-                    try {\n-                        waitForTopicsToBeCreated(runtimeConfig.getTrimmedTopics());\n-                    } catch (InterruptedException e) {\n-                        Thread.currentThread().interrupt();\n-                        return;\n-                    }\n-                    LOGGER.debug(\"Starting Kafka Streams pipeline\");\n-                    streams.start();\n-                }\n-            });\n-        }\n-\n-        return streams;\n-    }\n-\n-    private void waitForTopicsToBeCreated(Collection<String> topicsToAwait)\n-            throws InterruptedException {\n-        try (AdminClient adminClient = AdminClient.create(adminClientConfig)) {\n-            Set<String> lastMissingTopics = null;\n-            while (true) {\n-                try {\n-                    ListTopicsResult topics = adminClient.listTopics();\n-                    Set<String> existingTopics = topics.names().get(10, TimeUnit.SECONDS);\n-\n-                    if (existingTopics.containsAll(topicsToAwait)) {\n-                        LOGGER.debug(\"All expected topics created: \" + topicsToAwait);\n-                        return;\n-                    } else {\n-                        Set<String> missingTopics = new HashSet<>(topicsToAwait);\n-                        missingTopics.removeAll(existingTopics);\n-\n-                        // Do not spam warnings - topics may take time to be created by an operator like Strimzi\n-                        if (missingTopics.equals(lastMissingTopics)) {\n-                            LOGGER.debug(\"Waiting for topic(s) to be created: \" + missingTopics);\n-                        } else {\n-                            LOGGER.warn(\"Waiting for topic(s) to be created: \" + missingTopics);\n-                            lastMissingTopics = missingTopics;\n-                        }\n-                    }\n-\n-                    Thread.sleep(1_000);\n-                } catch (ExecutionException | TimeoutException e) {\n-                    LOGGER.error(\"Failed to get topic names from broker\", e);\n-                }\n-            }\n-        }\n-    }\n-\n-    public Set<String> getMissingTopics(Collection<String> topicsToCheck)\n-            throws InterruptedException {\n+    public Set<String> getMissingTopics(Collection<String> topicsToCheck) throws InterruptedException {\n         HashSet<String> missing = new HashSet<>(topicsToCheck);\n \n         try (AdminClient adminClient = AdminClient.create(adminClientConfig)) {\n"}}, {"oid": "a3584869ed848b9d8c2f7052ec9a5943f13866cc", "url": "https://github.com/quarkusio/quarkus/commit/a3584869ed848b9d8c2f7052ec9a5943f13866cc", "message": "Instantiate KafkaStreams in producer method", "committedDate": "2020-07-18T13:35:39Z", "type": "commit"}, {"oid": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "url": "https://github.com/quarkusio/quarkus/commit/8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "message": "Reorganize the KafkaStreams beans initialization\n\nThis will avoid race conditions and make sure everything is properly\ninitialized before consuming the beans.", "committedDate": "2020-07-18T13:35:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njc5NDQ0NA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r456794444", "bodyText": "This annotation shouldn't be necessary", "author": "geoand", "createdAt": "2020-07-18T14:19:23Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsProducer.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package io.quarkus.kafka.streams.runtime;\n+\n+import java.net.InetSocketAddress;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.PreDestroy;\n+import javax.enterprise.inject.Instance;\n+import javax.enterprise.inject.Produces;\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.ListTopicsResult;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreams.StateListener;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.jboss.logging.Logger;\n+\n+import io.quarkus.arc.Unremovable;\n+import io.quarkus.runtime.Startup;\n+\n+/**\n+ * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n+ * method returning a KS {@link Topology}, then this topology will be configured\n+ * and started. Optionally, before starting the pipeline, this manager will wait\n+ * for a given set of topics to be created, as KS itself will fail without all\n+ * input topics being created upfront.\n+ */\n+@Singleton\n+public class KafkaStreamsProducer {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaStreamsProducer.class.getName());\n+\n+    private final ExecutorService executorService;\n+    private final KafkaStreams kafkaStreams;\n+    private final KafkaStreamsTopologyManager kafkaStreamsTopologyManager;\n+\n+    @Inject", "originalCommit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njc5NDgzNg==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r456794836", "bodyText": "Yeah, I know it's not. It's purely cosmetic to make me think the parameters are injected :).", "author": "gsmet", "createdAt": "2020-07-18T14:23:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njc5NDQ0NA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA2NjExNg==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457066116", "bodyText": "This looks like it can be final now, correct?", "author": "geoand", "createdAt": "2020-07-20T05:32:34Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java", "diffHunk": "@@ -1,282 +1,29 @@\n package io.quarkus.kafka.streams.runtime;\n \n-import java.net.InetSocketAddress;\n-import java.time.Duration;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashSet;\n-import java.util.List;\n-import java.util.Objects;\n-import java.util.Optional;\n import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n \n-import javax.enterprise.context.ApplicationScoped;\n-import javax.enterprise.event.Observes;\n-import javax.enterprise.inject.Instance;\n-import javax.enterprise.inject.Produces;\n-import javax.inject.Inject;\n-import javax.inject.Singleton;\n-\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.AdminClientConfig;\n import org.apache.kafka.clients.admin.ListTopicsResult;\n-import org.apache.kafka.common.config.SaslConfigs;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.streams.KafkaClientSupplier;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreams.StateListener;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n import org.jboss.logging.Logger;\n \n-import io.quarkus.runtime.ShutdownEvent;\n-import io.quarkus.runtime.StartupEvent;\n-\n-/**\n- * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n- * method returning a KS {@link Topology}, then this topology will be configured\n- * and started. Optionally, before starting the pipeline, this manager will wait\n- * for a given set of topics to be created, as KS itself will fail without all\n- * input topics being created upfront.\n- */\n-@ApplicationScoped\n public class KafkaStreamsTopologyManager {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaStreamsTopologyManager.class.getName());\n \n-    private final ExecutorService executor;\n-    private volatile KafkaStreams streams;\n-    private volatile KafkaStreamsRuntimeConfig runtimeConfig;\n-    private volatile Instance<Topology> topology;\n-    private volatile Properties properties;\n     private volatile Properties adminClientConfig;", "originalCommit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODEzMA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457138130", "bodyText": "Ah yeah, forgot to remove it.", "author": "gsmet", "createdAt": "2020-07-20T07:41:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA2NjExNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE0MDA0NA==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457140044", "bodyText": "Fixed. Removed volatile and added final.", "author": "gsmet", "createdAt": "2020-07-20T07:43:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA2NjExNg=="}], "type": "inlineReview", "revised_code": {"commit": "849fe83e813b0763c3f39c3565b8715aeaf551ec", "chunk": "diff --git a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\nindex 95f7a4101c..ac696df956 100644\n--- a/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n+++ b/extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsTopologyManager.java\n\n@@ -17,7 +17,7 @@ public class KafkaStreamsTopologyManager {\n \n     private static final Logger LOGGER = Logger.getLogger(KafkaStreamsTopologyManager.class.getName());\n \n-    private volatile Properties adminClientConfig;\n+    private final Properties adminClientConfig;\n \n     public KafkaStreamsTopologyManager(Properties adminClientConfig) {\n         this.adminClientConfig = adminClientConfig;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjIxOQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457072219", "bodyText": "Is it certain that this needs to be @Unremovable?", "author": "geoand", "createdAt": "2020-07-20T05:45:39Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsProducer.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package io.quarkus.kafka.streams.runtime;\n+\n+import java.net.InetSocketAddress;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.PreDestroy;\n+import javax.enterprise.inject.Instance;\n+import javax.enterprise.inject.Produces;\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.ListTopicsResult;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreams.StateListener;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.jboss.logging.Logger;\n+\n+import io.quarkus.arc.Unremovable;\n+import io.quarkus.runtime.Startup;\n+\n+/**\n+ * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n+ * method returning a KS {@link Topology}, then this topology will be configured\n+ * and started. Optionally, before starting the pipeline, this manager will wait\n+ * for a given set of topics to be created, as KS itself will fail without all\n+ * input topics being created upfront.\n+ */\n+@Singleton\n+public class KafkaStreamsProducer {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaStreamsProducer.class.getName());\n+\n+    private final ExecutorService executorService;\n+    private final KafkaStreams kafkaStreams;\n+    private final KafkaStreamsTopologyManager kafkaStreamsTopologyManager;\n+\n+    @Inject\n+    public KafkaStreamsProducer(KafkaStreamsSupport kafkaStreamsSupport, KafkaStreamsRuntimeConfig runtimeConfig,\n+            Instance<Topology> topology, Instance<KafkaClientSupplier> kafkaClientSupplier,\n+            Instance<StateListener> stateListener, Instance<StateRestoreListener> globalStateRestoreListener) {\n+        // No producer for Topology -> nothing to do\n+        if (topology.isUnsatisfied()) {\n+            LOGGER.debug(\"No Topology producer; Kafka Streams will not be started\");\n+            this.executorService = null;\n+            this.kafkaStreams = null;\n+            this.kafkaStreamsTopologyManager = null;\n+            return;\n+        }\n+\n+        Properties buildTimeProperties = kafkaStreamsSupport.getProperties();\n+\n+        String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n+        Properties kafkaStreamsProperties = getStreamsProperties(buildTimeProperties, bootstrapServersConfig, runtimeConfig);\n+        Properties adminClientConfig = getAdminClientConfig(kafkaStreamsProperties);\n+\n+        this.executorService = Executors.newSingleThreadExecutor();\n+\n+        this.kafkaStreams = initializeKafkaStreams(kafkaStreamsProperties, runtimeConfig, adminClientConfig, topology.get(),\n+                kafkaClientSupplier, stateListener, globalStateRestoreListener, executorService);\n+        this.kafkaStreamsTopologyManager = new KafkaStreamsTopologyManager(adminClientConfig);\n+    }\n+\n+    @Produces\n+    @Singleton\n+    @Unremovable", "originalCommit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjI2Mg==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457072262", "bodyText": "Same here", "author": "geoand", "createdAt": "2020-07-20T05:45:47Z", "path": "extensions/kafka-streams/runtime/src/main/java/io/quarkus/kafka/streams/runtime/KafkaStreamsProducer.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package io.quarkus.kafka.streams.runtime;\n+\n+import java.net.InetSocketAddress;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import javax.annotation.PreDestroy;\n+import javax.enterprise.inject.Instance;\n+import javax.enterprise.inject.Produces;\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.ListTopicsResult;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreams.StateListener;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.jboss.logging.Logger;\n+\n+import io.quarkus.arc.Unremovable;\n+import io.quarkus.runtime.Startup;\n+\n+/**\n+ * Manages the lifecycle of a Kafka Streams pipeline. If there's a producer\n+ * method returning a KS {@link Topology}, then this topology will be configured\n+ * and started. Optionally, before starting the pipeline, this manager will wait\n+ * for a given set of topics to be created, as KS itself will fail without all\n+ * input topics being created upfront.\n+ */\n+@Singleton\n+public class KafkaStreamsProducer {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaStreamsProducer.class.getName());\n+\n+    private final ExecutorService executorService;\n+    private final KafkaStreams kafkaStreams;\n+    private final KafkaStreamsTopologyManager kafkaStreamsTopologyManager;\n+\n+    @Inject\n+    public KafkaStreamsProducer(KafkaStreamsSupport kafkaStreamsSupport, KafkaStreamsRuntimeConfig runtimeConfig,\n+            Instance<Topology> topology, Instance<KafkaClientSupplier> kafkaClientSupplier,\n+            Instance<StateListener> stateListener, Instance<StateRestoreListener> globalStateRestoreListener) {\n+        // No producer for Topology -> nothing to do\n+        if (topology.isUnsatisfied()) {\n+            LOGGER.debug(\"No Topology producer; Kafka Streams will not be started\");\n+            this.executorService = null;\n+            this.kafkaStreams = null;\n+            this.kafkaStreamsTopologyManager = null;\n+            return;\n+        }\n+\n+        Properties buildTimeProperties = kafkaStreamsSupport.getProperties();\n+\n+        String bootstrapServersConfig = asString(runtimeConfig.bootstrapServers);\n+        Properties kafkaStreamsProperties = getStreamsProperties(buildTimeProperties, bootstrapServersConfig, runtimeConfig);\n+        Properties adminClientConfig = getAdminClientConfig(kafkaStreamsProperties);\n+\n+        this.executorService = Executors.newSingleThreadExecutor();\n+\n+        this.kafkaStreams = initializeKafkaStreams(kafkaStreamsProperties, runtimeConfig, adminClientConfig, topology.get(),\n+                kafkaClientSupplier, stateListener, globalStateRestoreListener, executorService);\n+        this.kafkaStreamsTopologyManager = new KafkaStreamsTopologyManager(adminClientConfig);\n+    }\n+\n+    @Produces\n+    @Singleton\n+    @Unremovable\n+    @Startup\n+    public KafkaStreams getKafkaStreams() {\n+        return kafkaStreams;\n+    }\n+\n+    @Produces\n+    @Singleton\n+    @Unremovable", "originalCommit": "8e2ebbb0f08f54d23c4d18d815dab56ee312e6bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODY0MQ==", "url": "https://github.com/quarkusio/quarkus/pull/10726#discussion_r457138641", "bodyText": "It was before so I mimic the existing behavior.", "author": "gsmet", "createdAt": "2020-07-20T07:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzA3MjI2Mg=="}], "type": "inlineReview", "revised_code": null}, {"oid": "849fe83e813b0763c3f39c3565b8715aeaf551ec", "url": "https://github.com/quarkusio/quarkus/commit/849fe83e813b0763c3f39c3565b8715aeaf551ec", "message": "Reorganize the KafkaStreams beans initialization\n\nThis will avoid race conditions and make sure everything is properly\ninitialized before consuming the beans.", "committedDate": "2020-07-20T07:43:02Z", "type": "commit"}, {"oid": "849fe83e813b0763c3f39c3565b8715aeaf551ec", "url": "https://github.com/quarkusio/quarkus/commit/849fe83e813b0763c3f39c3565b8715aeaf551ec", "message": "Reorganize the KafkaStreams beans initialization\n\nThis will avoid race conditions and make sure everything is properly\ninitialized before consuming the beans.", "committedDate": "2020-07-20T07:43:02Z", "type": "forcePushed"}]}