{"pr_number": 713, "pr_title": "[IOTDB-418] New series reader", "pr_createdAt": "2020-01-08T03:26:29Z", "pr_url": "https://github.com/apache/iotdb/pull/713", "timeline": [{"oid": "89181044d4ccfaba2c0aec9f3435c014197c612f", "url": "https://github.com/apache/iotdb/commit/89181044d4ccfaba2c0aec9f3435c014197c612f", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-17T01:51:21Z", "type": "commit"}, {"oid": "c92895b3fc58bafdfda98ea0e197551a441b8c2a", "url": "https://github.com/apache/iotdb/commit/c92895b3fc58bafdfda98ea0e197551a441b8c2a", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-17T02:17:30Z", "type": "commit"}, {"oid": "ec9b912bcb11774933175ffb717cb5a441bc172a", "url": "https://github.com/apache/iotdb/commit/ec9b912bcb11774933175ffb717cb5a441bc172a", "message": "fix review", "committedDate": "2020-02-17T02:51:46Z", "type": "commit"}, {"oid": "171949b2e25e53826907ee42596ecfabeb47af5c", "url": "https://github.com/apache/iotdb/commit/171949b2e25e53826907ee42596ecfabeb47af5c", "message": "fix sonar bug", "committedDate": "2020-02-17T02:57:59Z", "type": "commit"}, {"oid": "2bb1a8bcf2a3b5d383f433817a2b39b863a705aa", "url": "https://github.com/apache/iotdb/commit/2bb1a8bcf2a3b5d383f433817a2b39b863a705aa", "message": "Add more mergeAggrOnOneSeriesTest", "committedDate": "2020-02-17T03:01:54Z", "type": "commit"}, {"oid": "964180ee31c9b843e2c8e69a099a97b2a845bade", "url": "https://github.com/apache/iotdb/commit/964180ee31c9b843e2c8e69a099a97b2a845bade", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-17T03:02:10Z", "type": "commit"}, {"oid": "014d27c190bc790030ad65c101ac6fba7392e56f", "url": "https://github.com/apache/iotdb/commit/014d27c190bc790030ad65c101ac6fba7392e56f", "message": "Remove useless UnSupportedDataTypeException string", "committedDate": "2020-02-17T03:04:58Z", "type": "commit"}, {"oid": "a6f752fbb6a3bb4bc142690ffc473d2223bc2321", "url": "https://github.com/apache/iotdb/commit/a6f752fbb6a3bb4bc142690ffc473d2223bc2321", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-17T03:52:17Z", "type": "commit"}, {"oid": "855fef4d1c064025bf6a0ba71b5fa2866c944e83", "url": "https://github.com/apache/iotdb/commit/855fef4d1c064025bf6a0ba71b5fa2866c944e83", "message": "add group by device", "committedDate": "2020-02-17T04:13:18Z", "type": "commit"}, {"oid": "dd3f44ab710207bc10f484033e4c32e6b7fe75c4", "url": "https://github.com/apache/iotdb/commit/dd3f44ab710207bc10f484033e4c32e6b7fe75c4", "message": "Update avg method and test values", "committedDate": "2020-02-17T04:36:13Z", "type": "commit"}, {"oid": "67e4f64eb6973e2bcc749f9cca3f3e1cc78c7d49", "url": "https://github.com/apache/iotdb/commit/67e4f64eb6973e2bcc749f9cca3f3e1cc78c7d49", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-17T04:36:34Z", "type": "commit"}, {"oid": "9a54e6e0160048b596fbdef3a85fcb46139850fe", "url": "https://github.com/apache/iotdb/commit/9a54e6e0160048b596fbdef3a85fcb46139850fe", "message": "fix align by device doc", "committedDate": "2020-02-17T05:45:41Z", "type": "commit"}, {"oid": "153d4335c19ef6cb54e2e3eb727cdf58e551e655", "url": "https://github.com/apache/iotdb/commit/153d4335c19ef6cb54e2e3eb727cdf58e551e655", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-17T05:46:05Z", "type": "commit"}, {"oid": "9eb97668bc52a77672acbd9e7782f473f2ef4ca3", "url": "https://github.com/apache/iotdb/commit/9eb97668bc52a77672acbd9e7782f473f2ef4ca3", "message": "rename raw query data set with value filter", "committedDate": "2020-02-17T06:14:35Z", "type": "commit"}, {"oid": "c5df90c09d9f34e10dcc8eac6c00e81a33238a0d", "url": "https://github.com/apache/iotdb/commit/c5df90c09d9f34e10dcc8eac6c00e81a33238a0d", "message": "add some comments", "committedDate": "2020-02-17T06:21:23Z", "type": "commit"}, {"oid": "d46a4c4ae936e3482da99dc6803ed8d5dfe6df83", "url": "https://github.com/apache/iotdb/commit/d46a4c4ae936e3482da99dc6803ed8d5dfe6df83", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-17T06:21:46Z", "type": "commit"}, {"oid": "719a93812dd354bb81111a76c247b501e7263a8a", "url": "https://github.com/apache/iotdb/commit/719a93812dd354bb81111a76c247b501e7263a8a", "message": "Cached chunk (#802)\n\n* add chunk cache in server", "committedDate": "2020-02-17T07:20:11Z", "type": "commit"}, {"oid": "ae09b5bf5400360004283fe4a5a0be30b749c911", "url": "https://github.com/apache/iotdb/commit/ae09b5bf5400360004283fe4a5a0be30b749c911", "message": "add ttl in seriesreader with value filter", "committedDate": "2020-02-17T07:29:21Z", "type": "commit"}, {"oid": "2b2f45825a2b31c338beaf149325b9eaecfeda0f", "url": "https://github.com/apache/iotdb/commit/2b2f45825a2b31c338beaf149325b9eaecfeda0f", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-17T07:29:50Z", "type": "commit"}, {"oid": "91a37f8f33d5871dd5bdb4ab1ba80cb23fe61d7e", "url": "https://github.com/apache/iotdb/commit/91a37f8f33d5871dd5bdb4ab1ba80cb23fe61d7e", "message": "fix a bug in tsfile test", "committedDate": "2020-02-17T10:21:32Z", "type": "commit"}, {"oid": "1509a8bd603e95333f777342a6901f277bcef503", "url": "https://github.com/apache/iotdb/commit/1509a8bd603e95333f777342a6901f277bcef503", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-17T10:22:34Z", "type": "commit"}, {"oid": "a0577808ebdc86e8abb7cd521886af5f98d3e66a", "url": "https://github.com/apache/iotdb/commit/a0577808ebdc86e8abb7cd521886af5f98d3e66a", "message": "do not print \"cant get cpu ratio\" in Tests", "committedDate": "2020-02-17T12:45:38Z", "type": "commit"}, {"oid": "a421890ad7b9ad814e48f48950c91bb942628bb9", "url": "https://github.com/apache/iotdb/commit/a421890ad7b9ad814e48f48950c91bb942628bb9", "message": "fix bug in ActiveTimeSeriesCounter Test", "committedDate": "2020-02-17T12:47:05Z", "type": "commit"}, {"oid": "42fd2b72f61bfa32a11795b13009e45b3f5128a7", "url": "https://github.com/apache/iotdb/commit/42fd2b72f61bfa32a11795b13009e45b3f5128a7", "message": "wait at most 30s for finishing merge tasks before stop daemon.", "committedDate": "2020-02-17T12:47:50Z", "type": "commit"}, {"oid": "4c0390fe91cd07c94851d70cb89ab368bfceb080", "url": "https://github.com/apache/iotdb/commit/4c0390fe91cd07c94851d70cb89ab368bfceb080", "message": "Merge branch 'new_series_reader' of github.com:apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-17T12:48:20Z", "type": "commit"}, {"oid": "70ba6c8de5c8654e57137cb9465d58847d7175f6", "url": "https://github.com/apache/iotdb/commit/70ba6c8de5c8654e57137cb9465d58847d7175f6", "message": "try to connect 6667 to check whether the socket is closed", "committedDate": "2020-02-17T15:41:55Z", "type": "commit"}, {"oid": "b453bbae93e0885e21d835d1230634675a958553", "url": "https://github.com/apache/iotdb/commit/b453bbae93e0885e21d835d1230634675a958553", "message": "try to connect 6667 to check whether the socket is closed", "committedDate": "2020-02-17T15:42:46Z", "type": "commit"}, {"oid": "9e3a00453dd26a217871e75a61e63758a455e731", "url": "https://github.com/apache/iotdb/commit/9e3a00453dd26a217871e75a61e63758a455e731", "message": "check whether jmx port is closed", "committedDate": "2020-02-17T16:44:19Z", "type": "commit"}, {"oid": "e33be28526c8c763b707025ae22853b7a6bf150a", "url": "https://github.com/apache/iotdb/commit/e33be28526c8c763b707025ae22853b7a6bf150a", "message": "try 8181 port after environmentutil.cleanup", "committedDate": "2020-02-18T00:57:29Z", "type": "commit"}, {"oid": "6658a0a2fc93bd1e75ef0d2ce683b7eb0999a311", "url": "https://github.com/apache/iotdb/commit/6658a0a2fc93bd1e75ef0d2ce683b7eb0999a311", "message": "[IOTDB-496]An extra line in CLI when the result is empty (#819)\n\n* [IOTDB-496]An extra line in align by device query", "committedDate": "2020-02-18T01:11:09Z", "type": "commit"}, {"oid": "3c2b803da524a2f7c9e6e3ecf2565f79a1598ecc", "url": "https://github.com/apache/iotdb/commit/3c2b803da524a2f7c9e6e3ecf2565f79a1598ecc", "message": "print temp logs for checking why 8181 can not be closed", "committedDate": "2020-02-18T01:42:31Z", "type": "commit"}, {"oid": "c148346e81c94c37f9dae8f89241eaee1452f954", "url": "https://github.com/apache/iotdb/commit/c148346e81c94c37f9dae8f89241eaee1452f954", "message": "print temp logs for checking why 8181 can not be closed", "committedDate": "2020-02-18T01:55:18Z", "type": "commit"}, {"oid": "2e8fe371f12f605f07ce75153782a13b21f1bbbb", "url": "https://github.com/apache/iotdb/commit/2e8fe371f12f605f07ce75153782a13b21f1bbbb", "message": "remove unused daemon in IT", "committedDate": "2020-02-18T02:42:44Z", "type": "commit"}, {"oid": "46643be08679a1081091394ad8ecb1e6a67d3d28", "url": "https://github.com/apache/iotdb/commit/46643be08679a1081091394ad8ecb1e6a67d3d28", "message": "fix two daemon conflict in IoTDBRecoverIT", "committedDate": "2020-02-18T04:29:15Z", "type": "commit"}, {"oid": "4c81b0c930cedbdd8b11730b245983072fb62ba2", "url": "https://github.com/apache/iotdb/commit/4c81b0c930cedbdd8b11730b245983072fb62ba2", "message": "fix activeTimeseries null pointer", "committedDate": "2020-02-18T05:56:52Z", "type": "commit"}, {"oid": "5cde236271f0e8320db1752ddb994658c14801d5", "url": "https://github.com/apache/iotdb/commit/5cde236271f0e8320db1752ddb994658c14801d5", "message": "print which files are left after cleaning a folder", "committedDate": "2020-02-18T05:57:12Z", "type": "commit"}, {"oid": "2a434dd01021f780b3316880770835b4ba030ee0", "url": "https://github.com/apache/iotdb/commit/2a434dd01021f780b3316880770835b4ba030ee0", "message": "merge with master", "committedDate": "2020-02-18T06:02:17Z", "type": "commit"}, {"oid": "2571fbfa03a97b492b3d852d91938cbb1cd901a2", "url": "https://github.com/apache/iotdb/commit/2571fbfa03a97b492b3d852d91938cbb1cd901a2", "message": "merge with master", "committedDate": "2020-02-18T06:02:27Z", "type": "commit"}, {"oid": "d0b2b639d88fd355a11c51781a8e67456d4f4182", "url": "https://github.com/apache/iotdb/commit/d0b2b639d88fd355a11c51781a8e67456d4f4182", "message": "try to fix 8181 port not closed successful", "committedDate": "2020-02-18T08:49:46Z", "type": "commit"}, {"oid": "7a161eded61193c0871ade7385f4ec15aa5e62b9", "url": "https://github.com/apache/iotdb/commit/7a161eded61193c0871ade7385f4ec15aa5e62b9", "message": "[IOTDB-482] Vectorized TimeGenerator (#818)\n\n* rename and abstract TimeGenerator, combine LeafNode\r\n* remove unused cache in AbstractFileSeriesReader", "committedDate": "2020-02-18T09:46:44Z", "type": "commit"}, {"oid": "32b6a79a0b2808f702516b239a76556bdde66d6d", "url": "https://github.com/apache/iotdb/commit/32b6a79a0b2808f702516b239a76556bdde66d6d", "message": "debug why files can not be deleted", "committedDate": "2020-02-18T11:11:33Z", "type": "commit"}, {"oid": "05c45a36b25ab9cc9c50a92cba69ee7eb565e302", "url": "https://github.com/apache/iotdb/commit/05c45a36b25ab9cc9c50a92cba69ee7eb565e302", "message": "debug why win can not delete files", "committedDate": "2020-02-18T13:23:53Z", "type": "commit"}, {"oid": "9a69d9731bed76383b45ab002f54153fcc060b6f", "url": "https://github.com/apache/iotdb/commit/9a69d9731bed76383b45ab002f54153fcc060b6f", "message": "fix a bug in TsFileIOWriter that the file field is not initialized", "committedDate": "2020-02-18T13:34:47Z", "type": "commit"}, {"oid": "112e264aef4164a4116016be97ecf50939bdce95", "url": "https://github.com/apache/iotdb/commit/112e264aef4164a4116016be97ecf50939bdce95", "message": "add a log to split test functions in a Test", "committedDate": "2020-02-18T15:02:07Z", "type": "commit"}, {"oid": "207daa47821b54a7950c77ec9eab3f4b7aab1932", "url": "https://github.com/apache/iotdb/commit/207daa47821b54a7950c77ec9eab3f4b7aab1932", "message": "for debug", "committedDate": "2020-02-18T15:03:08Z", "type": "commit"}, {"oid": "0daa79145a1e4c3c9cd511730b9569a946e898f1", "url": "https://github.com/apache/iotdb/commit/0daa79145a1e4c3c9cd511730b9569a946e898f1", "message": "for debug reader and writer", "committedDate": "2020-02-18T16:00:52Z", "type": "commit"}, {"oid": "aaf121d354b64d1a15b0811b8d7e56f0527067b4", "url": "https://github.com/apache/iotdb/commit/aaf121d354b64d1a15b0811b8d7e56f0527067b4", "message": "for debug", "committedDate": "2020-02-19T07:52:18Z", "type": "commit"}, {"oid": "aa829e46b49eaea77899a3548cedd081a1aa9e69", "url": "https://github.com/apache/iotdb/commit/aa829e46b49eaea77899a3548cedd081a1aa9e69", "message": "for debug", "committedDate": "2020-02-19T09:24:43Z", "type": "commit"}, {"oid": "506dd87556645900e89be3ec9299e8b4a75e8283", "url": "https://github.com/apache/iotdb/commit/506dd87556645900e89be3ec9299e8b4a75e8283", "message": "fix a reader not closed before delete data in UT", "committedDate": "2020-02-19T16:44:50Z", "type": "commit"}, {"oid": "f9da016369a6c94d19c7be142bea243618c4c803", "url": "https://github.com/apache/iotdb/commit/f9da016369a6c94d19c7be142bea243618c4c803", "message": "fix a reader not closed before delete data in UT", "committedDate": "2020-02-19T23:21:12Z", "type": "commit"}, {"oid": "6c8380d5f8e37f412bcd426218880dafad678895", "url": "https://github.com/apache/iotdb/commit/6c8380d5f8e37f412bcd426218880dafad678895", "message": "add FileMonitor logger to monitot file open and close event", "committedDate": "2020-02-20T00:11:49Z", "type": "commit"}, {"oid": "2410ff2d34ab5ccb269e5861c7636ff9ce02251f", "url": "https://github.com/apache/iotdb/commit/2410ff2d34ab5ccb269e5861c7636ff9ce02251f", "message": "merge with ut_close_socket", "committedDate": "2020-02-20T00:59:10Z", "type": "commit"}, {"oid": "4dc7261aff90582532430ccf03bd3ff2143aa127", "url": "https://github.com/apache/iotdb/commit/4dc7261aff90582532430ccf03bd3ff2143aa127", "message": "fix bugs that file is not closed in", "committedDate": "2020-02-20T04:29:09Z", "type": "commit"}, {"oid": "396771c14a471c0dc3ec72bc311c8c4573742cad", "url": "https://github.com/apache/iotdb/commit/396771c14a471c0dc3ec72bc311c8c4573742cad", "message": "move an error log to info log.(resourceLogger)", "committedDate": "2020-02-20T04:48:02Z", "type": "commit"}, {"oid": "90c513cc6b1b263d0fb9491d46a5978299f938a0", "url": "https://github.com/apache/iotdb/commit/90c513cc6b1b263d0fb9491d46a5978299f938a0", "message": "merge with ut_close_socket", "committedDate": "2020-02-20T04:49:10Z", "type": "commit"}, {"oid": "b1dbcb3e26abcab53f6ae303d57ffa12ed38255f", "url": "https://github.com/apache/iotdb/commit/b1dbcb3e26abcab53f6ae303d57ffa12ed38255f", "message": "enable mergeLog for checking why merging is hanged", "committedDate": "2020-02-20T04:59:29Z", "type": "commit"}, {"oid": "ed7b5483abc7bd9cd272dffb15082ab5fa5824ea", "url": "https://github.com/apache/iotdb/commit/ed7b5483abc7bd9cd272dffb15082ab5fa5824ea", "message": "try to set stoptime as 0 to accelerate MetrcsService close", "committedDate": "2020-02-20T07:11:36Z", "type": "commit"}, {"oid": "487850dcf0633d002ace8892bc5eeb76c60c24d0", "url": "https://github.com/apache/iotdb/commit/487850dcf0633d002ace8892bc5eeb76c60c24d0", "message": "try to solve the bug 'if the flushing thread is too fast, the tmpMemTable.wait() may never wakeup' in TsFileProcessor syncFlush()", "committedDate": "2020-02-20T08:09:51Z", "type": "commit"}, {"oid": "78bf35180b3f4ad94b6387cf4881db34e731e107", "url": "https://github.com/apache/iotdb/commit/78bf35180b3f4ad94b6387cf4881db34e731e107", "message": "try to solve the bug 'if the flushing thread is too fast, the tmpMemTable.wait() may never wakeup' in TsFileProcessor syncFlush() 2", "committedDate": "2020-02-20T11:04:42Z", "type": "commit"}, {"oid": "2f2e1e07909b3d6266a49a25fe6f2fcc03efe512", "url": "https://github.com/apache/iotdb/commit/2f2e1e07909b3d6266a49a25fe6f2fcc03efe512", "message": "debug why 8181 is not closed", "committedDate": "2020-02-20T11:05:33Z", "type": "commit"}, {"oid": "a87d9e422d52f9f6cc63871ce09d414ad723c74a", "url": "https://github.com/apache/iotdb/commit/a87d9e422d52f9f6cc63871ce09d414ad723c74a", "message": "[IOTDB-510][IOTDB-501]Fix nullPointException bug in TsProcessor (#831)\n\n* add stop timeout for 8181, disable 8181 port when test\r\n* fix bug IOTDB-510, shouldClose is set to true but a new memtable is put to the flushing queue", "committedDate": "2020-02-23T03:34:36Z", "type": "commit"}, {"oid": "9570e2a1a850263428870c6215b2d41836169c33", "url": "https://github.com/apache/iotdb/commit/9570e2a1a850263428870c6215b2d41836169c33", "message": "resolve conflict after merging master", "committedDate": "2020-02-23T04:03:53Z", "type": "commit"}, {"oid": "4aa9ff37f00d450d7d7a15601856e09fede78b1b", "url": "https://github.com/apache/iotdb/commit/4aa9ff37f00d450d7d7a15601856e09fede78b1b", "message": "fix sonar bug and smell", "committedDate": "2020-02-23T04:24:34Z", "type": "commit"}, {"oid": "77a01a5c688a1729d139e9632db7a16fc337dbd7", "url": "https://github.com/apache/iotdb/commit/77a01a5c688a1729d139e9632db7a16fc337dbd7", "message": "add license", "committedDate": "2020-02-23T04:25:59Z", "type": "commit"}, {"oid": "f96fafd2830012b67717de1b5ae92829acaecb77", "url": "https://github.com/apache/iotdb/commit/f96fafd2830012b67717de1b5ae92829acaecb77", "message": "add some test", "committedDate": "2020-02-23T05:43:33Z", "type": "commit"}, {"oid": "f0e59f10215c45cd44629d4cb177a6cde6f8e468", "url": "https://github.com/apache/iotdb/commit/f0e59f10215c45cd44629d4cb177a6cde6f8e468", "message": "add header", "committedDate": "2020-02-23T06:07:09Z", "type": "commit"}, {"oid": "3b06b1ff789eb660964ffe39f39015193b45453a", "url": "https://github.com/apache/iotdb/commit/3b06b1ff789eb660964ffe39f39015193b45453a", "message": "Extract series test util", "committedDate": "2020-02-06T02:42:23Z", "type": "commit"}, {"oid": "d7e4a549afc4e53d0999ae404544cc3ca69c7841", "url": "https://github.com/apache/iotdb/commit/d7e4a549afc4e53d0999ae404544cc3ca69c7841", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T02:45:38Z", "type": "commit"}, {"oid": "5570ec2b4ad9b6b9f18dc359667c2b534ea164ef", "url": "https://github.com/apache/iotdb/commit/5570ec2b4ad9b6b9f18dc359667c2b534ea164ef", "message": "Add license", "committedDate": "2020-02-06T04:16:07Z", "type": "commit"}, {"oid": "3404293a089b7e9a890a874d5a949b6514db03ca", "url": "https://github.com/apache/iotdb/commit/3404293a089b7e9a890a874d5a949b6514db03ca", "message": "Merge branch 'master' into new_series_reader", "committedDate": "2020-02-06T09:44:22Z", "type": "commit"}, {"oid": "4e998ab35ad9a2294d81af6ba70c4867b2f257fb", "url": "https://github.com/apache/iotdb/commit/4e998ab35ad9a2294d81af6ba70c4867b2f257fb", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T09:53:54Z", "type": "commit"}, {"oid": "88f5a669a26f1abab44416cfdc4b3630d6738bdc", "url": "https://github.com/apache/iotdb/commit/88f5a669a26f1abab44416cfdc4b3630d6738bdc", "message": "Enhance IT test for aggregation query with more than one functions on one series", "committedDate": "2020-02-06T10:21:23Z", "type": "commit"}, {"oid": "838953099a39cec502bf2708e5eeb6d5f34e859e", "url": "https://github.com/apache/iotdb/commit/838953099a39cec502bf2708e5eeb6d5f34e859e", "message": "Rename", "committedDate": "2020-02-06T10:37:06Z", "type": "commit"}, {"oid": "cdd8e61ee79162679554adee489f2c12ce5c7d69", "url": "https://github.com/apache/iotdb/commit/cdd8e61ee79162679554adee489f2c12ce5c7d69", "message": "add raw data query document", "committedDate": "2020-02-06T10:39:25Z", "type": "commit"}, {"oid": "cfdc6b5f8e208e0b4e1b4cfdf3341fe13132b028", "url": "https://github.com/apache/iotdb/commit/cfdc6b5f8e208e0b4e1b4cfdf3341fe13132b028", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-06T10:39:43Z", "type": "commit"}, {"oid": "98f3d317ee9004224af7942674d3def23d2c8055", "url": "https://github.com/apache/iotdb/commit/98f3d317ee9004224af7942674d3def23d2c8055", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T10:57:22Z", "type": "commit"}, {"oid": "979fe85ec0a7f3331a9de5bdc3a5e70cf92478af", "url": "https://github.com/apache/iotdb/commit/979fe85ec0a7f3331a9de5bdc3a5e70cf92478af", "message": "fix bug and add more tests", "committedDate": "2020-02-06T11:15:06Z", "type": "commit"}, {"oid": "b1eb11042ee80062fa9240a245c8afdf1607f86c", "url": "https://github.com/apache/iotdb/commit/b1eb11042ee80062fa9240a245c8afdf1607f86c", "message": "update raw data query and series reader doc", "committedDate": "2020-02-06T11:38:55Z", "type": "commit"}, {"oid": "bd9517b92f3bba035ae0e63ffc99cc729902e826", "url": "https://github.com/apache/iotdb/commit/bd9517b92f3bba035ae0e63ffc99cc729902e826", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T11:39:38Z", "type": "commit"}, {"oid": "b71fd95722519d8048401da82a87eb2126d29591", "url": "https://github.com/apache/iotdb/commit/b71fd95722519d8048401da82a87eb2126d29591", "message": "[IOTDB-452] Do all aggregations of one series at one pass in GroupBy (#769)", "committedDate": "2020-02-06T12:00:35Z", "type": "commit"}, {"oid": "eb7565e2095f9fda34607df22adc36739485abde", "url": "https://github.com/apache/iotdb/commit/eb7565e2095f9fda34607df22adc36739485abde", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T12:05:25Z", "type": "commit"}, {"oid": "eddc108b04f8baaea0cc3219ccbbd285038a78c3", "url": "https://github.com/apache/iotdb/commit/eddc108b04f8baaea0cc3219ccbbd285038a78c3", "message": "Enhance", "committedDate": "2020-02-06T12:05:41Z", "type": "commit"}, {"oid": "0ee0a4ff446b2d32a3779e91bf58c50fb46bccc5", "url": "https://github.com/apache/iotdb/commit/0ee0a4ff446b2d32a3779e91bf58c50fb46bccc5", "message": "Add Aggregation desgin document", "committedDate": "2020-02-06T12:07:22Z", "type": "commit"}, {"oid": "7deb6eb3dce530039d8df6d851d4c5fd4c10d072", "url": "https://github.com/apache/iotdb/commit/7deb6eb3dce530039d8df6d851d4c5fd4c10d072", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-06T12:08:19Z", "type": "commit"}, {"oid": "b45c505bd27810f55abad832c07b5c24bb94c604", "url": "https://github.com/apache/iotdb/commit/b45c505bd27810f55abad832c07b5c24bb94c604", "message": "Add SeriesReaderByTimestampTest for SeriesReaderByTimestamp (#773)", "committedDate": "2020-02-06T14:15:33Z", "type": "commit"}, {"oid": "e6ff2a5310807c7883cabceb0aefc6bf4948ed9e", "url": "https://github.com/apache/iotdb/commit/e6ff2a5310807c7883cabceb0aefc6bf4948ed9e", "message": "optimze data query doc", "committedDate": "2020-02-06T14:21:58Z", "type": "commit"}, {"oid": "6c1cf27590f81b56acfd7101d3bdd642690d9add", "url": "https://github.com/apache/iotdb/commit/6c1cf27590f81b56acfd7101d3bdd642690d9add", "message": "Add Group By design document", "committedDate": "2020-02-06T15:55:00Z", "type": "commit"}, {"oid": "cc383cd361f7e9285c6cccd8117590eef3ba3bfb", "url": "https://github.com/apache/iotdb/commit/cc383cd361f7e9285c6cccd8117590eef3ba3bfb", "message": "merge", "committedDate": "2020-02-07T01:06:11Z", "type": "commit"}, {"oid": "2bf629e2cad00613b36261514531e21019ffdd72", "url": "https://github.com/apache/iotdb/commit/2bf629e2cad00613b36261514531e21019ffdd72", "message": "remove print in test", "committedDate": "2020-02-07T01:15:48Z", "type": "commit"}, {"oid": "4e2ad7553848b0feabb85c7f76d7abf7015ee117", "url": "https://github.com/apache/iotdb/commit/4e2ad7553848b0feabb85c7f76d7abf7015ee117", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-07T01:18:33Z", "type": "commit"}, {"oid": "4db49d52e37b02409034eec56061c8ca3f3d9ba3", "url": "https://github.com/apache/iotdb/commit/4db49d52e37b02409034eec56061c8ca3f3d9ba3", "message": "move package", "committedDate": "2020-02-07T01:23:23Z", "type": "commit"}, {"oid": "3fb17fbf41c71319850aa2b24dabe236d4a734de", "url": "https://github.com/apache/iotdb/commit/3fb17fbf41c71319850aa2b24dabe236d4a734de", "message": "simplify PlanExecutor", "committedDate": "2020-02-07T02:59:29Z", "type": "commit"}, {"oid": "a3e2367a8ad1f0b2a97798ed99f397265e73e149", "url": "https://github.com/apache/iotdb/commit/a3e2367a8ad1f0b2a97798ed99f397265e73e149", "message": "Fix typo", "committedDate": "2020-02-07T08:12:01Z", "type": "commit"}, {"oid": "eb920ce66115bf2777a62ca77b1b357a0e2fc3a5", "url": "https://github.com/apache/iotdb/commit/eb920ce66115bf2777a62ca77b1b357a0e2fc3a5", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-07T09:01:13Z", "type": "commit"}, {"oid": "a5c59614256df06e4f85959d1f3b93620fcd36a2", "url": "https://github.com/apache/iotdb/commit/a5c59614256df06e4f85959d1f3b93620fcd36a2", "message": "review planner and RawDataQueryExecutor (#779)", "committedDate": "2020-02-07T09:21:05Z", "type": "commit"}, {"oid": "9501fcdeea74164e61589fda1123685505b2c6e2", "url": "https://github.com/apache/iotdb/commit/9501fcdeea74164e61589fda1123685505b2c6e2", "message": "optimize QueryRouter", "committedDate": "2020-02-07T09:41:36Z", "type": "commit"}, {"oid": "9daae58989ff86dd0a0576efa88378a8388b3c42", "url": "https://github.com/apache/iotdb/commit/9daae58989ff86dd0a0576efa88378a8388b3c42", "message": "add more test", "committedDate": "2020-02-07T11:39:37Z", "type": "commit"}, {"oid": "46542d3804f4a1ad015b236a0ef90af5847ad566", "url": "https://github.com/apache/iotdb/commit/46542d3804f4a1ad015b236a0ef90af5847ad566", "message": "delete useless local variable in groupby", "committedDate": "2020-02-07T12:51:06Z", "type": "commit"}, {"oid": "d04339da22d0866fc1b3e76f53edfc664318c28b", "url": "https://github.com/apache/iotdb/commit/d04339da22d0866fc1b3e76f53edfc664318c28b", "message": "enable session handle duplicate columns", "committedDate": "2020-02-08T04:00:12Z", "type": "commit"}, {"oid": "f4b76296b85876d2ee247b430e8ab1e575f83b06", "url": "https://github.com/apache/iotdb/commit/f4b76296b85876d2ee247b430e8ab1e575f83b06", "message": "remove unused code", "committedDate": "2020-02-08T04:01:57Z", "type": "commit"}, {"oid": "2ea5ff791ef72b0125e70f2250f49d1aa5ec42eb", "url": "https://github.com/apache/iotdb/commit/2ea5ff791ef72b0125e70f2250f49d1aa5ec42eb", "message": "Resolve conflict from IOTDB-447", "committedDate": "2020-02-08T05:27:26Z", "type": "commit"}, {"oid": "96cf31e6213388f86fe575d125602506bc2a5118", "url": "https://github.com/apache/iotdb/commit/96cf31e6213388f86fe575d125602506bc2a5118", "message": "resolve conflict from IOTDB-447", "committedDate": "2020-02-08T05:34:10Z", "type": "commit"}, {"oid": "6db2b91daeedd85785ba71918ed7d63251024097", "url": "https://github.com/apache/iotdb/commit/6db2b91daeedd85785ba71918ed7d63251024097", "message": "fix test", "committedDate": "2020-02-08T06:03:59Z", "type": "commit"}, {"oid": "9fab09b824632311da7698b8a30438155d2759ee", "url": "https://github.com/apache/iotdb/commit/9fab09b824632311da7698b8a30438155d2759ee", "message": "enable warn level log for UTs", "committedDate": "2020-02-08T14:38:39Z", "type": "commit"}, {"oid": "7414e4835f70fb48fec0068c278ed63a1a2469a8", "url": "https://github.com/apache/iotdb/commit/7414e4835f70fb48fec0068c278ed63a1a2469a8", "message": "Update TsFile query document (#778)\n\n* add Tsfile query document", "committedDate": "2020-02-09T04:59:12Z", "type": "commit"}, {"oid": "2cb7ffe24065f399555a764ca8c446255a0f236d", "url": "https://github.com/apache/iotdb/commit/2cb7ffe24065f399555a764ca8c446255a0f236d", "message": "remove print in IoTDBSessionIT", "committedDate": "2020-02-09T05:02:03Z", "type": "commit"}, {"oid": "b31a856b7ed3d4908712424b6075a0d347c2fb77", "url": "https://github.com/apache/iotdb/commit/b31a856b7ed3d4908712424b6075a0d347c2fb77", "message": "fix typo in RawDataQuery.md", "committedDate": "2020-02-09T16:50:14Z", "type": "commit"}, {"oid": "760da1137d8def454eec4c44b8a5b7a593fbbc0e", "url": "https://github.com/apache/iotdb/commit/760da1137d8def454eec4c44b8a5b7a593fbbc0e", "message": "Merge remote-tracking branch 'origin/master' into ut_close_socket", "committedDate": "2020-02-10T00:56:50Z", "type": "commit"}, {"oid": "5f47303ed936e0679ed5aa5a9ebb0906b2fd62e5", "url": "https://github.com/apache/iotdb/commit/5f47303ed936e0679ed5aa5a9ebb0906b2fd62e5", "message": "fix travis", "committedDate": "2020-02-10T01:57:01Z", "type": "commit"}, {"oid": "8c4c1564709134a76bced711b35fb637b7214411", "url": "https://github.com/apache/iotdb/commit/8c4c1564709134a76bced711b35fb637b7214411", "message": "fix travis", "committedDate": "2020-02-10T02:27:03Z", "type": "commit"}, {"oid": "8c5172475dea2c2b7322993c9e160172598a3a78", "url": "https://github.com/apache/iotdb/commit/8c5172475dea2c2b7322993c9e160172598a3a78", "message": "add some tests", "committedDate": "2020-02-10T05:58:02Z", "type": "commit"}, {"oid": "7d16f11f8ac1492c86788413a10394b6df31df47", "url": "https://github.com/apache/iotdb/commit/7d16f11f8ac1492c86788413a10394b6df31df47", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-10T05:58:23Z", "type": "commit"}, {"oid": "bb3595305c8fb5190046a8cdd2441907d939b48f", "url": "https://github.com/apache/iotdb/commit/bb3595305c8fb5190046a8cdd2441907d939b48f", "message": "Add Apache Header", "committedDate": "2020-02-10T06:52:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA3NzMxNA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r377077314", "bodyText": "Why is this statement always false here?", "author": "LeiRui", "createdAt": "2020-02-10T13:58:00Z", "path": "server/src/test/java/org/apache/iotdb/db/query/reader/series/SeriesAggregateReaderTest.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.db.query.reader.series;\n+\n+import static org.apache.iotdb.db.conf.IoTDBConstant.PATH_SEPARATOR;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.exception.StorageEngineException;\n+import org.apache.iotdb.db.exception.metadata.MetadataException;\n+import org.apache.iotdb.db.exception.path.PathException;\n+import org.apache.iotdb.db.exception.query.QueryProcessException;\n+import org.apache.iotdb.db.query.aggregation.AggregateResult;\n+import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.db.query.factory.AggreResultFactory;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class SeriesAggregateReaderTest {\n+\n+  private static final String SERIES_READER_TEST_SG = \"root.seriesReaderTest\";\n+  private List<String> deviceIds = new ArrayList<>();\n+  private List<MeasurementSchema> measurementSchemas = new ArrayList<>();\n+\n+  private List<TsFileResource> seqResources = new ArrayList<>();\n+  private List<TsFileResource> unseqResources = new ArrayList<>();\n+\n+\n+  @Before\n+  public void setUp() throws MetadataException, PathException, IOException, WriteProcessException {\n+    SeriesReaderTestUtil.setUp(measurementSchemas, deviceIds, seqResources, unseqResources);\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException, StorageEngineException {\n+    SeriesReaderTestUtil.tearDown(seqResources, unseqResources);\n+  }\n+\n+  @Test\n+  public void aggregateTest() {\n+    try {\n+      Path path = new Path(SERIES_READER_TEST_SG + PATH_SEPARATOR + \"device0\", \"sensor0\");\n+      QueryDataSource queryDataSource = new QueryDataSource(path, seqResources, unseqResources);\n+      SeriesAggregateReader seriesReader = new SeriesAggregateReader(path, TSDataType.INT32,\n+          new QueryContext(), queryDataSource, null, null);\n+      AggregateResult aggregateResult = AggreResultFactory\n+          .getAggrResultByName(\"count\", TSDataType.INT32);\n+      while (seriesReader.hasNextChunk()) {\n+        if (seriesReader.canUseCurrentChunkStatistics()) {\n+          Statistics chunkStatistics = seriesReader.currentChunkStatistics();\n+          aggregateResult.updateResultFromStatistics(chunkStatistics);\n+          seriesReader.skipCurrentChunk();\n+          continue;\n+        }\n+        int loopTime = 0;\n+        while (seriesReader.hasNextPage()) {\n+          if (seriesReader.canUseCurrentPageStatistics()) {", "originalCommit": "bb3595305c8fb5190046a8cdd2441907d939b48f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzExMDc2OQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r377110769", "bodyText": "Because there is only one page in a chunk of his test data, the non-overlapped pages will re-enter the chunk layer to judge once and then the are calculated directly using chunkMetaData.", "author": "liutaohua", "createdAt": "2020-02-10T14:54:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA3NzMxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQyNTAzMQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r377425031", "bodyText": "Then this test should be fixed.", "author": "LeiRui", "createdAt": "2020-02-11T02:41:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA3NzMxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQzMzUxOA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r377433518", "bodyText": "Sorry @LeiRui , it's my fault. I have fixed this test in the latest commit. Thanks for your further review!", "author": "samperson1997", "createdAt": "2020-02-11T03:30:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzA3NzMxNA=="}], "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/test/java/org/apache/iotdb/db/query/reader/series/SeriesAggregateReaderTest.java b/server/src/test/java/org/apache/iotdb/db/query/reader/series/SeriesAggregateReaderTest.java\nindex a342d6f5d9..1b2b291580 100644\n--- a/server/src/test/java/org/apache/iotdb/db/query/reader/series/SeriesAggregateReaderTest.java\n+++ b/server/src/test/java/org/apache/iotdb/db/query/reader/series/SeriesAggregateReaderTest.java\n\n@@ -74,6 +74,7 @@ public class SeriesAggregateReaderTest {\n           new QueryContext(), queryDataSource, null, null);\n       AggregateResult aggregateResult = AggreResultFactory\n           .getAggrResultByName(\"count\", TSDataType.INT32);\n+      int loopTime = 0;\n       while (seriesReader.hasNextChunk()) {\n         if (seriesReader.canUseCurrentChunkStatistics()) {\n           Statistics chunkStatistics = seriesReader.currentChunkStatistics();\n"}}, {"oid": "1d799d5ef61fc27af14c326b71f30814df07c533", "url": "https://github.com/apache/iotdb/commit/1d799d5ef61fc27af14c326b71f30814df07c533", "message": "fix tsfile read doc", "committedDate": "2020-02-11T03:20:00Z", "type": "commit"}, {"oid": "3e01c49a093f3ceaa081b510fc1d2a9f9cc636aa", "url": "https://github.com/apache/iotdb/commit/3e01c49a093f3ceaa081b510fc1d2a9f9cc636aa", "message": "Fix SeriesAggregateReaderTest", "committedDate": "2020-02-11T03:29:56Z", "type": "commit"}, {"oid": "851e04104b812131ad5c5520da51bea0b4e31124", "url": "https://github.com/apache/iotdb/commit/851e04104b812131ad5c5520da51bea0b4e31124", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-02-11T03:30:15Z", "type": "commit"}, {"oid": "e04acb2bca9b57053e954bb5dbb9334535f1bb76", "url": "https://github.com/apache/iotdb/commit/e04acb2bca9b57053e954bb5dbb9334535f1bb76", "message": "solve the group by device bug", "committedDate": "2020-02-11T03:41:15Z", "type": "commit"}, {"oid": "a937845746772532aa72c8e09068a7de6ac14f3a", "url": "https://github.com/apache/iotdb/commit/a937845746772532aa72c8e09068a7de6ac14f3a", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-11T03:41:30Z", "type": "commit"}, {"oid": "414fdbe2d6a98b2d83b13da9c7dd322d2dc2218c", "url": "https://github.com/apache/iotdb/commit/414fdbe2d6a98b2d83b13da9c7dd322d2dc2218c", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-11T04:06:16Z", "type": "commit"}, {"oid": "dd3385cbce7aed4ee56093042bc541e05ce52418", "url": "https://github.com/apache/iotdb/commit/dd3385cbce7aed4ee56093042bc541e05ce52418", "message": "merge master", "committedDate": "2020-02-11T05:38:29Z", "type": "commit"}, {"oid": "f28cd4a276713fea6d694146b5e90d073bc6813a", "url": "https://github.com/apache/iotdb/commit/f28cd4a276713fea6d694146b5e90d073bc6813a", "message": "fix doc", "committedDate": "2020-02-11T06:51:18Z", "type": "commit"}, {"oid": "511ac745c01fd66f3fc4f0a197f80071f3d291a3", "url": "https://github.com/apache/iotdb/commit/511ac745c01fd66f3fc4f0a197f80071f3d291a3", "message": "fix fill filter", "committedDate": "2020-02-12T03:10:08Z", "type": "commit"}, {"oid": "29303a417703867dab3abecebd92c9408fa03069", "url": "https://github.com/apache/iotdb/commit/29303a417703867dab3abecebd92c9408fa03069", "message": "solve the fill bug", "committedDate": "2020-02-12T03:18:13Z", "type": "commit"}, {"oid": "8a0e49f464df8128b8b5788905ad46a856bc530d", "url": "https://github.com/apache/iotdb/commit/8a0e49f464df8128b8b5788905ad46a856bc530d", "message": "fix upperbound filter", "committedDate": "2020-02-12T03:26:46Z", "type": "commit"}, {"oid": "27f0852b0eff9a25e673f5d515631066b4ddf72b", "url": "https://github.com/apache/iotdb/commit/27f0852b0eff9a25e673f5d515631066b4ddf72b", "message": "fix upperbound filter", "committedDate": "2020-02-12T03:27:33Z", "type": "commit"}, {"oid": "b190f925787afd92f227b34f65768db22eaa31f2", "url": "https://github.com/apache/iotdb/commit/b190f925787afd92f227b34f65768db22eaa31f2", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-12T03:28:07Z", "type": "commit"}, {"oid": "15630d0f562a1866449fba452ddd40f099b44dc8", "url": "https://github.com/apache/iotdb/commit/15630d0f562a1866449fba452ddd40f099b44dc8", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-12T03:52:41Z", "type": "commit"}, {"oid": "5668225c6aa47a55a0603268494be639f96bf70b", "url": "https://github.com/apache/iotdb/commit/5668225c6aa47a55a0603268494be639f96bf70b", "message": "delete useless IPointReader in IFill", "committedDate": "2020-02-12T06:23:03Z", "type": "commit"}, {"oid": "6c9a4e8a9bf347b4286dc8b128b13811b1db3843", "url": "https://github.com/apache/iotdb/commit/6c9a4e8a9bf347b4286dc8b128b13811b1db3843", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-12T06:23:39Z", "type": "commit"}, {"oid": "7bad43b4e0ea280d551dffc5794d370e7a769b4d", "url": "https://github.com/apache/iotdb/commit/7bad43b4e0ea280d551dffc5794d370e7a769b4d", "message": "change the Linear Fill logic", "committedDate": "2020-02-12T06:35:44Z", "type": "commit"}, {"oid": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "url": "https://github.com/apache/iotdb/commit/305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "message": "add some comments", "committedDate": "2020-02-12T06:39:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODIzMDU2MA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378230560", "bodyText": "what's this comment mean...", "author": "EJTTianYu", "createdAt": "2020-02-12T12:51:09Z", "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBDescriptor.java", "diffHunk": "@@ -309,6 +309,11 @@ private void loadProps() {\n       conf.setMemtableNumInEachStorageGroup(\n           Integer.parseInt(properties.getProperty(\"memtable_num_in_each_storage_group\", String.valueOf(conf.getMemtableNumInEachStorageGroup()))));\n \n+      // the num of memtables in each storage group", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg1NzA5NQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378857095", "bodyText": "Fixed. Sorry for misunderstanding. This should be \"the default fill interval in LinearFill and PreviousFill\".", "author": "jack870131", "createdAt": "2020-02-13T13:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODIzMDU2MA=="}], "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/conf/IoTDBDescriptor.java b/server/src/main/java/org/apache/iotdb/db/conf/IoTDBDescriptor.java\nindex dcf02cf643..dd67f9afcd 100644\n--- a/server/src/main/java/org/apache/iotdb/db/conf/IoTDBDescriptor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/conf/IoTDBDescriptor.java\n\n@@ -309,7 +314,7 @@ public class IoTDBDescriptor {\n       conf.setMemtableNumInEachStorageGroup(\n           Integer.parseInt(properties.getProperty(\"memtable_num_in_each_storage_group\", String.valueOf(conf.getMemtableNumInEachStorageGroup()))));\n \n-      // the num of memtables in each storage group\n+      // the default fill interval in LinearFill and PreviousFill\n       conf.setDefaultFillInterval(\n           Integer.parseInt(properties.getProperty(\"default_fill_interval\", String.valueOf(conf.getDefaultFillInterval()))));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODIzODc0OA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378238748", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n               * <p> This method is calculate the aggregation using the common timestamps of cross series\n          \n          \n            \n               * <p> This method is calculate the aggregation using the common timestamps of cross series\n          \n      \n    \n    \n  \n\ncalculate here, grammar miss", "author": "EJTTianYu", "createdAt": "2020-02-12T13:08:46Z", "path": "server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java", "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.db.query.aggregation;\n+\n+import org.apache.iotdb.db.exception.query.QueryProcessException;\n+import org.apache.iotdb.db.query.reader.series.IReaderByTimestamp;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+\n+import java.io.IOException;\n+\n+public abstract class AggregateResult {\n+\n+  protected TSDataType dataType;\n+\n+  private boolean booleanRet;\n+  private int intRet;\n+  private long longRet;\n+  private float floatRet;\n+  private double doubleRet;\n+  private Binary binaryRet;\n+\n+  private boolean hasResult;\n+\n+  /**\n+   * construct.\n+   *\n+   * @param dataType result data type.\n+   */\n+  public AggregateResult(TSDataType dataType) {\n+    this.dataType = dataType;\n+    this.hasResult = false;\n+  }\n+\n+  public abstract Object getResult();\n+\n+  /**\n+   * Calculate the aggregation using Statistics\n+   *\n+   * @param statistics chunkStatistics or pageStatistics\n+   */\n+  public abstract void updateResultFromStatistics(Statistics statistics)\n+      throws QueryProcessException;\n+\n+  /**\n+   * Aggregate results cannot be calculated using Statistics directly, using the data in each page\n+   *\n+   * @param dataInThisPage the data in Page\n+   */\n+  public abstract void updateResultFromPageData(BatchData dataInThisPage) throws IOException;\n+  /**\n+   * Aggregate results cannot be calculated using Statistics directly, using the data in each page\n+   *\n+   * @param dataInThisPage the data in Page\n+   * @param bound calculate points whose time < bound\n+   */\n+  public abstract void updateResultFromPageData(BatchData dataInThisPage, long bound)\n+      throws IOException;\n+\n+  /**\n+   * <p> This method is calculate the aggregation using the common timestamps of cross series", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODg1NjcxMw==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378856713", "bodyText": "Fixed. Thx.", "author": "jack870131", "createdAt": "2020-02-13T13:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODIzODc0OA=="}], "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java b/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\nindex d65fb71285..6862d62f4c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\n\n@@ -78,7 +78,7 @@ public abstract class AggregateResult {\n       throws IOException;\n \n   /**\n-   * <p> This method is calculate the aggregation using the common timestamps of cross series\n+   * <p> This method calculates the aggregation using common timestamps of the cross series\n    * filter. </p>\n    *\n    * @throws IOException TsFile data read error\n"}}, {"oid": "753239a74366e4116b29036728102d6f19c24aae", "url": "https://github.com/apache/iotdb/commit/753239a74366e4116b29036728102d6f19c24aae", "message": "[IoTDB-468] Restructure QueryPlan (#796)\n\n* Restructure QueryPlan and AlignByDevicePlan", "committedDate": "2020-02-13T07:06:20Z", "type": "commit"}, {"oid": "2b0a724510a45a6823e3b8385efb81680f1688c7", "url": "https://github.com/apache/iotdb/commit/2b0a724510a45a6823e3b8385efb81680f1688c7", "message": "fix comments in AggregateResult and IoTDBDescriptor", "committedDate": "2020-02-13T13:24:19Z", "type": "commit"}, {"oid": "ec267afa483d7c00725bb121177c538cfa9eddef", "url": "https://github.com/apache/iotdb/commit/ec267afa483d7c00725bb121177c538cfa9eddef", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-13T13:28:54Z", "type": "commit"}, {"oid": "5a563b8271df95885e1ec89c1aadd7ade30cd9d8", "url": "https://github.com/apache/iotdb/commit/5a563b8271df95885e1ec89c1aadd7ade30cd9d8", "message": "fix OOM when there are too many columns in one query by adding a IChunkLoader cache", "committedDate": "2020-02-13T14:19:16Z", "type": "commit"}, {"oid": "4326b382470f9fa0693d34d9c892815be0331797", "url": "https://github.com/apache/iotdb/commit/4326b382470f9fa0693d34d9c892815be0331797", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-13T14:20:01Z", "type": "commit"}, {"oid": "792f60c6d5e05b407499342b7cfef910344a5fa5", "url": "https://github.com/apache/iotdb/commit/792f60c6d5e05b407499342b7cfef910344a5fa5", "message": "fix conflict", "committedDate": "2020-02-14T01:22:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjgzNzkzOA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r376837938", "bodyText": "If upper level use this method without check seriesReader.isPageOverlapped(). This may cause a problem. Maybe we should check this and then do the work", "author": "SilverNarcissus", "createdAt": "2020-02-10T01:29:42Z", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java", "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.query.reader.series;\n+\n+import org.apache.iotdb.db.engine.cache.DeviceMetaDataCache;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n+import org.apache.iotdb.db.engine.querycontext.ReadOnlyMemChunk;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.db.query.control.FileReaderManager;\n+import org.apache.iotdb.db.query.reader.chunk.MemChunkLoader;\n+import org.apache.iotdb.db.query.reader.chunk.MemChunkReader;\n+import org.apache.iotdb.db.query.reader.universal.PriorityMergeReader;\n+import org.apache.iotdb.db.utils.QueryUtils;\n+import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.TimeValuePair;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.common.Chunk;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.controller.ChunkLoaderImpl;\n+import org.apache.iotdb.tsfile.read.controller.IChunkLoader;\n+import org.apache.iotdb.tsfile.read.filter.basic.Filter;\n+import org.apache.iotdb.tsfile.read.filter.basic.UnaryFilter;\n+import org.apache.iotdb.tsfile.read.reader.IChunkReader;\n+import org.apache.iotdb.tsfile.read.reader.IPageReader;\n+import org.apache.iotdb.tsfile.read.reader.chunk.ChunkReader;\n+\n+import java.io.IOException;\n+import java.util.*;\n+\n+public class SeriesReader {\n+\n+  private final Path seriesPath;\n+  private final TSDataType dataType;\n+  private final QueryContext context;\n+  private final Filter timeFilter;\n+  private final Filter valueFilter;\n+\n+  private final List<TsFileResource> seqFileResource;\n+  private final PriorityQueue<TsFileResource> unseqFileResource;\n+\n+  private final List<ChunkMetaData> seqChunkMetadatas = new LinkedList<>();\n+  private final PriorityQueue<ChunkMetaData> unseqChunkMetadatas =\n+      new PriorityQueue<>(Comparator.comparingLong(ChunkMetaData::getStartTime));\n+\n+  private boolean hasCachedFirstChunkMetadata;\n+  private ChunkMetaData firstChunkMetaData;\n+\n+  private PriorityQueue<VersionPair<IPageReader>> overlappedPageReaders =\n+      new PriorityQueue<>(\n+          Comparator.comparingLong(pageReader -> pageReader.data.getStatistics().getStartTime()));\n+\n+  private PriorityMergeReader mergeReader = new PriorityMergeReader();\n+\n+  private boolean hasCachedNextBatch;\n+  private BatchData cachedBatchData;\n+\n+\n+  public SeriesReader(Path seriesPath, TSDataType dataType, QueryContext context,\n+      QueryDataSource dataSource, Filter timeFilter, Filter valueFilter) {\n+    this.seriesPath = seriesPath;\n+    this.dataType = dataType;\n+    this.context = context;\n+    this.seqFileResource = dataSource.getSeqResources();\n+    this.unseqFileResource = sortUnSeqFileResources(dataSource.getUnseqResources());\n+    this.timeFilter = timeFilter;\n+    this.valueFilter = valueFilter;\n+  }\n+\n+  @TestOnly\n+  public SeriesReader(Path seriesPath, TSDataType dataType, QueryContext context,\n+      List<TsFileResource> seqFileResource, List<TsFileResource> unseqFileResource,\n+      Filter timeFilter, Filter valueFilter) {\n+    this.seriesPath = seriesPath;\n+    this.dataType = dataType;\n+    this.context = context;\n+    this.seqFileResource = seqFileResource;\n+    this.unseqFileResource = sortUnSeqFileResources(unseqFileResource);\n+    this.timeFilter = timeFilter;\n+    this.valueFilter = valueFilter;\n+  }\n+\n+\n+  public boolean hasNextChunk() throws IOException {\n+    if (hasCachedFirstChunkMetadata) {\n+      return true;\n+    }\n+    // init first chunk metadata whose startTime is minimum\n+    tryToInitFirstChunk();\n+\n+    return hasCachedFirstChunkMetadata;\n+  }\n+\n+  /**\n+   * Because seq data and unseq data intersect, the minimum startTime taken from two files at a time\n+   * is used as the reference time to start reading data\n+   */\n+  private void tryToInitFirstChunk() throws IOException {\n+    tryToFillChunkMetadatas();\n+    hasCachedFirstChunkMetadata = true;\n+    if (!seqChunkMetadatas.isEmpty() && unseqChunkMetadatas.isEmpty()) {\n+      // only has seq\n+      firstChunkMetaData = seqChunkMetadatas.remove(0);\n+    } else if (seqChunkMetadatas.isEmpty() && !unseqChunkMetadatas.isEmpty()) {\n+      // only has unseq\n+      firstChunkMetaData = unseqChunkMetadatas.poll();\n+    } else if (!seqChunkMetadatas.isEmpty()) {\n+      // has seq and unseq\n+      if (seqChunkMetadatas.get(0).getStartTime() <= unseqChunkMetadatas.peek().getStartTime()) {\n+        firstChunkMetaData = seqChunkMetadatas.remove(0);\n+      } else {\n+        firstChunkMetaData = unseqChunkMetadatas.poll();\n+      }\n+    } else {\n+      // no seq nor unseq\n+      hasCachedFirstChunkMetadata = false;\n+    }\n+    tryToFillChunkMetadatas();\n+  }\n+\n+  public boolean isChunkOverlapped() {\n+    Statistics chunkStatistics = firstChunkMetaData.getStatistics();\n+    return mergeReader.hasNextTimeValuePair()\n+        || (!seqChunkMetadatas.isEmpty()\n+        && chunkStatistics.getEndTime() >= seqChunkMetadatas.get(0).getStartTime())\n+        || (!unseqChunkMetadatas.isEmpty()\n+        && chunkStatistics.getEndTime() >= unseqChunkMetadatas.peek().getStartTime());\n+  }\n+\n+  public Statistics currentChunkStatistics() {\n+    return firstChunkMetaData.getStatistics();\n+  }\n+\n+  public void skipCurrentChunk() {\n+    hasCachedFirstChunkMetadata = false;\n+    firstChunkMetaData = null;\n+  }\n+\n+  public boolean hasNextPage() throws IOException {\n+    if (!overlappedPageReaders.isEmpty()) {\n+      return true;\n+    }\n+\n+    fillOverlappedPageReaders();\n+\n+    return !overlappedPageReaders.isEmpty();\n+  }\n+\n+  private void fillOverlappedPageReaders() throws IOException {\n+    if (!hasCachedFirstChunkMetadata) {\n+      return;\n+    }\n+    unpackOneChunkMetaData(firstChunkMetaData);\n+    hasCachedFirstChunkMetadata = false;\n+    firstChunkMetaData = null;\n+  }\n+\n+  private void unpackOneChunkMetaData(ChunkMetaData chunkMetaData) throws IOException {\n+    initChunkReader(chunkMetaData)\n+        .getPageReaderList()\n+        .forEach(\n+            pageReader ->\n+                overlappedPageReaders.add(\n+                    new VersionPair(chunkMetaData.getVersion(), pageReader)));\n+  }\n+\n+\n+  protected BatchData nextPage() throws IOException {", "originalCommit": "b31a856b7ed3d4908712424b6075a0d347c2fb77", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\nindex 5140dbb904..6a262acb6e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\n\n@@ -204,7 +204,11 @@ public class SeriesReader {\n     return batchData;\n   }\n \n-  protected boolean isPageOverlapped() {\n+  protected boolean isPageOverlapped() throws IOException {\n+    if (overlappedPageReaders.isEmpty()) {\n+      throw new IOException(\"overlappedPageReaders is empty, hasNextPage should be called first\");\n+    }\n+\n     Statistics pageStatistics = overlappedPageReaders.peek().data.getStatistics();\n     return mergeReader.hasNextTimeValuePair()\n         || (!seqChunkMetadatas.isEmpty()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5Nzg5OA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r377997898", "bodyText": "OOM error may appear in this method. The reason is if many pages are over lapped, returned batch data will be very large.", "author": "SilverNarcissus", "createdAt": "2020-02-12T01:37:52Z", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java", "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.query.reader.series;\n+\n+import org.apache.iotdb.db.engine.cache.DeviceMetaDataCache;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n+import org.apache.iotdb.db.engine.querycontext.ReadOnlyMemChunk;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.db.query.control.FileReaderManager;\n+import org.apache.iotdb.db.query.reader.chunk.MemChunkLoader;\n+import org.apache.iotdb.db.query.reader.chunk.MemChunkReader;\n+import org.apache.iotdb.db.query.reader.universal.PriorityMergeReader;\n+import org.apache.iotdb.db.utils.QueryUtils;\n+import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.TimeValuePair;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.common.Chunk;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.controller.ChunkLoaderImpl;\n+import org.apache.iotdb.tsfile.read.controller.IChunkLoader;\n+import org.apache.iotdb.tsfile.read.filter.basic.Filter;\n+import org.apache.iotdb.tsfile.read.filter.basic.UnaryFilter;\n+import org.apache.iotdb.tsfile.read.reader.IChunkReader;\n+import org.apache.iotdb.tsfile.read.reader.IPageReader;\n+import org.apache.iotdb.tsfile.read.reader.chunk.ChunkReader;\n+\n+import java.io.IOException;\n+import java.util.*;\n+\n+public class SeriesReader {\n+\n+  private final Path seriesPath;\n+  private final TSDataType dataType;\n+  private final QueryContext context;\n+  private final Filter timeFilter;\n+  private final Filter valueFilter;\n+\n+  private final List<TsFileResource> seqFileResource;\n+  private final PriorityQueue<TsFileResource> unseqFileResource;\n+\n+  private final List<ChunkMetaData> seqChunkMetadatas = new LinkedList<>();\n+  private final PriorityQueue<ChunkMetaData> unseqChunkMetadatas =\n+      new PriorityQueue<>(Comparator.comparingLong(ChunkMetaData::getStartTime));\n+\n+  private boolean hasCachedFirstChunkMetadata;\n+  private ChunkMetaData firstChunkMetaData;\n+\n+  private PriorityQueue<VersionPair<IPageReader>> overlappedPageReaders =\n+      new PriorityQueue<>(\n+          Comparator.comparingLong(pageReader -> pageReader.data.getStatistics().getStartTime()));\n+\n+  private PriorityMergeReader mergeReader = new PriorityMergeReader();\n+\n+  private boolean hasCachedNextBatch;\n+  private BatchData cachedBatchData;\n+\n+\n+  public SeriesReader(Path seriesPath, TSDataType dataType, QueryContext context,\n+      QueryDataSource dataSource, Filter timeFilter, Filter valueFilter) {\n+    this.seriesPath = seriesPath;\n+    this.dataType = dataType;\n+    this.context = context;\n+    this.seqFileResource = dataSource.getSeqResources();\n+    this.unseqFileResource = sortUnSeqFileResources(dataSource.getUnseqResources());\n+    this.timeFilter = timeFilter;\n+    this.valueFilter = valueFilter;\n+  }\n+\n+  @TestOnly\n+  public SeriesReader(Path seriesPath, TSDataType dataType, QueryContext context,\n+      List<TsFileResource> seqFileResource, List<TsFileResource> unseqFileResource,\n+      Filter timeFilter, Filter valueFilter) {\n+    this.seriesPath = seriesPath;\n+    this.dataType = dataType;\n+    this.context = context;\n+    this.seqFileResource = seqFileResource;\n+    this.unseqFileResource = sortUnSeqFileResources(unseqFileResource);\n+    this.timeFilter = timeFilter;\n+    this.valueFilter = valueFilter;\n+  }\n+\n+\n+  public boolean hasNextChunk() throws IOException {\n+    if (hasCachedFirstChunkMetadata) {\n+      return true;\n+    }\n+    // init first chunk metadata whose startTime is minimum\n+    tryToInitFirstChunk();\n+\n+    return hasCachedFirstChunkMetadata;\n+  }\n+\n+  /**\n+   * Because seq data and unseq data intersect, the minimum startTime taken from two files at a time\n+   * is used as the reference time to start reading data\n+   */\n+  private void tryToInitFirstChunk() throws IOException {\n+    tryToFillChunkMetadatas();\n+    hasCachedFirstChunkMetadata = true;\n+    if (!seqChunkMetadatas.isEmpty() && unseqChunkMetadatas.isEmpty()) {\n+      // only has seq\n+      firstChunkMetaData = seqChunkMetadatas.remove(0);\n+    } else if (seqChunkMetadatas.isEmpty() && !unseqChunkMetadatas.isEmpty()) {\n+      // only has unseq\n+      firstChunkMetaData = unseqChunkMetadatas.poll();\n+    } else if (!seqChunkMetadatas.isEmpty()) {\n+      // has seq and unseq\n+      if (seqChunkMetadatas.get(0).getStartTime() <= unseqChunkMetadatas.peek().getStartTime()) {\n+        firstChunkMetaData = seqChunkMetadatas.remove(0);\n+      } else {\n+        firstChunkMetaData = unseqChunkMetadatas.poll();\n+      }\n+    } else {\n+      // no seq nor unseq\n+      hasCachedFirstChunkMetadata = false;\n+    }\n+    tryToFillChunkMetadatas();\n+  }\n+\n+  public boolean isChunkOverlapped() {\n+    Statistics chunkStatistics = firstChunkMetaData.getStatistics();\n+    return mergeReader.hasNextTimeValuePair()\n+        || (!seqChunkMetadatas.isEmpty()\n+        && chunkStatistics.getEndTime() >= seqChunkMetadatas.get(0).getStartTime())\n+        || (!unseqChunkMetadatas.isEmpty()\n+        && chunkStatistics.getEndTime() >= unseqChunkMetadatas.peek().getStartTime());\n+  }\n+\n+  public Statistics currentChunkStatistics() {\n+    return firstChunkMetaData.getStatistics();\n+  }\n+\n+  public void skipCurrentChunk() {\n+    hasCachedFirstChunkMetadata = false;\n+    firstChunkMetaData = null;\n+  }\n+\n+  public boolean hasNextPage() throws IOException {\n+    if (!overlappedPageReaders.isEmpty()) {\n+      return true;\n+    }\n+\n+    fillOverlappedPageReaders();\n+\n+    return !overlappedPageReaders.isEmpty();\n+  }\n+\n+  private void fillOverlappedPageReaders() throws IOException {\n+    if (!hasCachedFirstChunkMetadata) {\n+      return;\n+    }\n+    unpackOneChunkMetaData(firstChunkMetaData);\n+    hasCachedFirstChunkMetadata = false;\n+    firstChunkMetaData = null;\n+  }\n+\n+  private void unpackOneChunkMetaData(ChunkMetaData chunkMetaData) throws IOException {\n+    initChunkReader(chunkMetaData)\n+        .getPageReaderList()\n+        .forEach(\n+            pageReader ->\n+                overlappedPageReaders.add(\n+                    new VersionPair(chunkMetaData.getVersion(), pageReader)));\n+  }\n+\n+\n+  protected BatchData nextPage() throws IOException {\n+    BatchData pageData = Objects\n+        .requireNonNull(overlappedPageReaders.poll().data, \"No Batch data\")\n+        .getAllSatisfiedPageData();\n+    // only need to consider valueFilter because timeFilter has been set into the page reader\n+    if (valueFilter == null) {\n+      return pageData;\n+    }\n+    BatchData batchData = new BatchData(pageData.getDataType());\n+    while (pageData.hasCurrent()) {\n+      if (valueFilter.satisfy(pageData.currentTime(), pageData.currentValue())) {\n+        batchData.putAnObject(pageData.currentTime(), pageData.currentValue());\n+      }\n+      pageData.next();\n+    }\n+    return batchData;\n+  }\n+\n+  protected boolean isPageOverlapped() {\n+    Statistics pageStatistics = overlappedPageReaders.peek().data.getStatistics();\n+    return mergeReader.hasNextTimeValuePair()\n+        || (!seqChunkMetadatas.isEmpty()\n+        && pageStatistics.getEndTime() >= seqChunkMetadatas.get(0).getStartTime())\n+        || (!unseqChunkMetadatas.isEmpty()\n+        && pageStatistics.getEndTime() >= unseqChunkMetadatas.peek().getStartTime());\n+  }\n+\n+  public Statistics currentPageStatistics() throws IOException {\n+    if (overlappedPageReaders.isEmpty() || overlappedPageReaders.peek().data == null) {\n+      throw new IOException(\"No next page statistics.\");\n+    }\n+    return overlappedPageReaders.peek().data.getStatistics();\n+  }\n+\n+  public void skipCurrentPage() {\n+    overlappedPageReaders.poll();\n+  }\n+\n+  public boolean hasNextOverlappedPage() throws IOException {", "originalCommit": "f28cd4a276713fea6d694146b5e90d073bc6813a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2Mjk3OQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379962979", "bodyText": "Hi, I created a jira to do this in the future.", "author": "qiaojialin", "createdAt": "2020-02-17T02:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5Nzg5OA=="}], "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\nindex 5140dbb904..6a262acb6e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesReader.java\n\n@@ -204,7 +204,11 @@ public class SeriesReader {\n     return batchData;\n   }\n \n-  protected boolean isPageOverlapped() {\n+  protected boolean isPageOverlapped() throws IOException {\n+    if (overlappedPageReaders.isEmpty()) {\n+      throw new IOException(\"overlappedPageReaders is empty, hasNextPage should be called first\");\n+    }\n+\n     Statistics pageStatistics = overlappedPageReaders.peek().data.getStatistics();\n     return mergeReader.hasNextTimeValuePair()\n         || (!seqChunkMetadatas.isEmpty()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4NDY0Nw==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378084647", "bodyText": "May be we should use result rather than Ret. When I first look at this name, I think it is return rather than result", "author": "SilverNarcissus", "createdAt": "2020-02-12T07:49:20Z", "path": "server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java", "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.db.query.aggregation;\n+\n+import org.apache.iotdb.db.exception.query.QueryProcessException;\n+import org.apache.iotdb.db.query.reader.series.IReaderByTimestamp;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+\n+import java.io.IOException;\n+\n+public abstract class AggregateResult {\n+\n+  protected TSDataType dataType;\n+\n+  private boolean booleanRet;", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MzQ1MA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379963450", "bodyText": "fixed to booleanValue", "author": "qiaojialin", "createdAt": "2020-02-17T02:31:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4NDY0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java b/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\nindex d65fb71285..6862d62f4c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggregateResult.java\n\n@@ -78,7 +78,7 @@ public abstract class AggregateResult {\n       throws IOException;\n \n   /**\n-   * <p> This method is calculate the aggregation using the common timestamps of cross series\n+   * <p> This method calculates the aggregation using common timestamps of the cross series\n    * filter. </p>\n    *\n    * @throws IOException TsFile data read error\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4NTg3Mw==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378085873", "bodyText": "I have a question. Why we need this class? This class's methods are identical with TimeValuePair. Could we just use TimeValuePair?", "author": "SilverNarcissus", "createdAt": "2020-02-12T07:52:57Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairInMemTable.java", "diffHunk": "@@ -20,7 +20,7 @@\n \n import java.util.Objects;\n \n-import org.apache.iotdb.db.utils.TimeValuePair;\n+import org.apache.iotdb.tsfile.read.TimeValuePair;\n import org.apache.iotdb.tsfile.utils.TsPrimitiveType;\n \n public class TimeValuePairInMemTable extends TimeValuePair {", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MzgzNg==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379963836", "bodyText": "Hi, we didn't even notice there exist such a class, I have removed it.", "author": "qiaojialin", "createdAt": "2020-02-17T02:33:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4NTg3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ec9b912bcb11774933175ffb717cb5a441bc172a", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairInMemTable.java b/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairInMemTable.java\ndeleted file mode 100644\nindex 3f6328f98a..0000000000\n--- a/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairInMemTable.java\n+++ /dev/null\n\n@@ -1,45 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iotdb.db.engine.memtable;\n-\n-import java.util.Objects;\n-\n-import org.apache.iotdb.tsfile.read.TimeValuePair;\n-import org.apache.iotdb.tsfile.utils.TsPrimitiveType;\n-\n-public class TimeValuePairInMemTable extends TimeValuePair {\n-\n-  public TimeValuePairInMemTable(long timestamp, TsPrimitiveType value) {\n-    super(timestamp, value);\n-  }\n-\n-  @Override\n-  public boolean equals(Object object) {\n-    if (!(object instanceof TimeValuePairInMemTable)) {\n-      return false;\n-    }\n-    TimeValuePairInMemTable o = (TimeValuePairInMemTable) object;\n-    return o.getTimestamp() == this.getTimestamp();\n-  }\n-\n-  @Override\n-  public int hashCode() {\n-    return Objects.hash(this.getTimestamp());\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4Njc3Mw==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378086773", "bodyText": "This class is unused. Remove it or give an explanation.", "author": "SilverNarcissus", "createdAt": "2020-02-12T07:55:29Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairSorter.java", "diffHunk": "@@ -21,7 +21,7 @@\n import java.util.Iterator;\n import java.util.List;\n \n-import org.apache.iotdb.db.utils.TimeValuePair;\n+import org.apache.iotdb.tsfile.read.TimeValuePair;\n \n public interface TimeValuePairSorter {", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2Mzk0MA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379963940", "bodyText": "already removed", "author": "qiaojialin", "createdAt": "2020-02-17T02:34:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4Njc3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ec9b912bcb11774933175ffb717cb5a441bc172a", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairSorter.java b/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairSorter.java\ndeleted file mode 100644\nindex d4d8819870..0000000000\n--- a/server/src/main/java/org/apache/iotdb/db/engine/memtable/TimeValuePairSorter.java\n+++ /dev/null\n\n@@ -1,55 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iotdb.db.engine.memtable;\n-\n-import java.util.Iterator;\n-import java.util.List;\n-\n-import org.apache.iotdb.tsfile.read.TimeValuePair;\n-\n-public interface TimeValuePairSorter {\n-\n-  /**\n-   * get the distinct sorted startTime. Only for query.\n-   *\n-   * @return a List which contains all distinct {@link TimeValuePair}s in ascending order by\n-   * timestamp.\n-   */\n-  List<TimeValuePair> getSortedTimeValuePairList();\n-\n-  /**\n-   * notice, by default implementation, calling this method will cause calling\n-   * getSortedTimeValuePairList().\n-   *\n-   * @return an iterator of data in this class.\n-   */\n-  default Iterator<TimeValuePair> getIterator() {\n-    return getSortedTimeValuePairList().iterator();\n-  }\n-\n-  /**\n-   * notice, by default implementation, calling this method will cause calling\n-   * getSortedTimeValuePairList().\n-   *\n-   * @return if there is no data in this sorter, return true.\n-   */\n-  default boolean isEmpty() {\n-    return getSortedTimeValuePairList().isEmpty();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4ODk5NA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378088994", "bodyText": "The sum value here may overflow when we query large number of data. May be we should use this method to update:\navg(n + 1) = avg(n) * (n / n + 1) + (1 / n + 1) * val(n + 1)", "author": "SilverNarcissus", "createdAt": "2020-02-12T08:01:23Z", "path": "server/src/main/java/org/apache/iotdb/db/query/aggregation/impl/AvgAggrResult.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.db.query.aggregation.impl;\n+\n+import java.io.IOException;\n+import org.apache.iotdb.db.query.aggregation.AggregateResult;\n+import org.apache.iotdb.db.query.reader.series.IReaderByTimestamp;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+\n+public class AvgAggrResult extends AggregateResult {\n+\n+  private static final String AVG_AGGR_NAME = \"AVG\";\n+  private TSDataType seriesDataType;\n+  protected double sum = 0.0;\n+  protected int cnt = 0;\n+\n+  public AvgAggrResult(TSDataType seriesDataType) {\n+    super(TSDataType.DOUBLE);\n+    this.seriesDataType = seriesDataType;\n+    reset();\n+    sum = 0.0;\n+    cnt = 0;\n+  }\n+\n+  @Override\n+  public Double getResult() {\n+    if (cnt > 0) {\n+      setDoubleRet(sum / cnt);\n+    }\n+    return hasResult() ? getDoubleRet() : null;\n+  }\n+\n+  @Override\n+  public void updateResultFromStatistics(Statistics statistics) {\n+    cnt += statistics.getCount();\n+    sum += statistics.getSumValue();\n+  }\n+\n+  @Override\n+  public void updateResultFromPageData(BatchData dataInThisPage) throws IOException {\n+    updateResultFromPageData(dataInThisPage, Long.MAX_VALUE);\n+  }\n+\n+  @Override\n+  public void updateResultFromPageData(BatchData dataInThisPage, long bound) throws IOException {\n+    while (dataInThisPage.hasCurrent()) {\n+      if (dataInThisPage.currentTime() >= bound) {\n+        break;\n+      }\n+      updateAvg(seriesDataType, dataInThisPage.currentValue());\n+      dataInThisPage.next();\n+    }\n+  }\n+\n+  @Override\n+  public void updateResultUsingTimestamps(long[] timestamps, int length,\n+      IReaderByTimestamp dataReader) throws IOException {\n+    for (int i = 0; i < length; i++) {\n+      Object value = dataReader.getValueInTimestamp(timestamps[i]);\n+      if (value != null) {\n+        updateAvg(seriesDataType, value);\n+      }\n+    }\n+  }\n+\n+  private void updateAvg(TSDataType type, Object sumVal) throws IOException {", "originalCommit": "305b02a13398cf9a0b607d157cb0c2c5f3f1706b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk4MjU1NA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379982554", "bodyText": "Fixed. Thanks for your suggestion!", "author": "samperson1997", "createdAt": "2020-02-17T04:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODA4ODk5NA=="}], "type": "inlineReview", "revised_code": {"commit": "ec9b912bcb11774933175ffb717cb5a441bc172a", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/aggregation/impl/AvgAggrResult.java b/server/src/main/java/org/apache/iotdb/db/query/aggregation/impl/AvgAggrResult.java\nindex 1e243bb932..e5b1531edf 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/aggregation/impl/AvgAggrResult.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/aggregation/impl/AvgAggrResult.java\n\n@@ -44,9 +44,9 @@ public class AvgAggrResult extends AggregateResult {\n   @Override\n   public Double getResult() {\n     if (cnt > 0) {\n-      setDoubleRet(sum / cnt);\n+      setDoubleValue(sum / cnt);\n     }\n-    return hasResult() ? getDoubleRet() : null;\n+    return hasResult() ? getDoubleValue() : null;\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyNzAxNA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378827014", "bodyText": "Why we can just put chunk statistics into page reader?", "author": "SilverNarcissus", "createdAt": "2020-02-13T12:21:36Z", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/chunk/MemChunkReader.java", "diffHunk": "@@ -112,12 +116,9 @@ public void close() {\n   }\n \n   @Override\n-  public PageHeader nextPageHeader() throws IOException {\n-    return null;\n+  public List<IPageReader> getPageReaderList() throws IOException {\n+    return Collections.singletonList(\n+        new MemPageReader(nextPageData(), readOnlyMemChunk.getChunkMetaData().getStatistics()));", "originalCommit": "753239a74366e4116b29036728102d6f19c24aae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2NDI4OA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379964288", "bodyText": "cause memchunk only has one page", "author": "qiaojialin", "createdAt": "2020-02-17T02:36:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgyNzAxNA=="}], "type": "inlineReview", "revised_code": {"commit": "ec9b912bcb11774933175ffb717cb5a441bc172a", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/reader/chunk/MemChunkReader.java b/server/src/main/java/org/apache/iotdb/db/query/reader/chunk/MemChunkReader.java\nindex 74e467da1d..7f97a85f79 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/reader/chunk/MemChunkReader.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/reader/chunk/MemChunkReader.java\n\n@@ -117,6 +117,7 @@ public class MemChunkReader implements IChunkReader, IPointReader {\n \n   @Override\n   public List<IPageReader> getPageReaderList() throws IOException {\n+    // we treat one ReadOnlyMemChunk as one Page\n     return Collections.singletonList(\n         new MemPageReader(nextPageData(), readOnlyMemChunk.getChunkMetaData().getStatistics()));\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMDY5MQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r378830691", "bodyText": "While statement doesn't loop. You can use if to replace while", "author": "SilverNarcissus", "createdAt": "2020-02-13T12:30:16Z", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesRawDataPointReader.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.query.reader.series;\n+\n+import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n+import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.read.TimeValuePair;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.filter.basic.Filter;\n+import org.apache.iotdb.tsfile.read.reader.IPointReader;\n+\n+import java.io.IOException;\n+\n+\n+public class SeriesRawDataPointReader implements IPointReader {\n+\n+  private final SeriesReader seriesReader;\n+\n+  private boolean hasCachedTimeValuePair;\n+  private BatchData batchData;\n+  private TimeValuePair timeValuePair;\n+\n+\n+  public SeriesRawDataPointReader(SeriesReader seriesReader) {\n+    this.seriesReader = seriesReader;\n+  }\n+\n+  public SeriesRawDataPointReader(Path seriesPath, TSDataType dataType, QueryContext context,\n+      QueryDataSource dataSource, Filter timeFilter, Filter valueFilter) {\n+    this.seriesReader = new SeriesReader(seriesPath, dataType, context, dataSource, timeFilter,\n+        valueFilter);\n+  }\n+\n+  private boolean hasNext() throws IOException {\n+    while (seriesReader.hasNextChunk()) {\n+      while (seriesReader.hasNextPage()) {\n+        if (seriesReader.hasNextOverlappedPage()) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  private boolean hasNextSatisfiedInCurrentBatch() {\n+    while (batchData != null && batchData.hasCurrent()) {\n+      timeValuePair = new TimeValuePair(batchData.currentTime(),\n+          batchData.currentTsPrimitiveType());\n+      hasCachedTimeValuePair = true;\n+      batchData.next();\n+      return true;\n+    }", "originalCommit": "753239a74366e4116b29036728102d6f19c24aae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2NDU2Ng==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379964566", "bodyText": "fixed", "author": "qiaojialin", "createdAt": "2020-02-17T02:38:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODgzMDY5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "ec9b912bcb11774933175ffb717cb5a441bc172a", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesRawDataPointReader.java b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesRawDataPointReader.java\nindex 76ee61031c..6532c0af76 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesRawDataPointReader.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/reader/series/SeriesRawDataPointReader.java\n\n@@ -61,7 +61,7 @@ public class SeriesRawDataPointReader implements IPointReader {\n   }\n \n   private boolean hasNextSatisfiedInCurrentBatch() {\n-    while (batchData != null && batchData.hasCurrent()) {\n+    if (batchData != null && batchData.hasCurrent()) {\n       timeValuePair = new TimeValuePair(batchData.currentTime(),\n           batchData.currentTsPrimitiveType());\n       hasCachedTimeValuePair = true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2MDQ0NQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379260445", "bodyText": "Maybe we need more tests here. Consider the order of columns, mergeable or unmergeable columns. Such as following sql:\n\"SELECT sum(s0), count(s0), avg(s2), avg(s0) FROM root.vehicle.d0 WHERE time >= 6000 AND time <= 9000\"\n\"SELECT sum(s2), count(s0), avg(s2), avg(s1), count(s2),sum(s0) FROM root.vehicle.d0 WHERE time >= 6000 AND time <= 9000\"", "author": "SilverNarcissus", "createdAt": "2020-02-14T05:51:00Z", "path": "server/src/test/java/org/apache/iotdb/db/integration/IoTDBAggregationIT.java", "diffHunk": "@@ -508,6 +509,57 @@ public void avgSumErrorTest() throws SQLException {\n     }\n   }\n \n+  /**\n+   * test aggregation query with more than one functions on one series\n+   */\n+  @Test\n+  public void mergeAggrOnOneSeriesTest() {", "originalCommit": "792f60c6d5e05b407499342b7cfef910344a5fa5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2NjEzNg==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r379966136", "bodyText": "Thanks. I'd like to add more tests. @qiaojialin", "author": "samperson1997", "createdAt": "2020-02-17T02:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2MDQ0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "2bb1a8bcf2a3b5d383f433817a2b39b863a705aa", "chunk": "diff --git a/server/src/test/java/org/apache/iotdb/db/integration/IoTDBAggregationIT.java b/server/src/test/java/org/apache/iotdb/db/integration/IoTDBAggregationIT.java\nindex fb784c5f5e..0aed0ed2a4 100644\n--- a/server/src/test/java/org/apache/iotdb/db/integration/IoTDBAggregationIT.java\n+++ b/server/src/test/java/org/apache/iotdb/db/integration/IoTDBAggregationIT.java\n\n@@ -516,7 +516,10 @@ public class IoTDBAggregationIT {\n   public void mergeAggrOnOneSeriesTest() {\n     String[] retArray = new String[]{\n         \"0,1.4508E7,7250.374812593703,7250.374812593703,1.4508E7\",\n-        \"0,626750.0,1250.998003992016,1250.998003992016,626750.0\"\n+        \"0,626750.0,1250.998003992016,1250.998003992016,626750.0\",\n+        \"0,1.4508E7,2001,7250.374812593703,7250.374812593703\",\n+        \"0,1.4508E7,2001,7250.374812593703,7250.374812593703,2001,1.4508E7\"\n+\n     };\n     try (Connection connection = DriverManager.\n         getConnection(\"jdbc:iotdb://127.0.0.1:6667/\", \"root\", \"root\");\n"}}, {"oid": "8747fa0c683e85d55b151a2128fb04b4e20f7769", "url": "https://github.com/apache/iotdb/commit/8747fa0c683e85d55b151a2128fb04b4e20f7769", "message": "[IoTDB-475] Improve the logic of AlignByDevicePlan in PhysicalGenerator (#803)\n\n* optimize PhysicalGenerator", "committedDate": "2020-02-14T13:16:42Z", "type": "commit"}, {"oid": "8e4a01a4fe4defb7e8db75031254120908e419c0", "url": "https://github.com/apache/iotdb/commit/8e4a01a4fe4defb7e8db75031254120908e419c0", "message": "[IOTDB-466] group by device -> align by device && group by time range [] -> [) (#805)", "committedDate": "2020-02-15T03:00:32Z", "type": "commit"}, {"oid": "812d73933d2de45d91b122217f2c0282990c97dd", "url": "https://github.com/apache/iotdb/commit/812d73933d2de45d91b122217f2c0282990c97dd", "message": "[IOTDB-464] Vectorize the IReaderByTimestamp (#812)", "committedDate": "2020-02-15T11:33:39Z", "type": "commit"}, {"oid": "0353cdfcae3a6af3cef60b801bb684c644704746", "url": "https://github.com/apache/iotdb/commit/0353cdfcae3a6af3cef60b801bb684c644704746", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-15T11:35:00Z", "type": "commit"}, {"oid": "c4dd6261d638f817f6b60c1ee2cec306a38e782b", "url": "https://github.com/apache/iotdb/commit/c4dd6261d638f817f6b60c1ee2cec306a38e782b", "message": "optimize codes", "committedDate": "2020-02-15T11:46:29Z", "type": "commit"}, {"oid": "ce5f509c56f9c3d087b01af8a835428acc818e7a", "url": "https://github.com/apache/iotdb/commit/ce5f509c56f9c3d087b01af8a835428acc818e7a", "message": "add config parameter", "committedDate": "2020-02-15T14:12:25Z", "type": "commit"}, {"oid": "ca636e712bb459c3c02882444b86b8f1573490e9", "url": "https://github.com/apache/iotdb/commit/ca636e712bb459c3c02882444b86b8f1573490e9", "message": "add config parameter", "committedDate": "2020-02-15T14:13:28Z", "type": "commit"}, {"oid": "e2bf45788e698fb1dce60e6a9d8efdddea32db6e", "url": "https://github.com/apache/iotdb/commit/e2bf45788e698fb1dce60e6a9d8efdddea32db6e", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-15T14:14:02Z", "type": "commit"}, {"oid": "484617c03b037a03777f34fe611e65f13c054ff5", "url": "https://github.com/apache/iotdb/commit/484617c03b037a03777f34fe611e65f13c054ff5", "message": "optimize tsfile read doc", "committedDate": "2020-02-16T06:03:35Z", "type": "commit"}, {"oid": "7c870369a8f58b7b599d45b51ef7f9ff78b73e5b", "url": "https://github.com/apache/iotdb/commit/7c870369a8f58b7b599d45b51ef7f9ff78b73e5b", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-16T06:04:14Z", "type": "commit"}, {"oid": "35e1d2c2e3b37d3d68d8487eef621f30a73cf4f9", "url": "https://github.com/apache/iotdb/commit/35e1d2c2e3b37d3d68d8487eef621f30a73cf4f9", "message": "print warn log if waitForAllTsProcessorClosed() too long time", "committedDate": "2020-02-16T18:08:01Z", "type": "commit"}, {"oid": "6b060ef5242bb430218fc1f21535c36c8f08324f", "url": "https://github.com/apache/iotdb/commit/6b060ef5242bb430218fc1f21535c36c8f08324f", "message": "add comments if a tsfile is closed", "committedDate": "2020-02-17T00:25:21Z", "type": "commit"}, {"oid": "aa34ad8107b0285d80baf2246639833c9a2b7779", "url": "https://github.com/apache/iotdb/commit/aa34ad8107b0285d80baf2246639833c9a2b7779", "message": "set jmxport =31999 by default if there is no config file", "committedDate": "2020-02-17T00:42:21Z", "type": "commit"}, {"oid": "34666834f8566a178535649e910f8959aa9c3b0b", "url": "https://github.com/apache/iotdb/commit/34666834f8566a178535649e910f8959aa9c3b0b", "message": "remove priority in ChunkMetadata and add check in isPageOverlapped in SeriesReader", "committedDate": "2020-02-17T01:50:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE3NTk1Ng==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r366175956", "bodyText": "remove unused field", "author": "samperson1997", "createdAt": "2020-01-14T06:55:51Z", "path": "jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java", "diffHunk": "@@ -41,8 +41,10 @@\n public class IoTDBQueryResultSet implements ResultSet {\n \n   private static final String TIMESTAMP_STR = \"Time\";\n+  private static final int TIMESTAMP_STR_LENGTH = 4;", "originalCommit": "ad08f92972fd1c7c536a8e9644e9fcb4858e069c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java b/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\nindex eaa5993adf..d0f466ec20 100644\n--- a/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\n+++ b/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\n\n@@ -21,1269 +21,181 @@ package org.apache.iotdb.jdbc;\n \n import org.apache.iotdb.rpc.IoTDBRPCException;\n import org.apache.iotdb.rpc.RpcUtils;\n-import org.apache.iotdb.service.rpc.thrift.*;\n+import org.apache.iotdb.service.rpc.thrift.TSFetchResultsReq;\n+import org.apache.iotdb.service.rpc.thrift.TSFetchResultsResp;\n+import org.apache.iotdb.service.rpc.thrift.TSIService;\n+import org.apache.iotdb.service.rpc.thrift.TSQueryDataSet;\n import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n import org.apache.iotdb.tsfile.utils.BytesUtils;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.apache.thrift.TException;\n \n-import java.io.InputStream;\n-import java.io.Reader;\n-import java.math.BigDecimal;\n-import java.math.MathContext;\n-import java.net.URL;\n import java.nio.ByteBuffer;\n-import java.sql.Date;\n-import java.sql.*;\n-import java.util.*;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Objects;\n \n-public class IoTDBQueryResultSet implements ResultSet {\n+public class IoTDBQueryResultSet extends AbstractIoTDBResultSet {\n \n-  private static final String TIMESTAMP_STR = \"Time\";\n-  private static final int TIMESTAMP_STR_LENGTH = 4;\n   private static final int START_INDEX = 2;\n   private static final String VALUE_IS_NULL = \"The value got by %s (column name) is NULL.\";\n-  private static final String EMPTY_STR = \"\";\n-  private Statement statement = null;\n-  private String sql;\n-  private SQLWarning warningChain = null;\n-  private boolean isClosed = false;\n-  private TSIService.Iface client = null;\n-  private List<String> columnInfoList; // no deduplication\n-  private List<String> columnTypeList; // no deduplication\n-  private Map<String, Integer> columnInfoMap; // used because the server returns deduplicated columns\n-  private List<String> columnTypeDeduplicatedList; // deduplicated from columnTypeList\n   private int rowsIndex = 0; // used to record the row index in current TSQueryDataSet\n-  private int fetchSize;\n-  private boolean emptyResultSet = false;\n   private boolean align = true;\n \n   private TSQueryDataSet tsQueryDataSet = null;\n   private byte[] time; // used to cache the current time value\n-  private byte[][] values; // used to cache the current row record value\n   private byte[] currentBitmap; // used to cache the current bitmap for every column\n   private static final int FLAG = 0x80; // used to do `and` operation with bitmap to judge whether the value is null\n \n-  private long sessionId;\n-  private long queryId;\n-  private boolean ignoreTimeStamp = false;\n-\n-  public IoTDBQueryResultSet() {\n-    // do nothing\n-  }\n \n   public IoTDBQueryResultSet(Statement statement, List<String> columnNameList,\n       List<String> columnTypeList, boolean ignoreTimeStamp, TSIService.Iface client,\n       String sql, long queryId, long sessionId, TSQueryDataSet dataset)\n       throws SQLException {\n-    this.statement = statement;\n-    this.fetchSize = statement.getFetchSize();\n-    this.columnTypeList = columnTypeList;\n-\n+    super(statement, columnNameList, columnTypeList, ignoreTimeStamp, client, sql, queryId, sessionId);\n     time = new byte[Long.BYTES];\n-    values = new byte[columnNameList.size()][];\n     currentBitmap = new byte[columnNameList.size()];\n-\n-    this.columnInfoList = new ArrayList<>();\n-    if(!ignoreTimeStamp) {\n-      this.columnInfoList.add(TIMESTAMP_STR);\n-    }\n-    // deduplicate and map\n-    this.columnInfoMap = new HashMap<>();\n-    if(!ignoreTimeStamp) {\n-      this.columnInfoMap.put(TIMESTAMP_STR, 1);\n-    }\n-    this.columnTypeDeduplicatedList = new ArrayList<>();\n-    int index = START_INDEX;\n-    for (int i = 0; i < columnNameList.size(); i++) {\n-      String name = columnNameList.get(i);\n-      columnInfoList.add(name);\n-      if (!columnInfoMap.containsKey(name)) {\n-        columnInfoMap.put(name, index++);\n-        columnTypeDeduplicatedList.add(columnTypeList.get(i));\n-      }\n-    }\n-\n-    this.ignoreTimeStamp = ignoreTimeStamp;\n-    this.client = client;\n-    this.sql = sql;\n-    this.queryId = queryId;\n     this.tsQueryDataSet = dataset;\n-    this.sessionId = sessionId;\n-  }\n-\n-  @Override\n-  public boolean isWrapperFor(Class<?> iface) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T unwrap(Class<T> iface) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean absolute(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void afterLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void beforeFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void cancelRowUpdates() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void clearWarnings() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void close() throws SQLException {\n-    if (isClosed) {\n-      return;\n-    }\n-    if (client != null) {\n-      try {\n-        TSCloseOperationReq closeReq = new TSCloseOperationReq(sessionId);\n-        closeReq.setQueryId(queryId);\n-        TSStatus closeResp = client.closeOperation(closeReq);\n-        RpcUtils.verifySuccess(closeResp);\n-      } catch (IoTDBRPCException e) {\n-        throw new SQLException(\"Error occurs for close opeation in server side becasuse \" + e);\n-      } catch (TException e) {\n-        throw new SQLException(\n-            \"Error occurs when connecting to server for close operation, becasue: \" + e);\n-      }\n-    }\n-    client = null;\n-    isClosed = true;\n-  }\n-\n-\n-  @Override\n-  public void deleteRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int findColumn(String columnName) {\n-    return columnInfoMap.get(columnName);\n-  }\n-\n-  @Override\n-  public boolean first() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Array getArray(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Array getArray(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getAsciiStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getAsciiStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(int columnIndex) throws SQLException {\n-    return getBigDecimal(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(String columnName) throws SQLException {\n-    return new BigDecimal(Objects.requireNonNull(getValueByName(columnName)));\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(int columnIndex, int scale) throws SQLException {\n-    MathContext mc = new MathContext(scale);\n-    return getBigDecimal(columnIndex).round(mc);\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(String columnName, int scale) throws SQLException {\n-    return getBigDecimal(findColumn(columnName), scale);\n-  }\n-\n-  @Override\n-  public InputStream getBinaryStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getBinaryStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Blob getBlob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Blob getBlob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n   }\n \n   @Override\n-  public boolean getBoolean(int columnIndex) throws SQLException {\n-    return getBoolean(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public boolean getBoolean(String columnName) throws SQLException {\n+  public long getLong(String columnName) throws SQLException {\n     checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToBool(values[index]);\n-    }\n-    else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+    if (columnName.equals(TIMESTAMP_STR)) {\n+      return BytesUtils.bytesToLong(time);\n     }\n-  }\n-\n-  @Override\n-  public byte getByte(int columnIndex) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte getByte(String columnName) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte[] getBytes(int columnIndex) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte[] getBytes(String columnName) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getCharacterStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getCharacterStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Clob getClob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Clob getClob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getConcurrency() {\n-    return ResultSet.CONCUR_READ_ONLY;\n-  }\n-\n-  @Override\n-  public String getCursorName() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Date getDate(int columnIndex) throws SQLException {\n-    return new Date(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Date getDate(String columnName) throws SQLException {\n-    return getDate(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Date getDate(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Date getDate(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public double getDouble(int columnIndex) throws SQLException {\n-    return getDouble(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public double getDouble(String columnName) throws SQLException {\n-    checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n+    int index = columnOrdinalMap.get(columnName) - START_INDEX;\n     if (values[index] != null) {\n-      return BytesUtils.bytesToDouble(values[index]);\n+      return BytesUtils.bytesToLong(values[index]);\n     } else {\n       throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n     }\n   }\n \n   @Override\n-  public int getFetchDirection() throws SQLException {\n-    return ResultSet.FETCH_FORWARD;\n-  }\n-\n-  @Override\n-  public void setFetchDirection(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getFetchSize() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n+  protected boolean fetchResults() throws SQLException {\n+    rowsIndex = 0;\n+    TSFetchResultsReq req = new TSFetchResultsReq(sessionId, sql, fetchSize, queryId, align);\n+    try {\n+      TSFetchResultsResp resp = client.fetchResults(req);\n \n-  @Override\n-  public void setFetchSize(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n+      try {\n+        RpcUtils.verifySuccess(resp.getStatus());\n+      } catch (IoTDBRPCException e) {\n+        throw new IoTDBSQLException(e.getMessage(), resp.getStatus());\n+      }\n+      if (!resp.hasResultSet) {\n+        emptyResultSet = true;\n+      } else {\n+        tsQueryDataSet = resp.getQueryDataSet();\n+      }\n+      return resp.hasResultSet;\n+    } catch (TException e) {\n+      throw new SQLException(\n+          \"Cannot fetch result from server, because of network connection: {} \", e);\n+    }\n   }\n \n   @Override\n-  public float getFloat(int columnIndex) throws SQLException {\n-    return getFloat(findColumnNameByIndex(columnIndex));\n+  protected boolean hasCachedResults() {\n+    return (tsQueryDataSet != null && tsQueryDataSet.time.hasRemaining());\n   }\n \n   @Override\n-  public float getFloat(String columnName) throws SQLException {\n-    checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToFloat(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+  protected void constructOneRow() {\n+    tsQueryDataSet.time.get(time);\n+    for (int i = 0; i < tsQueryDataSet.bitmapList.size(); i++) {\n+      ByteBuffer bitmapBuffer = tsQueryDataSet.bitmapList.get(i);\n+      // another new 8 row, should move the bitmap buffer position to next byte\n+      if (rowsIndex % 8 == 0) {\n+        currentBitmap[i] = bitmapBuffer.get();\n+      }\n+      values[i] = null;\n+      if (!isNull(i, rowsIndex)) {\n+        ByteBuffer valueBuffer = tsQueryDataSet.valueList.get(i);\n+        TSDataType dataType = columnTypeDeduplicatedList.get(i);\n+        switch (dataType) {\n+          case BOOLEAN:\n+            if (values[i] == null) {\n+              values[i] = new byte[1];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case INT32:\n+            if (values[i] == null) {\n+              values[i] = new byte[Integer.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case INT64:\n+            if (values[i] == null) {\n+              values[i] = new byte[Long.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case FLOAT:\n+            if (values[i] == null) {\n+              values[i] = new byte[Float.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case DOUBLE:\n+            if (values[i] == null) {\n+              values[i] = new byte[Double.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case TEXT:\n+            int length = valueBuffer.getInt();\n+            values[i] = ReadWriteIOUtils.readBytes(valueBuffer, length);\n+            break;\n+          default:\n+            throw new UnSupportedDataTypeException(\n+                String.format(\"Data type %s is not supported.\", columnTypeDeduplicatedList.get(i)));\n+        }\n+      }\n     }\n+    rowsIndex++;\n   }\n \n-  @Override\n-  public int getHoldability() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n+  /**\n+   * judge whether the specified column value is null in the current position\n+   *\n+   * @param index series index\n+   * @param rowNum current position\n+   */\n+  private boolean isNull(int index, int rowNum) {\n+    byte bitmap = currentBitmap[index];\n+    int shift = rowNum % 8;\n+    return ((FLAG >>> shift) & bitmap) == 0;\n   }\n \n   @Override\n-  public int getInt(int columnIndex) throws SQLException {\n-    return getInt(findColumnNameByIndex(columnIndex));\n+  protected void checkRecord() throws SQLException {\n+    if (Objects.isNull(tsQueryDataSet)) {\n+      throw new SQLException(\"No record remains\");\n+    }\n   }\n \n   @Override\n-  public int getInt(String columnName) throws SQLException {\n+  protected String getValueByName(String columnName) throws SQLException {\n     checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToInt(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+    if (columnName.equals(TIMESTAMP_STR)) {\n+      return String.valueOf(BytesUtils.bytesToLong(time));\n+    }\n+    int index = columnOrdinalMap.get(columnName) - START_INDEX;\n+    if (index < 0 || index >= values.length || values[index] == null) {\n+      return null;\n     }\n+    return getString(index, columnTypeDeduplicatedList.get(index), values);\n   }\n \n-  @Override\n-  public long getLong(int columnIndex) throws SQLException {\n-    return getLong(findColumnNameByIndex(columnIndex));\n+  public boolean isIgnoreTimeStamp() {\n+    return ignoreTimeStamp;\n   }\n \n-  @Override\n-  public long getLong(String columnName) throws SQLException {\n-    checkRecord();\n-    if (columnName.equals(TIMESTAMP_STR)) {\n-      return BytesUtils.bytesToLong(time);\n-    }\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToLong(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n-    }\n-  }\n-\n-  @Override\n-  public ResultSetMetaData getMetaData() {\n-    return new IoTDBResultMetadata(columnInfoList, columnTypeList, ignoreTimeStamp);\n-  }\n-\n-  @Override\n-  public Reader getNCharacterStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getNCharacterStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public NClob getNClob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public NClob getNClob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public String getNString(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public String getNString(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Object getObject(int columnIndex) throws SQLException {\n-    return getObject(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public Object getObject(String columnName) throws SQLException {\n-    return getValueByName(columnName);\n-  }\n-\n-  @Override\n-  public Object getObject(int arg0, Map<String, Class<?>> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Object getObject(String arg0, Map<String, Class<?>> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T getObject(int arg0, Class<T> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T getObject(String arg0, Class<T> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Ref getRef(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Ref getRef(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public RowId getRowId(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public RowId getRowId(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLXML getSQLXML(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLXML getSQLXML(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public short getShort(int columnIndex) throws SQLException {\n-    return getShort(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public short getShort(String columnName) throws SQLException {\n-    return Short.parseShort(Objects.requireNonNull(getValueByName(columnName)));\n-  }\n-\n-  @Override\n-  public Statement getStatement() {\n-    return this.statement;\n-  }\n-\n-  @Override\n-  public String getString(int columnIndex) throws SQLException {\n-    return getString(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public String getString(String columnName) throws SQLException {\n-    return getValueByName(columnName);\n-  }\n-\n-  @Override\n-  public Time getTime(int columnIndex) throws SQLException {\n-    return new Time(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Time getTime(String columnName) throws SQLException {\n-    return getTime(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Time getTime(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Time getTime(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(int columnIndex) throws SQLException {\n-    return new Timestamp(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(String columnName) throws SQLException {\n-    return getTimestamp(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getType() {\n-    return ResultSet.TYPE_FORWARD_ONLY;\n-  }\n-\n-  @Override\n-  public URL getURL(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public URL getURL(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getUnicodeStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getUnicodeStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLWarning getWarnings() {\n-    return warningChain;\n-  }\n-\n-  @Override\n-  public void insertRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isAfterLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isBeforeFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isClosed() {\n-    return isClosed;\n-  }\n-\n-  @Override\n-  public boolean isFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean last() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void moveToCurrentRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void moveToInsertRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean next() throws SQLException {\n-    if (hasCachedResults()) {\n-      constructOneRow();\n-      return true;\n-    }\n-    if (emptyResultSet) {\n-      return false;\n-    }\n-    if (fetchResults()) {\n-      constructOneRow();\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-\n-  /**\n-   * @return true means has results\n-   */\n-  private boolean fetchResults() throws SQLException {\n-    rowsIndex = 0;\n-    TSFetchResultsReq req = new TSFetchResultsReq(sessionId, sql, fetchSize, queryId, align);\n-    try {\n-      TSFetchResultsResp resp = client.fetchResults(req);\n-\n-      try {\n-        RpcUtils.verifySuccess(resp.getStatus());\n-      } catch (IoTDBRPCException e) {\n-        throw new IoTDBSQLException(e.getMessage(), resp.getStatus());\n-      }\n-      if (!resp.hasResultSet) {\n-        emptyResultSet = true;\n-      } else {\n-        tsQueryDataSet = resp.getQueryDataSet();\n-      }\n-      return resp.hasResultSet;\n-    } catch (TException e) {\n-      throw new SQLException(\n-          \"Cannot fetch result from server, because of network connection: {} \", e);\n-    }\n-  }\n-\n-  private boolean hasCachedResults() {\n-    return (tsQueryDataSet != null && tsQueryDataSet.time.hasRemaining());\n-  }\n-\n-  private void constructOneRow() {\n-    tsQueryDataSet.time.get(time);\n-    for (int i = 0; i < tsQueryDataSet.bitmapList.size(); i++) {\n-      ByteBuffer bitmapBuffer = tsQueryDataSet.bitmapList.get(i);\n-      // another new 8 row, should move the bitmap buffer position to next byte\n-      if (rowsIndex % 8 == 0) {\n-        currentBitmap[i] = bitmapBuffer.get();\n-      }\n-      values[i] = null;\n-      if (!isNull(i, rowsIndex)) {\n-        ByteBuffer valueBuffer = tsQueryDataSet.valueList.get(i);\n-        TSDataType dataType = TSDataType.valueOf(columnTypeDeduplicatedList.get(i));\n-        switch (dataType) {\n-          case BOOLEAN:\n-            if (values[i] == null) {\n-              values[i] = new byte[1];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case INT32:\n-            if (values[i] == null) {\n-              values[i] = new byte[Integer.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case INT64:\n-            if (values[i] == null) {\n-              values[i] = new byte[Long.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case FLOAT:\n-            if (values[i] == null) {\n-              values[i] = new byte[Float.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case DOUBLE:\n-            if (values[i] == null) {\n-              values[i] = new byte[Double.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case TEXT:\n-            int length = valueBuffer.getInt();\n-            values[i] = ReadWriteIOUtils.readBytes(valueBuffer, length);\n-            break;\n-          default:\n-            throw new UnSupportedDataTypeException(\n-                String.format(\"Data type %s is not supported.\", columnTypeDeduplicatedList.get(i)));\n-        }\n-      }\n-    }\n-    rowsIndex++;\n-  }\n-\n-  /**\n-   * judge whether the specified column value is null in the current position\n-   *\n-   * @param index series index\n-   * @param rowNum current position\n-   */\n-  private boolean isNull(int index, int rowNum) {\n-    byte bitmap = currentBitmap[index];\n-    int shift = rowNum % 8;\n-    return ((FLAG >>> shift) & bitmap) == 0;\n-  }\n-\n-  @Override\n-  public boolean previous() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void refreshRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean relative(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowDeleted() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowInserted() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowUpdated() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateArray(int arg0, Array arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateArray(String arg0, Array arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBigDecimal(int arg0, BigDecimal arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBigDecimal(String arg0, BigDecimal arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, Blob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, Blob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBoolean(int arg0, boolean arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBoolean(String arg0, boolean arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateByte(int arg0, byte arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateByte(String arg0, byte arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBytes(int arg0, byte[] arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBytes(String arg0, byte[] arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Clob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Clob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDate(int arg0, Date arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDate(String arg0, Date arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDouble(int arg0, double arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDouble(String arg0, double arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateFloat(int arg0, float arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateFloat(String arg0, float arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateInt(int arg0, int arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateInt(String arg0, int arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateLong(int arg0, long arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateLong(String arg0, long arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, NClob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, NClob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNString(int arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNString(String arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNull(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNull(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(int arg0, Object arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(String arg0, Object arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(int arg0, Object arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(String arg0, Object arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRef(int arg0, Ref arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRef(String arg0, Ref arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRowId(int arg0, RowId arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRowId(String arg0, RowId arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateSQLXML(int arg0, SQLXML arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateSQLXML(String arg0, SQLXML arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateShort(int arg0, short arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateShort(String arg0, short arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateString(int arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateString(String arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTime(int arg0, Time arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTime(String arg0, Time arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTimestamp(int arg0, Timestamp arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTimestamp(String arg0, Timestamp arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean wasNull() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  private void checkRecord() throws SQLException {\n-    if (Objects.isNull(tsQueryDataSet)) {\n-      throw new SQLException(\"No record remains\");\n-    }\n-  }\n-\n-  private String findColumnNameByIndex(int columnIndex) throws SQLException {\n-    if (columnIndex <= 0) {\n-      throw new SQLException(\"column index should start from 1\");\n-    }\n-    if (columnIndex > columnInfoList.size()) {\n-      throw new SQLException(\n-          String.format(\"column index %d out of range %d\", columnIndex, columnInfoList.size()));\n-    }\n-    return columnInfoList.get(columnIndex - 1);\n-  }\n-\n-  private String getValueByName(String columnName) throws SQLException {\n-    checkRecord();\n-    if (columnName.equals(TIMESTAMP_STR)) {\n-      return String.valueOf(BytesUtils.bytesToLong(time));\n-    }\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (index < 0 || index >= values.length || values[index] == null) {\n-      return null;\n-    }\n-    TSDataType dataType = TSDataType.valueOf(columnTypeDeduplicatedList.get(index));\n-    switch (dataType) {\n-      case BOOLEAN:\n-        return String.valueOf(BytesUtils.bytesToBool(values[index]));\n-      case INT32:\n-        return String.valueOf(BytesUtils.bytesToInt(values[index]));\n-      case INT64:\n-        return String.valueOf(BytesUtils.bytesToLong(values[index]));\n-      case FLOAT:\n-        return String.valueOf(BytesUtils.bytesToFloat(values[index]));\n-      case DOUBLE:\n-        return String.valueOf(BytesUtils.bytesToDouble(values[index]));\n-      case TEXT:\n-        return new String(values[index]);\n-      default:\n-        return null;\n-    }\n-  }\n-\n-  public boolean isIgnoreTimeStamp() {\n-    return ignoreTimeStamp;\n-  }\n-\n-  public void setIgnoreTimeStamp(boolean ignoreTimeStamp) {\n-    this.ignoreTimeStamp = ignoreTimeStamp;\n-  }\n   \n   public boolean isAlign() {\n     return align;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE3NjAwMw==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r366176003", "bodyText": "remove unused field", "author": "samperson1997", "createdAt": "2020-01-14T06:56:00Z", "path": "jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java", "diffHunk": "@@ -41,8 +41,10 @@\n public class IoTDBQueryResultSet implements ResultSet {\n \n   private static final String TIMESTAMP_STR = \"Time\";\n+  private static final int TIMESTAMP_STR_LENGTH = 4;\n   private static final int START_INDEX = 2;\n   private static final String VALUE_IS_NULL = \"The value got by %s (column name) is NULL.\";\n+  private static final String EMPTY_STR = \"\";", "originalCommit": "ad08f92972fd1c7c536a8e9644e9fcb4858e069c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java b/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\nindex eaa5993adf..d0f466ec20 100644\n--- a/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\n+++ b/jdbc/src/main/java/org/apache/iotdb/jdbc/IoTDBQueryResultSet.java\n\n@@ -21,1269 +21,181 @@ package org.apache.iotdb.jdbc;\n \n import org.apache.iotdb.rpc.IoTDBRPCException;\n import org.apache.iotdb.rpc.RpcUtils;\n-import org.apache.iotdb.service.rpc.thrift.*;\n+import org.apache.iotdb.service.rpc.thrift.TSFetchResultsReq;\n+import org.apache.iotdb.service.rpc.thrift.TSFetchResultsResp;\n+import org.apache.iotdb.service.rpc.thrift.TSIService;\n+import org.apache.iotdb.service.rpc.thrift.TSQueryDataSet;\n import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n import org.apache.iotdb.tsfile.utils.BytesUtils;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.apache.thrift.TException;\n \n-import java.io.InputStream;\n-import java.io.Reader;\n-import java.math.BigDecimal;\n-import java.math.MathContext;\n-import java.net.URL;\n import java.nio.ByteBuffer;\n-import java.sql.Date;\n-import java.sql.*;\n-import java.util.*;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Objects;\n \n-public class IoTDBQueryResultSet implements ResultSet {\n+public class IoTDBQueryResultSet extends AbstractIoTDBResultSet {\n \n-  private static final String TIMESTAMP_STR = \"Time\";\n-  private static final int TIMESTAMP_STR_LENGTH = 4;\n   private static final int START_INDEX = 2;\n   private static final String VALUE_IS_NULL = \"The value got by %s (column name) is NULL.\";\n-  private static final String EMPTY_STR = \"\";\n-  private Statement statement = null;\n-  private String sql;\n-  private SQLWarning warningChain = null;\n-  private boolean isClosed = false;\n-  private TSIService.Iface client = null;\n-  private List<String> columnInfoList; // no deduplication\n-  private List<String> columnTypeList; // no deduplication\n-  private Map<String, Integer> columnInfoMap; // used because the server returns deduplicated columns\n-  private List<String> columnTypeDeduplicatedList; // deduplicated from columnTypeList\n   private int rowsIndex = 0; // used to record the row index in current TSQueryDataSet\n-  private int fetchSize;\n-  private boolean emptyResultSet = false;\n   private boolean align = true;\n \n   private TSQueryDataSet tsQueryDataSet = null;\n   private byte[] time; // used to cache the current time value\n-  private byte[][] values; // used to cache the current row record value\n   private byte[] currentBitmap; // used to cache the current bitmap for every column\n   private static final int FLAG = 0x80; // used to do `and` operation with bitmap to judge whether the value is null\n \n-  private long sessionId;\n-  private long queryId;\n-  private boolean ignoreTimeStamp = false;\n-\n-  public IoTDBQueryResultSet() {\n-    // do nothing\n-  }\n \n   public IoTDBQueryResultSet(Statement statement, List<String> columnNameList,\n       List<String> columnTypeList, boolean ignoreTimeStamp, TSIService.Iface client,\n       String sql, long queryId, long sessionId, TSQueryDataSet dataset)\n       throws SQLException {\n-    this.statement = statement;\n-    this.fetchSize = statement.getFetchSize();\n-    this.columnTypeList = columnTypeList;\n-\n+    super(statement, columnNameList, columnTypeList, ignoreTimeStamp, client, sql, queryId, sessionId);\n     time = new byte[Long.BYTES];\n-    values = new byte[columnNameList.size()][];\n     currentBitmap = new byte[columnNameList.size()];\n-\n-    this.columnInfoList = new ArrayList<>();\n-    if(!ignoreTimeStamp) {\n-      this.columnInfoList.add(TIMESTAMP_STR);\n-    }\n-    // deduplicate and map\n-    this.columnInfoMap = new HashMap<>();\n-    if(!ignoreTimeStamp) {\n-      this.columnInfoMap.put(TIMESTAMP_STR, 1);\n-    }\n-    this.columnTypeDeduplicatedList = new ArrayList<>();\n-    int index = START_INDEX;\n-    for (int i = 0; i < columnNameList.size(); i++) {\n-      String name = columnNameList.get(i);\n-      columnInfoList.add(name);\n-      if (!columnInfoMap.containsKey(name)) {\n-        columnInfoMap.put(name, index++);\n-        columnTypeDeduplicatedList.add(columnTypeList.get(i));\n-      }\n-    }\n-\n-    this.ignoreTimeStamp = ignoreTimeStamp;\n-    this.client = client;\n-    this.sql = sql;\n-    this.queryId = queryId;\n     this.tsQueryDataSet = dataset;\n-    this.sessionId = sessionId;\n-  }\n-\n-  @Override\n-  public boolean isWrapperFor(Class<?> iface) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T unwrap(Class<T> iface) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean absolute(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void afterLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void beforeFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void cancelRowUpdates() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void clearWarnings() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void close() throws SQLException {\n-    if (isClosed) {\n-      return;\n-    }\n-    if (client != null) {\n-      try {\n-        TSCloseOperationReq closeReq = new TSCloseOperationReq(sessionId);\n-        closeReq.setQueryId(queryId);\n-        TSStatus closeResp = client.closeOperation(closeReq);\n-        RpcUtils.verifySuccess(closeResp);\n-      } catch (IoTDBRPCException e) {\n-        throw new SQLException(\"Error occurs for close opeation in server side becasuse \" + e);\n-      } catch (TException e) {\n-        throw new SQLException(\n-            \"Error occurs when connecting to server for close operation, becasue: \" + e);\n-      }\n-    }\n-    client = null;\n-    isClosed = true;\n-  }\n-\n-\n-  @Override\n-  public void deleteRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int findColumn(String columnName) {\n-    return columnInfoMap.get(columnName);\n-  }\n-\n-  @Override\n-  public boolean first() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Array getArray(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Array getArray(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getAsciiStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getAsciiStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(int columnIndex) throws SQLException {\n-    return getBigDecimal(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(String columnName) throws SQLException {\n-    return new BigDecimal(Objects.requireNonNull(getValueByName(columnName)));\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(int columnIndex, int scale) throws SQLException {\n-    MathContext mc = new MathContext(scale);\n-    return getBigDecimal(columnIndex).round(mc);\n-  }\n-\n-  @Override\n-  public BigDecimal getBigDecimal(String columnName, int scale) throws SQLException {\n-    return getBigDecimal(findColumn(columnName), scale);\n-  }\n-\n-  @Override\n-  public InputStream getBinaryStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getBinaryStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Blob getBlob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Blob getBlob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n   }\n \n   @Override\n-  public boolean getBoolean(int columnIndex) throws SQLException {\n-    return getBoolean(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public boolean getBoolean(String columnName) throws SQLException {\n+  public long getLong(String columnName) throws SQLException {\n     checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToBool(values[index]);\n-    }\n-    else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+    if (columnName.equals(TIMESTAMP_STR)) {\n+      return BytesUtils.bytesToLong(time);\n     }\n-  }\n-\n-  @Override\n-  public byte getByte(int columnIndex) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte getByte(String columnName) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte[] getBytes(int columnIndex) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public byte[] getBytes(String columnName) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getCharacterStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getCharacterStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Clob getClob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Clob getClob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getConcurrency() {\n-    return ResultSet.CONCUR_READ_ONLY;\n-  }\n-\n-  @Override\n-  public String getCursorName() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Date getDate(int columnIndex) throws SQLException {\n-    return new Date(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Date getDate(String columnName) throws SQLException {\n-    return getDate(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Date getDate(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Date getDate(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public double getDouble(int columnIndex) throws SQLException {\n-    return getDouble(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public double getDouble(String columnName) throws SQLException {\n-    checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n+    int index = columnOrdinalMap.get(columnName) - START_INDEX;\n     if (values[index] != null) {\n-      return BytesUtils.bytesToDouble(values[index]);\n+      return BytesUtils.bytesToLong(values[index]);\n     } else {\n       throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n     }\n   }\n \n   @Override\n-  public int getFetchDirection() throws SQLException {\n-    return ResultSet.FETCH_FORWARD;\n-  }\n-\n-  @Override\n-  public void setFetchDirection(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getFetchSize() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n+  protected boolean fetchResults() throws SQLException {\n+    rowsIndex = 0;\n+    TSFetchResultsReq req = new TSFetchResultsReq(sessionId, sql, fetchSize, queryId, align);\n+    try {\n+      TSFetchResultsResp resp = client.fetchResults(req);\n \n-  @Override\n-  public void setFetchSize(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n+      try {\n+        RpcUtils.verifySuccess(resp.getStatus());\n+      } catch (IoTDBRPCException e) {\n+        throw new IoTDBSQLException(e.getMessage(), resp.getStatus());\n+      }\n+      if (!resp.hasResultSet) {\n+        emptyResultSet = true;\n+      } else {\n+        tsQueryDataSet = resp.getQueryDataSet();\n+      }\n+      return resp.hasResultSet;\n+    } catch (TException e) {\n+      throw new SQLException(\n+          \"Cannot fetch result from server, because of network connection: {} \", e);\n+    }\n   }\n \n   @Override\n-  public float getFloat(int columnIndex) throws SQLException {\n-    return getFloat(findColumnNameByIndex(columnIndex));\n+  protected boolean hasCachedResults() {\n+    return (tsQueryDataSet != null && tsQueryDataSet.time.hasRemaining());\n   }\n \n   @Override\n-  public float getFloat(String columnName) throws SQLException {\n-    checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToFloat(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+  protected void constructOneRow() {\n+    tsQueryDataSet.time.get(time);\n+    for (int i = 0; i < tsQueryDataSet.bitmapList.size(); i++) {\n+      ByteBuffer bitmapBuffer = tsQueryDataSet.bitmapList.get(i);\n+      // another new 8 row, should move the bitmap buffer position to next byte\n+      if (rowsIndex % 8 == 0) {\n+        currentBitmap[i] = bitmapBuffer.get();\n+      }\n+      values[i] = null;\n+      if (!isNull(i, rowsIndex)) {\n+        ByteBuffer valueBuffer = tsQueryDataSet.valueList.get(i);\n+        TSDataType dataType = columnTypeDeduplicatedList.get(i);\n+        switch (dataType) {\n+          case BOOLEAN:\n+            if (values[i] == null) {\n+              values[i] = new byte[1];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case INT32:\n+            if (values[i] == null) {\n+              values[i] = new byte[Integer.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case INT64:\n+            if (values[i] == null) {\n+              values[i] = new byte[Long.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case FLOAT:\n+            if (values[i] == null) {\n+              values[i] = new byte[Float.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case DOUBLE:\n+            if (values[i] == null) {\n+              values[i] = new byte[Double.BYTES];\n+            }\n+            valueBuffer.get(values[i]);\n+            break;\n+          case TEXT:\n+            int length = valueBuffer.getInt();\n+            values[i] = ReadWriteIOUtils.readBytes(valueBuffer, length);\n+            break;\n+          default:\n+            throw new UnSupportedDataTypeException(\n+                String.format(\"Data type %s is not supported.\", columnTypeDeduplicatedList.get(i)));\n+        }\n+      }\n     }\n+    rowsIndex++;\n   }\n \n-  @Override\n-  public int getHoldability() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n+  /**\n+   * judge whether the specified column value is null in the current position\n+   *\n+   * @param index series index\n+   * @param rowNum current position\n+   */\n+  private boolean isNull(int index, int rowNum) {\n+    byte bitmap = currentBitmap[index];\n+    int shift = rowNum % 8;\n+    return ((FLAG >>> shift) & bitmap) == 0;\n   }\n \n   @Override\n-  public int getInt(int columnIndex) throws SQLException {\n-    return getInt(findColumnNameByIndex(columnIndex));\n+  protected void checkRecord() throws SQLException {\n+    if (Objects.isNull(tsQueryDataSet)) {\n+      throw new SQLException(\"No record remains\");\n+    }\n   }\n \n   @Override\n-  public int getInt(String columnName) throws SQLException {\n+  protected String getValueByName(String columnName) throws SQLException {\n     checkRecord();\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToInt(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n+    if (columnName.equals(TIMESTAMP_STR)) {\n+      return String.valueOf(BytesUtils.bytesToLong(time));\n+    }\n+    int index = columnOrdinalMap.get(columnName) - START_INDEX;\n+    if (index < 0 || index >= values.length || values[index] == null) {\n+      return null;\n     }\n+    return getString(index, columnTypeDeduplicatedList.get(index), values);\n   }\n \n-  @Override\n-  public long getLong(int columnIndex) throws SQLException {\n-    return getLong(findColumnNameByIndex(columnIndex));\n+  public boolean isIgnoreTimeStamp() {\n+    return ignoreTimeStamp;\n   }\n \n-  @Override\n-  public long getLong(String columnName) throws SQLException {\n-    checkRecord();\n-    if (columnName.equals(TIMESTAMP_STR)) {\n-      return BytesUtils.bytesToLong(time);\n-    }\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (values[index] != null) {\n-      return BytesUtils.bytesToLong(values[index]);\n-    } else {\n-      throw new SQLException(String.format(VALUE_IS_NULL, columnName));\n-    }\n-  }\n-\n-  @Override\n-  public ResultSetMetaData getMetaData() {\n-    return new IoTDBResultMetadata(columnInfoList, columnTypeList, ignoreTimeStamp);\n-  }\n-\n-  @Override\n-  public Reader getNCharacterStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Reader getNCharacterStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public NClob getNClob(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public NClob getNClob(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public String getNString(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public String getNString(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Object getObject(int columnIndex) throws SQLException {\n-    return getObject(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public Object getObject(String columnName) throws SQLException {\n-    return getValueByName(columnName);\n-  }\n-\n-  @Override\n-  public Object getObject(int arg0, Map<String, Class<?>> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Object getObject(String arg0, Map<String, Class<?>> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T getObject(int arg0, Class<T> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public <T> T getObject(String arg0, Class<T> arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Ref getRef(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Ref getRef(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public RowId getRowId(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public RowId getRowId(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLXML getSQLXML(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLXML getSQLXML(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public short getShort(int columnIndex) throws SQLException {\n-    return getShort(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public short getShort(String columnName) throws SQLException {\n-    return Short.parseShort(Objects.requireNonNull(getValueByName(columnName)));\n-  }\n-\n-  @Override\n-  public Statement getStatement() {\n-    return this.statement;\n-  }\n-\n-  @Override\n-  public String getString(int columnIndex) throws SQLException {\n-    return getString(findColumnNameByIndex(columnIndex));\n-  }\n-\n-  @Override\n-  public String getString(String columnName) throws SQLException {\n-    return getValueByName(columnName);\n-  }\n-\n-  @Override\n-  public Time getTime(int columnIndex) throws SQLException {\n-    return new Time(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Time getTime(String columnName) throws SQLException {\n-    return getTime(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Time getTime(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Time getTime(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(int columnIndex) throws SQLException {\n-    return new Timestamp(getLong(columnIndex));\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(String columnName) throws SQLException {\n-    return getTimestamp(findColumn(columnName));\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(int arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public Timestamp getTimestamp(String arg0, Calendar arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public int getType() {\n-    return ResultSet.TYPE_FORWARD_ONLY;\n-  }\n-\n-  @Override\n-  public URL getURL(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public URL getURL(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getUnicodeStream(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public InputStream getUnicodeStream(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public SQLWarning getWarnings() {\n-    return warningChain;\n-  }\n-\n-  @Override\n-  public void insertRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isAfterLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isBeforeFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isClosed() {\n-    return isClosed;\n-  }\n-\n-  @Override\n-  public boolean isFirst() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean isLast() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean last() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void moveToCurrentRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void moveToInsertRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean next() throws SQLException {\n-    if (hasCachedResults()) {\n-      constructOneRow();\n-      return true;\n-    }\n-    if (emptyResultSet) {\n-      return false;\n-    }\n-    if (fetchResults()) {\n-      constructOneRow();\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-\n-  /**\n-   * @return true means has results\n-   */\n-  private boolean fetchResults() throws SQLException {\n-    rowsIndex = 0;\n-    TSFetchResultsReq req = new TSFetchResultsReq(sessionId, sql, fetchSize, queryId, align);\n-    try {\n-      TSFetchResultsResp resp = client.fetchResults(req);\n-\n-      try {\n-        RpcUtils.verifySuccess(resp.getStatus());\n-      } catch (IoTDBRPCException e) {\n-        throw new IoTDBSQLException(e.getMessage(), resp.getStatus());\n-      }\n-      if (!resp.hasResultSet) {\n-        emptyResultSet = true;\n-      } else {\n-        tsQueryDataSet = resp.getQueryDataSet();\n-      }\n-      return resp.hasResultSet;\n-    } catch (TException e) {\n-      throw new SQLException(\n-          \"Cannot fetch result from server, because of network connection: {} \", e);\n-    }\n-  }\n-\n-  private boolean hasCachedResults() {\n-    return (tsQueryDataSet != null && tsQueryDataSet.time.hasRemaining());\n-  }\n-\n-  private void constructOneRow() {\n-    tsQueryDataSet.time.get(time);\n-    for (int i = 0; i < tsQueryDataSet.bitmapList.size(); i++) {\n-      ByteBuffer bitmapBuffer = tsQueryDataSet.bitmapList.get(i);\n-      // another new 8 row, should move the bitmap buffer position to next byte\n-      if (rowsIndex % 8 == 0) {\n-        currentBitmap[i] = bitmapBuffer.get();\n-      }\n-      values[i] = null;\n-      if (!isNull(i, rowsIndex)) {\n-        ByteBuffer valueBuffer = tsQueryDataSet.valueList.get(i);\n-        TSDataType dataType = TSDataType.valueOf(columnTypeDeduplicatedList.get(i));\n-        switch (dataType) {\n-          case BOOLEAN:\n-            if (values[i] == null) {\n-              values[i] = new byte[1];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case INT32:\n-            if (values[i] == null) {\n-              values[i] = new byte[Integer.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case INT64:\n-            if (values[i] == null) {\n-              values[i] = new byte[Long.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case FLOAT:\n-            if (values[i] == null) {\n-              values[i] = new byte[Float.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case DOUBLE:\n-            if (values[i] == null) {\n-              values[i] = new byte[Double.BYTES];\n-            }\n-            valueBuffer.get(values[i]);\n-            break;\n-          case TEXT:\n-            int length = valueBuffer.getInt();\n-            values[i] = ReadWriteIOUtils.readBytes(valueBuffer, length);\n-            break;\n-          default:\n-            throw new UnSupportedDataTypeException(\n-                String.format(\"Data type %s is not supported.\", columnTypeDeduplicatedList.get(i)));\n-        }\n-      }\n-    }\n-    rowsIndex++;\n-  }\n-\n-  /**\n-   * judge whether the specified column value is null in the current position\n-   *\n-   * @param index series index\n-   * @param rowNum current position\n-   */\n-  private boolean isNull(int index, int rowNum) {\n-    byte bitmap = currentBitmap[index];\n-    int shift = rowNum % 8;\n-    return ((FLAG >>> shift) & bitmap) == 0;\n-  }\n-\n-  @Override\n-  public boolean previous() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void refreshRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean relative(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowDeleted() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowInserted() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean rowUpdated() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateArray(int arg0, Array arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateArray(String arg0, Array arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateAsciiStream(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBigDecimal(int arg0, BigDecimal arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBigDecimal(String arg0, BigDecimal arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBinaryStream(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, Blob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, Blob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, InputStream arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(int arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBlob(String arg0, InputStream arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBoolean(int arg0, boolean arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBoolean(String arg0, boolean arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateByte(int arg0, byte arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateByte(String arg0, byte arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBytes(int arg0, byte[] arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateBytes(String arg0, byte[] arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateCharacterStream(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Clob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Clob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateClob(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-\n-  }\n-\n-  @Override\n-  public void updateClob(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDate(int arg0, Date arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDate(String arg0, Date arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDouble(int arg0, double arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateDouble(String arg0, double arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateFloat(int arg0, float arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateFloat(String arg0, float arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateInt(int arg0, int arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateInt(String arg0, int arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateLong(int arg0, long arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateLong(String arg0, long arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNCharacterStream(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, NClob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, NClob arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, Reader arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(int arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNClob(String arg0, Reader arg1, long arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNString(int arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNString(String arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNull(int arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateNull(String arg0) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(int arg0, Object arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(String arg0, Object arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(int arg0, Object arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateObject(String arg0, Object arg1, int arg2) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRef(int arg0, Ref arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRef(String arg0, Ref arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRow() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRowId(int arg0, RowId arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateRowId(String arg0, RowId arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateSQLXML(int arg0, SQLXML arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateSQLXML(String arg0, SQLXML arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateShort(int arg0, short arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateShort(String arg0, short arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateString(int arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateString(String arg0, String arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTime(int arg0, Time arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTime(String arg0, Time arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTimestamp(int arg0, Timestamp arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public void updateTimestamp(String arg0, Timestamp arg1) throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  @Override\n-  public boolean wasNull() throws SQLException {\n-    throw new SQLException(Constant.METHOD_NOT_SUPPORTED);\n-  }\n-\n-  private void checkRecord() throws SQLException {\n-    if (Objects.isNull(tsQueryDataSet)) {\n-      throw new SQLException(\"No record remains\");\n-    }\n-  }\n-\n-  private String findColumnNameByIndex(int columnIndex) throws SQLException {\n-    if (columnIndex <= 0) {\n-      throw new SQLException(\"column index should start from 1\");\n-    }\n-    if (columnIndex > columnInfoList.size()) {\n-      throw new SQLException(\n-          String.format(\"column index %d out of range %d\", columnIndex, columnInfoList.size()));\n-    }\n-    return columnInfoList.get(columnIndex - 1);\n-  }\n-\n-  private String getValueByName(String columnName) throws SQLException {\n-    checkRecord();\n-    if (columnName.equals(TIMESTAMP_STR)) {\n-      return String.valueOf(BytesUtils.bytesToLong(time));\n-    }\n-    int index = columnInfoMap.get(columnName) - START_INDEX;\n-    if (index < 0 || index >= values.length || values[index] == null) {\n-      return null;\n-    }\n-    TSDataType dataType = TSDataType.valueOf(columnTypeDeduplicatedList.get(index));\n-    switch (dataType) {\n-      case BOOLEAN:\n-        return String.valueOf(BytesUtils.bytesToBool(values[index]));\n-      case INT32:\n-        return String.valueOf(BytesUtils.bytesToInt(values[index]));\n-      case INT64:\n-        return String.valueOf(BytesUtils.bytesToLong(values[index]));\n-      case FLOAT:\n-        return String.valueOf(BytesUtils.bytesToFloat(values[index]));\n-      case DOUBLE:\n-        return String.valueOf(BytesUtils.bytesToDouble(values[index]));\n-      case TEXT:\n-        return new String(values[index]);\n-      default:\n-        return null;\n-    }\n-  }\n-\n-  public boolean isIgnoreTimeStamp() {\n-    return ignoreTimeStamp;\n-  }\n-\n-  public void setIgnoreTimeStamp(boolean ignoreTimeStamp) {\n-    this.ignoreTimeStamp = ignoreTimeStamp;\n-  }\n   \n   public boolean isAlign() {\n     return align;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE3ODUwNQ==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r366178505", "bodyText": "remove unused import", "author": "samperson1997", "createdAt": "2020-01-14T07:06:15Z", "path": "server/src/main/java/org/apache/iotdb/db/query/executor/EngineExecutor.java", "diffHunk": "@@ -19,14 +19,19 @@\n package org.apache.iotdb.db.query.executor;\n \n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n import org.apache.iotdb.db.exception.StorageEngineException;\n import org.apache.iotdb.db.query.context.QueryContext;\n import org.apache.iotdb.db.query.dataset.EngineDataSetWithValueFilter;\n import org.apache.iotdb.db.query.dataset.NewEngineDataSetWithoutValueFilter;\n+import org.apache.iotdb.db.query.dataset.NonAlignEngineDataSet;\n import org.apache.iotdb.db.query.reader.IReaderByTimestamp;\n import org.apache.iotdb.db.query.reader.ManagedSeriesReader;\n+import org.apache.iotdb.db.query.reader.seriesRelated.RawDataReaderWithoutValueFilter;\n+import org.apache.iotdb.db.query.reader.seriesRelated.SeriesDataReaderWithoutValueFilter;", "originalCommit": "ad08f92972fd1c7c536a8e9644e9fcb4858e069c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/executor/EngineExecutor.java b/server/src/main/java/org/apache/iotdb/db/query/executor/RawDataQueryExecutor.java\nsimilarity index 59%\nrename from server/src/main/java/org/apache/iotdb/db/query/executor/EngineExecutor.java\nrename to server/src/main/java/org/apache/iotdb/db/query/executor/RawDataQueryExecutor.java\nindex 6fd1e90edc..6315a72989 100644\n--- a/server/src/main/java/org/apache/iotdb/db/query/executor/EngineExecutor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/query/executor/RawDataQueryExecutor.java\n\n@@ -18,20 +18,19 @@\n  */\n package org.apache.iotdb.db.query.executor;\n \n-\n-import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n import org.apache.iotdb.db.exception.StorageEngineException;\n+import org.apache.iotdb.db.qp.physical.crud.RawDataQueryPlan;\n import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.db.query.control.QueryResourceManager;\n import org.apache.iotdb.db.query.dataset.EngineDataSetWithValueFilter;\n-import org.apache.iotdb.db.query.dataset.NewEngineDataSetWithoutValueFilter;\n import org.apache.iotdb.db.query.dataset.NonAlignEngineDataSet;\n-import org.apache.iotdb.db.query.reader.IReaderByTimestamp;\n-import org.apache.iotdb.db.query.reader.ManagedSeriesReader;\n-import org.apache.iotdb.db.query.reader.seriesRelated.RawDataReaderWithoutValueFilter;\n-import org.apache.iotdb.db.query.reader.seriesRelated.SeriesDataReaderWithoutValueFilter;\n-import org.apache.iotdb.db.query.reader.seriesRelated.SeriesReaderByTimestamp;\n+import org.apache.iotdb.db.query.dataset.RawQueryDataSetWithoutValueFilter;\n+import org.apache.iotdb.db.query.reader.series.IReaderByTimestamp;\n+import org.apache.iotdb.db.query.reader.series.ManagedSeriesReader;\n+import org.apache.iotdb.db.query.reader.series.SeriesRawDataBatchReader;\n+import org.apache.iotdb.db.query.reader.series.SeriesReaderByTimestamp;\n import org.apache.iotdb.db.query.timegenerator.EngineTimeGenerator;\n import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n import org.apache.iotdb.tsfile.read.common.Path;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE4OTEyNA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r366189124", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class AggreResultData {\n          \n          \n            \n             public class AggrResultData {", "author": "samperson1997", "createdAt": "2020-01-14T07:45:09Z", "path": "server/src/main/java/org/apache/iotdb/db/query/aggregation/AggreResultData.java", "diffHunk": "@@ -25,7 +25,6 @@\n \n public class AggreResultData {", "originalCommit": "bd4e66feb3f1a3b5c79dbff199cb1c06aeddccd2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggreResultData.java b/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggreResultData.java\ndeleted file mode 100644\nindex 966174abeb..0000000000\n--- a/server/src/main/java/org/apache/iotdb/db/query/aggregation/AggreResultData.java\n+++ /dev/null\n\n@@ -1,167 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.iotdb.db.query.aggregation;\n-\n-import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n-import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n-import org.apache.iotdb.tsfile.utils.Binary;\n-\n-public class AggreResultData {\n-\n-  private TSDataType dataType;\n-\n-  private boolean booleanRet;\n-  private int intRet;\n-  private long longRet;\n-  private float floatRet;\n-  private double doubleRet;\n-  private Binary binaryRet;\n-\n-  private boolean hasResult;\n-\n-  public AggreResultData(TSDataType dataType) {\n-    this.dataType = dataType;\n-    this.hasResult = false;\n-  }\n-\n-  public void reset() {\n-    hasResult = false;\n-  }\n-\n-  public Object getValue() {\n-    switch (dataType) {\n-      case BOOLEAN:\n-        return booleanRet;\n-      case DOUBLE:\n-        return doubleRet;\n-      case TEXT:\n-        return binaryRet;\n-      case FLOAT:\n-        return floatRet;\n-      case INT32:\n-        return intRet;\n-      case INT64:\n-        return longRet;\n-      default:\n-        throw new UnSupportedDataTypeException(String.valueOf(dataType));\n-    }\n-  }\n-\n-  /**\n-   * set an object.\n-   *\n-   * @param v object value\n-   */\n-  public void setValue(Object v) {\n-    hasResult = true;\n-    switch (dataType) {\n-      case BOOLEAN:\n-        booleanRet = (Boolean) v;\n-        break;\n-      case DOUBLE:\n-        doubleRet = (Double) v;\n-        break;\n-      case TEXT:\n-        binaryRet = (Binary) v;\n-        break;\n-      case FLOAT:\n-        floatRet = (Float) v;\n-        break;\n-      case INT32:\n-        intRet = (Integer) v;\n-        break;\n-      case INT64:\n-        longRet = (Long) v;\n-        break;\n-      default:\n-        throw new UnSupportedDataTypeException(String.valueOf(dataType));\n-    }\n-  }\n-\n-  public TSDataType getDataType() {\n-    return dataType;\n-  }\n-\n-  public boolean isBooleanRet() {\n-    return booleanRet;\n-  }\n-\n-  public void setBooleanRet(boolean booleanRet) {\n-    this.hasResult = true;\n-    this.booleanRet = booleanRet;\n-  }\n-\n-  public int getIntRet() {\n-    return intRet;\n-  }\n-\n-  public void setIntRet(int intRet) {\n-    this.hasResult = true;\n-    this.intRet = intRet;\n-  }\n-\n-  public long getLongRet() {\n-    return longRet;\n-  }\n-\n-  public void setLongRet(long longRet) {\n-    this.hasResult = true;\n-    this.longRet = longRet;\n-  }\n-\n-  public float getFloatRet() {\n-    return floatRet;\n-  }\n-\n-  public void setFloatRet(float floatRet) {\n-    this.hasResult = true;\n-    this.floatRet = floatRet;\n-  }\n-\n-  public double getDoubleRet() {\n-    return doubleRet;\n-  }\n-\n-  public void setDoubleRet(double doubleRet) {\n-    this.hasResult = true;\n-    this.doubleRet = doubleRet;\n-  }\n-\n-  public Binary getBinaryRet() {\n-    return binaryRet;\n-  }\n-\n-  public void setBinaryRet(Binary binaryRet) {\n-    this.hasResult = true;\n-    this.binaryRet = binaryRet;\n-  }\n-\n-  public boolean hasResult() {\n-    return hasResult;\n-  }\n-\n-  public AggreResultData deepCopy() {\n-    AggreResultData aggreResultData = new AggreResultData(this.dataType);\n-    if (hasResult) {\n-      aggreResultData.setValue(this.getValue());\n-    }\n-    return aggreResultData;\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE5MDcxNA==", "url": "https://github.com/apache/iotdb/pull/713#discussion_r366190714", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private void unPackOverlappedChunks() throws IOException {\n          \n          \n            \n              private void unpackOverlappedChunks() throws IOException {", "author": "samperson1997", "createdAt": "2020-01-14T07:50:08Z", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/seriesRelated/AbstractDataReader.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.query.reader.seriesRelated;\n+\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeSet;\n+import org.apache.iotdb.db.engine.cache.DeviceMetaDataCache;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n+import org.apache.iotdb.db.engine.querycontext.ReadOnlyMemChunk;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.exception.StorageEngineException;\n+import org.apache.iotdb.db.query.context.QueryContext;\n+import org.apache.iotdb.db.query.control.QueryResourceManager;\n+import org.apache.iotdb.db.query.reader.ManagedSeriesReader;\n+import org.apache.iotdb.db.query.reader.MemChunkLoader;\n+import org.apache.iotdb.db.query.reader.chunkRelated.ChunkDataIterator;\n+import org.apache.iotdb.db.query.reader.chunkRelated.MemChunkReader;\n+import org.apache.iotdb.db.query.reader.universal.PriorityMergeReader;\n+import org.apache.iotdb.db.utils.QueryUtils;\n+import org.apache.iotdb.db.utils.TimeValuePair;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.common.Chunk;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.controller.ChunkLoaderImpl;\n+import org.apache.iotdb.tsfile.read.controller.IChunkLoader;\n+import org.apache.iotdb.tsfile.read.filter.basic.Filter;\n+import org.apache.iotdb.tsfile.read.reader.IChunkReader;\n+import org.apache.iotdb.tsfile.read.reader.chunk.ChunkReader;\n+\n+/*\n+ * This class implements a pause read method, pseudocode is:\n+ *\n+ *\n+ * while(hasNextChunk()){\n+ *    while(hasNextPage()){\n+ *      while(hasNextBatch()){\n+ *        nextBatch()\n+ *      }\n+ *    }\n+ * }\n+ */\n+public abstract class AbstractDataReader implements ManagedSeriesReader {\n+\n+  private final QueryDataSource queryDataSource;\n+  private final QueryContext context;\n+  private final Path seriesPath;\n+  private final TSDataType dataType;\n+  protected Filter filter;\n+\n+  private final List<TsFileResource> seqFileResource;\n+  private final TreeSet<TsFileResource> unseqFileResource;\n+\n+  private final List<ChunkMetaData> seqChunkMetadatas = new ArrayList<>();\n+  private final TreeSet<ChunkMetaData> unseqChunkMetadatas = new TreeSet<>(\n+      Comparator.comparingLong(ChunkMetaData::getStartTime));\n+\n+  private final List<IChunkLoader> openedChunkLoaders = new LinkedList<>();\n+\n+  protected boolean hasCachedNextChunk;\n+  private boolean isCurrentChunkReaderInit;\n+  protected IChunkReader chunkReader;\n+  protected ChunkMetaData chunkMetaData;\n+\n+  protected List<VersionPair<IChunkReader>> overlappedChunkReader = new ArrayList<>();\n+  protected List<VersionPair<IChunkReader>> overlappedPages = new ArrayList<>();\n+\n+  protected boolean hasCachedNextPage;\n+  protected PageHeader currentPage;\n+\n+  private boolean hasCachedNextBatch;\n+  protected PriorityMergeReader priorityMergeReader = new PriorityMergeReader();\n+  private long latestDirectlyOverlappedPageEndTime = Long.MAX_VALUE;\n+\n+  private boolean hasRemaining;\n+  private boolean managedByQueryManager;\n+\n+\n+  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n+      Filter filter, QueryContext context) throws StorageEngineException, IOException {\n+    queryDataSource = QueryResourceManager.getInstance()\n+        .getQueryDataSource(seriesPath, context);\n+    this.seriesPath = seriesPath;\n+    this.context = context;\n+    this.dataType = dataType;\n+\n+    this.filter = queryDataSource.setTTL(filter);\n+\n+    seqFileResource = queryDataSource.getSeqResources();\n+    unseqFileResource = sortUnSeqFileResources(queryDataSource.getUnseqResources());\n+\n+    removeInvalidFiles();\n+    tryToFillChunkMetadatas();\n+  }\n+\n+  //for test\n+  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n+      Filter filter, QueryContext context, QueryDataSource dataSource)\n+      throws IOException {\n+    queryDataSource = dataSource;\n+    this.seriesPath = seriesPath;\n+    this.context = context;\n+    this.dataType = dataType;\n+\n+    this.filter = queryDataSource.setTTL(filter);\n+\n+    seqFileResource = queryDataSource.getSeqResources();\n+    unseqFileResource = sortUnSeqFileResources(queryDataSource.getUnseqResources());\n+\n+    removeInvalidFiles();\n+    tryToFillChunkMetadatas();\n+  }\n+\n+  //for test\n+  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n+      Filter filter, QueryContext context, List<TsFileResource> seqResources) throws IOException {\n+    this.queryDataSource = null;\n+    this.seriesPath = seriesPath;\n+    this.context = context;\n+    this.dataType = dataType;\n+\n+    this.filter = filter;\n+\n+    this.seqFileResource = seqResources;\n+    this.unseqFileResource = new TreeSet<>();\n+\n+    removeInvalidFiles();\n+    tryToFillChunkMetadatas();\n+  }\n+\n+\n+  protected boolean hasNextChunk() throws IOException {\n+    if (hasCachedNextChunk) {\n+      return true;\n+    }\n+    removeInvalidChunks();\n+    searchMinimumChunk();\n+    //When the new chunk cannot be found, it is time to end all methods\n+    if (!hasCachedNextChunk) {\n+      return false;\n+    }\n+    unpackOverlappedFiles();\n+    unPackOverlappedChunks();\n+    return hasCachedNextChunk;\n+  }\n+\n+\n+  protected boolean hasNextPage() throws IOException {\n+    if (hasCachedNextPage) {\n+      return true;\n+    }\n+    if (!isCurrentChunkReaderInit) {\n+      chunkReader = initChunkReader(chunkMetaData);\n+      isCurrentChunkReaderInit = true;\n+    }\n+    if (chunkReader != null && chunkReader.hasNextSatisfiedPage()) {\n+      fillOverlappedPages();\n+      return hasCachedNextPage;\n+    }\n+\n+    isCurrentChunkReaderInit = false;\n+    chunkMetaData.getChunkLoader().close();\n+    hasCachedNextChunk = hasCachedNextPage;\n+    return hasCachedNextPage;\n+  }\n+\n+\n+  public boolean hasNextBatch() throws IOException {\n+    if (hasCachedNextBatch) {\n+      return true;\n+    }\n+    if (chunkReader.hasNextSatisfiedPage()) {\n+      priorityMergeReader\n+          .addReaderWithPriority(new ChunkDataIterator(chunkReader), chunkMetaData.getVersion());\n+      hasCachedNextBatch = true;\n+    }\n+    for (int i = 0; i < overlappedPages.size(); i++) {\n+      VersionPair<IChunkReader> reader = overlappedPages.get(i);\n+      priorityMergeReader\n+          .addReaderWithPriority(new ChunkDataIterator(reader.data), reader.version);\n+      hasCachedNextBatch = true;\n+    }\n+    overlappedPages.clear();\n+\n+    hasCachedNextPage = hasCachedNextBatch;\n+    return hasCachedNextBatch;\n+  }\n+\n+  public BatchData nextBatch() throws IOException {\n+    if (priorityMergeReader.hasNext()) {\n+      hasCachedNextBatch = false;\n+      return nextOverlappedPage();\n+    }\n+    throw new IOException(\"no next data\");\n+  }\n+\n+\n+  protected BatchData nextOverlappedPage() throws IOException {\n+    BatchData batchData = new BatchData(dataType);\n+    while (priorityMergeReader.hasNext()) {\n+      TimeValuePair timeValuePair = priorityMergeReader.current();\n+      //TODO should add a batchSize to limit the number of reads per time\n+      if (timeValuePair.getTimestamp() > latestDirectlyOverlappedPageEndTime) {\n+        break;\n+      }\n+      batchData.putAnObject(timeValuePair.getTimestamp(), timeValuePair.getValue().getValue());\n+      priorityMergeReader.next();\n+    }\n+    return batchData;\n+  }\n+\n+  private IChunkReader initChunkReader(ChunkMetaData metaData) throws IOException {\n+    if (metaData == null) {\n+      return null;\n+    }\n+    IChunkReader chunkReader;\n+    IChunkLoader chunkLoader = metaData.getChunkLoader();\n+    openedChunkLoaders.add(chunkLoader);\n+    if (chunkLoader instanceof MemChunkLoader) {\n+      MemChunkLoader memChunkLoader = (MemChunkLoader) chunkLoader;\n+      chunkReader = new MemChunkReader(memChunkLoader.getChunk(), filter);\n+    } else {\n+      Chunk chunk = chunkLoader.getChunk(metaData);\n+      chunkReader = new ChunkReader(chunk, filter);\n+      chunkReader.hasNextSatisfiedPage();\n+    }\n+    return chunkReader;\n+  }\n+\n+  private List<ChunkMetaData> loadChunkMetadatas(TsFileResource resource) throws IOException {\n+    List<ChunkMetaData> currentChunkMetaDataList;\n+    if (resource == null) {\n+      return new ArrayList<>();\n+    }\n+    if (resource.isClosed()) {\n+      currentChunkMetaDataList = DeviceMetaDataCache.getInstance().get(resource, seriesPath);\n+    } else {\n+      currentChunkMetaDataList = resource.getChunkMetaDataList();\n+    }\n+    List<Modification> pathModifications = context\n+        .getPathModifications(resource.getModFile(), seriesPath.getFullPath());\n+    for (ChunkMetaData data : currentChunkMetaDataList) {\n+      if (data.getChunkLoader() == null) {\n+        data.setChunkLoader(\n+            new ChunkLoaderImpl(new TsFileSequenceReader(resource.getFile().getAbsolutePath())));\n+      }\n+    }\n+    if (!pathModifications.isEmpty()) {\n+      QueryUtils.modifyChunkMetaData(currentChunkMetaDataList, pathModifications);\n+    }\n+    ReadOnlyMemChunk readOnlyMemChunk = resource.getReadOnlyMemChunk();\n+    if (readOnlyMemChunk != null) {\n+      currentChunkMetaDataList.add(readOnlyMemChunk.getChunkMetaData());\n+    }\n+    return currentChunkMetaDataList;\n+  }\n+\n+  private TreeSet<TsFileResource> sortUnSeqFileResources(List<TsFileResource> tsFileResources) {\n+    TreeSet<TsFileResource> unseqTsFilesSet = new TreeSet<>((o1, o2) -> {\n+      Map<String, Long> startTimeMap = o1.getStartTimeMap();\n+      Long minTimeOfO1 = startTimeMap.get(seriesPath.getDevice());\n+      Map<String, Long> startTimeMap2 = o2.getStartTimeMap();\n+      Long minTimeOfO2 = startTimeMap2.get(seriesPath.getDevice());\n+\n+      return Long.compare(minTimeOfO1, minTimeOfO2);\n+    });\n+    unseqTsFilesSet.addAll(tsFileResources);\n+    return unseqTsFilesSet;\n+  }\n+\n+  @Override\n+  public boolean isManagedByQueryManager() {\n+    return managedByQueryManager;\n+  }\n+\n+  @Override\n+  public void setManagedByQueryManager(boolean managedByQueryManager) {\n+    this.managedByQueryManager = managedByQueryManager;\n+  }\n+\n+  @Override\n+  public boolean hasRemaining() {\n+    return hasRemaining;\n+  }\n+\n+  @Override\n+  public void setHasRemaining(boolean hasRemaining) {\n+    this.hasRemaining = hasRemaining;\n+  }\n+\n+  /**\n+   * Because you get a list of all the files, some files are not necessary when filters exist. This\n+   * method filters out the available data files based on the filter\n+   */\n+  private void removeInvalidFiles() {\n+    //filter seq files\n+    while (filter != null && !seqFileResource.isEmpty()) {\n+      if (!isValid(seqFileResource.get(0))) {\n+        seqFileResource.remove(0);\n+        continue;\n+      }\n+      break;\n+    }\n+    //filter unseq files\n+    while (filter != null && !unseqFileResource.isEmpty()) {\n+      if (!isValid(unseqFileResource.first())) {\n+        unseqFileResource.pollFirst();\n+        continue;\n+      }\n+      break;\n+    }\n+  }\n+\n+  private boolean isValid(TsFileResource tsFileResource) {\n+    long startTime = tsFileResource.getStartTimeMap()\n+        .get(seriesPath.getDevice());\n+    long endTime = tsFileResource.getEndTimeMap()\n+        .getOrDefault(seriesPath.getDevice(), Long.MAX_VALUE);\n+    return filter.satisfyStartEndTime(startTime, endTime);\n+  }\n+\n+  /**\n+   * unseq files are very special files that intersect not only with sequence files, but also with\n+   * other unseq files. So we need to find all tsfiles that overlapped with current chunk and\n+   * extract chunks from the resource.\n+   */\n+  private void unpackOverlappedFiles() throws IOException {\n+    while (!unseqFileResource.isEmpty()) {\n+      Map<String, Long> startTimeMap = unseqFileResource.first().getStartTimeMap();\n+      Long unSeqStartTime = startTimeMap.getOrDefault(seriesPath.getDevice(), Long.MAX_VALUE);\n+      if (chunkMetaData.getEndTime() >= unSeqStartTime) {\n+        unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n+        continue;\n+      }\n+      break;\n+    }\n+    while (!seqFileResource.isEmpty()) {\n+      Map<String, Long> startTimeMap = seqFileResource.get(0).getStartTimeMap();\n+      Long seqStartTime = startTimeMap.getOrDefault(seriesPath.getDevice(), Long.MIN_VALUE);\n+      if (chunkMetaData.getEndTime() > seqStartTime) {\n+        seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n+        continue;\n+      }\n+      break;\n+    }\n+  }\n+\n+  /**\n+   * Because seq data and unseq data intersect, the minimum startTime taken from two files at a time\n+   * is used as the reference time to start reading data\n+   */\n+  private void searchMinimumChunk() {\n+    hasCachedNextChunk = true;\n+    if (!seqChunkMetadatas.isEmpty() && unseqChunkMetadatas.isEmpty()) {\n+      chunkMetaData = seqChunkMetadatas.remove(0);\n+    } else if (seqChunkMetadatas.isEmpty() && !unseqChunkMetadatas.isEmpty()) {\n+      chunkMetaData = unseqChunkMetadatas.pollFirst();\n+    } else if (!seqChunkMetadatas.isEmpty()) {\n+      // neither seqChunkMetadatas nor unseqChunkMetadatas is null\n+      if (seqChunkMetadatas.get(0).getStartTime() <= unseqChunkMetadatas.first().getStartTime()) {\n+        chunkMetaData = seqChunkMetadatas.remove(0);\n+      } else {\n+        chunkMetaData = unseqChunkMetadatas.pollFirst();\n+      }\n+    } else {\n+      hasCachedNextChunk = false;\n+    }\n+  }\n+\n+  /**\n+   * Before reading the chunks, should first clean up all the useless chunks, because in the file\n+   * hierarchy, although the files are available, some of the internal chunks are still unavailable\n+   */\n+  private void removeInvalidChunks() throws IOException {\n+    //remove seq chunks\n+    while (filter != null && (!seqChunkMetadatas.isEmpty() || !seqFileResource.isEmpty())) {\n+      while (seqChunkMetadatas.isEmpty() && !seqFileResource.isEmpty()) {\n+        seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n+      }\n+\n+      ChunkMetaData metaData = seqChunkMetadatas.get(0);\n+      if (!filter.satisfyStartEndTime(metaData.getStartTime(), metaData.getEndTime())) {\n+        seqChunkMetadatas.remove(0);\n+        continue;\n+      }\n+      break;\n+    }\n+    while (filter != null && (!unseqChunkMetadatas.isEmpty() || !unseqFileResource.isEmpty())) {\n+      while (unseqChunkMetadatas.isEmpty() && !unseqFileResource.isEmpty()) {\n+        unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n+      }\n+      ChunkMetaData metaData = unseqChunkMetadatas.first();\n+      if (!filter.satisfyStartEndTime(metaData.getStartTime(), metaData.getEndTime())) {\n+        unseqChunkMetadatas.pollFirst();\n+        continue;\n+      }\n+      break;\n+    }\n+    tryToFillChunkMetadatas();\n+  }\n+\n+  /**\n+   * Because there may be too many files in the scenario used by the user, we cannot open all the\n+   * chunks at once, which may OOM, so we can only fill one file at a time when needed. This\n+   * approach is likely to be ubiquitous, but it keeps the system running smoothly\n+   */\n+  private void tryToFillChunkMetadatas() throws IOException {\n+    while (seqChunkMetadatas.isEmpty() && !seqFileResource.isEmpty()) {\n+      seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n+    }\n+    while (unseqChunkMetadatas.isEmpty() && !unseqFileResource.isEmpty()) {\n+      unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n+    }\n+  }\n+\n+  /**\n+   * Before calling this method, you should make sure that all the intersecting files are filled in\n+   * the container, because the files intersect, but some chunks may still be useless, so you need\n+   * to clean up all the unused chunks and populate the container. It should be noted that this\n+   * chunk collection is not in order, and all chunks should be used at once\n+   */\n+  private void unPackOverlappedChunks() throws IOException {", "originalCommit": "bd4e66feb3f1a3b5c79dbff199cb1c06aeddccd2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "89181044d4ccfaba2c0aec9f3435c014197c612f", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/query/reader/seriesRelated/AbstractDataReader.java b/server/src/main/java/org/apache/iotdb/db/query/reader/seriesRelated/AbstractDataReader.java\ndeleted file mode 100644\nindex 144f8750fa..0000000000\n--- a/server/src/main/java/org/apache/iotdb/db/query/reader/seriesRelated/AbstractDataReader.java\n+++ /dev/null\n\n@@ -1,514 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iotdb.db.query.reader.seriesRelated;\n-\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Comparator;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.TreeSet;\n-import org.apache.iotdb.db.engine.cache.DeviceMetaDataCache;\n-import org.apache.iotdb.db.engine.modification.Modification;\n-import org.apache.iotdb.db.engine.querycontext.QueryDataSource;\n-import org.apache.iotdb.db.engine.querycontext.ReadOnlyMemChunk;\n-import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n-import org.apache.iotdb.db.exception.StorageEngineException;\n-import org.apache.iotdb.db.query.context.QueryContext;\n-import org.apache.iotdb.db.query.control.QueryResourceManager;\n-import org.apache.iotdb.db.query.reader.ManagedSeriesReader;\n-import org.apache.iotdb.db.query.reader.MemChunkLoader;\n-import org.apache.iotdb.db.query.reader.chunkRelated.ChunkDataIterator;\n-import org.apache.iotdb.db.query.reader.chunkRelated.MemChunkReader;\n-import org.apache.iotdb.db.query.reader.universal.PriorityMergeReader;\n-import org.apache.iotdb.db.utils.QueryUtils;\n-import org.apache.iotdb.db.utils.TimeValuePair;\n-import org.apache.iotdb.tsfile.file.header.PageHeader;\n-import org.apache.iotdb.tsfile.file.metadata.ChunkMetaData;\n-import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n-import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n-import org.apache.iotdb.tsfile.read.common.BatchData;\n-import org.apache.iotdb.tsfile.read.common.Chunk;\n-import org.apache.iotdb.tsfile.read.common.Path;\n-import org.apache.iotdb.tsfile.read.controller.ChunkLoaderImpl;\n-import org.apache.iotdb.tsfile.read.controller.IChunkLoader;\n-import org.apache.iotdb.tsfile.read.filter.basic.Filter;\n-import org.apache.iotdb.tsfile.read.reader.IChunkReader;\n-import org.apache.iotdb.tsfile.read.reader.chunk.ChunkReader;\n-\n-/*\n- * This class implements a pause read method, pseudocode is:\n- *\n- *\n- * while(hasNextChunk()){\n- *    while(hasNextPage()){\n- *      while(hasNextBatch()){\n- *        nextBatch()\n- *      }\n- *    }\n- * }\n- */\n-public abstract class AbstractDataReader implements ManagedSeriesReader {\n-\n-  private final QueryDataSource queryDataSource;\n-  private final QueryContext context;\n-  private final Path seriesPath;\n-  private final TSDataType dataType;\n-  protected Filter filter;\n-\n-  private final List<TsFileResource> seqFileResource;\n-  private final TreeSet<TsFileResource> unseqFileResource;\n-\n-  private final List<ChunkMetaData> seqChunkMetadatas = new ArrayList<>();\n-  private final TreeSet<ChunkMetaData> unseqChunkMetadatas = new TreeSet<>(\n-      Comparator.comparingLong(ChunkMetaData::getStartTime));\n-\n-  private final List<IChunkLoader> openedChunkLoaders = new LinkedList<>();\n-\n-  protected boolean hasCachedNextChunk;\n-  private boolean isCurrentChunkReaderInit;\n-  protected IChunkReader chunkReader;\n-  protected ChunkMetaData chunkMetaData;\n-\n-  protected List<VersionPair<IChunkReader>> overlappedChunkReader = new ArrayList<>();\n-  protected List<VersionPair<IChunkReader>> overlappedPages = new ArrayList<>();\n-\n-  protected boolean hasCachedNextPage;\n-  protected PageHeader currentPage;\n-\n-  private boolean hasCachedNextBatch;\n-  protected PriorityMergeReader priorityMergeReader = new PriorityMergeReader();\n-  private long latestDirectlyOverlappedPageEndTime = Long.MAX_VALUE;\n-\n-  private boolean hasRemaining;\n-  private boolean managedByQueryManager;\n-\n-\n-  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n-      Filter filter, QueryContext context) throws StorageEngineException, IOException {\n-    queryDataSource = QueryResourceManager.getInstance()\n-        .getQueryDataSource(seriesPath, context);\n-    this.seriesPath = seriesPath;\n-    this.context = context;\n-    this.dataType = dataType;\n-\n-    this.filter = queryDataSource.setTTL(filter);\n-\n-    seqFileResource = queryDataSource.getSeqResources();\n-    unseqFileResource = sortUnSeqFileResources(queryDataSource.getUnseqResources());\n-\n-    removeInvalidFiles();\n-    tryToFillChunkMetadatas();\n-  }\n-\n-  //for test\n-  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n-      Filter filter, QueryContext context, QueryDataSource dataSource)\n-      throws IOException {\n-    queryDataSource = dataSource;\n-    this.seriesPath = seriesPath;\n-    this.context = context;\n-    this.dataType = dataType;\n-\n-    this.filter = queryDataSource.setTTL(filter);\n-\n-    seqFileResource = queryDataSource.getSeqResources();\n-    unseqFileResource = sortUnSeqFileResources(queryDataSource.getUnseqResources());\n-\n-    removeInvalidFiles();\n-    tryToFillChunkMetadatas();\n-  }\n-\n-  //for test\n-  public AbstractDataReader(Path seriesPath, TSDataType dataType,\n-      Filter filter, QueryContext context, List<TsFileResource> seqResources) throws IOException {\n-    this.queryDataSource = null;\n-    this.seriesPath = seriesPath;\n-    this.context = context;\n-    this.dataType = dataType;\n-\n-    this.filter = filter;\n-\n-    this.seqFileResource = seqResources;\n-    this.unseqFileResource = new TreeSet<>();\n-\n-    removeInvalidFiles();\n-    tryToFillChunkMetadatas();\n-  }\n-\n-\n-  protected boolean hasNextChunk() throws IOException {\n-    if (hasCachedNextChunk) {\n-      return true;\n-    }\n-    removeInvalidChunks();\n-    searchMinimumChunk();\n-    //When the new chunk cannot be found, it is time to end all methods\n-    if (!hasCachedNextChunk) {\n-      return false;\n-    }\n-    unpackOverlappedFiles();\n-    unPackOverlappedChunks();\n-    return hasCachedNextChunk;\n-  }\n-\n-\n-  protected boolean hasNextPage() throws IOException {\n-    if (hasCachedNextPage) {\n-      return true;\n-    }\n-    if (!isCurrentChunkReaderInit) {\n-      chunkReader = initChunkReader(chunkMetaData);\n-      isCurrentChunkReaderInit = true;\n-    }\n-    if (chunkReader != null && chunkReader.hasNextSatisfiedPage()) {\n-      fillOverlappedPages();\n-      return hasCachedNextPage;\n-    }\n-\n-    isCurrentChunkReaderInit = false;\n-    chunkMetaData.getChunkLoader().close();\n-    hasCachedNextChunk = hasCachedNextPage;\n-    return hasCachedNextPage;\n-  }\n-\n-\n-  public boolean hasNextBatch() throws IOException {\n-    if (hasCachedNextBatch) {\n-      return true;\n-    }\n-    if (chunkReader.hasNextSatisfiedPage()) {\n-      priorityMergeReader\n-          .addReaderWithPriority(new ChunkDataIterator(chunkReader), chunkMetaData.getVersion());\n-      hasCachedNextBatch = true;\n-    }\n-    for (int i = 0; i < overlappedPages.size(); i++) {\n-      VersionPair<IChunkReader> reader = overlappedPages.get(i);\n-      priorityMergeReader\n-          .addReaderWithPriority(new ChunkDataIterator(reader.data), reader.version);\n-      hasCachedNextBatch = true;\n-    }\n-    overlappedPages.clear();\n-\n-    hasCachedNextPage = hasCachedNextBatch;\n-    return hasCachedNextBatch;\n-  }\n-\n-  public BatchData nextBatch() throws IOException {\n-    if (priorityMergeReader.hasNext()) {\n-      hasCachedNextBatch = false;\n-      return nextOverlappedPage();\n-    }\n-    throw new IOException(\"no next data\");\n-  }\n-\n-\n-  protected BatchData nextOverlappedPage() throws IOException {\n-    BatchData batchData = new BatchData(dataType);\n-    while (priorityMergeReader.hasNext()) {\n-      TimeValuePair timeValuePair = priorityMergeReader.current();\n-      //TODO should add a batchSize to limit the number of reads per time\n-      if (timeValuePair.getTimestamp() > latestDirectlyOverlappedPageEndTime) {\n-        break;\n-      }\n-      batchData.putAnObject(timeValuePair.getTimestamp(), timeValuePair.getValue().getValue());\n-      priorityMergeReader.next();\n-    }\n-    return batchData;\n-  }\n-\n-  private IChunkReader initChunkReader(ChunkMetaData metaData) throws IOException {\n-    if (metaData == null) {\n-      return null;\n-    }\n-    IChunkReader chunkReader;\n-    IChunkLoader chunkLoader = metaData.getChunkLoader();\n-    openedChunkLoaders.add(chunkLoader);\n-    if (chunkLoader instanceof MemChunkLoader) {\n-      MemChunkLoader memChunkLoader = (MemChunkLoader) chunkLoader;\n-      chunkReader = new MemChunkReader(memChunkLoader.getChunk(), filter);\n-    } else {\n-      Chunk chunk = chunkLoader.getChunk(metaData);\n-      chunkReader = new ChunkReader(chunk, filter);\n-      chunkReader.hasNextSatisfiedPage();\n-    }\n-    return chunkReader;\n-  }\n-\n-  private List<ChunkMetaData> loadChunkMetadatas(TsFileResource resource) throws IOException {\n-    List<ChunkMetaData> currentChunkMetaDataList;\n-    if (resource == null) {\n-      return new ArrayList<>();\n-    }\n-    if (resource.isClosed()) {\n-      currentChunkMetaDataList = DeviceMetaDataCache.getInstance().get(resource, seriesPath);\n-    } else {\n-      currentChunkMetaDataList = resource.getChunkMetaDataList();\n-    }\n-    List<Modification> pathModifications = context\n-        .getPathModifications(resource.getModFile(), seriesPath.getFullPath());\n-    for (ChunkMetaData data : currentChunkMetaDataList) {\n-      if (data.getChunkLoader() == null) {\n-        data.setChunkLoader(\n-            new ChunkLoaderImpl(new TsFileSequenceReader(resource.getFile().getAbsolutePath())));\n-      }\n-    }\n-    if (!pathModifications.isEmpty()) {\n-      QueryUtils.modifyChunkMetaData(currentChunkMetaDataList, pathModifications);\n-    }\n-    ReadOnlyMemChunk readOnlyMemChunk = resource.getReadOnlyMemChunk();\n-    if (readOnlyMemChunk != null) {\n-      currentChunkMetaDataList.add(readOnlyMemChunk.getChunkMetaData());\n-    }\n-    return currentChunkMetaDataList;\n-  }\n-\n-  private TreeSet<TsFileResource> sortUnSeqFileResources(List<TsFileResource> tsFileResources) {\n-    TreeSet<TsFileResource> unseqTsFilesSet = new TreeSet<>((o1, o2) -> {\n-      Map<String, Long> startTimeMap = o1.getStartTimeMap();\n-      Long minTimeOfO1 = startTimeMap.get(seriesPath.getDevice());\n-      Map<String, Long> startTimeMap2 = o2.getStartTimeMap();\n-      Long minTimeOfO2 = startTimeMap2.get(seriesPath.getDevice());\n-\n-      return Long.compare(minTimeOfO1, minTimeOfO2);\n-    });\n-    unseqTsFilesSet.addAll(tsFileResources);\n-    return unseqTsFilesSet;\n-  }\n-\n-  @Override\n-  public boolean isManagedByQueryManager() {\n-    return managedByQueryManager;\n-  }\n-\n-  @Override\n-  public void setManagedByQueryManager(boolean managedByQueryManager) {\n-    this.managedByQueryManager = managedByQueryManager;\n-  }\n-\n-  @Override\n-  public boolean hasRemaining() {\n-    return hasRemaining;\n-  }\n-\n-  @Override\n-  public void setHasRemaining(boolean hasRemaining) {\n-    this.hasRemaining = hasRemaining;\n-  }\n-\n-  /**\n-   * Because you get a list of all the files, some files are not necessary when filters exist. This\n-   * method filters out the available data files based on the filter\n-   */\n-  private void removeInvalidFiles() {\n-    //filter seq files\n-    while (filter != null && !seqFileResource.isEmpty()) {\n-      if (!isValid(seqFileResource.get(0))) {\n-        seqFileResource.remove(0);\n-        continue;\n-      }\n-      break;\n-    }\n-    //filter unseq files\n-    while (filter != null && !unseqFileResource.isEmpty()) {\n-      if (!isValid(unseqFileResource.first())) {\n-        unseqFileResource.pollFirst();\n-        continue;\n-      }\n-      break;\n-    }\n-  }\n-\n-  private boolean isValid(TsFileResource tsFileResource) {\n-    long startTime = tsFileResource.getStartTimeMap()\n-        .get(seriesPath.getDevice());\n-    long endTime = tsFileResource.getEndTimeMap()\n-        .getOrDefault(seriesPath.getDevice(), Long.MAX_VALUE);\n-    return filter.satisfyStartEndTime(startTime, endTime);\n-  }\n-\n-  /**\n-   * unseq files are very special files that intersect not only with sequence files, but also with\n-   * other unseq files. So we need to find all tsfiles that overlapped with current chunk and\n-   * extract chunks from the resource.\n-   */\n-  private void unpackOverlappedFiles() throws IOException {\n-    while (!unseqFileResource.isEmpty()) {\n-      Map<String, Long> startTimeMap = unseqFileResource.first().getStartTimeMap();\n-      Long unSeqStartTime = startTimeMap.getOrDefault(seriesPath.getDevice(), Long.MAX_VALUE);\n-      if (chunkMetaData.getEndTime() >= unSeqStartTime) {\n-        unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n-        continue;\n-      }\n-      break;\n-    }\n-    while (!seqFileResource.isEmpty()) {\n-      Map<String, Long> startTimeMap = seqFileResource.get(0).getStartTimeMap();\n-      Long seqStartTime = startTimeMap.getOrDefault(seriesPath.getDevice(), Long.MIN_VALUE);\n-      if (chunkMetaData.getEndTime() > seqStartTime) {\n-        seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n-        continue;\n-      }\n-      break;\n-    }\n-  }\n-\n-  /**\n-   * Because seq data and unseq data intersect, the minimum startTime taken from two files at a time\n-   * is used as the reference time to start reading data\n-   */\n-  private void searchMinimumChunk() {\n-    hasCachedNextChunk = true;\n-    if (!seqChunkMetadatas.isEmpty() && unseqChunkMetadatas.isEmpty()) {\n-      chunkMetaData = seqChunkMetadatas.remove(0);\n-    } else if (seqChunkMetadatas.isEmpty() && !unseqChunkMetadatas.isEmpty()) {\n-      chunkMetaData = unseqChunkMetadatas.pollFirst();\n-    } else if (!seqChunkMetadatas.isEmpty()) {\n-      // neither seqChunkMetadatas nor unseqChunkMetadatas is null\n-      if (seqChunkMetadatas.get(0).getStartTime() <= unseqChunkMetadatas.first().getStartTime()) {\n-        chunkMetaData = seqChunkMetadatas.remove(0);\n-      } else {\n-        chunkMetaData = unseqChunkMetadatas.pollFirst();\n-      }\n-    } else {\n-      hasCachedNextChunk = false;\n-    }\n-  }\n-\n-  /**\n-   * Before reading the chunks, should first clean up all the useless chunks, because in the file\n-   * hierarchy, although the files are available, some of the internal chunks are still unavailable\n-   */\n-  private void removeInvalidChunks() throws IOException {\n-    //remove seq chunks\n-    while (filter != null && (!seqChunkMetadatas.isEmpty() || !seqFileResource.isEmpty())) {\n-      while (seqChunkMetadatas.isEmpty() && !seqFileResource.isEmpty()) {\n-        seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n-      }\n-\n-      ChunkMetaData metaData = seqChunkMetadatas.get(0);\n-      if (!filter.satisfyStartEndTime(metaData.getStartTime(), metaData.getEndTime())) {\n-        seqChunkMetadatas.remove(0);\n-        continue;\n-      }\n-      break;\n-    }\n-    while (filter != null && (!unseqChunkMetadatas.isEmpty() || !unseqFileResource.isEmpty())) {\n-      while (unseqChunkMetadatas.isEmpty() && !unseqFileResource.isEmpty()) {\n-        unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n-      }\n-      ChunkMetaData metaData = unseqChunkMetadatas.first();\n-      if (!filter.satisfyStartEndTime(metaData.getStartTime(), metaData.getEndTime())) {\n-        unseqChunkMetadatas.pollFirst();\n-        continue;\n-      }\n-      break;\n-    }\n-    tryToFillChunkMetadatas();\n-  }\n-\n-  /**\n-   * Because there may be too many files in the scenario used by the user, we cannot open all the\n-   * chunks at once, which may OOM, so we can only fill one file at a time when needed. This\n-   * approach is likely to be ubiquitous, but it keeps the system running smoothly\n-   */\n-  private void tryToFillChunkMetadatas() throws IOException {\n-    while (seqChunkMetadatas.isEmpty() && !seqFileResource.isEmpty()) {\n-      seqChunkMetadatas.addAll(loadChunkMetadatas(seqFileResource.remove(0)));\n-    }\n-    while (unseqChunkMetadatas.isEmpty() && !unseqFileResource.isEmpty()) {\n-      unseqChunkMetadatas.addAll(loadChunkMetadatas(unseqFileResource.pollFirst()));\n-    }\n-  }\n-\n-  /**\n-   * Before calling this method, you should make sure that all the intersecting files are filled in\n-   * the container, because the files intersect, but some chunks may still be useless, so you need\n-   * to clean up all the unused chunks and populate the container. It should be noted that this\n-   * chunk collection is not in order, and all chunks should be used at once\n-   */\n-  private void unPackOverlappedChunks() throws IOException {\n-    while (!unseqChunkMetadatas.isEmpty()) {\n-      long startTime = unseqChunkMetadatas.first().getStartTime();\n-\n-      if (chunkMetaData.getEndTime() >= startTime) {\n-        ChunkMetaData metaData = unseqChunkMetadatas.pollFirst();\n-        if (metaData != null) {\n-          IChunkReader chunkReader = initChunkReader(metaData);\n-          //When data points overlap, there should be a weight\n-          overlappedChunkReader.add(new VersionPair<>(metaData.getVersion(), chunkReader));\n-        }\n-        continue;\n-      }\n-      break;\n-    }\n-    while (!seqChunkMetadatas.isEmpty()) {\n-      long startTime = seqChunkMetadatas.get(0).getStartTime();\n-\n-      if (chunkMetaData.getEndTime() >= startTime) {\n-        ChunkMetaData metaData = seqChunkMetadatas.remove(0);\n-        if (metaData != null) {\n-          IChunkReader chunkReader = initChunkReader(metaData);\n-          overlappedChunkReader.add(new VersionPair<>(metaData.getVersion(), chunkReader));\n-        }\n-        continue;\n-      }\n-      break;\n-    }\n-  }\n-\n-  /**\n-   * This is just a fake page container, because no matter how chaotic the situation is, the data in\n-   * a chunk is always in order, so when the first page of the chunk cannot be used, there is no\n-   * need to look at the whole chunk.\n-   */\n-  private void fillOverlappedPages() {\n-    currentPage = chunkReader.nextPageHeader();\n-    hasCachedNextPage = true;\n-    latestDirectlyOverlappedPageEndTime = currentPage.getEndTime();\n-    while (!overlappedChunkReader.isEmpty()) {\n-      VersionPair<IChunkReader> iChunkReader = overlappedChunkReader.get(0);\n-      if (currentPage.getEndTime() > iChunkReader.data.nextPageHeader().getStartTime()) {\n-        overlappedPages.add(overlappedChunkReader.remove(0));\n-      }\n-    }\n-  }\n-\n-  private class VersionPair<T> {\n-\n-    private long version;\n-    private T data;\n-\n-    public VersionPair(long version, T data) {\n-      this.version = version;\n-      this.data = data;\n-    }\n-  }\n-\n-  public void close() throws IOException {\n-    if (chunkMetaData != null) {\n-      chunkMetaData.getChunkLoader().close();\n-    }\n-    for (int i = 0; i < openedChunkLoaders.size(); i++) {\n-      openedChunkLoaders.get(i).close();\n-    }\n-  }\n-}\n"}}, {"oid": "2ba8ec7f0d7f9098ee0ed5ce76f14c4b3123e1a7", "url": "https://github.com/apache/iotdb/commit/2ba8ec7f0d7f9098ee0ed5ce76f14c4b3123e1a7", "message": "rename and optimize codes", "committedDate": "2020-01-14T08:39:26Z", "type": "commit"}, {"oid": "2a2c465ff25ddeea1bd0b5ab94052b5e4404ce3d", "url": "https://github.com/apache/iotdb/commit/2a2c465ff25ddeea1bd0b5ab94052b5e4404ce3d", "message": "remove unusned class", "committedDate": "2020-01-14T08:48:04Z", "type": "commit"}, {"oid": "8f77fa48dc83c7ec4bf830f7597a0630895cf32a", "url": "https://github.com/apache/iotdb/commit/8f77fa48dc83c7ec4bf830f7597a0630895cf32a", "message": "refactor 50% raw data query", "committedDate": "2020-01-14T10:59:33Z", "type": "commit"}, {"oid": "7c2dd96055f5ee96c1cb8a87f236863d2d955a0d", "url": "https://github.com/apache/iotdb/commit/7c2dd96055f5ee96c1cb8a87f236863d2d955a0d", "message": "refactor 75% raw data query", "committedDate": "2020-01-14T13:13:50Z", "type": "commit"}, {"oid": "e6f38ade4a4e0985f88b4402af3e3543cac052ab", "url": "https://github.com/apache/iotdb/commit/e6f38ade4a4e0985f88b4402af3e3543cac052ab", "message": "Merge AggregateResult and AggreResultData; move init() to constructor", "committedDate": "2020-01-15T02:03:38Z", "type": "commit"}, {"oid": "72b7613aa79ce34439d0b1f02832458d5f400ce9", "url": "https://github.com/apache/iotdb/commit/72b7613aa79ce34439d0b1f02832458d5f400ce9", "message": "update aggregate; exchange extend relationship of AVG and SUM", "committedDate": "2020-01-15T02:59:10Z", "type": "commit"}, {"oid": "8c3db53546760b6a088ff68bb0667924ef4e0311", "url": "https://github.com/apache/iotdb/commit/8c3db53546760b6a088ff68bb0667924ef4e0311", "message": "rename function to aggregate result", "committedDate": "2020-01-15T05:02:03Z", "type": "commit"}, {"oid": "3856be65e192744167619d5719721f2d03ec8ba7", "url": "https://github.com/apache/iotdb/commit/3856be65e192744167619d5719721f2d03ec8ba7", "message": "fix SeriesDataReaderWithoutValueFilter close", "committedDate": "2020-01-15T05:13:16Z", "type": "commit"}, {"oid": "4eb366162caa62f21b99b6c1ce5accd71e89a338", "url": "https://github.com/apache/iotdb/commit/4eb366162caa62f21b99b6c1ce5accd71e89a338", "message": "Merge disable align", "committedDate": "2020-01-15T06:19:43Z", "type": "commit"}, {"oid": "0028b388433510b31aed7a9e894d2f71d3d36933", "url": "https://github.com/apache/iotdb/commit/0028b388433510b31aed7a9e894d2f71d3d36933", "message": "Replace AggreResultDataPointReader and OldEngineDataSet by SingleDataSet", "committedDate": "2020-01-15T06:56:19Z", "type": "commit"}, {"oid": "c09f0cf9febb4c81aba391e715e1fecf033875e3", "url": "https://github.com/apache/iotdb/commit/c09f0cf9febb4c81aba391e715e1fecf033875e3", "message": "resolve sonarcloud", "committedDate": "2020-01-15T07:03:38Z", "type": "commit"}, {"oid": "45a59bbb6c45bf3d4950178c9523438b0aa0a8d8", "url": "https://github.com/apache/iotdb/commit/45a59bbb6c45bf3d4950178c9523438b0aa0a8d8", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-01-15T07:03:56Z", "type": "commit"}, {"oid": "301ea5c46da34917c803547da574435eff3892fe", "url": "https://github.com/apache/iotdb/commit/301ea5c46da34917c803547da574435eff3892fe", "message": "Merge branch 'master' into new_series_reader", "committedDate": "2020-01-15T07:30:40Z", "type": "commit"}, {"oid": "43f67d36c60c68f5149e043a48f80a066996373b", "url": "https://github.com/apache/iotdb/commit/43f67d36c60c68f5149e043a48f80a066996373b", "message": "Merge branch 'master' of https://github.com/apache/incubator-iotdb", "committedDate": "2020-01-15T07:54:30Z", "type": "commit"}, {"oid": "861d37269b510ac5a7318320bf503fae612ea304", "url": "https://github.com/apache/iotdb/commit/861d37269b510ac5a7318320bf503fae612ea304", "message": "fix aggregate SUM and AVG", "committedDate": "2020-01-15T08:07:49Z", "type": "commit"}, {"oid": "d357d9a9fe56ffcf1d5096daaf598d62c9af3c4a", "url": "https://github.com/apache/iotdb/commit/d357d9a9fe56ffcf1d5096daaf598d62c9af3c4a", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-01-15T08:08:00Z", "type": "commit"}, {"oid": "0c4523e27967fc9ec8f73244a868196a4e851f62", "url": "https://github.com/apache/iotdb/commit/0c4523e27967fc9ec8f73244a868196a4e851f62", "message": "fix some bug of new_series_reader (#744)\n\n* fix some bug after refactoring AggregateResult", "committedDate": "2020-01-15T08:16:30Z", "type": "commit"}, {"oid": "477144223274a4155882aa5f43fdcafec2b630d1", "url": "https://github.com/apache/iotdb/commit/477144223274a4155882aa5f43fdcafec2b630d1", "message": "fix GroupByWithoutValueFilterDataSet", "committedDate": "2020-01-15T08:48:14Z", "type": "commit"}, {"oid": "2d3cdc673b8fb74937a8298e635b8838b7878d41", "url": "https://github.com/apache/iotdb/commit/2d3cdc673b8fb74937a8298e635b8838b7878d41", "message": "Refactor rowRecord.addField()", "committedDate": "2020-01-15T09:13:18Z", "type": "commit"}, {"oid": "0ccdd73e8c87b644629b63a3204115c7ee0efdf3", "url": "https://github.com/apache/iotdb/commit/0ccdd73e8c87b644629b63a3204115c7ee0efdf3", "message": "leverage statistics in SeriesDataReaderWithValueFilter", "committedDate": "2020-01-15T10:33:40Z", "type": "commit"}, {"oid": "9f7aa139cf946532a5972fa72ac111fca1d10f47", "url": "https://github.com/apache/iotdb/commit/9f7aa139cf946532a5972fa72ac111fca1d10f47", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-01-15T10:43:10Z", "type": "commit"}, {"oid": "27b566bc308c934538c8768cc8c9bfd910b104ea", "url": "https://github.com/apache/iotdb/commit/27b566bc308c934538c8768cc8c9bfd910b104ea", "message": "Fix IoTDBFillIT", "committedDate": "2020-01-15T11:47:09Z", "type": "commit"}, {"oid": "be880674fbe2ac41e351cb0662d5bf97d92725c7", "url": "https://github.com/apache/iotdb/commit/be880674fbe2ac41e351cb0662d5bf97d92725c7", "message": "Fix some bugs", "committedDate": "2020-01-15T13:48:58Z", "type": "commit"}, {"oid": "5d54d4b7167816c78746f29b6a2649cb58039a71", "url": "https://github.com/apache/iotdb/commit/5d54d4b7167816c78746f29b6a2649cb58039a71", "message": "Revert extend relationship of AVG and SUM", "committedDate": "2020-01-16T01:56:33Z", "type": "commit"}, {"oid": "f6d853b3c7946a90cb87c91672679aee9f3c2971", "url": "https://github.com/apache/iotdb/commit/f6d853b3c7946a90cb87c91672679aee9f3c2971", "message": "Fix IOTDBGroupByIT", "committedDate": "2020-01-16T02:44:51Z", "type": "commit"}, {"oid": "c92387731922faf7b43e4fe355ee7a6f77a1006e", "url": "https://github.com/apache/iotdb/commit/c92387731922faf7b43e4fe355ee7a6f77a1006e", "message": "Fix IoTDBAggregationSmallDataIT", "committedDate": "2020-01-16T08:34:40Z", "type": "commit"}, {"oid": "cf9c736183a9d80b3ea7a81fef457d3c2f7771e3", "url": "https://github.com/apache/iotdb/commit/cf9c736183a9d80b3ea7a81fef457d3c2f7771e3", "message": "refactor abstract reader", "committedDate": "2020-01-16T10:54:16Z", "type": "commit"}, {"oid": "d73bb01110b37bc16502bc6f91215aceaa68cb86", "url": "https://github.com/apache/iotdb/commit/d73bb01110b37bc16502bc6f91215aceaa68cb86", "message": "resolve conflicts", "committedDate": "2020-01-16T10:56:56Z", "type": "commit"}, {"oid": "3b5ee81213c5ec5202c2dae8d7a4c9c617941676", "url": "https://github.com/apache/iotdb/commit/3b5ee81213c5ec5202c2dae8d7a4c9c617941676", "message": "resolve conflicts", "committedDate": "2020-01-16T11:33:43Z", "type": "commit"}, {"oid": "062d6a8c97a8e49c3f5e2705ffe1e2f6f8f0a991", "url": "https://github.com/apache/iotdb/commit/062d6a8c97a8e49c3f5e2705ffe1e2f6f8f0a991", "message": "Exchange extend relationship of AVG and SUM", "committedDate": "2020-01-16T12:12:42Z", "type": "commit"}, {"oid": "b6a322c5cded9db1510e93aa11760a34dc21ee04", "url": "https://github.com/apache/iotdb/commit/b6a322c5cded9db1510e93aa11760a34dc21ee04", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader", "committedDate": "2020-01-16T12:12:53Z", "type": "commit"}, {"oid": "fca96688fb6c567f2d07ebe5f6f4e2eb068402ac", "url": "https://github.com/apache/iotdb/commit/fca96688fb6c567f2d07ebe5f6f4e2eb068402ac", "message": "fix some bug", "committedDate": "2020-01-16T12:19:38Z", "type": "commit"}, {"oid": "ef6326b2a46a4b4cc65163c203b159ff6a004dea", "url": "https://github.com/apache/iotdb/commit/ef6326b2a46a4b4cc65163c203b159ff6a004dea", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-01-16T12:19:59Z", "type": "commit"}, {"oid": "d6196d02a918d521220022d4c78e57902c63fb0d", "url": "https://github.com/apache/iotdb/commit/d6196d02a918d521220022d4c78e57902c63fb0d", "message": "fix some bugs", "committedDate": "2020-01-16T12:55:58Z", "type": "commit"}, {"oid": "af4187be5c6a487e2088f1326fa65d8ce58cdb6f", "url": "https://github.com/apache/iotdb/commit/af4187be5c6a487e2088f1326fa65d8ce58cdb6f", "message": "fix bugs", "committedDate": "2020-01-16T12:59:18Z", "type": "commit"}, {"oid": "5e9f11c73d27668a8277a08e5447fce134507f14", "url": "https://github.com/apache/iotdb/commit/5e9f11c73d27668a8277a08e5447fce134507f14", "message": "fix some bug", "committedDate": "2020-01-16T13:10:04Z", "type": "commit"}, {"oid": "87a5491c0458ad1987c310fa9142b63d578223c0", "url": "https://github.com/apache/iotdb/commit/87a5491c0458ad1987c310fa9142b63d578223c0", "message": "Revert extend relationship of AVG and SUM", "committedDate": "2020-01-16T13:16:09Z", "type": "commit"}, {"oid": "e12cba927e3f36224279e49dcaa4f4c74ad170fe", "url": "https://github.com/apache/iotdb/commit/e12cba927e3f36224279e49dcaa4f4c74ad170fe", "message": "remove unused codes", "committedDate": "2020-01-16T13:23:01Z", "type": "commit"}, {"oid": "aa229d2068e259f7699ebfd80b936fc38ef1ca46", "url": "https://github.com/apache/iotdb/commit/aa229d2068e259f7699ebfd80b936fc38ef1ca46", "message": "Add iterators to TVList to prevent data copy (#753)\n\n* refactor ReadOnlyMemChunk to save copy on read", "committedDate": "2020-01-20T06:48:39Z", "type": "commit"}, {"oid": "0b38a38a32dbf57b000841303f45ebe2f648c753", "url": "https://github.com/apache/iotdb/commit/0b38a38a32dbf57b000841303f45ebe2f648c753", "message": "[IOTDB-434] Refactor SeriesReaderByTimestamp (#756)\n\n* add new byTimestampReader", "committedDate": "2020-01-21T14:58:25Z", "type": "commit"}, {"oid": "5a47f6142b7f40dbf4a33abb97a801ec0b5fb509", "url": "https://github.com/apache/iotdb/commit/5a47f6142b7f40dbf4a33abb97a801ec0b5fb509", "message": "fix filter bug", "committedDate": "2020-01-22T01:23:13Z", "type": "commit"}, {"oid": "ab0c74e1ba0d986e77853cfe28c448fac9233959", "url": "https://github.com/apache/iotdb/commit/ab0c74e1ba0d986e77853cfe28c448fac9233959", "message": "fix javadoc", "committedDate": "2020-01-22T01:38:36Z", "type": "commit"}, {"oid": "3224d8323d5248958d2ac27bce2c586a160b43f9", "url": "https://github.com/apache/iotdb/commit/3224d8323d5248958d2ac27bce2c586a160b43f9", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-01-22T01:38:49Z", "type": "commit"}, {"oid": "c497d416ad9c43a9dcc0be3fcf88c4c0d3a0329e", "url": "https://github.com/apache/iotdb/commit/c497d416ad9c43a9dcc0be3fcf88c4c0d3a0329e", "message": "refactor QueryProcessor to Planner", "committedDate": "2020-01-28T13:51:27Z", "type": "commit"}, {"oid": "cfac2a6b7dff1dae33bf2f4200f5f2c179c7777e", "url": "https://github.com/apache/iotdb/commit/cfac2a6b7dff1dae33bf2f4200f5f2c179c7777e", "message": "refactor EngineQueryRouter related", "committedDate": "2020-01-28T14:06:51Z", "type": "commit"}, {"oid": "1b00919f5392fbbf44440f9d626ec37dcd013861", "url": "https://github.com/apache/iotdb/commit/1b00919f5392fbbf44440f9d626ec37dcd013861", "message": "uncomment test", "committedDate": "2020-01-31T01:36:53Z", "type": "commit"}, {"oid": "953609a1ae327c10a983d26d9d7208050a695bef", "url": "https://github.com/apache/iotdb/commit/953609a1ae327c10a983d26d9d7208050a695bef", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-01-31T01:37:13Z", "type": "commit"}, {"oid": "086e84d40ef7b882521008fb6741bf96f8aaa2f7", "url": "https://github.com/apache/iotdb/commit/086e84d40ef7b882521008fb6741bf96f8aaa2f7", "message": "remove unused interface", "committedDate": "2020-01-31T02:20:57Z", "type": "commit"}, {"oid": "e80692852eb01ea7f89fa25293bbe20aac5cadcc", "url": "https://github.com/apache/iotdb/commit/e80692852eb01ea7f89fa25293bbe20aac5cadcc", "message": "[IOTDB-438] Modify SeriesReader to uniform usage (#758)\n\n* new SeriesReader", "committedDate": "2020-01-31T13:44:18Z", "type": "commit"}, {"oid": "f0ec06736365c13fc768bd922611fada16dcddfc", "url": "https://github.com/apache/iotdb/commit/f0ec06736365c13fc768bd922611fada16dcddfc", "message": "delete hasNext method in IReaderByTimestamp", "committedDate": "2020-02-01T12:24:12Z", "type": "commit"}, {"oid": "8103f53ac34c7ea7344c5f5ee6611bb5ba4b4bb2", "url": "https://github.com/apache/iotdb/commit/8103f53ac34c7ea7344c5f5ee6611bb5ba4b4bb2", "message": "refactor ByTimestampReaderAdapter", "committedDate": "2020-02-02T04:48:11Z", "type": "commit"}, {"oid": "94641c3ac5ec3563beab02b755cb4f4a117e6832", "url": "https://github.com/apache/iotdb/commit/94641c3ac5ec3563beab02b755cb4f4a117e6832", "message": "disable sonar for contributor pr", "committedDate": "2020-02-03T03:19:43Z", "type": "commit"}, {"oid": "c5c20f063d2c1e39d9568b61b736fffe66792e8e", "url": "https://github.com/apache/iotdb/commit/c5c20f063d2c1e39d9568b61b736fffe66792e8e", "message": "Merge branch 'master' of https://github.com/apache/incubator-iotdb", "committedDate": "2020-02-03T03:23:59Z", "type": "commit"}, {"oid": "e65cc7ae53c304615b0371e9472c13b2d53cad3c", "url": "https://github.com/apache/iotdb/commit/e65cc7ae53c304615b0371e9472c13b2d53cad3c", "message": "[IOTDB-449] Manage TsFileSequenceReader in query by FileReaderManager (#760)\n\n* manage TsFileSequenceReader in the query by FileReaderManager", "committedDate": "2020-02-03T05:48:17Z", "type": "commit"}, {"oid": "51a8788967d5935f12de6854073c53bd71407d89", "url": "https://github.com/apache/iotdb/commit/51a8788967d5935f12de6854073c53bd71407d89", "message": "modify group by", "committedDate": "2020-02-03T07:06:55Z", "type": "commit"}, {"oid": "326b1ea9dcfe9dd3b29d26a134aabc5113e80a8b", "url": "https://github.com/apache/iotdb/commit/326b1ea9dcfe9dd3b29d26a134aabc5113e80a8b", "message": "simplify aggregation", "committedDate": "2020-02-03T08:04:57Z", "type": "commit"}, {"oid": "0a81003dd396b155e43f56a07e8b2940da610b2b", "url": "https://github.com/apache/iotdb/commit/0a81003dd396b155e43f56a07e8b2940da610b2b", "message": "Merge remote-tracking branch 'origin/master' into new_series_reader", "committedDate": "2020-02-03T08:19:10Z", "type": "commit"}, {"oid": "68bb28b272ba0d9d51c3ad4842ae688ea4919dc7", "url": "https://github.com/apache/iotdb/commit/68bb28b272ba0d9d51c3ad4842ae688ea4919dc7", "message": "fix endTime in GroupByEngineDataSet", "committedDate": "2020-02-03T08:58:50Z", "type": "commit"}, {"oid": "92a9e5110f5a162966b4dbfc260425978f33ed6a", "url": "https://github.com/apache/iotdb/commit/92a9e5110f5a162966b4dbfc260425978f33ed6a", "message": "add override", "committedDate": "2020-02-03T09:08:22Z", "type": "commit"}, {"oid": "28e21862202f277197a959870406ecde906f251b", "url": "https://github.com/apache/iotdb/commit/28e21862202f277197a959870406ecde906f251b", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-03T09:08:29Z", "type": "commit"}, {"oid": "9937c5951f9e3f171843f643f6526c09994b2bbf", "url": "https://github.com/apache/iotdb/commit/9937c5951f9e3f171843f643f6526c09994b2bbf", "message": "add override in SeriesReader", "committedDate": "2020-02-03T09:11:24Z", "type": "commit"}, {"oid": "128f4d6274bb9fee15d6bee635f37da2396e9db2", "url": "https://github.com/apache/iotdb/commit/128f4d6274bb9fee15d6bee635f37da2396e9db2", "message": "refactor exception", "committedDate": "2020-02-03T10:28:05Z", "type": "commit"}, {"oid": "d98129b16a5e8fd5f6d6d783dc2670ab222df085", "url": "https://github.com/apache/iotdb/commit/d98129b16a5e8fd5f6d6d783dc2670ab222df085", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-03T10:28:17Z", "type": "commit"}, {"oid": "908068a114ef3e7731a0dcf475a3a1b05b493e29", "url": "https://github.com/apache/iotdb/commit/908068a114ef3e7731a0dcf475a3a1b05b493e29", "message": "fix IoTDBGroupbyDeviceIT and IoTDBGroupByIT", "committedDate": "2020-02-03T11:28:13Z", "type": "commit"}, {"oid": "bfbb1f3e1749ec40297d9a0e88d22be51dd2923d", "url": "https://github.com/apache/iotdb/commit/bfbb1f3e1749ec40297d9a0e88d22be51dd2923d", "message": "[IOTDB-443] Fix ReadOnlyMemChunk round float and double data incorrectly (#761)\n\n* [IOTDB-443] Fix ReadOnlyMemChunk round float and double data incorrectly", "committedDate": "2020-02-03T12:43:58Z", "type": "commit"}, {"oid": "1645ccc10dc402485b17e6b97eb142c08e6b0fd6", "url": "https://github.com/apache/iotdb/commit/1645ccc10dc402485b17e6b97eb142c08e6b0fd6", "message": "Merge branch 'master' of https://github.com/apache/incubator-iotdb", "committedDate": "2020-02-04T00:51:00Z", "type": "commit"}, {"oid": "0b82b29c97d9611556e124157095e6c7730fa518", "url": "https://github.com/apache/iotdb/commit/0b82b29c97d9611556e124157095e6c7730fa518", "message": "init", "committedDate": "2020-02-04T00:51:59Z", "type": "commit"}, {"oid": "d0e46c94a2580248a705a42b0dd3fb190819565a", "url": "https://github.com/apache/iotdb/commit/d0e46c94a2580248a705a42b0dd3fb190819565a", "message": "[developing] support select ConstValue column, support query on notExistColumns for group by device", "committedDate": "2020-02-04T02:04:09Z", "type": "commit"}, {"oid": "1990ec449691a4368c1a9e1b445efd5b531cd215", "url": "https://github.com/apache/iotdb/commit/1990ec449691a4368c1a9e1b445efd5b531cd215", "message": "test", "committedDate": "2020-02-04T02:36:10Z", "type": "commit"}, {"oid": "5e99422083b7976d0dc92d15d896e26335a4df37", "url": "https://github.com/apache/iotdb/commit/5e99422083b7976d0dc92d15d896e26335a4df37", "message": "Merge branch 'new_series_reader' of https://github.com/apache/incubator-iotdb into new_series_reader", "committedDate": "2020-02-04T02:36:29Z", "type": "commit"}, {"oid": "b8a9146fefd40ab3f62c4fc6dc8a5e0cab10ce53", "url": "https://github.com/apache/iotdb/commit/b8a9146fefd40ab3f62c4fc6dc8a5e0cab10ce53", "message": "move SeqTsFileRecoverTest output to target", "committedDate": "2020-02-04T03:34:09Z", "type": "commit"}, {"oid": "0241e4a58c58649f7dd651dd5ed676488d8aba29", "url": "https://github.com/apache/iotdb/commit/0241e4a58c58649f7dd651dd5ed676488d8aba29", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-04T03:34:33Z", "type": "commit"}, {"oid": "51ed987062f44f7c4496dc76762070be142709e2", "url": "https://github.com/apache/iotdb/commit/51ed987062f44f7c4496dc76762070be142709e2", "message": "Clear the relation of SeriesReaders (#764)\n\n* clear series readers", "committedDate": "2020-02-04T05:42:31Z", "type": "commit"}, {"oid": "1e95c05cf13ccdb295bf49e7acfce63d4d19c29b", "url": "https://github.com/apache/iotdb/commit/1e95c05cf13ccdb295bf49e7acfce63d4d19c29b", "message": "Add constant and not exist column", "committedDate": "2020-02-04T07:03:15Z", "type": "commit"}, {"oid": "594703b4b3befef81d503ebe579b9dfa846cd409", "url": "https://github.com/apache/iotdb/commit/594703b4b3befef81d503ebe579b9dfa846cd409", "message": "[IOTDB-335] Merge different aggregations on same timeseries (#765)\n\n* [IOTDB-335] Separate query executions of the same timeseries with different aggregate functions may be optimized", "committedDate": "2020-02-04T10:31:26Z", "type": "commit"}, {"oid": "a1390bbc5bbeeecfcbbcff418133ca6564e5ac64", "url": "https://github.com/apache/iotdb/commit/a1390bbc5bbeeecfcbbcff418133ca6564e5ac64", "message": "optimize aggregation", "committedDate": "2020-02-04T10:39:16Z", "type": "commit"}, {"oid": "551c99ec73789ad3ad1fcd090bec76cbd1d93a98", "url": "https://github.com/apache/iotdb/commit/551c99ec73789ad3ad1fcd090bec76cbd1d93a98", "message": "add some tests", "committedDate": "2020-02-04T11:27:40Z", "type": "commit"}, {"oid": "055bbcb21a336b34b93393e8cf20ab8aed201c1a", "url": "https://github.com/apache/iotdb/commit/055bbcb21a336b34b93393e8cf20ab8aed201c1a", "message": "Test", "committedDate": "2020-02-04T11:42:52Z", "type": "commit"}, {"oid": "1c8b37d52ba1071d73e9b14801bc1a90a972d38f", "url": "https://github.com/apache/iotdb/commit/1c8b37d52ba1071d73e9b14801bc1a90a972d38f", "message": "fix some code smell", "committedDate": "2020-02-04T12:19:25Z", "type": "commit"}, {"oid": "5c2d3d58188eb8d8d338d99ad2b1d1a85d125673", "url": "https://github.com/apache/iotdb/commit/5c2d3d58188eb8d8d338d99ad2b1d1a85d125673", "message": "Merge remote-tracking branch 'origin/new_series_reader' into new_series_reader", "committedDate": "2020-02-04T12:19:51Z", "type": "commit"}, {"oid": "72fd634115b362f7b9c161a74339236ec8eaa288", "url": "https://github.com/apache/iotdb/commit/72fd634115b362f7b9c161a74339236ec8eaa288", "message": "remove unused test class", "committedDate": "2020-02-04T12:22:44Z", "type": "commit"}, {"oid": "9dcaa2c540aa31af5221273cea2b031cdc6668ad", "url": "https://github.com/apache/iotdb/commit/9dcaa2c540aa31af5221273cea2b031cdc6668ad", "message": "rename RawDataBatchReader to SeriesRawDataBatchReader", "committedDate": "2020-02-05T01:13:19Z", "type": "commit"}, {"oid": "946fffc4f157b0fabe80fd34a47bb11d2b99cae8", "url": "https://github.com/apache/iotdb/commit/946fffc4f157b0fabe80fd34a47bb11d2b99cae8", "message": "rename AggregateReader to SeriesAggregateReader", "committedDate": "2020-02-05T01:28:27Z", "type": "commit"}, {"oid": "b87bfc76a83b55d2cb16e911b319e95dc9600d72", "url": "https://github.com/apache/iotdb/commit/b87bfc76a83b55d2cb16e911b319e95dc9600d72", "message": "Merge branch 'master' of https://github.com/apache/incubator-iotdb into IOTDB-447-query-as-much-as-possible", "committedDate": "2020-02-05T02:18:52Z", "type": "commit"}, {"oid": "2d3264f7beea6e80432d80accfd37a1fcc29ba38", "url": "https://github.com/apache/iotdb/commit/2d3264f7beea6e80432d80accfd37a1fcc29ba38", "message": "fix defaultFillInterval", "committedDate": "2020-02-05T03:24:21Z", "type": "commit"}, {"oid": "ceea14c45c553f888242d4a3f36a56d62641f101", "url": "https://github.com/apache/iotdb/commit/ceea14c45c553f888242d4a3f36a56d62641f101", "message": "fix default fill type of int/long/float/double to LinearFill", "committedDate": "2020-02-05T04:00:34Z", "type": "commit"}, {"oid": "aa0d67bad440078e565b1f6827c89dc471d2cadd", "url": "https://github.com/apache/iotdb/commit/aa0d67bad440078e565b1f6827c89dc471d2cadd", "message": "rename FillEngineExecutor to FillQueryExecutor", "committedDate": "2020-02-05T04:08:15Z", "type": "commit"}, {"oid": "0b636dc926f73764a61fd208d956a0bbaaae75b7", "url": "https://github.com/apache/iotdb/commit/0b636dc926f73764a61fd208d956a0bbaaae75b7", "message": "fixed new_series_reader problem reported in sonar (#768)\n\n* fix bug and code smell that sonar scans", "committedDate": "2020-02-05T06:34:42Z", "type": "commit"}, {"oid": "255d9cee1f91ec1fa23141e4523d9ecec1f5ad2c", "url": "https://github.com/apache/iotdb/commit/255d9cee1f91ec1fa23141e4523d9ecec1f5ad2c", "message": "some file package changes", "committedDate": "2020-02-05T07:07:07Z", "type": "commit"}, {"oid": "1222a15f8cab9f34373971a569b3b9c1f5e12516", "url": "https://github.com/apache/iotdb/commit/1222a15f8cab9f34373971a569b3b9c1f5e12516", "message": "resolve conflicts", "committedDate": "2020-02-05T07:28:36Z", "type": "commit"}, {"oid": "c311b36e5ddde3734e2d7bf18347ba56fa457483", "url": "https://github.com/apache/iotdb/commit/c311b36e5ddde3734e2d7bf18347ba56fa457483", "message": "Merge new_series_reader and solve conflicts", "committedDate": "2020-02-05T08:34:34Z", "type": "commit"}, {"oid": "e291c91c5c6bd557b101678611b80776da452b78", "url": "https://github.com/apache/iotdb/commit/e291c91c5c6bd557b101678611b80776da452b78", "message": "Rename", "committedDate": "2020-02-05T08:43:50Z", "type": "commit"}, {"oid": "740f6143cbf25bfad9bfb54e61f2c485b9f4adcc", "url": "https://github.com/apache/iotdb/commit/740f6143cbf25bfad9bfb54e61f2c485b9f4adcc", "message": "change package name", "committedDate": "2020-02-05T08:47:48Z", "type": "commit"}, {"oid": "132fe564c906fedd0b8bd5b18e9ada6f2bb8ce36", "url": "https://github.com/apache/iotdb/commit/132fe564c906fedd0b8bd5b18e9ada6f2bb8ce36", "message": "Merge remote-tracking branch 'upstream/new_series_reader' into new_series_reader\n\n# Conflicts:\n#\tserver/src/main/java/org/apache/iotdb/db/query/dataset/groupby/GroupByWithoutValueFilterDataSet.java\n#\tserver/src/main/java/org/apache/iotdb/db/query/executor/AggregationExecutor.java", "committedDate": "2020-02-05T08:49:51Z", "type": "commit"}, {"oid": "7715c3b9afc793c3374adf88033815472f081ab8", "url": "https://github.com/apache/iotdb/commit/7715c3b9afc793c3374adf88033815472f081ab8", "message": "Solve conflicts", "committedDate": "2020-02-05T08:51:50Z", "type": "commit"}, {"oid": "9e7a0767265924e1ec266ed98820ea9d1cc2499e", "url": "https://github.com/apache/iotdb/commit/9e7a0767265924e1ec266ed98820ea9d1cc2499e", "message": "add doc", "committedDate": "2020-02-05T11:29:35Z", "type": "commit"}, {"oid": "c45fbbb8eb5cb819b12213f807c51e4695f3b80e", "url": "https://github.com/apache/iotdb/commit/c45fbbb8eb5cb819b12213f807c51e4695f3b80e", "message": "resolve conflict after merging master", "committedDate": "2020-02-05T13:01:39Z", "type": "commit"}, {"oid": "03e18f74b7c4b635af56f68c6372ea1566055ea4", "url": "https://github.com/apache/iotdb/commit/03e18f74b7c4b635af56f68c6372ea1566055ea4", "message": "Add tests", "committedDate": "2020-02-05T15:00:36Z", "type": "commit"}, {"oid": "5a06b8d762fc77925a27082b3bfa098e1742e10a", "url": "https://github.com/apache/iotdb/commit/5a06b8d762fc77925a27082b3bfa098e1742e10a", "message": "Add test", "committedDate": "2020-02-06T00:25:27Z", "type": "commit"}]}