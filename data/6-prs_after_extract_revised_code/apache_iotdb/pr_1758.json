{"pr_number": 1758, "pr_title": "add device chunk point cache", "pr_createdAt": "2020-09-24T04:07:10Z", "pr_url": "https://github.com/apache/iotdb/pull/1758", "timeline": [{"oid": "4e2e3d03865a814e420918e00921735a49b00b98", "url": "https://github.com/apache/iotdb/commit/4e2e3d03865a814e420918e00921735a49b00b98", "message": "add device chunk point cache", "committedDate": "2020-09-24T04:06:32Z", "type": "commit"}, {"oid": "3997918477caf4d03e4065095737004ca5dd8060", "url": "https://github.com/apache/iotdb/commit/3997918477caf4d03e4065095737004ca5dd8060", "message": "add license", "committedDate": "2020-09-24T04:49:22Z", "type": "commit"}, {"oid": "ac32635d7593c481d84cce1c1fd20972fc433191", "url": "https://github.com/apache/iotdb/commit/ac32635d7593c481d84cce1c1fd20972fc433191", "message": "remove count by chunk point", "committedDate": "2020-09-24T06:40:02Z", "type": "commit"}, {"oid": "3bfc638c5e8a4e2892cb9683d475aa516642cf95", "url": "https://github.com/apache/iotdb/commit/3bfc638c5e8a4e2892cb9683d475aa516642cf95", "message": "remove more", "committedDate": "2020-09-24T07:11:16Z", "type": "commit"}, {"oid": "85a8ba1ae15bb8ba7d9f6cfe1ce606f39c106755", "url": "https://github.com/apache/iotdb/commit/85a8ba1ae15bb8ba7d9f6cfe1ce606f39c106755", "message": "Merge branch 'master' into add_device_chunk_point_cache", "committedDate": "2020-09-24T07:37:55Z", "type": "commit"}, {"oid": "e2304a949bee08be3bc7e5ce9c35c95ebe02e977", "url": "https://github.com/apache/iotdb/commit/e2304a949bee08be3bc7e5ce9c35c95ebe02e977", "message": "upgrade merge method", "committedDate": "2020-09-24T16:11:29Z", "type": "commit"}, {"oid": "c7fe6b37b43136a1a5d5d0890acd87f372536294", "url": "https://github.com/apache/iotdb/commit/c7fe6b37b43136a1a5d5d0890acd87f372536294", "message": "fix bug", "committedDate": "2020-09-24T17:13:31Z", "type": "commit"}, {"oid": "ecd5bd9584862df5460508cb96610d45beddf1a4", "url": "https://github.com/apache/iotdb/commit/ecd5bd9584862df5460508cb96610d45beddf1a4", "message": "fix bug", "committedDate": "2020-09-25T02:28:13Z", "type": "commit"}, {"oid": "76226ea87b3bc02def97e5bc5d035dbbb137165b", "url": "https://github.com/apache/iotdb/commit/76226ea87b3bc02def97e5bc5d035dbbb137165b", "message": "update version", "committedDate": "2020-09-25T04:43:32Z", "type": "commit"}, {"oid": "621fe1d6d9529e634f493630e9fdad065b7a7dcf", "url": "https://github.com/apache/iotdb/commit/621fe1d6d9529e634f493630e9fdad065b7a7dcf", "message": "change merge strategy\n\nUse the approximate average value to determine whether to merge", "committedDate": "2020-09-29T15:16:15Z", "type": "commit"}, {"oid": "be8f7aabfaa202d8eda9bac714fc59af2f8f511d", "url": "https://github.com/apache/iotdb/commit/be8f7aabfaa202d8eda9bac714fc59af2f8f511d", "message": "Merge pull request #3 from CRZbulabula/add_device_chunk_point_cache\n\nadd chunk point approximate estimate", "committedDate": "2020-10-04T05:51:20Z", "type": "commit"}, {"oid": "dc9c37b6b2d28cef0174a24a752426789f5f294c", "url": "https://github.com/apache/iotdb/commit/dc9c37b6b2d28cef0174a24a752426789f5f294c", "message": "add unseq merge config and logic with instance unseq merge", "committedDate": "2020-10-04T09:42:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0OTQzMg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r500049432", "bodyText": "this reader should be closed here", "author": "EJTTianYu", "createdAt": "2020-10-06T07:05:37Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.engine.cache;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetadata;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class FileChunkPointSizeCache {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(FileChunkPointSizeCache.class);\n+\n+  // (absolute path,avg chunk point size)\n+  private Map<String, Map<String, Long>> tsfileDeviceChunkPointCache;\n+\n+  private FileChunkPointSizeCache() {\n+    tsfileDeviceChunkPointCache = new HashMap<>();\n+  }\n+\n+  public static FileChunkPointSizeCache getInstance() {\n+    return FileChunkPointSizeCacheHolder.INSTANCE;\n+  }\n+\n+  public Map<String, Long> get(File tsfile) {\n+    String path = tsfile.getAbsolutePath();\n+    return tsfileDeviceChunkPointCache.computeIfAbsent(path, k -> {\n+      Map<String, Long> deviceChunkPointMap = new HashMap<>();\n+      try {\n+        if (tsfile.exists()) {\n+          TsFileSequenceReader reader = new TsFileSequenceReader(path);", "originalCommit": "dc9c37b6b2d28cef0174a24a752426789f5f294c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44d47316308ebf003d9babebf98d312db7f6e85c", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java b/server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java\ndeleted file mode 100644\nindex b331521fb..000000000\n--- a/server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java\n+++ /dev/null\n\n@@ -1,86 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iotdb.db.engine.cache;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import org.apache.iotdb.tsfile.file.metadata.ChunkMetadata;\n-import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n-import org.apache.iotdb.tsfile.read.common.Path;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-public class FileChunkPointSizeCache {\n-\n-  private static final Logger logger = LoggerFactory.getLogger(FileChunkPointSizeCache.class);\n-\n-  // (absolute path,avg chunk point size)\n-  private Map<String, Map<String, Long>> tsfileDeviceChunkPointCache;\n-\n-  private FileChunkPointSizeCache() {\n-    tsfileDeviceChunkPointCache = new HashMap<>();\n-  }\n-\n-  public static FileChunkPointSizeCache getInstance() {\n-    return FileChunkPointSizeCacheHolder.INSTANCE;\n-  }\n-\n-  public Map<String, Long> get(File tsfile) {\n-    String path = tsfile.getAbsolutePath();\n-    return tsfileDeviceChunkPointCache.computeIfAbsent(path, k -> {\n-      Map<String, Long> deviceChunkPointMap = new HashMap<>();\n-      try {\n-        if (tsfile.exists()) {\n-          TsFileSequenceReader reader = new TsFileSequenceReader(path);\n-          List<Path> pathList = reader.getAllPaths();\n-          for (Path sensorPath : pathList) {\n-            String device = sensorPath.getDevice();\n-            long chunkPointNum = deviceChunkPointMap.getOrDefault(device, 0L);\n-            List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n-            for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-              chunkPointNum += chunkMetadata.getNumOfPoints();\n-            }\n-            deviceChunkPointMap.put(device, chunkPointNum);\n-          }\n-        } else {\n-          logger.info(\"{} tsfile does not exist\", path);\n-        }\n-      } catch (IOException e) {\n-        logger.error(\n-            \"{} tsfile reader creates error\", path, e);\n-      }\n-      return deviceChunkPointMap;\n-    });\n-  }\n-\n-  public void put(String tsfilePath, Map<String, Long> deviceChunkPointMap) {\n-    tsfileDeviceChunkPointCache.put(tsfilePath, deviceChunkPointMap);\n-  }\n-\n-  /**\n-   * singleton pattern.\n-   */\n-  private static class FileChunkPointSizeCacheHolder {\n-\n-    private static final FileChunkPointSizeCache INSTANCE = new FileChunkPointSizeCache();\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDEwNjk4Ng==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r500106986", "bodyText": "seems this function is never used", "author": "EJTTianYu", "createdAt": "2020-10-06T08:44:27Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "diffHunk": "@@ -644,8 +650,29 @@ public void flushOneMemTable() {\n     }\n   }\n \n+  private void updateDeviceChunkPointSizeCache() {", "originalCommit": "dc9c37b6b2d28cef0174a24a752426789f5f294c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "44d47316308ebf003d9babebf98d312db7f6e85c", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\nindex 0b3f08da2..94cf00f44 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\n\n@@ -650,26 +648,6 @@ public class TsFileProcessor {\n     }\n   }\n \n-  private void updateDeviceChunkPointSizeCache() {\n-    Map<String, Map<String, List<ChunkMetadata>>> deviceMeasurementChunkMetadataMap = writer\n-        .getMetadatasForQuery();\n-    Map<String, Long> deviceChunkPointMap = new HashMap<>();\n-    for (Entry<String, Map<String, List<ChunkMetadata>>> deviceMeasurementChunkMetadataEntry : deviceMeasurementChunkMetadataMap\n-        .entrySet()) {\n-      String device = deviceMeasurementChunkMetadataEntry.getKey();\n-      long chunkPointNum = 0;\n-      for (Entry<String, List<ChunkMetadata>> measurementChunkMetadataEntry : deviceMeasurementChunkMetadataEntry\n-          .getValue().entrySet()) {\n-        for (ChunkMetadata chunkMetadata : measurementChunkMetadataEntry.getValue()) {\n-          chunkPointNum += chunkMetadata.getNumOfPoints();\n-        }\n-      }\n-      deviceChunkPointMap.put(device, chunkPointNum);\n-    }\n-    FileChunkPointSizeCache.getInstance()\n-        .put(tsFileResource.getTsFile().getAbsolutePath(), deviceChunkPointMap);\n-  }\n-\n   private void endFile() throws IOException, TsFileProcessorException {\n     long closeStartTime = System.currentTimeMillis();\n //    updateDeviceChunkPointSizeCache();\n"}}, {"oid": "c854f433a4d10823c2b07bf05e6338b07661a282", "url": "https://github.com/apache/iotdb/commit/c854f433a4d10823c2b07bf05e6338b07661a282", "message": "comment to fix oom temp", "committedDate": "2020-10-07T08:23:51Z", "type": "commit"}, {"oid": "11a28e0f5a59e7b70554ff0e38f1f5a3d9abd0e5", "url": "https://github.com/apache/iotdb/commit/11a28e0f5a59e7b70554ff0e38f1f5a3d9abd0e5", "message": "add merge page point number", "committedDate": "2020-10-08T05:07:44Z", "type": "commit"}, {"oid": "cc636ee7d86f55cc83ba5bd37c6e3284513f9e50", "url": "https://github.com/apache/iotdb/commit/cc636ee7d86f55cc83ba5bd37c6e3284513f9e50", "message": "Merge branch 'master' into add_device_chunk_point_cache\n\n# Conflicts:\n#\tserver/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n#\tserver/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n#\tserver/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java", "committedDate": "2020-10-08T07:23:57Z", "type": "commit"}, {"oid": "6cd1f0410729f0fd11077b7745493778d52c143b", "url": "https://github.com/apache/iotdb/commit/6cd1f0410729f0fd11077b7745493778d52c143b", "message": "add log", "committedDate": "2020-10-09T02:06:11Z", "type": "commit"}, {"oid": "7528cb515334b17aa4d0d147e614b7de95741af4", "url": "https://github.com/apache/iotdb/commit/7528cb515334b17aa4d0d147e614b7de95741af4", "message": "update log", "committedDate": "2020-10-09T04:33:02Z", "type": "commit"}, {"oid": "5441a61d621aa6044ee614b59b70d9fa30b28b9b", "url": "https://github.com/apache/iotdb/commit/5441a61d621aa6044ee614b59b70d9fa30b28b9b", "message": "add read compaction limiter", "committedDate": "2020-10-10T04:11:58Z", "type": "commit"}, {"oid": "77e050c46d82ed6f7b8636d99fd9f89923e51137", "url": "https://github.com/apache/iotdb/commit/77e050c46d82ed6f7b8636d99fd9f89923e51137", "message": "add thread pool limit", "committedDate": "2020-10-12T04:54:44Z", "type": "commit"}, {"oid": "ae1196ca292b9b5f2291ca1cf24b69ec176b7311", "url": "https://github.com/apache/iotdb/commit/ae1196ca292b9b5f2291ca1cf24b69ec176b7311", "message": "update test", "committedDate": "2020-10-14T07:42:53Z", "type": "commit"}, {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c", "url": "https://github.com/apache/iotdb/commit/44d47316308ebf003d9babebf98d312db7f6e85c", "message": "remove useless code", "committedDate": "2020-10-14T08:48:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5ODAxNg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504598016", "bodyText": "every time using getMergeWriteRateLimiter() will call this set method setWriteMergeRate ,if this para can be changed after IoTDB start, this is OK, otherwise...", "author": "EJTTianYu", "createdAt": "2020-10-14T11:20:17Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/merge/manage/MergeManager.java", "diffHunk": "@@ -74,13 +75,18 @@\n   private MergeManager() {\n   }\n \n-  public RateLimiter getMergeRateLimiter() {\n-    setMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeThroughputMbPerSec());\n-    return mergeRateLimiter;\n+  public RateLimiter getMergeWriteRateLimiter() {\n+    setWriteMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeWriteThroughputMbPerSec());", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1NzE0Ng==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508457146", "bodyText": "It may can be set afterward in cli", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:27:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5ODAxNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTUxMw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504599513", "bodyText": "it would be better to set it as a private variable", "author": "EJTTianYu", "createdAt": "2020-10-14T11:23:20Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -124,7 +116,7 @@\n  */\n public class StorageGroupProcessor {\n \n-  private static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";\n+  public static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODI0Mw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458243", "bodyText": "TsFileManage also use it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:28:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTUxMw=="}], "type": "inlineReview", "revised_code": {"commit": "5e76b91d910155b95946682b04f5e55db4020d70", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\nindex f613d422f..7ee710686 100755\n--- a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n\n@@ -117,6 +117,7 @@ import org.slf4j.LoggerFactory;\n public class StorageGroupProcessor {\n \n   public static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";\n+  private static final String FAIL_TO_UPGRADE_FOLDER = \"Failed to move {} to upgrade folder\";\n \n   /**\n    * All newly generated chunks after merge have version number 0, so we set merged Modification\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTk1Ng==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504599956", "bodyText": "it would be better to set it as a private variable", "author": "EJTTianYu", "createdAt": "2020-10-14T11:24:17Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODM1MA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458350", "bodyText": "okay, I will solve it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:28:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTk1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\nindex f613d422f..1153dc9a4 100755\n--- a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n\n@@ -200,7 +200,7 @@ public class StorageGroupProcessor {\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  public TsFileManagement tsFileManagement;\n+  private TsFileManagement tsFileManagement;\n \n   /**\n    * time partition id -> version controller which assigns a version for each MemTable and\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNDU2Mg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504604562", "bodyText": "if the comment is used, it shoule be deleted then", "author": "EJTTianYu", "createdAt": "2020-10-14T11:33:05Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "diffHunk": "@@ -647,6 +650,7 @@ public void flushOneMemTable() {\n \n   private void endFile() throws IOException, TsFileProcessorException {\n     long closeStartTime = System.currentTimeMillis();\n+//    updateDeviceChunkPointSizeCache();", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODQ3MA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458470", "bodyText": "okay", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:29:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNDU2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\nindex 94cf00f44..245a3ff7a 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java\n\n@@ -650,7 +650,6 @@ public class TsFileProcessor {\n \n   private void endFile() throws IOException, TsFileProcessorException {\n     long closeStartTime = System.currentTimeMillis();\n-//    updateDeviceChunkPointSizeCache();\n     tsFileResource.serialize();\n     writer.endFile();\n     tsFileResource.cleanCloseFlag();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNjIwNQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504616205", "bodyText": "unused comments should be deleted then", "author": "EJTTianYu", "createdAt": "2020-10-14T11:54:50Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -67,17 +74,24 @@\n   private final int maxLevelNum = IoTDBDescriptor.getInstance().getConfig().getMaxLevelNum();\n   private final int maxFileNumInEachLevel = IoTDBDescriptor.getInstance().getConfig()\n       .getMaxFileNumInEachLevel();\n+  private final int maxUnseqLevelNum = IoTDBDescriptor.getInstance().getConfig()\n+      .getMaxUnseqLevelNum();\n+  private final int maxUnseqFileNumInEachLevel = IoTDBDescriptor.getInstance().getConfig()\n+      .getMaxFileNumInEachLevel();\n   private final int maxChunkPointNum = IoTDBDescriptor.getInstance().getConfig()\n       .getMergeChunkPointNumberThreshold();\n+  private final boolean isForceFullMerge = IoTDBDescriptor.getInstance().getConfig()\n+      .isForceFullMerge();\n   // First map is partition list; Second list is level list; Third list is file list in level;\n   private final Map<Long, List<TreeSet<TsFileResource>>> sequenceTsFileResources = new ConcurrentSkipListMap<>();\n   private final Map<Long, List<List<TsFileResource>>> unSequenceTsFileResources = new ConcurrentSkipListMap<>();\n   private final List<List<TsFileResource>> forkedSequenceTsFileResources = new ArrayList<>();\n   private final List<List<TsFileResource>> forkedUnSequenceTsFileResources = new ArrayList<>();\n-  private long forkedSeqListPointNum = 0;\n-  private Map<Path, MeasurementSchema> forkedSeqListPathMeasurementSchemaMap = new HashMap<>();\n-  private long forkedUnSeqListPointNum = 0;\n-  private Map<Path, MeasurementSchema> forkedUnSeqListPathMeasurementSchemaMap = new HashMap<>();\n+\n+//  private double forkedSeqListPointNum = 0;\n+//  private double forkedSeqListMeasurementSize = 0;\n+//  private double forkedUnSeqListPointNum = 0;\n+//  private double forkedUnSeqListMeasurementSize = 0;", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f8ff28dca6b1004676240cd8bffa4e31e2e43f4d", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex a121871d7..d0e5d2b94 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -88,10 +88,10 @@ public class LevelTsFileManagement extends TsFileManagement {\n   private final List<List<TsFileResource>> forkedSequenceTsFileResources = new ArrayList<>();\n   private final List<List<TsFileResource>> forkedUnSequenceTsFileResources = new ArrayList<>();\n \n-//  private double forkedSeqListPointNum = 0;\n-//  private double forkedSeqListMeasurementSize = 0;\n-//  private double forkedUnSeqListPointNum = 0;\n-//  private double forkedUnSeqListMeasurementSize = 0;\n+  private double forkedSeqListPointNum = 0;\n+  private double forkedSeqListMeasurementSize = 0;\n+  private double forkedUnSeqListPointNum = 0;\n+  private double forkedUnSeqListMeasurementSize = 0;\n \n   public LevelTsFileManagement(String storageGroupName, String storageGroupDir) {\n     super(storageGroupName, storageGroupDir);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNzk3OA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504617978", "bodyText": "unused function here", "author": "EJTTianYu", "createdAt": "2020-10-14T11:58:01Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -113,10 +129,11 @@ private void flushAllFilesToLastLevel(long timePartitionId,\n       List<List<TsFileResource>> currMergeFiles,\n       HotCompactionLogger hotCompactionLogger, boolean sequence) throws IOException {", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODU3MA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458570", "bodyText": "use it now", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:29:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNzk3OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMDUwMA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504620500", "bodyText": "remove unused code here", "author": "EJTTianYu", "createdAt": "2020-10-14T12:02:30Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODY3NA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458674", "bodyText": "use it now", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:29:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMDUwMA=="}], "type": "inlineReview", "revised_code": {"commit": "f8ff28dca6b1004676240cd8bffa4e31e2e43f4d", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex a121871d7..d0e5d2b94 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -441,71 +441,14 @@ public class LevelTsFileManagement extends TsFileManagement {\n             .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-//  private Pair<Double, Double> forkTsFileList(\n-//      List<List<TsFileResource>> forkedTsFileResources,\n-//      List rawTsFileResources, int currMaxLevel) {\n-//    forkedTsFileResources.clear();\n-//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n-//    long pointNum = 0;\n-//    // all flush to target file\n-//    ICardinality measurementSet = new HyperLogLog(13);\n-//    for (int i = 0; i < currMaxLevel - 1; i++) {\n-//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n-//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n-//          .get(i);\n-//      synchronized (levelRawTsFileResources) {\n-//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-//          if (tsFileResource.isClosed()) {\n-//            String path = tsFileResource.getTsFile().getAbsolutePath();\n-//            try {\n-//              if (tsFileResource.getTsFile().exists()) {\n-//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n-//                List<Path> pathList = reader.getAllPaths();\n-//                for (Path sensorPath : pathList) {\n-//                  measurementSet.offer(sensorPath.getFullPath());\n-//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n-//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-//                    pointNum += chunkMetadata.getNumOfPoints();\n-//                  }\n-//                }\n-//              } else {\n-//                logger.info(\"{} tsfile does not exist\", path);\n-//              }\n-//            } catch (IOException e) {\n-//              logger.error(\n-//                  \"{} tsfile reader creates error\", path, e);\n-//            }\n-//          }\n-//          if (measurementSet.cardinality() > 0\n-//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n-//            forkedLevelTsFileResources.add(tsFileResource);\n-//            break;\n-//          }\n-//          forkedLevelTsFileResources.add(tsFileResource);\n-//        }\n-//      }\n-//\n-//      if (measurementSet.cardinality() > 0\n-//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n-//        forkedTsFileResources.add(forkedLevelTsFileResources);\n-//        break;\n-//      }\n-//      forkedTsFileResources.add(forkedLevelTsFileResources);\n-//    }\n-//\n-//    // fill in empty file\n-//    while (forkedTsFileResources.size() < currMaxLevel) {\n-//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n-//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n-//    }\n-//\n-//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n-//  }\n-\n-  private void forkTsFileList(\n+  private Pair<Double, Double> forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n       List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n+    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+    long pointNum = 0;\n+    // all flush to target file\n+    ICardinality measurementSet = new HyperLogLog(13);\n     for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyNzczMw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504627733", "bodyText": "remove unused comment", "author": "EJTTianYu", "createdAt": "2020-10-14T12:15:41Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }\n+\n+  private void forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n-    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n-    long pointNum = 0;\n-    // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            forkedLevelTsFileResources.add(tsFileResource);\n           }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n-              }\n-              pathMeasurementSchemaMap.computeIfAbsent(new Path(device, measurement), k ->\n-                  new MeasurementSchema(measurement, chunkMetadataList.get(0).getDataType()));\n-            }\n-          }\n-          writer.close();\n-          forkedLevelTsFileResources.add(tsFileResource);\n-        }\n-        if (pathMeasurementSchemaMap.size() > 0\n-            && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-          break;\n         }\n       }\n-      if (pathMeasurementSchemaMap.size() > 0\n-          && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-        break;\n-      }\n       forkedTsFileResources.add(forkedLevelTsFileResources);\n     }\n-    return new Pair<>(pointNum, pathMeasurementSchemaMap);\n+\n+    // fill in empty file\n+    while (forkedTsFileResources.size() < currMaxLevel) {\n+      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+    }\n   }\n \n   @Override\n   protected void merge(long timePartition) {\n-    merge(forkedSequenceTsFileResources, true, timePartition);\n-    merge(forkedUnSequenceTsFileResources, false, timePartition);\n+    merge(forkedSequenceTsFileResources, true, timePartition, maxLevelNum, maxFileNumInEachLevel);\n+    if (maxUnseqLevelNum <= 1) {\n+      merge(isForceFullMerge, getTsFileList(true), forkedUnSequenceTsFileResources.get(0),\n+          Long.MAX_VALUE);\n+    } else {\n+      merge(forkedUnSequenceTsFileResources, false, timePartition, maxUnseqLevelNum,\n+          maxUnseqFileNumInEachLevel);\n+    }\n   }\n \n   @SuppressWarnings(\"squid:S3776\")\n   private void merge(List<List<TsFileResource>> mergeResources, boolean sequence,\n-      long timePartition) {\n+      long timePartition, int currMaxLevel, int currMaxFileNumInEachLevel) {\n     long startTimeMillis = System.currentTimeMillis();\n     try {\n       logger.info(\"{} start to filter hot compaction condition\", storageGroupName);\n-      long pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n-      Map<Path, MeasurementSchema> pathMeasurementSchemaMap =\n-          sequence ? forkedSeqListPathMeasurementSchemaMap\n-              : forkedUnSeqListPathMeasurementSchemaMap;\n-      logger.info(\"{} current sg subLevel point num: {}, measurement num: {}\", storageGroupName,\n-          pointNum, pathMeasurementSchemaMap.size());\n+//      double pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n+//      double measurementSize =\n+//          sequence ? forkedSeqListMeasurementSize : forkedUnSeqListMeasurementSize;\n+//      logger\n+//          .info(\"{} current sg subLevel point num: {}, approximate measurement num: {}\",\n+//              storageGroupName, pointNum,\n+//              measurementSize);", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODgzMA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458830", "bodyText": "use it now", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:29:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyNzczMw=="}], "type": "inlineReview", "revised_code": {"commit": "f8ff28dca6b1004676240cd8bffa4e31e2e43f4d", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex a121871d7..d0e5d2b94 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -441,71 +441,14 @@ public class LevelTsFileManagement extends TsFileManagement {\n             .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-//  private Pair<Double, Double> forkTsFileList(\n-//      List<List<TsFileResource>> forkedTsFileResources,\n-//      List rawTsFileResources, int currMaxLevel) {\n-//    forkedTsFileResources.clear();\n-//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n-//    long pointNum = 0;\n-//    // all flush to target file\n-//    ICardinality measurementSet = new HyperLogLog(13);\n-//    for (int i = 0; i < currMaxLevel - 1; i++) {\n-//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n-//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n-//          .get(i);\n-//      synchronized (levelRawTsFileResources) {\n-//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-//          if (tsFileResource.isClosed()) {\n-//            String path = tsFileResource.getTsFile().getAbsolutePath();\n-//            try {\n-//              if (tsFileResource.getTsFile().exists()) {\n-//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n-//                List<Path> pathList = reader.getAllPaths();\n-//                for (Path sensorPath : pathList) {\n-//                  measurementSet.offer(sensorPath.getFullPath());\n-//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n-//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-//                    pointNum += chunkMetadata.getNumOfPoints();\n-//                  }\n-//                }\n-//              } else {\n-//                logger.info(\"{} tsfile does not exist\", path);\n-//              }\n-//            } catch (IOException e) {\n-//              logger.error(\n-//                  \"{} tsfile reader creates error\", path, e);\n-//            }\n-//          }\n-//          if (measurementSet.cardinality() > 0\n-//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n-//            forkedLevelTsFileResources.add(tsFileResource);\n-//            break;\n-//          }\n-//          forkedLevelTsFileResources.add(tsFileResource);\n-//        }\n-//      }\n-//\n-//      if (measurementSet.cardinality() > 0\n-//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n-//        forkedTsFileResources.add(forkedLevelTsFileResources);\n-//        break;\n-//      }\n-//      forkedTsFileResources.add(forkedLevelTsFileResources);\n-//    }\n-//\n-//    // fill in empty file\n-//    while (forkedTsFileResources.size() < currMaxLevel) {\n-//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n-//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n-//    }\n-//\n-//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n-//  }\n-\n-  private void forkTsFileList(\n+  private Pair<Double, Double> forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n       List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n+    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+    long pointNum = 0;\n+    // all flush to target file\n+    ICardinality measurementSet = new HyperLogLog(13);\n     for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODY5NQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504628695", "bodyText": "I think some comments should be added, for the software is open sourced", "author": "EJTTianYu", "createdAt": "2020-10-14T12:17:18Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n+      .getMergePagePointNumberThreshold();\n \n   private HotCompactionUtils() {\n     throw new IllegalStateException(\"Utility class\");\n   }\n \n-  private static Pair<ChunkMetadata, Chunk> writeSeqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId,\n-      String measurementId,\n-      List<TsFileResource> levelResources)\n-      throws IOException {\n+  private static Pair<ChunkMetadata, Chunk> readByAppendMerge(RateLimiter compactionReadRateLimiter,", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1OTEyNQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508459125", "bodyText": "okay, I will add some comment", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T12:30:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODY5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\nindex d7dcd4c89..2abfdf54c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n\n@@ -55,7 +55,7 @@ import org.slf4j.LoggerFactory;\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n-  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n+  private static final int MERGE_PAGE_POINT_NUM = IoTDBDescriptor.getInstance().getConfig()\n       .getMergePagePointNumberThreshold();\n \n   private HotCompactionUtils() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MDE5Mg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504640192", "bodyText": "This code is consistent with the if, can be extracted", "author": "EJTTianYu", "createdAt": "2020-10-14T12:36:00Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {\n+      return maxVersion;\n+    }\n+    IChunkWriter chunkWriter = new ChunkWriterImpl(\n+        new MeasurementSchema(entry.getKey(), chunkMetadataList.get(0).getDataType()));\n+    for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n+      writeTVPair(timeValuePair, chunkWriter);\n+      targetResource.updateStartTime(device, timeValuePair.getTimestamp());\n+      targetResource.updateEndTime(device, timeValuePair.getTimestamp());\n+    }\n+    // wait for limit write\n+    MergeManager\n+        .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n+    chunkWriter.writeToFileWriter(writer);\n+    return maxVersion;\n+  }\n+\n+  private static Set<String> getTsFileDevicesSet(List<TsFileResource> subLevelResources,\n       Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String storageGroup)\n       throws IOException {\n+    Set<String> tsFileDevicesSet = new HashSet<>();\n     for (TsFileResource levelResource : subLevelResources) {\n       TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n           tsFileSequenceReaderMap,\n           storageGroup);\n       if (reader == null) {\n         continue;\n       }\n-      List<Path> allPaths = reader.getAllPaths();\n-      Map<String, TSDataType> allMeasurements = reader.getAllMeasurements();\n-      // device, measurement -> chunk metadata list\n-      for (Path path : allPaths) {\n-        if (devices.contains(path.getDevice())) {\n-          continue;\n-        }\n-        Map<String, MeasurementSchema> measurementSchemaMap = deviceMeasurementMap\n-            .computeIfAbsent(path.getDevice(), k -> new HashMap<>());\n-\n-        // measurement, chunk metadata list\n-        measurementSchemaMap.computeIfAbsent(path.getMeasurement(), k ->\n-            new MeasurementSchema(k, allMeasurements.get(path.getMeasurement())));\n-      }\n+      tsFileDevicesSet.addAll(reader.getAllDevices());\n     }\n+    return tsFileDevicesSet;\n   }\n \n+  /**\n+   * @param targetResource the target resource to be merged to\n+   * @param tsFileResources the source resource to be merged\n+   * @param storageGroup the storage group name\n+   * @param hotCompactionLogger the logger\n+   * @param devices the devices to be skipped(used by recover)\n+   */\n   @SuppressWarnings(\"squid:S3776\") // Suppress high Cognitive Complexity warning\n   public static void merge(TsFileResource targetResource,\n       List<TsFileResource> tsFileResources, String storageGroup,\n       HotCompactionLogger hotCompactionLogger,\n       Set<String> devices, boolean sequence) throws IOException {\n     RestorableTsFileIOWriter writer = new RestorableTsFileIOWriter(targetResource.getTsFile());\n     Map<String, TsFileSequenceReader> tsFileSequenceReaderMap = new HashMap<>();\n-    Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap = new HashMap<>();\n-    RateLimiter compactionRateLimiter = MergeManager.getINSTANCE().getMergeRateLimiter();\n-    fillDeviceMeasurementMap(devices, deviceMeasurementMap, tsFileResources,\n-        tsFileSequenceReaderMap, storageGroup);\n-    if (!sequence) {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n+    RateLimiter compactionWriteRateLimiter = MergeManager.getINSTANCE().getMergeWriteRateLimiter();\n+    RateLimiter compactionReadRateLimiter = MergeManager.getINSTANCE().getMergeReadRateLimiter();\n+    Set<String> tsFileDevicesMap = getTsFileDevicesSet(tsFileResources, tsFileSequenceReaderMap,\n+        storageGroup);\n+    for (String device : tsFileDevicesMap) {\n+      if (devices.contains(device)) {\n+        continue;\n+      }\n+      writer.startChunkGroup(device);\n+      // sort chunkMeta by measurement\n+      Map<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> measurementChunkMetadataMap = new HashMap<>();\n+      for (TsFileResource levelResource : tsFileResources) {\n+        TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n+            tsFileSequenceReaderMap, storageGroup);\n+        Map<String, List<ChunkMetadata>> chunkMetadataMap = reader\n+            .readChunkMetadataInDevice(device);\n+        long chunkMetadataSize = 0;\n+        for (Entry<String, List<ChunkMetadata>> entry : chunkMetadataMap.entrySet()) {\n+          for (ChunkMetadata chunkMetadata : entry.getValue()) {\n+            chunkMetadataSize += chunkMetadata.getStatistics().calculateRamSize();\n+            Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap;\n+            String measurementUid = chunkMetadata.getMeasurementUid();\n+            if (measurementChunkMetadataMap.containsKey(measurementUid)) {\n+              readerChunkMetadataMap = measurementChunkMetadataMap.get(measurementUid);\n+            } else {\n+              readerChunkMetadataMap = new LinkedHashMap<>();\n+            }\n+            List<ChunkMetadata> chunkMetadataList;\n+            if (readerChunkMetadataMap.containsKey(reader)) {\n+              chunkMetadataList = readerChunkMetadataMap.get(reader);\n+            } else {\n+              chunkMetadataList = new ArrayList<>();\n+            }\n+            chunkMetadataList.add(chunkMetadata);\n+            readerChunkMetadataMap.put(reader, chunkMetadataList);\n+            measurementChunkMetadataMap\n+                .put(chunkMetadata.getMeasurementUid(), readerChunkMetadataMap);\n+          }\n+        }\n+        // wait for limit read\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkMetadataSize);\n+      }\n+      if (!sequence) {\n         long maxVersion = Long.MIN_VALUE;\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n-          maxVersion = readUnseqChunk(storageGroup, tsFileSequenceReaderMap, deviceId,\n-              maxVersion, measurementId, timeValuePairMap, tsFileResources);\n-          IChunkWriter chunkWriter = new ChunkWriterImpl(entry.getValue());\n-          for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n-            writeTVPair(timeValuePair, chunkWriter);\n-            targetResource.updateStartTime(deviceId, timeValuePair.getTimestamp());\n-            targetResource.updateEndTime(deviceId, timeValuePair.getTimestamp());\n-          }\n-          // wait for limit write\n-          MergeManager\n-              .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n-          chunkWriter.writeToFileWriter(writer);\n+          maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+              compactionReadRateLimiter,\n+              entry,\n+              targetResource, writer);\n         }\n-        writer.writeVersion(maxVersion);\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());\n         }\n-      }\n-    } else {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+      } else {\n+        long maxVersion = Long.MIN_VALUE;\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Pair<ChunkMetadata, Chunk> chunkPair = writeSeqChunk(storageGroup,\n-              tsFileSequenceReaderMap, deviceId, measurementId, tsFileResources);\n-          ChunkMetadata newChunkMetadata = chunkPair.left;\n-          Chunk newChunk = chunkPair.right;\n-          if (newChunkMetadata != null && newChunk != null) {\n-            // wait for limit write\n-            MergeManager.mergeRateLimiterAcquire(compactionRateLimiter,\n-                (long)newChunk.getHeader().getDataSize() + newChunk.getData().position());\n-            writer.writeChunk(newChunk, newChunkMetadata);\n-            targetResource.updateStartTime(deviceId, newChunkMetadata.getStartTime());\n-            targetResource.updateEndTime(deviceId, newChunkMetadata.getEndTime());\n+          Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap = entry.getValue();\n+          boolean isPageEnoughLarge = true;\n+          for (List<ChunkMetadata> chunkMetadatas : readerChunkMetadatasMap.values()) {\n+            for (ChunkMetadata chunkMetadata : chunkMetadatas) {\n+              if (chunkMetadata.getNumOfPoints() < mergePagePointNum) {\n+                isPageEnoughLarge = false;\n+                break;\n+              }\n+            }\n+          }\n+          if (isPageEnoughLarge) {\n+            logger.info(\"{} [Hot Compaction] page enough large, use append merge\", storageGroup);\n+            maxVersion = writeByAppendMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                readerChunkMetadatasMap, targetResource, writer);\n+          } else {\n+            logger\n+                .info(\"{} [Hot Compaction] page enough large, use deserialize merge\", storageGroup);\n+            maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                entry,\n+                targetResource, writer);\n           }\n         }\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUyMjQwNA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508522404", "bodyText": "good idea", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T13:51:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MDE5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\nindex d7dcd4c89..2abfdf54c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n\n@@ -125,7 +125,7 @@ public class HotCompactionUtils {\n       maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n       // wait for limit write\n       MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n-          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+          (long) newChunk.getHeader().getDataSize() + newChunk.getData().position());\n       writer.writeChunk(newChunk, newChunkMetadata);\n       targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n       targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MTc4OQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504641789", "bodyText": "why this UT is skipped", "author": "EJTTianYu", "createdAt": "2020-10-14T12:38:38Z", "path": "server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java", "diffHunk": "@@ -268,291 +270,8 @@ public void testSeqAndUnSeqSyncClose()\n   }\n \n   @Test\n-  public void testEnableDiscardOutOfOrderDataForInsertRowPlan()", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzMzUzNA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508533534", "bodyText": "revert it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:01:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MTc4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java b/server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java\nindex d33c92cec..b33280c8c 100644\n--- a/server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java\n+++ b/server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java\n\n@@ -269,6 +265,298 @@ public class StorageGroupProcessorTest {\n     }\n   }\n \n+  @Test\n+  public void testEnableDiscardOutOfOrderDataForInsertRowPlan()\n+      throws WriteProcessException, QueryProcessException, IllegalPathException, IOException {\n+    IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n+    boolean defaultValue = config.isEnableDiscardOutOfOrderData();\n+    config.setEnableDiscardOutOfOrderData(true);\n+\n+    for (int j = 21; j <= 30; j++) {\n+      TSRecord record = new TSRecord(j, deviceId);\n+      record.addTuple(DataPoint.getDataPoint(TSDataType.INT32, measurementId, String.valueOf(j)));\n+      insertToStorageGroupProcessor(record);\n+      processor.asyncCloseAllWorkingTsFileProcessors();\n+    }\n+    processor.syncCloseAllWorkingTsFileProcessors();\n+\n+    for (int j = 10; j >= 1; j--) {\n+      TSRecord record = new TSRecord(j, deviceId);\n+      record.addTuple(DataPoint.getDataPoint(TSDataType.INT32, measurementId, String.valueOf(j)));\n+      insertToStorageGroupProcessor(record);\n+      processor.asyncCloseAllWorkingTsFileProcessors();\n+    }\n+\n+    processor.syncCloseAllWorkingTsFileProcessors();\n+\n+    for (TsFileProcessor tsfileProcessor : processor.getWorkUnsequenceTsFileProcessor()) {\n+      tsfileProcessor.syncFlush();\n+    }\n+\n+    QueryDataSource queryDataSource = processor\n+        .query(new PartialPath(deviceId), measurementId, context,\n+            null, null);\n+    Assert.assertEquals(10, queryDataSource.getSeqResources().size());\n+    Assert.assertEquals(0, queryDataSource.getUnseqResources().size());\n+    for (TsFileResource resource : queryDataSource.getSeqResources()) {\n+      Assert.assertTrue(resource.isClosed());\n+    }\n+    for (TsFileResource resource : queryDataSource.getUnseqResources()) {\n+      Assert.assertTrue(resource.isClosed());\n+    }\n+\n+    config.setEnableDiscardOutOfOrderData(defaultValue);\n+  }\n+\n+  @Test\n+  public void testEnableDiscardOutOfOrderDataForInsertTablet1()\n+      throws QueryProcessException, IllegalPathException, IOException {\n+    IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n+    boolean defaultEnableDiscard = config.isEnableDiscardOutOfOrderData();\n+    long defaultTimePartition = config.getPartitionInterval();\n+    boolean defaultEnablePartition = config.isEnablePartition();\n+    config.setEnableDiscardOutOfOrderData(true);\n+    config.setEnablePartition(true);\n+    config.setPartitionInterval(100);\n+\n+    String[] measurements = new String[2];\n+    measurements[0] = \"s0\";\n+    measurements[1] = \"s1\";\n+    List<Integer> dataTypes = new ArrayList<>();\n+    dataTypes.add(TSDataType.INT32.ordinal());\n+    dataTypes.add(TSDataType.INT64.ordinal());\n+\n+    MeasurementMNode[] measurementMNodes = new MeasurementMNode[2];\n+    measurementMNodes[0] = new MeasurementMNode(null, \"s0\",\n+        new MeasurementSchema(\"s0\", TSDataType.INT32, TSEncoding.PLAIN), null);\n+    measurementMNodes[1] = new MeasurementMNode(null, \"s1\",\n+        new MeasurementSchema(\"s1\", TSDataType.INT64, TSEncoding.PLAIN), null);\n+\n+    InsertTabletPlan insertTabletPlan1 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    long[] times = new long[100];\n+    Object[] columns = new Object[2];\n+    columns[0] = new int[100];\n+    columns[1] = new long[100];\n+\n+    for (int r = 0; r < 100; r++) {\n+      times[r] = r;\n+      ((int[]) columns[0])[r] = 1;\n+      ((long[]) columns[1])[r] = 1;\n+    }\n+    insertTabletPlan1.setTimes(times);\n+    insertTabletPlan1.setColumns(columns);\n+    insertTabletPlan1.setRowCount(times.length);\n+    insertTabletPlan1.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan1);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+\n+    InsertTabletPlan insertTabletPlan2 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    for (int r = 149; r >= 50; r--) {\n+      times[r - 50] = r;\n+      ((int[]) columns[0])[r - 50] = 1;\n+      ((long[]) columns[1])[r - 50] = 1;\n+    }\n+    insertTabletPlan2.setTimes(times);\n+    insertTabletPlan2.setColumns(columns);\n+    insertTabletPlan2.setRowCount(times.length);\n+    insertTabletPlan2.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan2);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+    processor.syncCloseAllWorkingTsFileProcessors();\n+\n+    for (TsFileProcessor tsfileProcessor : processor.getWorkUnsequenceTsFileProcessor()) {\n+      tsfileProcessor.syncFlush();\n+    }\n+\n+    QueryDataSource queryDataSource = processor\n+        .query(new PartialPath(deviceId), measurementId, context,\n+            null, null);\n+\n+    Assert.assertEquals(2, queryDataSource.getSeqResources().size());\n+    Assert.assertEquals(0, queryDataSource.getUnseqResources().size());\n+    for (TsFileResource resource : queryDataSource.getSeqResources()) {\n+      Assert.assertTrue(resource.isClosed());\n+    }\n+\n+    config.setEnableDiscardOutOfOrderData(defaultEnableDiscard);\n+    config.setPartitionInterval(defaultTimePartition);\n+    config.setEnablePartition(defaultEnablePartition);\n+  }\n+\n+  @Test\n+  public void testEnableDiscardOutOfOrderDataForInsertTablet2()\n+      throws QueryProcessException, IllegalPathException, IOException {\n+    IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n+    boolean defaultEnableDiscard = config.isEnableDiscardOutOfOrderData();\n+    long defaultTimePartition = config.getPartitionInterval();\n+    boolean defaultEnablePartition = config.isEnablePartition();\n+    config.setEnableDiscardOutOfOrderData(true);\n+    config.setEnablePartition(true);\n+    config.setPartitionInterval(1200);\n+\n+    String[] measurements = new String[2];\n+    measurements[0] = \"s0\";\n+    measurements[1] = \"s1\";\n+    List<Integer> dataTypes = new ArrayList<>();\n+    dataTypes.add(TSDataType.INT32.ordinal());\n+    dataTypes.add(TSDataType.INT64.ordinal());\n+\n+    MeasurementMNode[] measurementMNodes = new MeasurementMNode[2];\n+    measurementMNodes[0] = new MeasurementMNode(null, \"s0\",\n+        new MeasurementSchema(\"s0\", TSDataType.INT32, TSEncoding.PLAIN), null);\n+    measurementMNodes[1] = new MeasurementMNode(null, \"s1\",\n+        new MeasurementSchema(\"s1\", TSDataType.INT64, TSEncoding.PLAIN), null);\n+\n+    InsertTabletPlan insertTabletPlan1 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    long[] times = new long[1200];\n+    Object[] columns = new Object[2];\n+    columns[0] = new int[1200];\n+    columns[1] = new long[1200];\n+\n+    for (int r = 0; r < 1200; r++) {\n+      times[r] = r;\n+      ((int[]) columns[0])[r] = 1;\n+      ((long[]) columns[1])[r] = 1;\n+    }\n+    insertTabletPlan1.setTimes(times);\n+    insertTabletPlan1.setColumns(columns);\n+    insertTabletPlan1.setRowCount(times.length);\n+    insertTabletPlan1.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan1);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+\n+    InsertTabletPlan insertTabletPlan2 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    for (int r = 1249; r >= 50; r--) {\n+      times[r - 50] = r;\n+      ((int[]) columns[0])[r - 50] = 1;\n+      ((long[]) columns[1])[r - 50] = 1;\n+    }\n+    insertTabletPlan2.setTimes(times);\n+    insertTabletPlan2.setColumns(columns);\n+    insertTabletPlan2.setRowCount(times.length);\n+    insertTabletPlan2.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan2);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+    processor.syncCloseAllWorkingTsFileProcessors();\n+\n+    for (TsFileProcessor tsfileProcessor : processor.getWorkUnsequenceTsFileProcessor()) {\n+      tsfileProcessor.syncFlush();\n+    }\n+\n+    QueryDataSource queryDataSource = processor\n+        .query(new PartialPath(deviceId), measurementId, context,\n+            null, null);\n+\n+    Assert.assertEquals(2, queryDataSource.getSeqResources().size());\n+    Assert.assertEquals(0, queryDataSource.getUnseqResources().size());\n+    for (TsFileResource resource : queryDataSource.getSeqResources()) {\n+      Assert.assertTrue(resource.isClosed());\n+    }\n+\n+    config.setEnableDiscardOutOfOrderData(defaultEnableDiscard);\n+    config.setPartitionInterval(defaultTimePartition);\n+    config.setEnablePartition(defaultEnablePartition);\n+  }\n+\n+  @Test\n+  public void testEnableDiscardOutOfOrderDataForInsertTablet3()\n+      throws QueryProcessException, IllegalPathException, IOException {\n+    IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n+    boolean defaultEnableDiscard = config.isEnableDiscardOutOfOrderData();\n+    long defaultTimePartition = config.getPartitionInterval();\n+    boolean defaultEnablePartition = config.isEnablePartition();\n+    config.setEnableDiscardOutOfOrderData(true);\n+    config.setEnablePartition(true);\n+    config.setPartitionInterval(1000);\n+\n+    String[] measurements = new String[2];\n+    measurements[0] = \"s0\";\n+    measurements[1] = \"s1\";\n+    List<Integer> dataTypes = new ArrayList<>();\n+    dataTypes.add(TSDataType.INT32.ordinal());\n+    dataTypes.add(TSDataType.INT64.ordinal());\n+\n+    MeasurementMNode[] measurementMNodes = new MeasurementMNode[2];\n+    measurementMNodes[0] = new MeasurementMNode(null, \"s0\",\n+        new MeasurementSchema(\"s0\", TSDataType.INT32, TSEncoding.PLAIN), null);\n+    measurementMNodes[1] = new MeasurementMNode(null, \"s1\",\n+        new MeasurementSchema(\"s1\", TSDataType.INT64, TSEncoding.PLAIN), null);\n+\n+    InsertTabletPlan insertTabletPlan1 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    long[] times = new long[1200];\n+    Object[] columns = new Object[2];\n+    columns[0] = new int[1200];\n+    columns[1] = new long[1200];\n+\n+    for (int r = 0; r < 1200; r++) {\n+      times[r] = r;\n+      ((int[]) columns[0])[r] = 1;\n+      ((long[]) columns[1])[r] = 1;\n+    }\n+    insertTabletPlan1.setTimes(times);\n+    insertTabletPlan1.setColumns(columns);\n+    insertTabletPlan1.setRowCount(times.length);\n+    insertTabletPlan1.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan1);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+\n+    InsertTabletPlan insertTabletPlan2 = new InsertTabletPlan(new PartialPath(\"root.vehicle.d0\"),\n+        measurements,\n+        dataTypes);\n+\n+    for (int r = 1249; r >= 50; r--) {\n+      times[r - 50] = r;\n+      ((int[]) columns[0])[r - 50] = 1;\n+      ((long[]) columns[1])[r - 50] = 1;\n+    }\n+    insertTabletPlan2.setTimes(times);\n+    insertTabletPlan2.setColumns(columns);\n+    insertTabletPlan2.setRowCount(times.length);\n+    insertTabletPlan2.setMeasurementMNodes(measurementMNodes);\n+\n+    processor.insertTablet(insertTabletPlan2);\n+    processor.asyncCloseAllWorkingTsFileProcessors();\n+    processor.syncCloseAllWorkingTsFileProcessors();\n+\n+    for (TsFileProcessor tsfileProcessor : processor.getWorkUnsequenceTsFileProcessor()) {\n+      tsfileProcessor.syncFlush();\n+    }\n+\n+    QueryDataSource queryDataSource = processor\n+        .query(new PartialPath(deviceId), measurementId, context,\n+            null, null);\n+\n+    Assert.assertEquals(2, queryDataSource.getSeqResources().size());\n+    Assert.assertEquals(0, queryDataSource.getUnseqResources().size());\n+    for (TsFileResource resource : queryDataSource.getSeqResources()) {\n+      Assert.assertTrue(resource.isClosed());\n+    }\n+\n+    config.setEnableDiscardOutOfOrderData(defaultEnableDiscard);\n+    config.setPartitionInterval(defaultTimePartition);\n+    config.setEnablePartition(defaultEnablePartition);\n+  }\n+\n   @Test\n   public void testMerge()\n       throws WriteProcessException, QueryProcessException, IllegalPathException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY2NDk4Nw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504664987", "bodyText": "any difference between this if , else\uff0cI think both can use get(level)", "author": "EJTTianYu", "createdAt": "2020-10-14T13:12:25Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -209,25 +232,25 @@ public void removeAll(List<TsFileResource> tsFileResourceList, boolean sequence)\n   public void add(TsFileResource tsFileResource, boolean sequence) {\n     long timePartitionId = tsFileResource.getTimePartition();\n     int level = getMergeLevel(tsFileResource.getTsFile());\n-    if (level <= maxLevelNum - 1) {\n-      if (sequence) {\n+    if (sequence) {\n+      if (level <= maxLevelNum - 1) {\n         sequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(level)\n             .add(tsFileResource);\n       } else {\n-        unSequenceTsFileResources\n-            .computeIfAbsent(timePartitionId, this::newUnSequenceTsFileResources).get(level)\n+        sequenceTsFileResources\n+            .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(maxLevelNum - 1)\n             .add(tsFileResource);", "originalCommit": "44d47316308ebf003d9babebf98d312db7f6e85c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzNTMxNQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508535315", "bodyText": "add comment", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:03:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY2NDk4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex a121871d7..49f31a24b 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -234,20 +227,24 @@ public class LevelTsFileManagement extends TsFileManagement {\n     int level = getMergeLevel(tsFileResource.getTsFile());\n     if (sequence) {\n       if (level <= maxLevelNum - 1) {\n+        // current file has too high level\n         sequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(level)\n             .add(tsFileResource);\n       } else {\n+        // current file has normal level\n         sequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(maxLevelNum - 1)\n             .add(tsFileResource);\n       }\n     } else {\n       if (level <= maxUnseqLevelNum - 1) {\n+        // current file has too high level\n         unSequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newUnSequenceTsFileResources).get(level)\n             .add(tsFileResource);\n       } else {\n+        // current file has normal level\n         unSequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newUnSequenceTsFileResources)\n             .get(maxUnseqLevelNum - 1).add(tsFileResource);\n"}}, {"oid": "3dfd8b4f5cd6086b790f6a3fe661bed57431c98e", "url": "https://github.com/apache/iotdb/commit/3dfd8b4f5cd6086b790f6a3fe661bed57431c98e", "message": "add config property", "committedDate": "2020-10-17T02:36:49Z", "type": "commit"}, {"oid": "f8ff28dca6b1004676240cd8bffa4e31e2e43f4d", "url": "https://github.com/apache/iotdb/commit/f8ff28dca6b1004676240cd8bffa4e31e2e43f4d", "message": "uncomment flush all code", "committedDate": "2020-10-19T07:27:09Z", "type": "commit"}, {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "url": "https://github.com/apache/iotdb/commit/726a0bdf3bd1bb2102db7540a6a237063b3032d2", "message": "update compaction", "committedDate": "2020-10-19T08:14:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY4ODUzMA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r507688530", "bodyText": "Remove the useless import", "author": "samperson1997", "createdAt": "2020-10-19T11:59:08Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -31,29 +31,36 @@\n import java.nio.file.Files;\n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Map.Entry;\n import java.util.Set;\n import java.util.TreeSet;\n import java.util.concurrent.ConcurrentSkipListMap;\n import java.util.concurrent.CopyOnWriteArrayList;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLog;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n import org.apache.iotdb.db.conf.IoTDBDescriptor;\n import org.apache.iotdb.db.engine.cache.ChunkMetadataCache;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzNTYxMw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508535613", "bodyText": "solve it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:03:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY4ODUzMA=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex 526b46d02..49f31a24b 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -26,6 +26,8 @@ import static org.apache.iotdb.db.engine.tsfilemanagement.utils.HotCompactionLog\n import static org.apache.iotdb.db.engine.tsfilemanagement.utils.HotCompactionLogger.TARGET_NAME;\n import static org.apache.iotdb.tsfile.common.constant.TsFileConstant.TSFILE_SUFFIX;\n \n+import com.clearspring.analytics.stream.cardinality.HyperLogLog;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0OTcxNg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508149716", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      mergeLog.delete();\n          \n          \n            \n                      Files.delete(mergeLog.toPath());", "author": "samperson1997", "createdAt": "2020-10-20T01:11:00Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -146,4 +178,207 @@ public void run() {\n       closeHotCompactionMergeCallBack.call();\n     }\n   }\n+\n+  public void merge(boolean fullMerge, List<TsFileResource> seqMergeList,\n+      List<TsFileResource> unSeqMergeList, long dataTTL) {\n+    if (isUnseqMerging) {\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} Last merge is ongoing, currently consumed time: {}ms\", storageGroupName,\n+            (System.currentTimeMillis() - mergeStartTime));\n+      }\n+      return;\n+    }\n+    logger.info(\"{} will close all files for starting a merge (fullmerge = {})\", storageGroupName,\n+        fullMerge);\n+\n+    if (seqMergeList.isEmpty()) {\n+      logger.info(\"{} no seq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    if (unSeqMergeList.isEmpty()) {\n+      logger.info(\"{} no unseq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    long budget = IoTDBDescriptor.getInstance().getConfig().getMergeMemoryBudget();\n+    long timeLowerBound = System.currentTimeMillis() - dataTTL;\n+    MergeResource mergeResource = new MergeResource(seqMergeList, unSeqMergeList, timeLowerBound);\n+\n+    IMergeFileSelector fileSelector = getMergeFileSelector(budget, mergeResource);\n+    try {\n+      List[] mergeFiles = fileSelector.select();\n+      if (mergeFiles.length == 0) {\n+        logger.info(\"{} cannot select merge candidates under the budget {}\", storageGroupName,\n+            budget);\n+        return;\n+      }\n+      // avoid pending tasks holds the metadata and streams\n+      mergeResource.clear();\n+      String taskName = storageGroupName + \"-\" + System.currentTimeMillis();\n+      // do not cache metadata until true candidates are chosen, or too much metadata will be\n+      // cached during selection\n+      mergeResource.setCacheDeviceMeta(true);\n+\n+      for (TsFileResource tsFileResource : mergeResource.getSeqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+      for (TsFileResource tsFileResource : mergeResource.getUnseqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+\n+      MergeTask mergeTask = new MergeTask(mergeResource, storageGroupDir,\n+          this::mergeEndAction, taskName, fullMerge, fileSelector.getConcurrentMergeNum(),\n+          storageGroupName);\n+      mergingModification = new ModificationFile(\n+          storageGroupDir + File.separator + MERGING_MODIFICATION_FILE_NAME);\n+      MergeManager.getINSTANCE().submitMainTask(mergeTask);\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} submits a merge task {}, merging {} seqFiles, {} unseqFiles\",\n+            storageGroupName, taskName, mergeFiles[0].size(), mergeFiles[1].size());\n+      }\n+      isUnseqMerging = true;\n+      mergeStartTime = System.currentTimeMillis();\n+\n+    } catch (MergeException | IOException e) {\n+      logger.error(\"{} cannot select file for merge\", storageGroupName, e);\n+    }\n+  }\n+\n+  private IMergeFileSelector getMergeFileSelector(long budget, MergeResource resource) {\n+    MergeFileStrategy strategy = IoTDBDescriptor.getInstance().getConfig().getMergeFileStrategy();\n+    switch (strategy) {\n+      case MAX_FILE_NUM:\n+        return new MaxFileMergeFileSelector(resource, budget);\n+      case MAX_SERIES_NUM:\n+        return new MaxSeriesMergeFileSelector(resource, budget);\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown MergeFileStrategy \" + strategy);\n+    }\n+  }\n+\n+  /**\n+   * acquire the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteLock(TsFileResource seqFile) {\n+    boolean fileLockGot;\n+    boolean mergeLockGot;\n+    boolean hotCompactionLockGot;\n+    while (true) {\n+      fileLockGot = seqFile.tryWriteLock();\n+      mergeLockGot = mergeLock.writeLock().tryLock();\n+      hotCompactionLockGot = tryWriteLock();\n+\n+      if (fileLockGot && mergeLockGot && hotCompactionLockGot) {\n+        break;\n+      } else {\n+        // did not get all of them, release the gotten one and retry\n+        if (hotCompactionLockGot) {\n+          writeUnlock();\n+        }\n+        if (mergeLockGot) {\n+          mergeLock.writeLock().unlock();\n+        }\n+        if (fileLockGot) {\n+          seqFile.writeUnlock();\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * release the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteUnlock(TsFileResource seqFile) {\n+    writeUnlock();\n+    mergeLock.writeLock().unlock();\n+    seqFile.writeUnlock();\n+  }\n+\n+  private void removeUnseqFiles(List<TsFileResource> unseqFiles) {\n+    mergeLock.writeLock().lock();\n+    writeLock();\n+    try {\n+      removeAll(unseqFiles, false);\n+    } finally {\n+      writeUnlock();\n+      mergeLock.writeLock().unlock();\n+    }\n+\n+    for (TsFileResource unseqFile : unseqFiles) {\n+      unseqFile.writeLock();\n+      try {\n+        unseqFile.remove();\n+      } finally {\n+        unseqFile.writeUnlock();\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"squid:S1141\")\n+  private void updateMergeModification(TsFileResource seqFile) {\n+    try {\n+      // remove old modifications and write modifications generated during merge\n+      seqFile.removeModFile();\n+      if (mergingModification != null) {\n+        for (Modification modification : mergingModification.getModifications()) {\n+          seqFile.getModFile().write(modification);\n+        }\n+        try {\n+          seqFile.getModFile().close();\n+        } catch (IOException e) {\n+          logger\n+              .error(\"Cannot close the ModificationFile {}\", seqFile.getModFile().getFilePath(), e);\n+        }\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot clean the ModificationFile of {} after merge\", storageGroupName,\n+          seqFile.getTsFile(), e);\n+    }\n+  }\n+\n+  private void removeMergingModification() {\n+    try {\n+      if (mergingModification != null) {\n+        mergingModification.remove();\n+        mergingModification = null;\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot remove merging modification \", storageGroupName, e);\n+    }\n+  }\n+\n+  public void mergeEndAction(List<TsFileResource> seqFiles, List<TsFileResource> unseqFiles,\n+      File mergeLog) {\n+    logger.info(\"{} a merge task is ending...\", storageGroupName);\n+\n+    if (unseqFiles.isEmpty()) {\n+      // merge runtime exception arose, just end this merge\n+      isUnseqMerging = false;\n+      logger.info(\"{} a merge task abnormally ends\", storageGroupName);\n+      return;\n+    }\n+\n+    removeUnseqFiles(unseqFiles);\n+\n+    for (int i = 0; i < seqFiles.size(); i++) {\n+      TsFileResource seqFile = seqFiles.get(i);\n+      // get both seqFile lock and merge lock\n+      doubleWriteLock(seqFile);\n+\n+      try {\n+        updateMergeModification(seqFile);\n+        if (i == seqFiles.size() - 1) {\n+          //FIXME if there is an exception, the the modification file will be not closed.\n+          removeMergingModification();\n+          isUnseqMerging = false;\n+          mergeLog.delete();", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODAwMg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538002", "bodyText": "accept it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:06:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0OTcxNg=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\nindex caa6c0295..b0d84833e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n\n@@ -372,8 +373,11 @@ public abstract class TsFileManagement {\n           //FIXME if there is an exception, the the modification file will be not closed.\n           removeMergingModification();\n           isUnseqMerging = false;\n-          mergeLog.delete();\n+          Files.delete(mergeLog.toPath());\n         }\n+      } catch (IOException e) {\n+        logger.error(\"{} a merge task ends but cannot delete log {}\", storageGroupName,\n+            mergeLog.toPath());\n       } finally {\n         doubleWriteUnlock(seqFile);\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDM4OA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150388", "bodyText": "reader should be closed", "author": "samperson1997", "createdAt": "2020-10-20T01:13:32Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,135 +430,152 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    Pair<Double, Double> seqStatisticsPair = forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkedSeqListPointNum = seqStatisticsPair.left;\n+    forkedSeqListMeasurementSize = seqStatisticsPair.right;\n+    Pair<Double, Double> unSeqStatisticsPair = forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n+    forkedUnSeqListPointNum = unSeqStatisticsPair.left;\n+    forkedUnSeqListMeasurementSize = unSeqStatisticsPair.right;\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+  private Pair<Double, Double> forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n     // just fork part of the TsFile list, controlled by max_merge_chunk_point\n     long pointNum = 0;\n     // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    ICardinality measurementSet = new HyperLogLog(13);\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n-          }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            String path = tsFileResource.getTsFile().getAbsolutePath();\n+            try {\n+              if (tsFileResource.getTsFile().exists()) {\n+                TsFileSequenceReader reader = new TsFileSequenceReader(path);", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODU4NA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538584", "bodyText": "close it afterward", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDM4OA=="}], "type": "inlineReview", "revised_code": {"commit": "88173ea1423b349d767d2c86f3f0a6b247ce7cc7", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\nindex 526b46d02..cc421022e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n\n@@ -461,9 +458,8 @@ public class LevelTsFileManagement extends TsFileManagement {\n         for (TsFileResource tsFileResource : levelRawTsFileResources) {\n           if (tsFileResource.isClosed()) {\n             String path = tsFileResource.getTsFile().getAbsolutePath();\n-            try {\n-              if (tsFileResource.getTsFile().exists()) {\n-                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+            if (tsFileResource.getTsFile().exists()) {\n+              try (TsFileSequenceReader reader = new TsFileSequenceReader(path)) {\n                 List<Path> pathList = reader.getAllPaths();\n                 for (Path sensorPath : pathList) {\n                   measurementSet.offer(sensorPath.getFullPath());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDUyOQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150529", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      newChunk.getHeader().getDataSize() + newChunk.getData().position());\n          \n          \n            \n                      (long) newChunk.getHeader().getDataSize() + newChunk.getData().position());", "author": "samperson1997", "createdAt": "2020-10-20T01:14:09Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODk1MQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538951", "bodyText": "accept", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:07:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDUyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\nindex d7dcd4c89..2abfdf54c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n\n@@ -125,7 +125,7 @@ public class HotCompactionUtils {\n       maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n       // wait for limit write\n       MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n-          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+          (long) newChunk.getHeader().getDataSize() + newChunk.getData().position());\n       writer.writeChunk(newChunk, newChunkMetadata);\n       targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n       targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDk3Mw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150973", "bodyText": "Why public?", "author": "samperson1997", "createdAt": "2020-10-20T01:15:40Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTEwMA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539100", "bodyText": "use outside in test", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:07:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDk3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\nindex f613d422f..1153dc9a4 100755\n--- a/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n\n@@ -200,7 +200,7 @@ public class StorageGroupProcessor {\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  public TsFileManagement tsFileManagement;\n+  private TsFileManagement tsFileManagement;\n \n   /**\n    * time partition id -> version controller which assigns a version for each MemTable and\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzA2Mw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508167063", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n          \n          \n            \n              private final ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();", "author": "samperson1997", "createdAt": "2020-10-20T02:14:46Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTQwMw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539403", "bodyText": "accept it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:08:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzA2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\nindex caa6c0295..b0d84833e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n\n@@ -23,6 +23,7 @@ import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERG\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzI0Mg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508167242", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public volatile boolean isUnseqMerging = false;\n          \n          \n            \n              private volatile boolean isUnseqMerging = false;", "author": "samperson1997", "createdAt": "2020-10-20T02:15:22Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTY5Ng==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539696", "bodyText": "use in test", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzI0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\nindex caa6c0295..b0d84833e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n\n@@ -23,6 +23,7 @@ import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERG\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2ODM2Mw==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508168363", "bodyText": "Please check the accessibility of all new fields ... Class variable fields should not have public accessibility\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public long mergeStartTime;\n          \n          \n            \n              private long mergeStartTime;", "author": "samperson1997", "createdAt": "2020-10-20T02:19:39Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;\n+  /**\n+   * This is the modification file of the result of the current merge. Because the merged file may\n+   * be invisible at this moment, without this, deletion/update during merge could be lost.\n+   */\n+  public ModificationFile mergingModification;\n+  public long mergeStartTime;", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDAyOA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540028", "bodyText": "accept it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2ODM2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\nindex caa6c0295..b0d84833e 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n\n@@ -23,6 +23,7 @@ import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERG\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTQzNQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508169435", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n          \n          \n            \n              private static final int MERGE_PAGE_POINT_NUM = IoTDBDescriptor.getInstance().getConfig()", "author": "samperson1997", "createdAt": "2020-10-20T02:23:35Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDUyMA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540520", "bodyText": "accept it", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:09:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTQzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\nindex d7dcd4c89..2abfdf54c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n\n@@ -55,7 +55,7 @@ import org.slf4j.LoggerFactory;\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n-  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n+  private static final int MERGE_PAGE_POINT_NUM = IoTDBDescriptor.getInstance().getConfig()\n       .getMergePagePointNumberThreshold();\n \n   private HotCompactionUtils() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTY0MQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508169641", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                if (chunkMetadataList.size() <= 0) {\n          \n          \n            \n                if (chunkMetadataList.isEmpty()) {", "author": "samperson1997", "createdAt": "2020-10-20T02:24:21Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {", "originalCommit": "726a0bdf3bd1bb2102db7540a6a237063b3032d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDk5MQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540991", "bodyText": "good idea", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T14:09:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTY0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\nindex d7dcd4c89..2abfdf54c 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java\n\n@@ -125,7 +125,7 @@ public class HotCompactionUtils {\n       maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n       // wait for limit write\n       MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n-          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+          (long) newChunk.getHeader().getDataSize() + newChunk.getData().position());\n       writer.writeChunk(newChunk, newChunkMetadata);\n       targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n       targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n"}}, {"oid": "f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "url": "https://github.com/apache/iotdb/commit/f4073b1be5b5d998c639c22f1132d8dd0a3bf143", "message": "fix conversation", "committedDate": "2020-10-20T14:10:13Z", "type": "commit"}, {"oid": "5e76b91d910155b95946682b04f5e55db4020d70", "url": "https://github.com/apache/iotdb/commit/5e76b91d910155b95946682b04f5e55db4020d70", "message": "Merge branch 'master' into add_device_chunk_point_cache\n\n# Conflicts:\n#\tserver/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java\n#\tserver/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java\n#\tserver/src/test/java/org/apache/iotdb/db/engine/merge/MergeManagerTest.java", "committedDate": "2020-10-20T14:19:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTIyOA==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508599228", "bodyText": "RejectedExecutionException is a runtime exception. No need to throw it.", "author": "samperson1997", "createdAt": "2020-10-20T15:18:33Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/HotCompactionMergeTaskPoolManager.java", "diffHunk": "@@ -100,7 +104,8 @@ public ServiceType getID() {\n     return ServiceType.HOT_COMPACTION_SERVICE;\n   }\n \n-  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask) {\n+  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask)\n+      throws RejectedExecutionException {", "originalCommit": "5e76b91d910155b95946682b04f5e55db4020d70", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYwMDk5Mg==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508600992", "bodyText": "No\uff0cI need to catch it out to close current compaction logic", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T15:20:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTIyOA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTY5NQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508599695", "bodyText": "Remove unused import", "author": "samperson1997", "createdAt": "2020-10-20T15:19:08Z", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,57 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;", "originalCommit": "5e76b91d910155b95946682b04f5e55db4020d70", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYwNzUyMQ==", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508607521", "bodyText": "okay, I will remove all useless import in whole project", "author": "zhanglingzhe0820", "createdAt": "2020-10-20T15:26:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTY5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "9653cceb4f918a8f1a6b159b492d7839a2335836", "chunk": "diff --git a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\nindex b0d84833e..8a581ee98 100644\n--- a/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n+++ b/server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java\n\n@@ -40,7 +40,6 @@ import org.apache.iotdb.db.engine.modification.Modification;\n import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n-import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n import org.apache.iotdb.db.exception.MergeException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n"}}, {"oid": "9653cceb4f918a8f1a6b159b492d7839a2335836", "url": "https://github.com/apache/iotdb/commit/9653cceb4f918a8f1a6b159b492d7839a2335836", "message": "remove all useless import in project", "committedDate": "2020-10-20T15:25:37Z", "type": "commit"}, {"oid": "b524401937e855f89a07d2d03fe0a2963edcf950", "url": "https://github.com/apache/iotdb/commit/b524401937e855f89a07d2d03fe0a2963edcf950", "message": "fix ci", "committedDate": "2020-10-21T07:19:53Z", "type": "commit"}, {"oid": "88173ea1423b349d767d2c86f3f0a6b247ce7cc7", "url": "https://github.com/apache/iotdb/commit/88173ea1423b349d767d2c86f3f0a6b247ce7cc7", "message": "fix sonar bug", "committedDate": "2020-10-21T07:47:00Z", "type": "commit"}]}