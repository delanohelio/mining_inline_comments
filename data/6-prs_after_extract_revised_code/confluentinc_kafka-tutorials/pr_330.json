{"pr_number": 330, "pr_title": "DEVX-1520: Add tutorial for KStreams Cogrouping", "pr_createdAt": "2020-04-23T19:58:34Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/330", "timeline": [{"oid": "d903428455d6b205c77b5d4df034b420e000b58b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d903428455d6b205c77b5d4df034b420e000b58b", "message": "removed the intermediate customers_with_area_code stream\nupdated customers_by_area_code to newer syntax; allows PARTITION BY on the UDF results", "committedDate": "2020-04-13T22:48:20Z", "type": "commit"}, {"oid": "fac68657a15c17f92deb2b1df3b8c76bf571e543", "url": "https://github.com/confluentinc/kafka-tutorials/commit/fac68657a15c17f92deb2b1df3b8c76bf571e543", "message": "typo fix", "committedDate": "2020-04-13T22:48:20Z", "type": "commit"}, {"oid": "f36ae7031340077172efc2b67c9ce81a417e0e40", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f36ae7031340077172efc2b67c9ce81a417e0e40", "message": "DEVX-1520: Add tutorial for KStreasm Cogrouping", "committedDate": "2020-04-23T19:55:45Z", "type": "commit"}, {"oid": "634a83a4dd1bf7a3bc0b0df272bbafe4e2831bfb", "url": "https://github.com/confluentinc/kafka-tutorials/commit/634a83a4dd1bf7a3bc0b0df272bbafe4e2831bfb", "message": "add actual-output.json to gitignore, update make-avro-dir.adoc text", "committedDate": "2020-04-23T20:11:12Z", "type": "commit"}, {"oid": "68910d2a968548a399575c0ff924ef8495ed7566", "url": "https://github.com/confluentinc/kafka-tutorials/commit/68910d2a968548a399575c0ff924ef8495ed7566", "message": "Upgrade to CP 5.5 fix template file that had harded coded path", "committedDate": "2020-04-28T19:42:56Z", "type": "commit"}, {"oid": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4f0fd6f1732dab41bca4e09aca88798fb1717b43", "message": "add KIP link", "committedDate": "2020-04-28T22:32:20Z", "type": "commit"}, {"oid": "c399f257519d9a060015f8844f6833552bfaaa4d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c399f257519d9a060015f8844f6833552bfaaa4d", "message": "add original KIP proposal link", "committedDate": "2020-04-28T22:38:45Z", "type": "commit"}, {"oid": "c9b860b1424c4761f799247dfe28a49fb751e784", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c9b860b1424c4761f799247dfe28a49fb751e784", "message": "Update _includes/tutorials/fk-joins/kstreams/markup/dev/make-topology.adoc\n\nCo-Authored-By: Yeva Byzek <ybyzek@users.noreply.github.com>", "committedDate": "2020-04-29T15:12:57Z", "type": "commit"}, {"oid": "ec267277850e14dcac604c5b875e906bc4dd5059", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ec267277850e14dcac604c5b875e906bc4dd5059", "message": "Merge pull request #335 from confluentinc/MINOR_add_kip_link_to_fk_joins\n\nMINOR: Add original KIP proposal link", "committedDate": "2020-04-29T15:14:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTY5Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417651692", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n          \n          \n            \n                    props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));", "author": "gAmUssA", "createdAt": "2020-04-29T22:33:47Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import org.apache.avro.specific.SpecificRecord;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Aggregator;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Produced;\n+\n+public class CogroupingStreams {\n+\n+\n+\tpublic Properties buildStreamsProperties(Properties envProps) {\n+        Properties props = new Properties();\n+\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTg2OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417651869", "bodyText": "AbstractKafkaAvroSerDeConfig is deprecated in 5.5", "author": "gAmUssA", "createdAt": "2020-04-29T22:34:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MTY5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "5959dd935463de66401abc0521284b167eaaa15e", "chunk": "diff --git a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java b/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\ndeleted file mode 100644\nindex 27602c73..00000000\n--- a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\n+++ /dev/null\n\n@@ -1,180 +0,0 @@\n-package io.confluent.developer;\n-\n-\n-import io.confluent.common.utils.TestUtils;\n-import io.confluent.developer.avro.LoginEvent;\n-import io.confluent.developer.avro.LoginRollup;\n-import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n-import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n-import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n-import io.confluent.kafka.serializers.KafkaAvroSerializer;\n-import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;\n-import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n-import java.io.FileInputStream;\n-import java.io.IOException;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import org.apache.avro.specific.SpecificRecord;\n-import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.common.serialization.Serde;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.kstream.Aggregator;\n-import org.apache.kafka.streams.kstream.Consumed;\n-import org.apache.kafka.streams.kstream.KGroupedStream;\n-import org.apache.kafka.streams.kstream.KStream;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.kstream.Produced;\n-\n-public class CogroupingStreams {\n-\n-\n-\tpublic Properties buildStreamsProperties(Properties envProps) {\n-        Properties props = new Properties();\n-\n-        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n-        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n-        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n-        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024);\n-        return props;\n-    }\n-\n-    public Topology buildTopology(Properties envProps) {\n-        final StreamsBuilder builder = new StreamsBuilder();\n-            final String appOneInputTopic = envProps.getProperty(\"app-one.topic.name\");\n-            final String appTwoInputTopic = envProps.getProperty(\"app-two.topic.name\");\n-            final String appThreeInputTopic = envProps.getProperty(\"app-three.topic.name\");\n-            final String totalResultOutputTopic = envProps.getProperty(\"output.topic.name\");\n-\n-        final Serde<String> stringSerde = getPrimitiveAvroSerde(envProps, true);\n-        final Serde<LoginEvent> loginEventSerde = getSpecificAvroSerde(envProps);\n-        final Serde<LoginRollup> loginRollupSerde = getSpecificAvroSerde(envProps);\n-\n-\n-        final KStream<String, LoginEvent> appOneStream = builder.stream(appOneInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appTwoStream = builder.stream(appTwoInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appThreeStream = builder.stream(appThreeInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-\n-        final Aggregator<String, LoginEvent, LoginRollup> loginAggregator = new LoginAggregator();\n-\n-        final KGroupedStream<String, LoginEvent> appOneGrouped = appOneStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appTwoGrouped = appTwoStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appThreeGrouped = appThreeStream.groupByKey();\n-\n-        appOneGrouped.cogroup(loginAggregator)\n-            .cogroup(appTwoGrouped, loginAggregator)\n-            .cogroup(appThreeGrouped, loginAggregator)\n-            .aggregate(() -> new LoginRollup(new HashMap<>()), Materialized.with(Serdes.String(), loginRollupSerde))\n-            .toStream().to(totalResultOutputTopic, Produced.with(stringSerde, loginRollupSerde));\n-\n-        return builder.build();\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    static <T> Serde<T> getPrimitiveAvroSerde(final Properties envProps, boolean isKey) {\n-        final KafkaAvroDeserializer deserializer = new KafkaAvroDeserializer();\n-        final KafkaAvroSerializer serializer = new KafkaAvroSerializer();\n-        final Map<String, String> config = new HashMap<>();\n-        config.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-        deserializer.configure(config, isKey);\n-        serializer.configure(config, isKey);\n-        return (Serde<T>)Serdes.serdeFrom(serializer, deserializer);\n-    }\n-\n-    static <T extends SpecificRecord> SpecificAvroSerde<T> getSpecificAvroSerde(final Properties envProps) {\n-        final SpecificAvroSerde<T> specificAvroSerde = new SpecificAvroSerde<>();\n-\n-        final HashMap<String, String> serdeConfig = new HashMap<>();\n-        serdeConfig.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-\n-        specificAvroSerde.configure(serdeConfig, false);\n-        return specificAvroSerde;\n-    }\n-\n-    public void createTopics(final Properties envProps) {\n-        final Map<String, Object> config = new HashMap<>();\n-        config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n-        try (final AdminClient client = AdminClient.create(config)) {\n-\n-        final List<NewTopic> topics = new ArrayList<>();\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-one.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-one.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-one.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-two.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-two.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-two.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-three.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-three.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-three.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"output.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n-\n-            client.createTopics(topics);\n-        }\n-    }\n-\n-    public Properties loadEnvProperties(String fileName) throws IOException {\n-        final Properties envProps = new Properties();\n-        final FileInputStream input = new FileInputStream(fileName);\n-        envProps.load(input);\n-        input.close();\n-\n-        return envProps;\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-\n-        if (args.length < 1) {\n-            throw new IllegalArgumentException(\"This program takes one argument: the path to an environment configuration file.\");\n-        }\n-\n-        final CogroupingStreams instance = new CogroupingStreams();\n-        final Properties envProps = instance.loadEnvProperties(args[0]);\n-        final Properties streamProps = instance.buildStreamsProperties(envProps);\n-        final Topology topology = instance.buildTopology(envProps);\n-\n-        instance.createTopics(envProps);\n-\n-        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n-        final CountDownLatch latch = new CountDownLatch(1);\n-\n-        // Attach shutdown handler to catch Control-C.\n-        Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n-            @Override\n-            public void run() {\n-                streams.close(Duration.ofSeconds(5));\n-                latch.countDown();\n-            }\n-        });\n-\n-        try {\n-            streams.start();\n-            latch.await();\n-        } catch (Throwable e) {\n-            System.exit(1);\n-        }\n-        System.exit(0);\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MjQ4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417652484", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;", "author": "gAmUssA", "createdAt": "2020-04-29T22:35:51Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5959dd935463de66401abc0521284b167eaaa15e", "chunk": "diff --git a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java b/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\ndeleted file mode 100644\nindex 27602c73..00000000\n--- a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\n+++ /dev/null\n\n@@ -1,180 +0,0 @@\n-package io.confluent.developer;\n-\n-\n-import io.confluent.common.utils.TestUtils;\n-import io.confluent.developer.avro.LoginEvent;\n-import io.confluent.developer.avro.LoginRollup;\n-import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n-import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n-import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n-import io.confluent.kafka.serializers.KafkaAvroSerializer;\n-import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;\n-import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n-import java.io.FileInputStream;\n-import java.io.IOException;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import org.apache.avro.specific.SpecificRecord;\n-import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.common.serialization.Serde;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.kstream.Aggregator;\n-import org.apache.kafka.streams.kstream.Consumed;\n-import org.apache.kafka.streams.kstream.KGroupedStream;\n-import org.apache.kafka.streams.kstream.KStream;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.kstream.Produced;\n-\n-public class CogroupingStreams {\n-\n-\n-\tpublic Properties buildStreamsProperties(Properties envProps) {\n-        Properties props = new Properties();\n-\n-        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n-        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n-        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n-        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024);\n-        return props;\n-    }\n-\n-    public Topology buildTopology(Properties envProps) {\n-        final StreamsBuilder builder = new StreamsBuilder();\n-            final String appOneInputTopic = envProps.getProperty(\"app-one.topic.name\");\n-            final String appTwoInputTopic = envProps.getProperty(\"app-two.topic.name\");\n-            final String appThreeInputTopic = envProps.getProperty(\"app-three.topic.name\");\n-            final String totalResultOutputTopic = envProps.getProperty(\"output.topic.name\");\n-\n-        final Serde<String> stringSerde = getPrimitiveAvroSerde(envProps, true);\n-        final Serde<LoginEvent> loginEventSerde = getSpecificAvroSerde(envProps);\n-        final Serde<LoginRollup> loginRollupSerde = getSpecificAvroSerde(envProps);\n-\n-\n-        final KStream<String, LoginEvent> appOneStream = builder.stream(appOneInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appTwoStream = builder.stream(appTwoInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appThreeStream = builder.stream(appThreeInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-\n-        final Aggregator<String, LoginEvent, LoginRollup> loginAggregator = new LoginAggregator();\n-\n-        final KGroupedStream<String, LoginEvent> appOneGrouped = appOneStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appTwoGrouped = appTwoStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appThreeGrouped = appThreeStream.groupByKey();\n-\n-        appOneGrouped.cogroup(loginAggregator)\n-            .cogroup(appTwoGrouped, loginAggregator)\n-            .cogroup(appThreeGrouped, loginAggregator)\n-            .aggregate(() -> new LoginRollup(new HashMap<>()), Materialized.with(Serdes.String(), loginRollupSerde))\n-            .toStream().to(totalResultOutputTopic, Produced.with(stringSerde, loginRollupSerde));\n-\n-        return builder.build();\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    static <T> Serde<T> getPrimitiveAvroSerde(final Properties envProps, boolean isKey) {\n-        final KafkaAvroDeserializer deserializer = new KafkaAvroDeserializer();\n-        final KafkaAvroSerializer serializer = new KafkaAvroSerializer();\n-        final Map<String, String> config = new HashMap<>();\n-        config.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-        deserializer.configure(config, isKey);\n-        serializer.configure(config, isKey);\n-        return (Serde<T>)Serdes.serdeFrom(serializer, deserializer);\n-    }\n-\n-    static <T extends SpecificRecord> SpecificAvroSerde<T> getSpecificAvroSerde(final Properties envProps) {\n-        final SpecificAvroSerde<T> specificAvroSerde = new SpecificAvroSerde<>();\n-\n-        final HashMap<String, String> serdeConfig = new HashMap<>();\n-        serdeConfig.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-\n-        specificAvroSerde.configure(serdeConfig, false);\n-        return specificAvroSerde;\n-    }\n-\n-    public void createTopics(final Properties envProps) {\n-        final Map<String, Object> config = new HashMap<>();\n-        config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n-        try (final AdminClient client = AdminClient.create(config)) {\n-\n-        final List<NewTopic> topics = new ArrayList<>();\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-one.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-one.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-one.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-two.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-two.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-two.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-three.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-three.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-three.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"output.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n-\n-            client.createTopics(topics);\n-        }\n-    }\n-\n-    public Properties loadEnvProperties(String fileName) throws IOException {\n-        final Properties envProps = new Properties();\n-        final FileInputStream input = new FileInputStream(fileName);\n-        envProps.load(input);\n-        input.close();\n-\n-        return envProps;\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-\n-        if (args.length < 1) {\n-            throw new IllegalArgumentException(\"This program takes one argument: the path to an environment configuration file.\");\n-        }\n-\n-        final CogroupingStreams instance = new CogroupingStreams();\n-        final Properties envProps = instance.loadEnvProperties(args[0]);\n-        final Properties streamProps = instance.buildStreamsProperties(envProps);\n-        final Topology topology = instance.buildTopology(envProps);\n-\n-        instance.createTopics(envProps);\n-\n-        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n-        final CountDownLatch latch = new CountDownLatch(1);\n-\n-        // Attach shutdown handler to catch Control-C.\n-        Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n-            @Override\n-            public void run() {\n-                streams.close(Duration.ofSeconds(5));\n-                latch.countDown();\n-            }\n-        });\n-\n-        try {\n-            streams.start();\n-            latch.await();\n-        } catch (Throwable e) {\n-            System.exit(1);\n-        }\n-        System.exit(0);\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1MjYyOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/330#discussion_r417652628", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;", "author": "gAmUssA", "createdAt": "2020-04-29T22:36:12Z", "path": "_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java", "diffHunk": "@@ -0,0 +1,180 @@\n+package io.confluent.developer;\n+\n+\n+import io.confluent.common.utils.TestUtils;\n+import io.confluent.developer.avro.LoginEvent;\n+import io.confluent.developer.avro.LoginRollup;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n+import io.confluent.kafka.serializers.KafkaAvroSerializer;\n+import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;", "originalCommit": "4f0fd6f1732dab41bca4e09aca88798fb1717b43", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5959dd935463de66401abc0521284b167eaaa15e", "chunk": "diff --git a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java b/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\ndeleted file mode 100644\nindex 27602c73..00000000\n--- a/_includes/tutorials/cogrouping-streams/kstreams/code/src/main/java/io/confluent/developer/CogroupingStreams.java\n+++ /dev/null\n\n@@ -1,180 +0,0 @@\n-package io.confluent.developer;\n-\n-\n-import io.confluent.common.utils.TestUtils;\n-import io.confluent.developer.avro.LoginEvent;\n-import io.confluent.developer.avro.LoginRollup;\n-import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n-import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n-import io.confluent.kafka.serializers.KafkaAvroDeserializer;\n-import io.confluent.kafka.serializers.KafkaAvroSerializer;\n-import io.confluent.kafka.streams.serdes.avro.PrimitiveAvroSerde;\n-import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n-import java.io.FileInputStream;\n-import java.io.IOException;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import org.apache.avro.specific.SpecificRecord;\n-import org.apache.kafka.clients.admin.AdminClient;\n-import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.common.serialization.Serde;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.kstream.Aggregator;\n-import org.apache.kafka.streams.kstream.Consumed;\n-import org.apache.kafka.streams.kstream.KGroupedStream;\n-import org.apache.kafka.streams.kstream.KStream;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.kstream.Produced;\n-\n-public class CogroupingStreams {\n-\n-\n-\tpublic Properties buildStreamsProperties(Properties envProps) {\n-        Properties props = new Properties();\n-\n-        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n-        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n-        props.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());\n-        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024);\n-        return props;\n-    }\n-\n-    public Topology buildTopology(Properties envProps) {\n-        final StreamsBuilder builder = new StreamsBuilder();\n-            final String appOneInputTopic = envProps.getProperty(\"app-one.topic.name\");\n-            final String appTwoInputTopic = envProps.getProperty(\"app-two.topic.name\");\n-            final String appThreeInputTopic = envProps.getProperty(\"app-three.topic.name\");\n-            final String totalResultOutputTopic = envProps.getProperty(\"output.topic.name\");\n-\n-        final Serde<String> stringSerde = getPrimitiveAvroSerde(envProps, true);\n-        final Serde<LoginEvent> loginEventSerde = getSpecificAvroSerde(envProps);\n-        final Serde<LoginRollup> loginRollupSerde = getSpecificAvroSerde(envProps);\n-\n-\n-        final KStream<String, LoginEvent> appOneStream = builder.stream(appOneInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appTwoStream = builder.stream(appTwoInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-        final KStream<String, LoginEvent> appThreeStream = builder.stream(appThreeInputTopic, Consumed.with(stringSerde, loginEventSerde));\n-\n-        final Aggregator<String, LoginEvent, LoginRollup> loginAggregator = new LoginAggregator();\n-\n-        final KGroupedStream<String, LoginEvent> appOneGrouped = appOneStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appTwoGrouped = appTwoStream.groupByKey();\n-        final KGroupedStream<String, LoginEvent> appThreeGrouped = appThreeStream.groupByKey();\n-\n-        appOneGrouped.cogroup(loginAggregator)\n-            .cogroup(appTwoGrouped, loginAggregator)\n-            .cogroup(appThreeGrouped, loginAggregator)\n-            .aggregate(() -> new LoginRollup(new HashMap<>()), Materialized.with(Serdes.String(), loginRollupSerde))\n-            .toStream().to(totalResultOutputTopic, Produced.with(stringSerde, loginRollupSerde));\n-\n-        return builder.build();\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    static <T> Serde<T> getPrimitiveAvroSerde(final Properties envProps, boolean isKey) {\n-        final KafkaAvroDeserializer deserializer = new KafkaAvroDeserializer();\n-        final KafkaAvroSerializer serializer = new KafkaAvroSerializer();\n-        final Map<String, String> config = new HashMap<>();\n-        config.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-        deserializer.configure(config, isKey);\n-        serializer.configure(config, isKey);\n-        return (Serde<T>)Serdes.serdeFrom(serializer, deserializer);\n-    }\n-\n-    static <T extends SpecificRecord> SpecificAvroSerde<T> getSpecificAvroSerde(final Properties envProps) {\n-        final SpecificAvroSerde<T> specificAvroSerde = new SpecificAvroSerde<>();\n-\n-        final HashMap<String, String> serdeConfig = new HashMap<>();\n-        serdeConfig.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,\n-                envProps.getProperty(\"schema.registry.url\"));\n-\n-        specificAvroSerde.configure(serdeConfig, false);\n-        return specificAvroSerde;\n-    }\n-\n-    public void createTopics(final Properties envProps) {\n-        final Map<String, Object> config = new HashMap<>();\n-        config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n-        try (final AdminClient client = AdminClient.create(config)) {\n-\n-        final List<NewTopic> topics = new ArrayList<>();\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-one.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-one.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-one.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-two.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-two.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-two.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"app-three.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"app-three.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"app-three.topic.replication.factor\"))));\n-\n-            topics.add(new NewTopic(\n-                    envProps.getProperty(\"output.topic.name\"),\n-                    Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n-                    Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n-\n-            client.createTopics(topics);\n-        }\n-    }\n-\n-    public Properties loadEnvProperties(String fileName) throws IOException {\n-        final Properties envProps = new Properties();\n-        final FileInputStream input = new FileInputStream(fileName);\n-        envProps.load(input);\n-        input.close();\n-\n-        return envProps;\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-\n-        if (args.length < 1) {\n-            throw new IllegalArgumentException(\"This program takes one argument: the path to an environment configuration file.\");\n-        }\n-\n-        final CogroupingStreams instance = new CogroupingStreams();\n-        final Properties envProps = instance.loadEnvProperties(args[0]);\n-        final Properties streamProps = instance.buildStreamsProperties(envProps);\n-        final Topology topology = instance.buildTopology(envProps);\n-\n-        instance.createTopics(envProps);\n-\n-        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n-        final CountDownLatch latch = new CountDownLatch(1);\n-\n-        // Attach shutdown handler to catch Control-C.\n-        Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n-            @Override\n-            public void run() {\n-                streams.close(Duration.ofSeconds(5));\n-                latch.countDown();\n-            }\n-        });\n-\n-        try {\n-            streams.start();\n-            latch.await();\n-        } catch (Throwable e) {\n-            System.exit(1);\n-        }\n-        System.exit(0);\n-    }\n-\n-}\n"}}, {"oid": "5959dd935463de66401abc0521284b167eaaa15e", "url": "https://github.com/confluentinc/kafka-tutorials/commit/5959dd935463de66401abc0521284b167eaaa15e", "message": "Merge pull request #327 from russau/rekeying-syntax-update\n\nRekeying syntax update", "committedDate": "2020-05-01T16:47:41Z", "type": "commit"}, {"oid": "6aab2b763a3643865b3173c00df01f180e08773a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6aab2b763a3643865b3173c00df01f180e08773a", "message": "Updated KSQL tutorials - 0.8.x (#337)\n\nTested KSQL joins tutorials against 0.8.1 and found some issues, which are fixed below:\r\n\r\n- PRINT command no longer needs the topic name quoting: it's now case sensitive. (fixed across all examples)\r\n- TIMESTAMPTOSTRING should be passed a timezone for consistent results\r\n- Added explicit ROWKEY columns to all CT/CS statements, as this is to be encouraged.\r\n- Fixed key format output of PRINT that was outputting MIXED, which is not a valid output.\r\n\r\nfixes #336\r\n\r\n\r\nCo-authored-by: Andy Coates <big-andy-coates@users.noreply.github.com>", "committedDate": "2020-05-01T16:54:48Z", "type": "commit"}, {"oid": "7d38529bec5820ad809c1556185a2f2f0b3784ed", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7d38529bec5820ad809c1556185a2f2f0b3784ed", "message": "Adapt expected output for cli column-width setting", "committedDate": "2020-05-01T17:41:19Z", "type": "commit"}, {"oid": "d8b974bb270951ee3801cc9fb40ac7459287a094", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d8b974bb270951ee3801cc9fb40ac7459287a094", "message": "DEVX-1520: Add tutorial for KStreasm Cogrouping", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "1281b81aaf09327ccfd1dbb8b05a33d7d470ca1d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/1281b81aaf09327ccfd1dbb8b05a33d7d470ca1d", "message": "add actual-output.json to gitignore, update make-avro-dir.adoc text", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "004f054e0081c68298a8534b4580c2553659af58", "url": "https://github.com/confluentinc/kafka-tutorials/commit/004f054e0081c68298a8534b4580c2553659af58", "message": "Upgrade to CP 5.5 fix template file that had harded coded path", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "f59d7aaee20442d3256ce6a242afc9033bd613c1", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f59d7aaee20442d3256ce6a242afc9033bd613c1", "message": "add KIP link", "committedDate": "2020-05-01T21:19:15Z", "type": "commit"}, {"oid": "e19c5de61b913bdcec44ae2cf53a424550676ae9", "url": "https://github.com/confluentinc/kafka-tutorials/commit/e19c5de61b913bdcec44ae2cf53a424550676ae9", "message": "Apply suggestions from code review\n\nCo-authored-by: Viktor Gamov <viktor@confluent.io>", "committedDate": "2020-05-01T21:23:19Z", "type": "commit"}, {"oid": "ef798ab67c2468010c37ce668d4ee97ff3ac36ae", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ef798ab67c2468010c37ce668d4ee97ff3ac36ae", "message": "fix merge conflict", "committedDate": "2020-05-01T21:27:26Z", "type": "commit"}, {"oid": "ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "url": "https://github.com/confluentinc/kafka-tutorials/commit/ad1d7afa2f9a6e50607ab49f651712bc8aa27496", "message": "ignore output files", "committedDate": "2020-05-01T21:35:53Z", "type": "commit"}, {"oid": "4ca77d4033953b157cd8903e5f2af6699994ce38", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4ca77d4033953b157cd8903e5f2af6699994ce38", "message": "Apply suggestions from code review\n\nCo-authored-by: Colin Hicks <colin.hicks@confluent.io>", "committedDate": "2020-05-04T20:48:50Z", "type": "commit"}, {"oid": "c44be55e156b5a6b11ea02165424023ce769077a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c44be55e156b5a6b11ea02165424023ce769077a", "message": "updates for gitignore", "committedDate": "2020-05-04T21:06:35Z", "type": "commit"}]}