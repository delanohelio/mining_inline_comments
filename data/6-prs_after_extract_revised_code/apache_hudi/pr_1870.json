{"pr_number": 1870, "pr_title": "[HUDI-808] Support cleaning bootstrap source data", "pr_createdAt": "2020-07-24T05:23:10Z", "pr_url": "https://github.com/apache/hudi/pull/1870", "timeline": [{"oid": "946b08eeab23a8fe78d5916b96e502714fa9bee7", "url": "https://github.com/apache/hudi/commit/946b08eeab23a8fe78d5916b96e502714fa9bee7", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-07-28T18:12:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI5MTg1Nw==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r463291857", "bodyText": "I see some private functions in this class to test cleaning based on commits and file versions, that are re-used by other tests. Is it possible to re-use some of these already created ones. May be introduce another boolean flag for bootstrap which can do additional creation of source files and later check if they are cleaned up.", "author": "umehrot2", "createdAt": "2020-07-30T21:51:09Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java", "diffHunk": "@@ -885,6 +888,109 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI\n         file2P0C1));\n   }\n \n+  @Test\n+  public void testBootstrapSourceFileCleanWithKeepLatestFileVersions() throws IOException {\n+    testBootstrapSourceFileClean(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS);\n+  }\n+\n+  @Test\n+  public void testBootstrapSourceFileCleanWithKeepLatestCommits() throws IOException {\n+    testBootstrapSourceFileClean(HoodieCleaningPolicy.KEEP_LATEST_COMMITS);\n+  }\n+\n+  /**\n+   * Test HoodieTable.clean() with Bootstrap source file clean enable.\n+   */\n+  @Test\n+  private void testBootstrapSourceFileClean(HoodieCleaningPolicy cleaningPolicy) throws IOException {\n+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n+        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n+            .withCleanBootstrapSourceFileEnabled(true)\n+            .withCleanerPolicy(cleaningPolicy).retainCommits(1).retainFileVersions(2).build())\n+        .build();", "originalCommit": "946b08eeab23a8fe78d5916b96e502714fa9bee7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM3MzQ3Nw==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467373477", "bodyText": "Rewrote the test part to make use of the original test code.", "author": "zhedoubushishi", "createdAt": "2020-08-08T07:19:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI5MTg1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "98cb7ccc736005fc9e907f19608e6be5d8596aaf", "chunk": "diff --git a/hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java b/hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java\nindex 97ff7645b..c15a21a55 100644\n--- a/hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java\n+++ b/hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java\n\n@@ -795,200 +837,127 @@ public class TestCleaner extends HoodieClientTestBase {\n       }\n     });\n     metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"001\"),\n+        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000002\"),\n         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n     List<HoodieCleanStat> hoodieCleanStatsTwo = runCleaner(config, simulateFailureRetry);\n     assertEquals(0, hoodieCleanStatsTwo.size(), \"Must not scan any partitions and clean any files\");\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"001\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\",\n         file2P0C1));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, \"001\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, \"00000000000002\",\n         file2P1C1));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"000\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\",\n         file1P0C0));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, \"000\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, \"00000000000001\",\n         file1P1C0));\n \n     // make next commit, with 2 updates to existing files, and 1 insert\n-    HoodieTestUtils.createInflightCommitFiles(basePath, \"002\");\n+    HoodieTestUtils.createInflightCommitFiles(basePath, \"00000000000003\");\n     metaClient = HoodieTableMetaClient.reload(metaClient);\n \n     HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\", file1P0C0); // update\n+        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\", file1P0C0); // update\n     HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\", file2P0C1); // update\n+        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\", file2P0C1); // update\n     String file3P0C2 =\n-        HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\");\n+        HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\");\n \n     commitMetadata = generateCommitMetadata(CollectionUtils\n         .createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n             CollectionUtils.createImmutableList(file1P0C0, file2P0C1, file3P0C2)));\n     metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"002\"),\n+        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000003\"),\n         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n \n     List<HoodieCleanStat> hoodieCleanStatsThree = runCleaner(config, simulateFailureRetry);\n     assertEquals(0, hoodieCleanStatsThree.size(),\n         \"Must not clean any file. We have to keep 1 version before the latest commit time to keep\");\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"000\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\",\n         file1P0C0));\n \n     // make next commit, with 2 updates to existing files, and 1 insert\n-    HoodieTestUtils.createInflightCommitFiles(basePath, \"003\");\n+    HoodieTestUtils.createInflightCommitFiles(basePath, \"00000000000004\");\n     metaClient = HoodieTableMetaClient.reload(metaClient);\n \n     HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"003\", file1P0C0); // update\n+        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000004\", file1P0C0); // update\n     HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"003\", file2P0C1); // update\n+        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000004\", file2P0C1); // update\n     String file4P0C3 =\n-        HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"003\");\n+        HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000004\");\n     commitMetadata = generateCommitMetadata(CollectionUtils.createImmutableMap(\n         HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, CollectionUtils.createImmutableList(file1P0C0, file2P0C1, file4P0C3)));\n     metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"003\"),\n+        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000004\"),\n         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n \n     List<HoodieCleanStat> hoodieCleanStatsFour = runCleaner(config, simulateFailureRetry);\n-    assertEquals(1,\n+    // enableBootstrapSourceClean would delete the bootstrap base file as the same time\n+    assertEquals(enableBootstrapSourceClean ? 2 : 1,\n         getCleanStat(hoodieCleanStatsFour, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH).getSuccessDeleteFiles()\n-            .size(), \"Must not clean one old file\");\n+            .size(), \"Must clean at least one old file\");\n \n-    assertFalse(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"000\",\n+    assertFalse(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\",\n         file1P0C0));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"001\",\n+    if (enableBootstrapSourceClean) {\n+      assertFalse(new File(bootstrapMapping.get(\n+          HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH).get(0).getBoostrapFileStatus().getPath().getUri()).exists());\n+    }\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\",\n         file1P0C0));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\",\n         file1P0C0));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"001\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\",\n         file2P0C1));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\",\n         file2P0C1));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"002\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\",\n         file3P0C2));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"003\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000004\",\n         file4P0C3));\n \n     // No cleaning on partially written file, with no commit.\n     HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"004\", file3P0C2); // update\n+        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000005\", file3P0C2); // update\n     commitMetadata = generateCommitMetadata(CollectionUtils.createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n         CollectionUtils.createImmutableList(file3P0C2)));\n     metaClient.getActiveTimeline().createNewInstant(\n-        new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, \"004\"));\n+        new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, \"00000000000005\"));\n     metaClient.getActiveTimeline().transitionRequestedToInflight(\n-        new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, \"004\"),\n+        new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, \"00000000000005\"),\n         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n     List<HoodieCleanStat> hoodieCleanStatsFive = runCleaner(config, simulateFailureRetry);\n     HoodieCleanStat cleanStat = getCleanStat(hoodieCleanStatsFive, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH);\n     assertEquals(0,\n         cleanStat != null ? cleanStat.getSuccessDeleteFiles().size() : 0, \"Must not clean any files\");\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"001\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\",\n         file1P0C0));\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"001\",\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\",\n         file2P0C1));\n   }\n \n-  @Test\n-  public void testBootstrapSourceFileCleanWithKeepLatestFileVersions() throws IOException {\n-    testBootstrapSourceFileClean(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS);\n-  }\n-\n-  @Test\n-  public void testBootstrapSourceFileCleanWithKeepLatestCommits() throws IOException {\n-    testBootstrapSourceFileClean(HoodieCleaningPolicy.KEEP_LATEST_COMMITS);\n-  }\n-\n   /**\n-   * Test HoodieTable.clean() with Bootstrap source file clean enable.\n+   * Generate Bootstrap index, bootstrap base file and corresponding metaClient.\n+   * @return Partition to BootstrapFileMapping Map\n+   * @throws IOException\n    */\n-  @Test\n-  private void testBootstrapSourceFileClean(HoodieCleaningPolicy cleaningPolicy) throws IOException {\n-    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n-        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withCleanBootstrapSourceFileEnabled(true)\n-            .withCleanerPolicy(cleaningPolicy).retainCommits(1).retainFileVersions(2).build())\n-        .build();\n-\n-    // create source data path\n+  private Map<String, List<BootstrapFileMapping>> generateBootstrapIndexAndSourceData() throws IOException {\n+    // create bootstrap source data path\n     java.nio.file.Path sourcePath = tempDir.resolve(\"data\");\n     java.nio.file.Files.createDirectories(sourcePath);\n     assertTrue(new File(sourcePath.toString()).exists());\n \n-    // generate bootstrap index\n-    List<BootstrapSourceFileMapping> mappingList = HoodieTestUtils.generateBootstrapIndex(\n-        metaClient, sourcePath.toString(), new String[] {HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH}, 2)\n-        .get(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH);\n-\n-    // create source data file\n-    new File(sourcePath.toString() + \"/\" + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH).mkdirs();\n-    new File(mappingList.get(0).getSourceFileStatus().getPath().getUri()).createNewFile();\n-    new File(mappingList.get(1).getSourceFileStatus().getPath().getUri()).createNewFile();\n-\n-    assertTrue(new File(mappingList.get(0).getSourceFileStatus().getPath().getUri()).exists());\n-    assertTrue(new File(mappingList.get(1).getSourceFileStatus().getPath().getUri()).exists());\n-\n-    // make 1 bootstrap commit, with 2 files per partition\n-    HoodieTestUtils.createInflightCommitFiles(basePath, \"00000000000001\");\n-\n-    String file1P0C0 = mappingList.get(0).getHudiFileId();\n-    String file2P0C0 = mappingList.get(1).getHudiFileId();\n-    HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\", file1P0C0); // insert\n-    HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\", file2P0C0); // insert\n-\n-    HoodieCommitMetadata commitMetadata = generateCommitMetadata(\n-        Collections.unmodifiableMap(new HashMap<String, List<String>>() {\n-          {\n-            put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, CollectionUtils.createImmutableList(file1P0C0, file2P0C0));\n-          }\n-        })\n-    );\n-\n-    metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000001\"),\n-        Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-\n-    // make next commit, with 2 updates to existing files\n-    HoodieTestUtils.createInflightCommitFiles(basePath, \"00000000000002\");\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n-\n-    HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\", file1P0C0); // update\n-    HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000002\", file2P0C0); // update\n-\n-    commitMetadata = generateCommitMetadata(CollectionUtils\n-        .createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-            CollectionUtils.createImmutableList(file1P0C0, file2P0C0)));\n-    metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000002\"),\n-        Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-\n-    // make next commit, with 1 update to existing file, and 1 insert\n-    HoodieTestUtils.createInflightCommitFiles(basePath, \"00000000000003\");\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n-    String file3P0C2 =\n-        HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\"); // insert\n-    HoodieTestUtils\n-        .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\", file1P0C0); // update\n-\n-    commitMetadata = generateCommitMetadata(CollectionUtils\n-        .createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-            CollectionUtils.createImmutableList(file1P0C0, file3P0C2)));\n-    metaClient.getActiveTimeline().saveAsComplete(\n-        new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, \"00000000000003\"),\n-        Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // recreate metaClient with Bootstrap base path\n+    metaClient = HoodieTestUtils.init(basePath, getTableType(), sourcePath.toString());\n \n-    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config, false);\n-    HoodieCleanStat cleanStat = getCleanStat(hoodieCleanStats, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH);\n+    // generate bootstrap index\n+    Map<String, List<BootstrapFileMapping>> bootstrapMapping = TestBootstrapIndex.generateBootstrapIndex(metaClient, sourcePath.toString(),\n+        new String[] {HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH}, 1);\n \n-    assertNotEquals(0, cleanStat.getSuccessDeleteFiles().size());\n-    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000003\", file1P0C0));\n-    assertFalse(HoodieTestUtils.doesDataFileExist(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, \"00000000000001\", file1P0C0));\n-    // make sure the source data file is deleted\n-    assertFalse(new File(mappingList.get(0).getSourceFileStatus().getPath().getUri()).exists());\n+    for (Map.Entry<String, List<BootstrapFileMapping>> entry : bootstrapMapping.entrySet()) {\n+      new File(sourcePath.toString() + \"/\" + entry.getKey()).mkdirs();\n+      assertTrue(new File(entry.getValue().get(0).getBoostrapFileStatus().getPath().getUri()).createNewFile());\n+    }\n+    return bootstrapMapping;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI0MzA1Ng==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r466243056", "bodyText": "can we move these to the test class itself? these are very specific to bootstrap.", "author": "vinothchandar", "createdAt": "2020-08-06T08:43:21Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -513,4 +522,41 @@ public static void writeRecordsToLogFiles(FileSystem fs, String basePath, Schema\n     }\n     return writeStatList;\n   }\n+\n+  public static Map<String, List<BootstrapSourceFileMapping>> generateBootstrapIndex(HoodieTableMetaClient metaClient,", "originalCommit": "946b08eeab23a8fe78d5916b96e502714fa9bee7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0MjQwOQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r466742409", "bodyText": "Sure. Done.", "author": "zhedoubushishi", "createdAt": "2020-08-06T23:40:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI0MzA1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "98cb7ccc736005fc9e907f19608e6be5d8596aaf", "chunk": "diff --git a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\nindex 0a7fb4de1..92d431c2d 100644\n--- a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\n+++ b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\n\n@@ -522,41 +508,4 @@ public class HoodieTestUtils {\n     }\n     return writeStatList;\n   }\n-\n-  public static Map<String, List<BootstrapSourceFileMapping>> generateBootstrapIndex(HoodieTableMetaClient metaClient,\n-      String sourceBasePath, String[] partitions, int numEntriesPerPartition) {\n-    Map<String, List<BootstrapSourceFileMapping>> bootstrapMapping =\n-        generateBootstrapMapping(sourceBasePath, partitions, numEntriesPerPartition);\n-    BootstrapIndex index = new HFileBasedBootstrapIndex(metaClient);\n-    try (IndexWriter writer = index.createWriter(sourceBasePath)) {\n-      writer.begin();\n-      bootstrapMapping.entrySet().stream().forEach(e -> writer.appendNextPartition(e.getKey(), e.getValue()));\n-      writer.finish();\n-    }\n-    return bootstrapMapping;\n-  }\n-\n-  private static Map<String, List<BootstrapSourceFileMapping>> generateBootstrapMapping(String sourceBasePath,\n-      String[] partitions, int numEntriesPerPartition) {\n-    return Arrays.stream(partitions).map(partition -> {\n-      return Pair.of(partition, IntStream.range(0, numEntriesPerPartition).mapToObj(idx -> {\n-        String hudiFileId = UUID.randomUUID().toString();\n-        System.out.println(\" hudiFileId :\" + hudiFileId + \", partition :\" + partition);\n-        String sourceFileName = idx + \".parquet\";\n-        HoodieFileStatus sourceFileStatus = HoodieFileStatus.newBuilder()\n-            .setPath(HoodiePath.newBuilder().setUri(sourceBasePath + \"/\" + partition + \"/\" + sourceFileName).build())\n-            .setLength(256 * 1024 * 1024L)\n-            .setAccessTime(new Date().getTime())\n-            .setModificationTime(new Date().getTime() + 99999)\n-            .setBlockReplication(2)\n-            .setOwner(\"hudi\")\n-            .setGroup(\"hudi\")\n-            .setBlockSize(128 * 1024 * 1024L)\n-            .setPermission(HoodieFSPermission.newBuilder().setUserAction(FsAction.ALL.name())\n-            .setGroupAction(FsAction.READ.name()).setOtherAction(FsAction.NONE.name()).setStickyBit(true).build())\n-            .build();\n-        return new BootstrapSourceFileMapping(sourceBasePath, partition, partition, sourceFileStatus, hudiFileId);\n-      }).collect(Collectors.toList()));\n-    }).collect(Collectors.toMap(Pair::getKey, Pair::getValue));\n-  }\n }\n"}}, {"oid": "98cb7ccc736005fc9e907f19608e6be5d8596aaf", "url": "https://github.com/apache/hudi/commit/98cb7ccc736005fc9e907f19608e6be5d8596aaf", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-08T07:08:55Z", "type": "forcePushed"}, {"oid": "e25ea60f77252ca81b212e1147ecef36f25d63d2", "url": "https://github.com/apache/hudi/commit/e25ea60f77252ca81b212e1147ecef36f25d63d2", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-08T07:18:26Z", "type": "forcePushed"}, {"oid": "82edf2206f83574f1577175cfa2606039683ffea", "url": "https://github.com/apache/hudi/commit/82edf2206f83574f1577175cfa2606039683ffea", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-08T08:34:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQwMTg3Mg==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467401872", "bodyText": "@zhedoubushishi : I think this is an unnecessary call we will be making per file deletion. With embedded timeline service, this request will be routed to driver. Instead, I think we can handle it using versioning in clean plan.  Thoughts ?", "author": "bvaradar", "createdAt": "2020-08-08T08:50:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java", "diffHunk": "@@ -116,6 +119,19 @@ HoodieCleanerPlan requestClean(JavaSparkContext jsc) {\n         PartitionCleanStat partitionCleanStat = partitionCleanStatMap.get(partitionPath);\n         partitionCleanStat.addDeleteFilePatterns(deletePath.getName());\n         partitionCleanStat.addDeletedFileResult(deletePath.getName(), deletedFileResult);\n+\n+        // If CleanBootstrapSourceFileEnabled and it is a metadata bootstrap commit, also delete the corresponding source file\n+        if (cleanBootstrapSourceFileEnabled && !FSUtils.isLogFile(deletePath)\n+            && FSUtils.getCommitTime(delFileName).equals(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS)) {\n+          Option<HoodieBaseFile> baseFile = fileSystemView.getBaseFileOn(partitionPath,", "originalCommit": "82edf2206f83574f1577175cfa2606039683ffea", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQ5Njk0NQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467496945", "bodyText": "I see what you mean. Yea we should avoid making unnecessary calls. So regarding to versioning, do you mean create a class like TimelineLayoutVersion or just simply handle this logic in the CleanActionExecutor. For example, if the parameter is a full file path then delete it. Else if the parameter is a file name, generate the file path and then delete it.", "author": "zhedoubushishi", "createdAt": "2020-08-08T19:34:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQwMTg3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUyNDUwOQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467524509", "bodyText": "@zhedoubushishi : Tried reaching you on slack :) to discuss an approach. I went ahead and implemented it in the interest of time.\nThe basic idea is to ensure that Cleaner plan stores necessary information related to files to be deleted including bootstrap base files. The cleaner executor will simply read the cleaner plan and be able to distinguish normal vs bootstrap base files. It goes ahead and deletes those files. For bootstrap base files, it records the complete path of the file it deleted in a separate (new) avro field. This is very important in order to ensure incremental timeline syncing (which reads these metadata) to work properly.\nPlease take a look at this code changes if possible.\n( @vinothchandar  @umehrot2  : FYI )", "author": "bvaradar", "createdAt": "2020-08-09T01:56:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQwMTg3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUyNDY0NQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467524645", "bodyText": "@zhedoubushishi : Please also note that for ensuring backwards compatibility, I add a migrator class (CleanPlanMigrator.java) to handle pre and post 0.6  .clean.requested files.", "author": "bvaradar", "createdAt": "2020-08-09T01:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQwMTg3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "60c5bfb67ef09972a5ccaa5c45085aac41d84932", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\nindex 6afff7b8d..3e3f3c8b1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n\n@@ -84,53 +83,44 @@ public class CleanActionExecutor extends BaseActionExecutor<HoodieCleanMetadata>\n       LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n \n       jsc.setJobGroup(this.getClass().getSimpleName(), \"Generates list of file slices to be cleaned\");\n-      Map<String, List<String>> cleanOps = jsc\n+      Map<String, List<HoodieCleanFileInfo>> cleanOps = jsc\n           .parallelize(partitionsToClean, cleanerParallelism)\n           .map(partitionPathToClean -> Pair.of(partitionPathToClean, planner.getDeletePaths(partitionPathToClean)))\n           .collect().stream()\n-          .collect(Collectors.toMap(Pair::getKey, Pair::getValue));\n+          .collect(Collectors.toMap(Pair::getKey,\n+            (y) -> y.getValue().stream().map(CleanFileInfo::toHoodieFileCleanInfo).collect(Collectors.toList())));\n \n       return new HoodieCleanerPlan(earliestInstant\n           .map(x -> new HoodieActionInstant(x.getTimestamp(), x.getAction(), x.getState().name())).orElse(null),\n-          config.getCleanerPolicy().name(), cleanOps, 1);\n+          config.getCleanerPolicy().name(), null, CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n     } catch (IOException e) {\n       throw new HoodieIOException(\"Failed to schedule clean operation\", e);\n     }\n   }\n \n-  private static PairFlatMapFunction<Iterator<Tuple2<String, String>>, String, PartitionCleanStat> deleteFilesFunc(\n-      HoodieTable table, Boolean cleanBootstrapSourceFileEnabled) {\n-    return (PairFlatMapFunction<Iterator<Tuple2<String, String>>, String, PartitionCleanStat>) iter -> {\n+  private static PairFlatMapFunction<Iterator<Tuple2<String, CleanFileInfo>>, String, PartitionCleanStat>\n+        deleteFilesFunc(HoodieTable table) {\n+    return (PairFlatMapFunction<Iterator<Tuple2<String, CleanFileInfo>>, String, PartitionCleanStat>) iter -> {\n       Map<String, PartitionCleanStat> partitionCleanStatMap = new HashMap<>();\n-\n-      SyncableFileSystemView fileSystemView = table.getHoodieView();\n       FileSystem fs = table.getMetaClient().getFs();\n-      Path basePath = new Path(table.getMetaClient().getBasePath());\n       while (iter.hasNext()) {\n-        Tuple2<String, String> partitionDelFileTuple = iter.next();\n+        Tuple2<String, CleanFileInfo> partitionDelFileTuple = iter.next();\n         String partitionPath = partitionDelFileTuple._1();\n-        String delFileName = partitionDelFileTuple._2();\n-        Path deletePath = FSUtils.getPartitionPath(FSUtils.getPartitionPath(basePath, partitionPath), delFileName);\n+        Path deletePath = new Path(partitionDelFileTuple._2().getFilePath());\n         String deletePathStr = deletePath.toString();\n         Boolean deletedFileResult = deleteFileAndGetResult(fs, deletePathStr);\n         if (!partitionCleanStatMap.containsKey(partitionPath)) {\n           partitionCleanStatMap.put(partitionPath, new PartitionCleanStat(partitionPath));\n         }\n+        boolean isBootstrapBasePathFile = partitionDelFileTuple._2().isBootstrapBaseFile();\n         PartitionCleanStat partitionCleanStat = partitionCleanStatMap.get(partitionPath);\n-        partitionCleanStat.addDeleteFilePatterns(deletePath.getName());\n-        partitionCleanStat.addDeletedFileResult(deletePath.getName(), deletedFileResult);\n-\n-        // If CleanBootstrapSourceFileEnabled and it is a metadata bootstrap commit, also delete the corresponding source file\n-        if (cleanBootstrapSourceFileEnabled && !FSUtils.isLogFile(deletePath)\n-            && FSUtils.getCommitTime(delFileName).equals(HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS)) {\n-          Option<HoodieBaseFile> baseFile = fileSystemView.getBaseFileOn(partitionPath,\n-              FSUtils.getCommitTime(delFileName), FSUtils.getFileId(delFileName));\n-          if (baseFile.isPresent() && baseFile.get().getBootstrapBaseFile().isPresent()) {\n-            String deleteBootstrapBaseFilePath = baseFile.get().getBootstrapBaseFile().get().getPath();\n-            Boolean deletedBootstrapBaseFileResult = deleteFileAndGetResult(fs, deleteBootstrapBaseFilePath);\n-            partitionCleanStat.addDeleteFilePatterns(new Path(deleteBootstrapBaseFilePath).getName());\n-            partitionCleanStat.addDeletedFileResult(new Path(deleteBootstrapBaseFilePath).getName(), deletedBootstrapBaseFileResult);\n-          }\n+        if (isBootstrapBasePathFile) {\n+          // For Bootstrap Base file deletions, store the full file path.\n+          partitionCleanStat.addDeleteFilePatterns(deletePath.toString(), true);\n+          partitionCleanStat.addDeletedFileResult(deletePath.toString(), deletedFileResult, true);\n+        } else {\n+          partitionCleanStat.addDeleteFilePatterns(deletePath.getName(), false);\n+          partitionCleanStat.addDeletedFileResult(deletePath.getName(), deletedFileResult, false);\n         }\n       }\n       return partitionCleanStatMap.entrySet().stream().map(e -> new Tuple2<>(e.getKey(), e.getValue()))\n"}}, {"oid": "60c5bfb67ef09972a5ccaa5c45085aac41d84932", "url": "https://github.com/apache/hudi/commit/60c5bfb67ef09972a5ccaa5c45085aac41d84932", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-09T01:45:12Z", "type": "forcePushed"}, {"oid": "8344982d6c125e70faa3ea83ac0dcd612d14694f", "url": "https://github.com/apache/hudi/commit/8344982d6c125e70faa3ea83ac0dcd612d14694f", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-09T02:09:39Z", "type": "forcePushed"}, {"oid": "6174d890b8d6fa91e3a8d1b960443222350446bd", "url": "https://github.com/apache/hudi/commit/6174d890b8d6fa91e3a8d1b960443222350446bd", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-09T02:26:45Z", "type": "forcePushed"}, {"oid": "c0bd452d0367eee8b8c37502f6b56c2ee645ef24", "url": "https://github.com/apache/hudi/commit/c0bd452d0367eee8b8c37502f6b56c2ee645ef24", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-09T05:35:05Z", "type": "forcePushed"}, {"oid": "896828d0340183369b8faf0c96721f446c30135f", "url": "https://github.com/apache/hudi/commit/896828d0340183369b8faf0c96721f446c30135f", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-09T16:07:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYyODY0OQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467628649", "bodyText": "rename : hoodie.cleaner.delete.bootstrap.base.file ?", "author": "vinothchandar", "createdAt": "2020-08-09T21:04:09Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -52,6 +52,8 @@\n   public static final String MAX_COMMITS_TO_KEEP_PROP = \"hoodie.keep.max.commits\";\n   public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n   public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n+  // Set true to clean bootstrap source files when necessary\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.bootstrap.base.file\";", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODM4MzQzMQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468383431", "bodyText": "Done", "author": "bvaradar", "createdAt": "2020-08-11T07:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYyODY0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\nindex 98e592222..08f377407 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java\n\n@@ -53,7 +53,7 @@ public class HoodieCompactionConfig extends DefaultHoodieConfig {\n   public static final String MIN_COMMITS_TO_KEEP_PROP = \"hoodie.keep.min.commits\";\n   public static final String COMMITS_ARCHIVAL_BATCH_SIZE_PROP = \"hoodie.commits.archival.batch\";\n   // Set true to clean bootstrap source files when necessary\n-  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.bootstrap.base.file\";\n+  public static final String CLEANER_BOOTSTRAP_BASE_FILE_ENABLED = \"hoodie.cleaner.delete.bootstrap.base.file\";\n   // Upsert uses this file size to compact new data onto existing files..\n   public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n   // By default, treat any file <= 100MB as a small file.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk0NDE5Ng==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467944196", "bodyText": "stylistic: in general, a stream within stream is a bit hard to read. flatMap() first? but guess this is a map. probably using a named lambda function may help", "author": "vinothchandar", "createdAt": "2020-08-10T14:29:58Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java", "diffHunk": "@@ -82,40 +83,45 @@ HoodieCleanerPlan requestClean(JavaSparkContext jsc) {\n       LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n \n       jsc.setJobGroup(this.getClass().getSimpleName(), \"Generates list of file slices to be cleaned\");\n-      Map<String, List<String>> cleanOps = jsc\n+      Map<String, List<HoodieCleanFileInfo>> cleanOps = jsc\n           .parallelize(partitionsToClean, cleanerParallelism)\n           .map(partitionPathToClean -> Pair.of(partitionPathToClean, planner.getDeletePaths(partitionPathToClean)))\n           .collect().stream()\n-          .collect(Collectors.toMap(Pair::getKey, Pair::getValue));\n+          .collect(Collectors.toMap(Pair::getKey,", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODM4Mjg0Mg==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468382842", "bodyText": "Done", "author": "bvaradar", "createdAt": "2020-08-11T07:33:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk0NDE5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\nindex fd1d9b761..52614476e 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n\n@@ -87,12 +88,12 @@ public class CleanActionExecutor extends BaseActionExecutor<HoodieCleanMetadata>\n           .parallelize(partitionsToClean, cleanerParallelism)\n           .map(partitionPathToClean -> Pair.of(partitionPathToClean, planner.getDeletePaths(partitionPathToClean)))\n           .collect().stream()\n-          .collect(Collectors.toMap(Pair::getKey,\n-            (y) -> y.getValue().stream().map(CleanFileInfo::toHoodieFileCleanInfo).collect(Collectors.toList())));\n+          .collect(Collectors.toMap(Pair::getKey, y -> CleanerUtils.convertToHoodieCleanFileInfoList(y.getValue())));\n \n       return new HoodieCleanerPlan(earliestInstant\n           .map(x -> new HoodieActionInstant(x.getTimestamp(), x.getAction(), x.getState().name())).orElse(null),\n-          config.getCleanerPolicy().name(), null, CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n+          config.getCleanerPolicy().name(), CollectionUtils.createImmutableMap(),\n+          CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n     } catch (IOException e) {\n       throw new HoodieIOException(\"Failed to schedule clean operation\", e);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk0NzA5OA==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467947098", "bodyText": "this is same as\npartitionCleanStat.addDeleteFilePatterns(deletePath.toString(), isBootstrapBasePathFile);\npartitionCleanStat.addDeletedFileResult(deletePath.toString(), deletedFileResult, isBootstrapBasePathFile);\n\nright", "author": "vinothchandar", "createdAt": "2020-08-10T14:34:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java", "diffHunk": "@@ -82,40 +83,45 @@ HoodieCleanerPlan requestClean(JavaSparkContext jsc) {\n       LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n \n       jsc.setJobGroup(this.getClass().getSimpleName(), \"Generates list of file slices to be cleaned\");\n-      Map<String, List<String>> cleanOps = jsc\n+      Map<String, List<HoodieCleanFileInfo>> cleanOps = jsc\n           .parallelize(partitionsToClean, cleanerParallelism)\n           .map(partitionPathToClean -> Pair.of(partitionPathToClean, planner.getDeletePaths(partitionPathToClean)))\n           .collect().stream()\n-          .collect(Collectors.toMap(Pair::getKey, Pair::getValue));\n+          .collect(Collectors.toMap(Pair::getKey,\n+            (y) -> y.getValue().stream().map(CleanFileInfo::toHoodieFileCleanInfo).collect(Collectors.toList())));\n \n       return new HoodieCleanerPlan(earliestInstant\n           .map(x -> new HoodieActionInstant(x.getTimestamp(), x.getAction(), x.getState().name())).orElse(null),\n-          config.getCleanerPolicy().name(), cleanOps, 1);\n+          config.getCleanerPolicy().name(), null, CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n     } catch (IOException e) {\n       throw new HoodieIOException(\"Failed to schedule clean operation\", e);\n     }\n   }\n \n-  private static PairFlatMapFunction<Iterator<Tuple2<String, String>>, String, PartitionCleanStat> deleteFilesFunc(\n-      HoodieTable table) {\n-    return (PairFlatMapFunction<Iterator<Tuple2<String, String>>, String, PartitionCleanStat>) iter -> {\n+  private static PairFlatMapFunction<Iterator<Tuple2<String, CleanFileInfo>>, String, PartitionCleanStat>\n+        deleteFilesFunc(HoodieTable table) {\n+    return (PairFlatMapFunction<Iterator<Tuple2<String, CleanFileInfo>>, String, PartitionCleanStat>) iter -> {\n       Map<String, PartitionCleanStat> partitionCleanStatMap = new HashMap<>();\n-\n       FileSystem fs = table.getMetaClient().getFs();\n-      Path basePath = new Path(table.getMetaClient().getBasePath());\n       while (iter.hasNext()) {\n-        Tuple2<String, String> partitionDelFileTuple = iter.next();\n+        Tuple2<String, CleanFileInfo> partitionDelFileTuple = iter.next();\n         String partitionPath = partitionDelFileTuple._1();\n-        String delFileName = partitionDelFileTuple._2();\n-        Path deletePath = FSUtils.getPartitionPath(FSUtils.getPartitionPath(basePath, partitionPath), delFileName);\n+        Path deletePath = new Path(partitionDelFileTuple._2().getFilePath());\n         String deletePathStr = deletePath.toString();\n         Boolean deletedFileResult = deleteFileAndGetResult(fs, deletePathStr);\n         if (!partitionCleanStatMap.containsKey(partitionPath)) {\n           partitionCleanStatMap.put(partitionPath, new PartitionCleanStat(partitionPath));\n         }\n+        boolean isBootstrapBasePathFile = partitionDelFileTuple._2().isBootstrapBaseFile();\n         PartitionCleanStat partitionCleanStat = partitionCleanStatMap.get(partitionPath);\n-        partitionCleanStat.addDeleteFilePatterns(deletePath.getName());\n-        partitionCleanStat.addDeletedFileResult(deletePath.getName(), deletedFileResult);\n+        if (isBootstrapBasePathFile) {", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODM3NTEzNg==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468375136", "bodyText": "deletePath.toString() returns a full path whereas deletePath.getName() returns only the file name.  That was the reason why it was in if-else block.", "author": "bvaradar", "createdAt": "2020-08-11T07:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk0NzA5OA=="}], "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\nindex fd1d9b761..52614476e 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/clean/CleanActionExecutor.java\n\n@@ -87,12 +88,12 @@ public class CleanActionExecutor extends BaseActionExecutor<HoodieCleanMetadata>\n           .parallelize(partitionsToClean, cleanerParallelism)\n           .map(partitionPathToClean -> Pair.of(partitionPathToClean, planner.getDeletePaths(partitionPathToClean)))\n           .collect().stream()\n-          .collect(Collectors.toMap(Pair::getKey,\n-            (y) -> y.getValue().stream().map(CleanFileInfo::toHoodieFileCleanInfo).collect(Collectors.toList())));\n+          .collect(Collectors.toMap(Pair::getKey, y -> CleanerUtils.convertToHoodieCleanFileInfoList(y.getValue())));\n \n       return new HoodieCleanerPlan(earliestInstant\n           .map(x -> new HoodieActionInstant(x.getTimestamp(), x.getAction(), x.getState().name())).orElse(null),\n-          config.getCleanerPolicy().name(), null, CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n+          config.getCleanerPolicy().name(), CollectionUtils.createImmutableMap(),\n+          CleanPlanner.LATEST_CLEAN_PLAN_VERSION, cleanOps);\n     } catch (IOException e) {\n       throw new HoodieIOException(\"Failed to schedule clean operation\", e);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk1Mzk5OQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r467953999", "bodyText": "CollectionUtils.emptyList or something?", "author": "vinothchandar", "createdAt": "2020-08-10T14:44:08Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java", "diffHunk": "@@ -39,17 +40,34 @@\n   private final List<String> successDeleteFiles;\n   // Files that could not be deleted\n   private final List<String> failedDeleteFiles;\n+  // Boostrap Base Path patterns that were generated for the delete operation\n+  private final List<String> deleteBootstrapBasePathPatterns;\n+  private final List<String> successDeleteBootstrapBaseFiles;\n+  // Files that could not be deleted\n+  private final List<String> failedDeleteBootstrapBaseFiles;\n   // Earliest commit that was retained in this clean\n   private final String earliestCommitToRetain;\n \n   public HoodieCleanStat(HoodieCleaningPolicy policy, String partitionPath, List<String> deletePathPatterns,\n       List<String> successDeleteFiles, List<String> failedDeleteFiles, String earliestCommitToRetain) {\n+    this(policy, partitionPath, deletePathPatterns, successDeleteFiles, failedDeleteFiles, earliestCommitToRetain,\n+        new ArrayList<>(), new ArrayList<>(), new ArrayList<>());", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\nindex 7f8b3515d..e9de502f7 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n\n@@ -40,7 +40,7 @@ public class HoodieCleanStat implements Serializable {\n   private final List<String> successDeleteFiles;\n   // Files that could not be deleted\n   private final List<String> failedDeleteFiles;\n-  // Boostrap Base Path patterns that were generated for the delete operation\n+  // Bootstrap Base Path patterns that were generated for the delete operation\n   private final List<String> deleteBootstrapBasePathPatterns;\n   private final List<String> successDeleteBootstrapBaseFiles;\n   // Files that could not be deleted\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA2OTY5OQ==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468069699", "bodyText": "[typo] boostrap -> bootstrap", "author": "zhedoubushishi", "createdAt": "2020-08-10T17:39:33Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java", "diffHunk": "@@ -39,17 +40,34 @@\n   private final List<String> successDeleteFiles;\n   // Files that could not be deleted\n   private final List<String> failedDeleteFiles;\n+  // Boostrap Base Path patterns that were generated for the delete operation", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\nindex 7f8b3515d..e9de502f7 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n\n@@ -40,7 +40,7 @@ public class HoodieCleanStat implements Serializable {\n   private final List<String> successDeleteFiles;\n   // Files that could not be deleted\n   private final List<String> failedDeleteFiles;\n-  // Boostrap Base Path patterns that were generated for the delete operation\n+  // Bootstrap Base Path patterns that were generated for the delete operation\n   private final List<String> deleteBootstrapBasePathPatterns;\n   private final List<String> successDeleteBootstrapBaseFiles;\n   // Files that could not be deleted\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA3MDQ3OA==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468070478", "bodyText": "[nit] wrong line", "author": "zhedoubushishi", "createdAt": "2020-08-10T17:40:59Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java", "diffHunk": "@@ -18,6 +18,7 @@\n \n package org.apache.hudi.common;\n \n+import java.util.ArrayList;", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\nindex 7f8b3515d..e9de502f7 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/HoodieCleanStat.java\n\n@@ -18,9 +18,9 @@\n \n package org.apache.hudi.common;\n \n-import java.util.ArrayList;\n import org.apache.hudi.common.model.HoodieCleaningPolicy;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.CollectionUtils;\n import org.apache.hudi.common.util.Option;\n \n import java.io.Serializable;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA3NjE0Mg==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468076142", "bodyText": "[nit] wrong line", "author": "zhedoubushishi", "createdAt": "2020-08-10T17:50:50Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.versioning.clean;\n+\n+import java.util.HashMap;", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java\nindex 14a17e7ca..e141e9a15 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/versioning/clean/CleanPlanV2MigrationHandler.java\n\n@@ -18,7 +18,6 @@\n \n package org.apache.hudi.common.table.timeline.versioning.clean;\n \n-import java.util.HashMap;\n import org.apache.hudi.avro.model.HoodieCleanFileInfo;\n import org.apache.hudi.avro.model.HoodieCleanerPlan;\n import org.apache.hudi.common.fs.FSUtils;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA3OTEzMw==", "url": "https://github.com/apache/hudi/pull/1870#discussion_r468079133", "bodyText": "[nit] wrong line", "author": "zhedoubushishi", "createdAt": "2020-08-10T17:56:00Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java", "diffHunk": "@@ -26,39 +26,49 @@\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataMigrator;\n-import org.apache.hudi.common.table.timeline.versioning.clean.CleanV1MigrationHandler;\n-import org.apache.hudi.common.table.timeline.versioning.clean.CleanV2MigrationHandler;\n+import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataV1MigrationHandler;\n+import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataV2MigrationHandler;\n \n import java.io.IOException;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import org.apache.hudi.common.table.timeline.versioning.clean.CleanPlanMigrator;", "originalCommit": "896828d0340183369b8faf0c96721f446c30135f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java\nindex fcee2db8e..6049ee307 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java\n\n@@ -18,22 +18,25 @@\n \n package org.apache.hudi.common.util;\n \n+import java.util.stream.Collectors;\n+import org.apache.hudi.avro.model.HoodieCleanFileInfo;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCleanPartitionMetadata;\n import org.apache.hudi.avro.model.HoodieCleanerPlan;\n import org.apache.hudi.common.HoodieCleanStat;\n+import org.apache.hudi.common.model.CleanFileInfo;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataMigrator;\n import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataV1MigrationHandler;\n import org.apache.hudi.common.table.timeline.versioning.clean.CleanMetadataV2MigrationHandler;\n+import org.apache.hudi.common.table.timeline.versioning.clean.CleanPlanMigrator;\n \n import java.io.IOException;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import org.apache.hudi.common.table.timeline.versioning.clean.CleanPlanMigrator;\n \n public class CleanerUtils {\n   public static final Integer CLEAN_METADATA_VERSION_1 = CleanMetadataV1MigrationHandler.VERSION;\n"}}, {"oid": "dcad7aa9afca86e204dba67f678c2330bdc3fa41", "url": "https://github.com/apache/hudi/commit/dcad7aa9afca86e204dba67f678c2330bdc3fa41", "message": "[HUDI-808] Support cleaning bootstrap source data", "committedDate": "2020-08-11T05:19:53Z", "type": "commit"}, {"oid": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "url": "https://github.com/apache/hudi/commit/ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "message": "Address review comments", "committedDate": "2020-08-11T07:45:23Z", "type": "commit"}, {"oid": "ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "url": "https://github.com/apache/hudi/commit/ee0e71b0416b4cd58c4e971f1d4a6807b08a34aa", "message": "Address review comments", "committedDate": "2020-08-11T07:45:23Z", "type": "forcePushed"}]}