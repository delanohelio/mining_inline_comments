{"pr_number": 2382, "pr_title": "[HUDI-1477] Support CopyOnWriteTable in java client", "pr_createdAt": "2020-12-26T15:38:09Z", "pr_url": "https://github.com/apache/hudi/pull/2382", "timeline": [{"oid": "fd8ad54ff4eb0eeff60f8e965e4a04aaa1d7dff3", "url": "https://github.com/apache/hudi/commit/fd8ad54ff4eb0eeff60f8e965e4a04aaa1d7dff3", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2020-12-27T02:59:02Z", "type": "forcePushed"}, {"oid": "715e09a7dfd283754f4c34199848c3026100297e", "url": "https://github.com/apache/hudi/commit/715e09a7dfd283754f4c34199848c3026100297e", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2021-01-10T07:30:51Z", "type": "forcePushed"}, {"oid": "498109ca22942b99df14b4fed885456430d57acc", "url": "https://github.com/apache/hudi/commit/498109ca22942b99df14b4fed885456430d57acc", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2021-01-25T01:37:02Z", "type": "forcePushed"}, {"oid": "2103800a5275d95283652072e3f9db288db6cfdb", "url": "https://github.com/apache/hudi/commit/2103800a5275d95283652072e3f9db288db6cfdb", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2021-01-25T11:47:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3NTIwNA==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573675204", "bodyText": "spark -> java", "author": "leesf", "createdAt": "2021-02-10T12:06:44Z", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.JavaLazyInsertIterable;\n+import org.apache.hudi.execution.bulkinsert.JavaBulkInsertInternalPartitionerFactory;\n+import org.apache.hudi.io.CreateHandleFactory;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * A spark implementation of {@link AbstractBulkInsertHelper}.", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "194188ac6dcd42f1665444d4f4ccff364450e028", "chunk": "diff --git a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java\nindex 46ce656c0..cce8ad1b0 100644\n--- a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java\n+++ b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java\n\n@@ -37,7 +37,7 @@ import java.util.ArrayList;\n import java.util.List;\n \n /**\n- * A spark implementation of {@link AbstractBulkInsertHelper}.\n+ * A java implementation of {@link AbstractBulkInsertHelper}.\n  *\n  * @param <T>\n  */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3NzIzOQ==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573677239", "bodyText": "move to upper line?", "author": "leesf", "createdAt": "2021-02-10T12:10:08Z", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class JavaInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseJavaCommitActionExecutor<T> {\n+\n+  private final List<HoodieRecord<T>> inputRecords;\n+\n+  public JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext context,\n+                                                 HoodieWriteConfig config, HoodieTable table,\n+                                                 String instantTime, List<HoodieRecord<T>> inputRecords) {\n+    this(context, config, table, instantTime, inputRecords, WriteOperationType.INSERT_OVERWRITE);\n+  }\n+\n+  public JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<HoodieRecord<T>> inputRecords,\n+                                                  WriteOperationType writeOperationType) {\n+    super(context, config, table, instantTime, writeOperationType);\n+    this.inputRecords = inputRecords;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<List<WriteStatus>> execute() {\n+    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(List<WriteStatus> writeStatuses) {\n+    return context.mapToPair(\n+        writeStatuses.stream().map(status -> status.getStat().getPartitionPath()).distinct().collect(Collectors.toList()),\n+        partitionPath ->\n+            Pair.of(partitionPath, getAllExistingFileIds(partitionPath)),\n+        1", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "194188ac6dcd42f1665444d4f4ccff364450e028", "chunk": "diff --git a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java\nindex b524edc3a..519cb76fc 100644\n--- a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java\n+++ b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java\n\n@@ -68,8 +68,7 @@ public class JavaInsertOverwriteCommitActionExecutor<T extends HoodieRecordPaylo\n     return context.mapToPair(\n         writeStatuses.stream().map(status -> status.getStat().getPartitionPath()).distinct().collect(Collectors.toList()),\n         partitionPath ->\n-            Pair.of(partitionPath, getAllExistingFileIds(partitionPath)),\n-        1\n+            Pair.of(partitionPath, getAllExistingFileIds(partitionPath)), 1\n     );\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3OTgxNQ==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573679815", "bodyText": "setJobStatus is useless in java client, remove this?", "author": "leesf", "createdAt": "2021-02-10T12:14:34Z", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class JavaInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends JavaInsertOverwriteCommitActionExecutor<T> {\n+\n+  public JavaInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                      HoodieWriteConfig config, HoodieTable table,\n+                                                      String instantTime, List<HoodieRecord<T>> inputRecords) {\n+    super(context, config, table, instantTime, inputRecords, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  protected List<String> getAllExistingFileIds(String partitionPath) {\n+    return table.getSliceView().getLatestFileSlices(partitionPath)\n+        .map(fg -> fg.getFileId()).distinct().collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(List<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();\n+    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context,\n+        table.getMetaClient().getBasePath(), config.useFileListingMetadata(),\n+        config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());\n+\n+    if (partitionPaths != null && partitionPaths.size() > 0) {\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "194188ac6dcd42f1665444d4f4ccff364450e028", "chunk": "diff --git a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java\nindex 8104190d0..ca6885ccf 100644\n--- a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java\n+++ b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java\n\n@@ -55,7 +55,6 @@ public class JavaInsertOverwriteTableCommitActionExecutor<T extends HoodieRecord\n         config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());\n \n     if (partitionPaths != null && partitionPaths.size() > 0) {\n-      context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");\n       partitionToExistingFileIds = context.mapToPair(partitionPaths,\n           partitionPath -> Pair.of(partitionPath, getAllExistingFileIds(partitionPath)), 1);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4MTk2OQ==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573681969", "bodyText": "this code duplicates with SparkCopyOnWriteRestoreActionExcutor, would we do some refactor or file a jira ticket to track this?", "author": "leesf", "createdAt": "2021-02-10T12:18:14Z", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/restore/JavaCopyOnWriteRestoreActionExecutor.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.restore;\n+\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieJavaEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.rollback.JavaCopyOnWriteRollbackActionExecutor;\n+\n+import java.util.List;\n+\n+public class JavaCopyOnWriteRestoreActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseRestoreActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public JavaCopyOnWriteRestoreActionExecutor(HoodieJavaEngineContext context,\n+                                              HoodieWriteConfig config,\n+                                              HoodieTable table,\n+                                              String instantTime,\n+                                              String restoreInstantTime) {\n+    super(context, config, table, instantTime, restoreInstantTime);\n+  }\n+\n+  @Override\n+  protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback) {\n+    table.getMetaClient().reloadActiveTimeline();\n+    JavaCopyOnWriteRollbackActionExecutor rollbackActionExecutor = new JavaCopyOnWriteRollbackActionExecutor(\n+        context,\n+        config,\n+        table,\n+        HoodieActiveTimeline.createNewInstantTime(),\n+        instantToRollback,\n+        true,\n+        true,\n+        false);\n+    if (!instantToRollback.getAction().equals(HoodieTimeline.COMMIT_ACTION)\n+        && !instantToRollback.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)) {\n+      throw new HoodieRollbackException(\"Unsupported action in rollback instant:\" + instantToRollback);\n+    }\n+    return rollbackActionExecutor.execute();", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3NTE2NDcxMw==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r575164713", "bodyText": "Sure, I will add a jira ticket to track it.", "author": "shenh062326", "createdAt": "2021-02-12T11:40:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4MTk2OQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4Mzg2MA==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573683860", "bodyText": "move above", "author": "leesf", "createdAt": "2021-02-10T12:21:30Z", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Performs Rollback of Hoodie Tables.\n+ */\n+public class JavaListingBasedRollbackHelper implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(JavaListingBasedRollbackHelper.class);\n+\n+  private final HoodieTableMetaClient metaClient;\n+  private final HoodieWriteConfig config;\n+\n+  public JavaListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n+    this.metaClient = metaClient;\n+    this.config = config;\n+  }\n+\n+  /**\n+   * Performs all rollback actions that we have collected in parallel.\n+   */\n+  public List<HoodieRollbackStat> performRollback(HoodieEngineContext context, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n+    Map<String, HoodieRollbackStat> partitionPathRollbackStatsPairs = maybeDeleteAndCollectStats(context, instantToRollback, rollbackRequests, true);\n+\n+    Map<String, List<Pair<String, HoodieRollbackStat>>> collect = partitionPathRollbackStatsPairs.entrySet()\n+        .stream()\n+        .map(x -> Pair.of(x.getKey(), x.getValue())).collect(Collectors.groupingBy(Pair::getLeft));\n+    return collect.values().stream()\n+        .map(pairs -> pairs.stream().map(Pair::getRight).reduce(RollbackUtils::mergeRollbackStat).orElse(null))\n+        .filter(Objects::nonNull)\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Collect all file info that needs to be rollbacked.\n+   */\n+  public List<HoodieRollbackStat> collectRollbackStats(HoodieEngineContext context, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n+    Map<String, HoodieRollbackStat> partitionPathRollbackStatsPairs = maybeDeleteAndCollectStats(context, instantToRollback, rollbackRequests, false);\n+    return new ArrayList<>(partitionPathRollbackStatsPairs.values());\n+  }\n+\n+  /**\n+   * May be delete interested files and collect stats or collect stats only.\n+   *\n+   * @param context           instance of {@link HoodieEngineContext} to use.\n+   * @param instantToRollback {@link HoodieInstant} of interest for which deletion or collect stats is requested.\n+   * @param rollbackRequests  List of {@link ListingBasedRollbackRequest} to be operated on.\n+   * @param doDelete          {@code true} if deletion has to be done. {@code false} if only stats are to be collected w/o performing any deletes.\n+   * @return stats collected with or w/o actual deletions.\n+   */\n+  Map<String, HoodieRollbackStat> maybeDeleteAndCollectStats(HoodieEngineContext context,\n+                                                             HoodieInstant instantToRollback,\n+                                                             List<ListingBasedRollbackRequest> rollbackRequests,\n+                                                             boolean doDelete) {\n+    return context.mapToPair(rollbackRequests, rollbackRequest -> {\n+      switch (rollbackRequest.getType()) {\n+        case DELETE_DATA_FILES_ONLY: {\n+          final Map<FileStatus, Boolean> filesToDeletedStatus = deleteBaseFiles(metaClient, config, instantToRollback.getTimestamp(),\n+              rollbackRequest.getPartitionPath(), doDelete);\n+          return new ImmutablePair<>(rollbackRequest.getPartitionPath(),\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withDeletedFileResults(filesToDeletedStatus).build());\n+        }\n+        case DELETE_DATA_AND_LOG_FILES: {\n+          final Map<FileStatus, Boolean> filesToDeletedStatus = deleteBaseAndLogFiles(metaClient, config, instantToRollback.getTimestamp(), rollbackRequest.getPartitionPath(), doDelete);\n+          return new ImmutablePair<>(rollbackRequest.getPartitionPath(),\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withDeletedFileResults(filesToDeletedStatus).build());\n+        }\n+        case APPEND_ROLLBACK_BLOCK: {\n+          HoodieLogFormat.Writer writer = null;\n+          try {\n+            writer = HoodieLogFormat.newWriterBuilder()\n+                .onParentPath(FSUtils.getPartitionPath(metaClient.getBasePath(), rollbackRequest.getPartitionPath()))\n+                .withFileId(rollbackRequest.getFileId().get())\n+                .overBaseCommit(rollbackRequest.getLatestBaseInstant().get()).withFs(metaClient.getFs())\n+                .withFileExtension(HoodieLogFile.DELTA_EXTENSION).build();\n+\n+            // generate metadata\n+            if (doDelete) {\n+              Map<HoodieLogBlock.HeaderMetadataType, String> header = generateHeader(instantToRollback.getTimestamp());\n+              // if update belongs to an existing log file\n+              writer.appendBlock(new HoodieCommandBlock(header));\n+            }\n+          } catch (IOException | InterruptedException io) {\n+            throw new HoodieRollbackException(\"Failed to rollback for instant \" + instantToRollback, io);\n+          } finally {\n+            try {\n+              if (writer != null) {\n+                writer.close();\n+              }\n+            } catch (IOException io) {\n+              throw new HoodieIOException(\"Error appending rollback block..\", io);\n+            }\n+          }\n+\n+          // This step is intentionally done after writer is closed. Guarantees that\n+          // getFileStatus would reflect correct stats and FileNotFoundException is not thrown in\n+          // cloud-storage : HUDI-168\n+          Map<FileStatus, Long> filesToNumBlocksRollback = Collections.singletonMap(\n+              metaClient.getFs().getFileStatus(Objects.requireNonNull(writer).getLogFile().getPath()),\n+              1L", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "194188ac6dcd42f1665444d4f4ccff364450e028", "chunk": "diff --git a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java\nindex 066991a4b..5331ca589 100644\n--- a/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java\n+++ b/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java\n\n@@ -146,8 +146,7 @@ public class JavaListingBasedRollbackHelper implements Serializable {\n           // getFileStatus would reflect correct stats and FileNotFoundException is not thrown in\n           // cloud-storage : HUDI-168\n           Map<FileStatus, Long> filesToNumBlocksRollback = Collections.singletonMap(\n-              metaClient.getFs().getFileStatus(Objects.requireNonNull(writer).getLogFile().getPath()),\n-              1L\n+              metaClient.getFs().getFileStatus(Objects.requireNonNull(writer).getLogFile().getPath()), 1L\n           );\n           return new ImmutablePair<>(rollbackRequest.getPartitionPath(),\n               HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4OTYzMA==", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573689630", "bodyText": "remove this line?", "author": "leesf", "createdAt": "2021-02-10T12:31:22Z", "path": "hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.HoodieJavaWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.engine.EngineType;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.testutils.Transformations;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+import org.apache.hudi.io.HoodieCreateHandle;\n+import org.apache.hudi.table.HoodieJavaCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieJavaTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hudi.testutils.HoodieJavaClientTestBase;\n+import org.apache.hudi.testutils.MetadataMergeWriteStatus;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroReadSupport;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.testutils.HoodieTestTable.makeNewCommitTime;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestJavaCopyOnWriteActionExecutor extends HoodieJavaClientTestBase {\n+\n+  private static final Logger LOG = LogManager.getLogger(TestJavaCopyOnWriteActionExecutor.class);\n+  private static final Schema SCHEMA = getSchemaFromResource(TestJavaCopyOnWriteActionExecutor.class, \"/exampleSchema.avsc\");\n+\n+  @Test\n+  public void testMakeNewPath() {\n+    String fileName = UUID.randomUUID().toString();\n+    String partitionPath = \"2016/05/04\";\n+\n+    String instantTime = makeNewCommitTime();\n+    HoodieWriteConfig config = makeHoodieClientConfig();\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    HoodieTable table = HoodieJavaTable.create(config, context, metaClient);\n+\n+    Pair<Path, String> newPathWithWriteToken = Arrays.asList(1).stream().map(x -> {\n+      HoodieRecord record = mock(HoodieRecord.class);\n+      when(record.getPartitionPath()).thenReturn(partitionPath);\n+      String writeToken = FSUtils.makeWriteToken(context.getTaskContextSupplier().getPartitionIdSupplier().get(),\n+          context.getTaskContextSupplier().getStageIdSupplier().get(),\n+          context.getTaskContextSupplier().getAttemptIdSupplier().get());\n+      HoodieCreateHandle io = new HoodieCreateHandle(config, instantTime, table, partitionPath, fileName,\n+          context.getTaskContextSupplier());\n+      return Pair.of(io.makeNewPath(record.getPartitionPath()), writeToken);\n+    }).collect(Collectors.toList()).get(0);\n+\n+    assertEquals(newPathWithWriteToken.getKey().toString(), Paths.get(this.basePath, partitionPath,\n+        FSUtils.makeDataFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());\n+  }\n+\n+  private HoodieWriteConfig makeHoodieClientConfig() {\n+    return makeHoodieClientConfigBuilder().build();\n+  }\n+\n+  private HoodieWriteConfig.Builder makeHoodieClientConfigBuilder() {\n+    // Prepare the AvroParquetIO\n+    return HoodieWriteConfig.newBuilder()\n+        .withEngineType(EngineType.JAVA)\n+        .withPath(basePath)\n+        .withSchema(SCHEMA.toString());\n+  }\n+\n+  // TODO (weiy): Add testcases for crossing file writing.", "originalCommit": "2103800a5275d95283652072e3f9db288db6cfdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "194188ac6dcd42f1665444d4f4ccff364450e028", "chunk": "diff --git a/hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java b/hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java\nindex 01d485e4d..17b174279 100644\n--- a/hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java\n+++ b/hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java\n\n@@ -117,7 +117,6 @@ public class TestJavaCopyOnWriteActionExecutor extends HoodieJavaClientTestBase\n         .withSchema(SCHEMA.toString());\n   }\n \n-  // TODO (weiy): Add testcases for crossing file writing.\n   @Test\n   public void testUpdateRecords() throws Exception {\n     // Prepare the AvroParquetIO\n"}}, {"oid": "194188ac6dcd42f1665444d4f4ccff364450e028", "url": "https://github.com/apache/hudi/commit/194188ac6dcd42f1665444d4f4ccff364450e028", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2021-02-12T11:42:46Z", "type": "forcePushed"}, {"oid": "aae460eefd595d664deaf60a4bb426a4af285faf", "url": "https://github.com/apache/hudi/commit/aae460eefd595d664deaf60a4bb426a4af285faf", "message": "[HUDI-1477] Support CopyOnWriteTable in java client", "committedDate": "2021-02-12T11:50:28Z", "type": "forcePushed"}, {"oid": "2d4fe473ab0d186331d056ae8ab21c0e4219de06", "url": "https://github.com/apache/hudi/commit/2d4fe473ab0d186331d056ae8ab21c0e4219de06", "message": "[HUDI-1477] Support copyOnWriteTable in java client", "committedDate": "2021-02-13T01:41:17Z", "type": "forcePushed"}, {"oid": "c5e6ae2dea8c9ee651a30c3f59886f3969a81a9e", "url": "https://github.com/apache/hudi/commit/c5e6ae2dea8c9ee651a30c3f59886f3969a81a9e", "message": "[HUDI-1477] Support copyOnWriteTable in java client", "committedDate": "2021-02-13T07:55:45Z", "type": "commit"}, {"oid": "c5e6ae2dea8c9ee651a30c3f59886f3969a81a9e", "url": "https://github.com/apache/hudi/commit/c5e6ae2dea8c9ee651a30c3f59886f3969a81a9e", "message": "[HUDI-1477] Support copyOnWriteTable in java client", "committedDate": "2021-02-13T07:55:45Z", "type": "forcePushed"}]}