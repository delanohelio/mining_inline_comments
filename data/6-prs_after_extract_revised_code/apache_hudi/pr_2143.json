{"pr_number": 2143, "pr_title": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "pr_createdAt": "2020-10-04T17:10:27Z", "pr_url": "https://github.com/apache/hudi/pull/2143", "timeline": [{"oid": "08ca27f468b87150a6c1568fe74f03155b5df3b6", "url": "https://github.com/apache/hudi/commit/08ca27f468b87150a6c1568fe74f03155b5df3b6", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "committedDate": "2020-10-04T17:34:31Z", "type": "forcePushed"}, {"oid": "b4298c36d59f29b607642f0e456fdd2907e3604d", "url": "https://github.com/apache/hudi/commit/b4298c36d59f29b607642f0e456fdd2907e3604d", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "committedDate": "2020-10-04T18:12:16Z", "type": "forcePushed"}, {"oid": "b7aa3df2b5eacc1a8d090cb00fc1e43b3e92fa8e", "url": "https://github.com/apache/hudi/commit/b7aa3df2b5eacc1a8d090cb00fc1e43b3e92fa8e", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "committedDate": "2020-10-04T18:20:28Z", "type": "forcePushed"}, {"oid": "6fb5d84167389c39fd8d02779b759ed816508453", "url": "https://github.com/apache/hudi/commit/6fb5d84167389c39fd8d02779b759ed816508453", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "committedDate": "2020-10-04T21:59:36Z", "type": "forcePushed"}, {"oid": "3be9ec7ec9081ae61eab6315dc8ed773f13c0531", "url": "https://github.com/apache/hudi/commit/3be9ec7ec9081ae61eab6315dc8ed773f13c0531", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "committedDate": "2020-10-05T00:02:38Z", "type": "forcePushed"}, {"oid": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "url": "https://github.com/apache/hudi/commit/1d9dac8ea23598dc778a4d86058b919580da0d3d", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable\n\nRemove APIs in `HoodieTestUtils`\n- listAllDataFilesAndLogFilesInPath\n- listAllLogFilesInPath\n- listAllDataFilesInPath\n- writeRecordsToLogFiles\n- createCleanFiles\n- createPendingCleanFiles\n\nMigrate the callers to use `HoodieTestTable` and `HoodieWriteableTestTable` with new APIs added\n- listAllBaseAndLogFiles\n- listAllLogFiles\n- listAllBaseFiles\n- withLogAppends\n- addClean\n- addInflightClean\n\nAlso added related APIs in `FileCreateUtils`\n- createCleanFile\n- createRequestedCleanFile\n- createInflightCleanFile", "committedDate": "2020-10-05T02:15:41Z", "type": "commit"}, {"oid": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "url": "https://github.com/apache/hudi/commit/1d9dac8ea23598dc778a4d86058b919580da0d3d", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable\n\nRemove APIs in `HoodieTestUtils`\n- listAllDataFilesAndLogFilesInPath\n- listAllLogFilesInPath\n- listAllDataFilesInPath\n- writeRecordsToLogFiles\n- createCleanFiles\n- createPendingCleanFiles\n\nMigrate the callers to use `HoodieTestTable` and `HoodieWriteableTestTable` with new APIs added\n- listAllBaseAndLogFiles\n- listAllLogFiles\n- listAllBaseFiles\n- withLogAppends\n- addClean\n- addInflightClean\n\nAlso added related APIs in `FileCreateUtils`\n- createCleanFile\n- createRequestedCleanFile\n- createInflightCleanFile", "committedDate": "2020-10-05T02:15:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMyMDE4OQ==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r499320189", "bodyText": "refer to hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java#writeRecordsToLogFiles", "author": "xushiyan", "createdAt": "2020-10-05T02:24:55Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -128,4 +148,37 @@ public HoodieWriteableTestTable withInserts(String partition, String fileId, Hoo\n \n     return this;\n   }\n+\n+  public HoodieWriteableTestTable withLogAppends(HoodieRecord... records) throws Exception {\n+    return withLogAppends(Arrays.asList(records));\n+  }\n+\n+  public HoodieWriteableTestTable withLogAppends(List<HoodieRecord> records) throws Exception {\n+    for (List<HoodieRecord> groupedRecords: records.stream()\n+        .collect(Collectors.groupingBy(HoodieRecord::getCurrentLocation)).values()) {\n+      appendRecordsToLogFile(groupedRecords);\n+    }\n+    return this;\n+  }\n+\n+  private void appendRecordsToLogFile(List<HoodieRecord> groupedRecords) throws Exception {\n+    String partitionPath = groupedRecords.get(0).getPartitionPath();\n+    HoodieRecordLocation location = groupedRecords.get(0).getCurrentLocation();\n+    try (HoodieLogFormat.Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(new Path(basePath, partitionPath))\n+        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(location.getFileId())\n+        .overBaseCommit(location.getInstantTime()).withFs(fs).build()) {\n+      Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n+      header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, location.getInstantTime());\n+      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n+      logWriter.appendBlock(new HoodieAvroDataBlock(groupedRecords.stream().map(r -> {\n+        try {\n+          GenericRecord val = (GenericRecord) r.getData().getInsertValue(schema).get();\n+          HoodieAvroUtils.addHoodieKeyToRecord(val, r.getRecordKey(), r.getPartitionPath(), \"\");\n+          return (IndexedRecord) val;\n+        } catch (IOException e) {\n+          return null;\n+        }\n+      }).collect(Collectors.toList()), header));", "originalCommit": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cabcf317c2bdf1874f3b4e2fc8478b8e63e730a6", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\nindex ac9c6858da..e167a0f4b6 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n\n@@ -176,6 +179,7 @@ public class HoodieWriteableTestTable extends HoodieTestTable {\n           HoodieAvroUtils.addHoodieKeyToRecord(val, r.getRecordKey(), r.getPartitionPath(), \"\");\n           return (IndexedRecord) val;\n         } catch (IOException e) {\n+          LOG.warn(\"Failed to convert record \" + r.toString(), e);\n           return null;\n         }\n       }).collect(Collectors.toList()), header));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMyMDY2Ng==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r499320666", "bodyText": "change to return array to work with HoodieTableFileSystemView APIs which take in array", "author": "xushiyan", "createdAt": "2020-10-05T02:27:28Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestTable.java", "diffHunk": "@@ -348,12 +372,36 @@ public String getBaseFileNameById(String fileId) {\n     return baseFileName(currentInstantTime, fileId);\n   }\n \n-  public List<FileStatus> listAllFiles(String partitionPath) throws IOException {\n-    return FileSystemTestUtils.listRecursive(fs, new Path(Paths.get(basePath, partitionPath).toString()));\n+  public FileStatus[] listAllBaseFiles() throws IOException {\n+    return listAllBaseFiles(HoodieFileFormat.PARQUET.getFileExtension());\n+  }\n+\n+  public FileStatus[] listAllBaseFiles(String fileExtension) throws IOException {\n+    return FileSystemTestUtils.listRecursive(fs, new Path(basePath)).stream()\n+        .filter(status -> status.getPath().getName().endsWith(fileExtension))\n+        .toArray(FileStatus[]::new);\n+  }\n+\n+  public FileStatus[] listAllLogFiles() throws IOException {\n+    return listAllLogFiles(HoodieFileFormat.HOODIE_LOG.getFileExtension());\n+  }\n+\n+  public FileStatus[] listAllLogFiles(String fileExtension) throws IOException {\n+    return FileSystemTestUtils.listRecursive(fs, new Path(basePath)).stream()\n+        .filter(status -> status.getPath().getName().contains(fileExtension))\n+        .toArray(FileStatus[]::new);\n+  }\n+\n+  public FileStatus[] listAllBaseAndLogFiles() throws IOException {\n+    return Stream.concat(Stream.of(listAllBaseFiles()), Stream.of(listAllLogFiles())).toArray(FileStatus[]::new);\n+  }\n+\n+  public FileStatus[] listAllFilesInPartition(String partitionPath) throws IOException {\n+    return FileSystemTestUtils.listRecursive(fs, new Path(Paths.get(basePath, partitionPath).toString())).toArray(new FileStatus[0]);\n   }\n \n-  public List<FileStatus> listAllFilesInTempFolder() throws IOException {\n-    return FileSystemTestUtils.listRecursive(fs, new Path(Paths.get(basePath, HoodieTableMetaClient.TEMPFOLDER_NAME).toString()));\n+  public FileStatus[] listAllFilesInTempFolder() throws IOException {\n+    return FileSystemTestUtils.listRecursive(fs, new Path(Paths.get(basePath, HoodieTableMetaClient.TEMPFOLDER_NAME).toString())).toArray(new FileStatus[0]);", "originalCommit": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEwMzE2OQ==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r502103169", "bodyText": "Considering the fluent APIs have one general pattern: method withXXX returns HoodieWriteableTestTable  itself. However, this method and above break this rule. WDYT about renaming them to getFileIdWithInserts or returnFileIdWithInserts?", "author": "yanghua", "createdAt": "2020-10-09T00:50:25Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -94,6 +106,10 @@ public String withInserts(String partition) throws Exception {\n   }\n \n   public String withInserts(String partition, HoodieRecord... records) throws Exception {\n+    return withInserts(partition, Arrays.asList(records));\n+  }\n+\n+  public String withInserts(String partition, List<HoodieRecord> records) throws Exception {", "originalCommit": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEyMzQ5Mw==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r502123493", "bodyText": "sounds good.", "author": "xushiyan", "createdAt": "2020-10-09T01:28:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEwMzE2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "cabcf317c2bdf1874f3b4e2fc8478b8e63e730a6", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\nindex ac9c6858da..e167a0f4b6 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n\n@@ -101,15 +104,15 @@ public class HoodieWriteableTestTable extends HoodieTestTable {\n     return (HoodieWriteableTestTable) super.forCommit(instantTime);\n   }\n \n-  public String withInserts(String partition) throws Exception {\n-    return withInserts(partition, new HoodieRecord[0]);\n+  public String getFileIdWithInserts(String partition) throws Exception {\n+    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n   }\n \n-  public String withInserts(String partition, HoodieRecord... records) throws Exception {\n-    return withInserts(partition, Arrays.asList(records));\n+  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n+    return getFileIdWithInserts(partition, Arrays.asList(records));\n   }\n \n-  public String withInserts(String partition, List<HoodieRecord> records) throws Exception {\n+  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n     String fileId = UUID.randomUUID().toString();\n     withInserts(partition, fileId, records);\n     return fileId;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEwMzc1NQ==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r502103755", "bodyText": "Logging the detailed exception information looks better?", "author": "yanghua", "createdAt": "2020-10-09T00:51:30Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -128,4 +148,37 @@ public HoodieWriteableTestTable withInserts(String partition, String fileId, Hoo\n \n     return this;\n   }\n+\n+  public HoodieWriteableTestTable withLogAppends(HoodieRecord... records) throws Exception {\n+    return withLogAppends(Arrays.asList(records));\n+  }\n+\n+  public HoodieWriteableTestTable withLogAppends(List<HoodieRecord> records) throws Exception {\n+    for (List<HoodieRecord> groupedRecords: records.stream()\n+        .collect(Collectors.groupingBy(HoodieRecord::getCurrentLocation)).values()) {\n+      appendRecordsToLogFile(groupedRecords);\n+    }\n+    return this;\n+  }\n+\n+  private void appendRecordsToLogFile(List<HoodieRecord> groupedRecords) throws Exception {\n+    String partitionPath = groupedRecords.get(0).getPartitionPath();\n+    HoodieRecordLocation location = groupedRecords.get(0).getCurrentLocation();\n+    try (HoodieLogFormat.Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(new Path(basePath, partitionPath))\n+        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(location.getFileId())\n+        .overBaseCommit(location.getInstantTime()).withFs(fs).build()) {\n+      Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n+      header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, location.getInstantTime());\n+      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n+      logWriter.appendBlock(new HoodieAvroDataBlock(groupedRecords.stream().map(r -> {\n+        try {\n+          GenericRecord val = (GenericRecord) r.getData().getInsertValue(schema).get();\n+          HoodieAvroUtils.addHoodieKeyToRecord(val, r.getRecordKey(), r.getPartitionPath(), \"\");\n+          return (IndexedRecord) val;\n+        } catch (IOException e) {", "originalCommit": "1d9dac8ea23598dc778a4d86058b919580da0d3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEyNDg3NA==", "url": "https://github.com/apache/hudi/pull/2143#discussion_r502124874", "bodyText": "@yanghua ok fixed.", "author": "xushiyan", "createdAt": "2020-10-09T01:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEwMzc1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "cabcf317c2bdf1874f3b4e2fc8478b8e63e730a6", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\nindex ac9c6858da..e167a0f4b6 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java\n\n@@ -176,6 +179,7 @@ public class HoodieWriteableTestTable extends HoodieTestTable {\n           HoodieAvroUtils.addHoodieKeyToRecord(val, r.getRecordKey(), r.getPartitionPath(), \"\");\n           return (IndexedRecord) val;\n         } catch (IOException e) {\n+          LOG.warn(\"Failed to convert record \" + r.toString(), e);\n           return null;\n         }\n       }).collect(Collectors.toList()), header));\n"}}, {"oid": "cabcf317c2bdf1874f3b4e2fc8478b8e63e730a6", "url": "https://github.com/apache/hudi/commit/cabcf317c2bdf1874f3b4e2fc8478b8e63e730a6", "message": "add log warn and fix API namings", "committedDate": "2020-10-09T01:34:22Z", "type": "commit"}]}