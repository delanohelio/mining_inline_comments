{"pr_number": 2283, "pr_title": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "pr_createdAt": "2020-11-26T07:51:11Z", "pr_url": "https://github.com/apache/hudi/pull/2283", "timeline": [{"oid": "2038dda8bfdd03c0e6e57f575a51773fe570c76b", "url": "https://github.com/apache/hudi/commit/2038dda8bfdd03c0e6e57f575a51773fe570c76b", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql", "committedDate": "2020-11-30T15:26:42Z", "type": "forcePushed"}, {"oid": "63e88099492dae817e428cc973d0d8c452dcb039", "url": "https://github.com/apache/hudi/commit/63e88099492dae817e428cc973d0d8c452dcb039", "message": "[HUDI-1415] refactor same code", "committedDate": "2020-12-06T12:23:28Z", "type": "forcePushed"}, {"oid": "0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "url": "https://github.com/apache/hudi/commit/0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code", "committedDate": "2020-12-06T12:26:45Z", "type": "forcePushed"}, {"oid": "98e229343ed140f252d5058813855ce689030396", "url": "https://github.com/apache/hudi/commit/98e229343ed140f252d5058813855ce689030396", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case", "committedDate": "2020-12-06T14:36:44Z", "type": "forcePushed"}, {"oid": "a5e896a04d6fb4227874067ffd47073d0e23ab71", "url": "https://github.com/apache/hudi/commit/a5e896a04d6fb4227874067ffd47073d0e23ab71", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2020-12-07T07:53:30Z", "type": "forcePushed"}, {"oid": "c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "url": "https://github.com/apache/hudi/commit/c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "message": "add log for test case", "committedDate": "2020-12-10T03:25:33Z", "type": "forcePushed"}, {"oid": "bc59a67f3d01dd17beb24592957ede089a5fa9a8", "url": "https://github.com/apache/hudi/commit/bc59a67f3d01dd17beb24592957ede089a5fa9a8", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2020-12-10T03:25:32Z", "type": "forcePushed"}, {"oid": "63cbf0148a033fe511d49b381691d126f78f8828", "url": "https://github.com/apache/hudi/commit/63cbf0148a033fe511d49b381691d126f78f8828", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2021-01-07T15:53:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzU3MA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247570", "bodyText": "since this is abstract class and not every implementation will have some concrete override, can we make this empty here so that HoodieDLAClient does not need to do a no op override.", "author": "nsivabalan", "createdAt": "2021-01-30T13:42:11Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -75,6 +76,8 @@ public abstract void createTable(String tableName, MessageType storageSchema,\n \n   public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n \n+  public abstract void updateTableProperties(String tableName, Map<String, String> tableProperties);", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "618ac88270a3e1745fade239ff5d742cec7bfe66", "chunk": "diff --git a/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java b/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\nindex 7030ec490..d3fe3fe68 100644\n--- a/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\n+++ b/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\n\n@@ -76,7 +85,7 @@ public abstract class AbstractSyncHoodieClient {\n \n   public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n \n-  public abstract void updateTableProperties(String tableName, Map<String, String> tableProperties);\n+  public  void updateTableProperties(String tableName, Map<String, String> tableProperties) {}\n \n   public abstract Map<String, String> getTableSchema(String tableName);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzY3NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247674", "bodyText": "I understand it's not part of this diff. But wondering if you can add some java docs to this class in general. I realized we don't have any one. (at line 41 ish) .", "author": "nsivabalan", "createdAt": "2021-01-30T13:43:29Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -63,7 +63,8 @@ public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning,\n   }\n \n   public abstract void createTable(String tableName, MessageType storageSchema,\n-                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+                                   String inputFormatClass, String outputFormatClass,", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "618ac88270a3e1745fade239ff5d742cec7bfe66", "chunk": "diff --git a/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java b/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\nindex 7030ec490..d3fe3fe68 100644\n--- a/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\n+++ b/hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java\n\n@@ -62,6 +62,15 @@ public abstract class AbstractSyncHoodieClient {\n     this.fs = fs;\n   }\n \n+  /**\n+   * Create the table.\n+   * @param tableName The table name.\n+   * @param storageSchema The table schema.\n+   * @param inputFormatClass The input format class of this table.\n+   * @param outputFormatClass The output format class of this table.\n+   * @param serdeClass The serde class of this table.\n+   * @param serdeProperties The serde properites of this table.\n+   */\n   public abstract void createTable(String tableName, MessageType storageSchema,\n                                    String inputFormatClass, String outputFormatClass,\n                                    String serdeClass, Map<String, String> serdeProperties);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248354", "bodyText": "minor. \"Failed to update...\". remove extra \"get\"", "author": "nsivabalan", "createdAt": "2021-01-30T13:49:41Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {\n+      return;\n+    }\n+    try {\n+      Table table = client.getTable(syncConfig.databaseName, tableName);\n+      for (Map.Entry<String, String> entry: tableProperties.entrySet()) {\n+        table.putToParameters(entry.getKey(), entry.getValue());\n+      }\n+      client.alter_table(syncConfig.databaseName, tableName, table);\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM0NzM2Ng==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570347366", "bodyText": "Thanks for your correct.", "author": "pengzhiwei2018", "createdAt": "2021-02-04T16:07:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA=="}], "type": "inlineReview", "revised_code": {"commit": "618ac88270a3e1745fade239ff5d742cec7bfe66", "chunk": "diff --git a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\nindex 70aa4aebe..a67dc1e75 100644\n--- a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n+++ b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n\n@@ -154,7 +154,7 @@ public class HoodieHiveClient extends AbstractSyncHoodieClient {\n       }\n       client.alter_table(syncConfig.databaseName, tableName, table);\n     } catch (Exception e) {\n-      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"\n+      throw new HoodieHiveSyncException(\"Failed to update table properties for table: \"\n           + tableName, e);\n     }\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248528", "bodyText": "Would be nice if you write a test for the actual problem you faced as per the title/desc. And ensure that the test fails w/o this patch and succeeds with this patch.", "author": "nsivabalan", "createdAt": "2021-01-30T13:51:09Z", "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -249,6 +255,54 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)\n         \"The last commit that was sycned should be 100\");\n   }\n \n+  @ParameterizedTest\n+  @MethodSource({\"useJdbcAndSchemaFromCommitMetadata\"})\n+  public void testSyncWithProperties(boolean useJdbc, boolean useSchemaFromCommitMetadata) throws Exception {", "originalCommit": "63cbf0148a033fe511d49b381691d126f78f8828", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI1MjQ0Ng==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567252446", "bodyText": "Thanks @nsivabalan  for these nice suggestions, I will spend some time to process.", "author": "pengzhiwei2018", "createdAt": "2021-01-30T14:28:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA=="}], "type": "inlineReview", "revised_code": {"commit": "8bc3097f3683a62429b5e2d2b54833c87773fc5e", "chunk": "diff --git a/hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java b/hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java\nindex 41ddc2f47..300e9378a 100644\n--- a/hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java\n+++ b/hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java\n\n@@ -252,7 +253,7 @@ public class TestHiveSyncTool {\n     List<Partition> tablePartitions = hiveClient.scanTablePartitions(HiveTestUtil.hiveSyncConfig.tableName);\n     assertEquals(6, tablePartitions.size(), \"The one partition we wrote should be added to hive\");\n     assertEquals(instantTime, hiveClient.getLastCommitTimeSynced(HiveTestUtil.hiveSyncConfig.tableName).get(),\n-        \"The last commit that was sycned should be 100\");\n+        \"The last commit that was synced should be 100\");\n   }\n \n   @ParameterizedTest\n"}}, {"oid": "4b568770a4559f3b3c46694e45a13cf2673277d9", "url": "https://github.com/apache/hudi/commit/4b568770a4559f3b3c46694e45a13cf2673277d9", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception", "committedDate": "2021-01-30T14:37:53Z", "type": "forcePushed"}, {"oid": "618ac88270a3e1745fade239ff5d742cec7bfe66", "url": "https://github.com/apache/hudi/commit/618ac88270a3e1745fade239ff5d742cec7bfe66", "message": "fix some code review", "committedDate": "2021-02-19T02:46:32Z", "type": "forcePushed"}, {"oid": "c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "url": "https://github.com/apache/hudi/commit/c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review", "committedDate": "2021-02-19T03:20:42Z", "type": "forcePushed"}, {"oid": "71363392e720bcc8d32c3c90c601b020f3d6366c", "url": "https://github.com/apache/hudi/commit/71363392e720bcc8d32c3c90c601b020f3d6366c", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review", "committedDate": "2021-02-19T03:22:40Z", "type": "forcePushed"}, {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "url": "https://github.com/apache/hudi/commit/872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-02-20T11:40:45Z", "type": "forcePushed"}, {"oid": "ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "url": "https://github.com/apache/hudi/commit/ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-07T06:08:02Z", "type": "forcePushed"}, {"oid": "6343d09682dbcfb6e9716312889d876178d6349b", "url": "https://github.com/apache/hudi/commit/6343d09682dbcfb6e9716312889d876178d6349b", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-12T12:19:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTI4NA==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435284", "bodyText": "Can you improve the javadoc ? It has missing properties and descriptions.", "author": "umehrot2", "createdAt": "2021-04-15T22:42:49Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "chunk": "diff --git a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\nindex a67dc1e75..db2463ddb 100644\n--- a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n+++ b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n\n@@ -140,11 +140,10 @@ public class HoodieHiveClient extends AbstractSyncHoodieClient {\n \n   /**\n    * Update the table properties to the table.\n-   * @param tableProperties\n    */\n   @Override\n   public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n-    if (tableProperties == null || tableProperties.size() == 0) {\n+    if (tableProperties == null || tableProperties.isEmpty()) {\n       return;\n     }\n     try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTg0Mw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435843", "bodyText": "nit: tableProperties.isEmpty() ?", "author": "umehrot2", "createdAt": "2021-04-15T22:44:20Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "chunk": "diff --git a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\nindex a67dc1e75..db2463ddb 100644\n--- a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n+++ b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java\n\n@@ -140,11 +140,10 @@ public class HoodieHiveClient extends AbstractSyncHoodieClient {\n \n   /**\n    * Update the table properties to the table.\n-   * @param tableProperties\n    */\n   @Override\n   public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n-    if (tableProperties == null || tableProperties.size() == 0) {\n+    if (tableProperties == null || tableProperties.isEmpty()) {\n       return;\n     }\n     try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614457527", "bodyText": "Can't we sync this while creating the table itself, like you are doing for serde properties ?", "author": "umehrot2", "createdAt": "2021-04-15T23:20:55Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNTg1Mg==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615525852", "bodyText": "Well, the tableProperties may change if the schema has changed. So we need to update the table properties  by a separate interface.", "author": "pengzhiwei2018", "createdAt": "2021-04-19T03:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI4ODQ1Nw==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616288457", "bodyText": "@pengzhiwei2018 I understand that. In the syncSchema() there is a check for schema difference. Perhaps we should move this inside that to avoid this being run every time. And for the first time when table is created, we can do it as part of create table. Thoughts ?", "author": "umehrot2", "createdAt": "2021-04-20T01:55:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI5NzczOQ==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616297739", "bodyText": "Yeah, +1 for this.", "author": "pengzhiwei2018", "createdAt": "2021-04-20T02:25:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}], "type": "inlineReview", "revised_code": {"commit": "8bc3097f3683a62429b5e2d2b54833c87773fc5e", "chunk": "diff --git a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java\nindex 2de434b20..80894b37a 100644\n--- a/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java\n+++ b/hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java\n\n@@ -165,13 +165,6 @@ public class HiveSyncTool extends AbstractSyncTool {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-    // Sync the table properties if need\n-    if (cfg.tableProperties != null) {\n-      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n-      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n-      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n-          + cfg.tableProperties);\n-    }\n     hoodieHiveClient.updateLastCommitTimeSynced(tableName);\n     LOG.info(\"Sync complete for \" + tableName);\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614461315", "bodyText": "Can you update the toString() in this class ?", "author": "umehrot2", "createdAt": "2021-04-15T23:31:26Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "diffHunk": "@@ -88,6 +88,12 @@\n   @Parameter(names = {\"--verify-metadata-file-listing\"}, description = \"Verify file listing from Hudi's metadata against file system\")\n   public Boolean verifyMetadataFileListing = HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE;\n \n+  @Parameter(names = {\"--table-properties\"}, description = \"Table properties to hive table\")\n+  public String tableProperties;\n+\n+  @Parameter(names = {\"--serde-properties\"}, description = \"Serde properties to hive table\")\n+  public String serdeProperties;\n+", "originalCommit": "6343d09682dbcfb6e9716312889d876178d6349b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNjAxMg==", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615526012", "bodyText": "Yes, thanks for remind me.", "author": "pengzhiwei2018", "createdAt": "2021-04-19T03:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "url": "https://github.com/apache/hudi/commit/33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-19T02:37:47Z", "type": "forcePushed"}, {"oid": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "url": "https://github.com/apache/hudi/commit/1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished", "committedDate": "2021-04-19T07:57:07Z", "type": "forcePushed"}, {"oid": "348cf1f629913dc193c17b3377b85351c2596ed9", "url": "https://github.com/apache/hudi/commit/348cf1f629913dc193c17b3377b85351c2596ed9", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-19T07:58:00Z", "type": "forcePushed"}, {"oid": "8bc3097f3683a62429b5e2d2b54833c87773fc5e", "url": "https://github.com/apache/hudi/commit/8bc3097f3683a62429b5e2d2b54833c87773fc5e", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:39:53Z", "type": "forcePushed"}, {"oid": "173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "url": "https://github.com/apache/hudi/commit/173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:45:16Z", "type": "forcePushed"}, {"oid": "657bf34ca752c67ad4ae902d4458471e6e72601d", "url": "https://github.com/apache/hudi/commit/657bf34ca752c67ad4ae902d4458471e6e72601d", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T02:58:12Z", "type": "forcePushed"}, {"oid": "5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "url": "https://github.com/apache/hudi/commit/5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T03:30:46Z", "type": "forcePushed"}, {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T05:04:54Z", "type": "commit"}, {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "committedDate": "2021-04-20T05:04:54Z", "type": "forcePushed"}]}