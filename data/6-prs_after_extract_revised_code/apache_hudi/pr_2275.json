{"pr_number": 2275, "pr_title": "[HUDI-1354] Block updates and replace on file groups in clustering", "pr_createdAt": "2020-11-23T16:52:14Z", "pr_url": "https://github.com/apache/hudi/pull/2275", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA2MjU2Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533062562", "bodyText": "please avoid sleep in tests. It tends to cause flakiness. If we can write the test such that it waits til a certain condition is met (or times out), that would be much more preferable.", "author": "vinothchandar", "createdAt": "2020-12-01T04:17:20Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java", "diffHunk": "@@ -456,4 +479,95 @@ public void testBulkInsertRecords(String bulkInsertMode) throws Exception {\n   public void testBulkInsertRecordsWithGlobalSort(String bulkInsertMode) throws Exception {\n     testBulkInsertRecords(bulkInsertMode);\n   }\n+\n+  protected HoodieInstant createRequestedReplaceInstant(HoodieTableMetaClient metaClient, String clusterTime, List<FileSlice>[] fileSlices) throws IOException {\n+    HoodieClusteringPlan clusteringPlan =\n+        ClusteringUtils.createClusteringPlan(CLUSTERING_STRATEGY_CLASS, STRATEGY_PARAMS, fileSlices, Collections.emptyMap());\n+\n+    HoodieInstant clusteringInstant = new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, clusterTime);\n+    HoodieRequestedReplaceMetadata requestedReplaceMetadata = HoodieRequestedReplaceMetadata.newBuilder()\n+        .setClusteringPlan(clusteringPlan).setOperationType(WriteOperationType.CLUSTER.name()).build();\n+    metaClient.getActiveTimeline().saveToPendingReplaceCommit(clusteringInstant, TimelineMetadataUtils.serializeRequestedReplaceMetadata(requestedReplaceMetadata));\n+    return clusteringInstant;\n+  }\n+\n+  @Test\n+  public void testUpdateRejectForClustering() throws Exception {\n+    Properties properties = new Properties();\n+    // set max bytes small can easy generate multi file group\n+    properties.setProperty(HoodieStorageConfig.PARQUET_FILE_MAX_BYTES, \"1024\");\n+    HoodieWriteConfig config = makeHoodieClientConfig(properties);\n+    String firstCommitTime = makeNewCommitTime();\n+    SparkRDDWriteClient writeClient = getHoodieWriteClient(config);\n+    writeClient.startCommitWithTime(firstCommitTime);\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    String partitionPath = \"2016/01/31\";\n+\n+    // 1. insert three record with two filegroup\n+    HoodieSparkCopyOnWriteTable table = (HoodieSparkCopyOnWriteTable) HoodieSparkTable.create(config, context, metaClient);\n+    // Get some records belong to the same partition (2016/01/31)\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+\n+    List<HoodieRecord> records = new ArrayList<>();\n+    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n+    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n+    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n+\n+    writeClient.insert(jsc.parallelize(records, 1), firstCommitTime);\n+    List<String> firstInsertFileGroupIds = table.getFileSystemView().getAllFileGroups(partitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    List<List<FileSlice>> firstInsertFileSlicesList = table.getFileSystemView().getAllFileGroups(partitionPath)\n+        .map(fileGroup -> fileGroup.getAllFileSlices().collect(Collectors.toList())).collect(Collectors.toList());\n+    assertEquals(2, firstInsertFileGroupIds.size());\n+    List<FileSlice>[] fileSlices = (List<FileSlice>[])firstInsertFileSlicesList.toArray(new List[firstInsertFileSlicesList.size()]);\n+\n+    // 2. generate clustering plan the filegroups\n+    String clusterTime1 = \"1\";\n+    createRequestedReplaceInstant(this.metaClient, clusterTime1, fileSlices);\n+\n+    // 3. insert one record with no updating reject exception\n+    String insertRecordStr4 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc40pp0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n+    RawTripTestPayload rowChange4 = new RawTripTestPayload(insertRecordStr4);\n+    HoodieRecord insertedRecord1 =\n+        new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);\n+    List<HoodieRecord> insertRecords = Arrays.asList(insertedRecord1);\n+    Thread.sleep(1000);\n+    String secondCommitTime = makeNewCommitTime();\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    writeClient.startCommitWithTime(secondCommitTime);\n+    List<WriteStatus> statuses = writeClient.upsert(jsc.parallelize(insertRecords), secondCommitTime).collect();\n+    assertEquals(1, statuses.size());\n+\n+    // 4. insert one record and update one record\n+    String insertRecordStr5 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n+    // We update the 1st record & add a new record\n+    String updateRecordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    RawTripTestPayload updateRowChanges1 = new RawTripTestPayload(updateRecordStr1);\n+    HoodieRecord updatedRecord1 = new HoodieRecord(\n+        new HoodieKey(updateRowChanges1.getRowKey(), updateRowChanges1.getPartitionPath()), updateRowChanges1);\n+    RawTripTestPayload rowChange5 = new RawTripTestPayload(insertRecordStr5);\n+    HoodieRecord insertedRecord2 =\n+        new HoodieRecord(new HoodieKey(rowChange5.getRowKey(), rowChange5.getPartitionPath()), rowChange5);\n+    List<HoodieRecord> updatedRecords = Arrays.asList(updatedRecord1, insertedRecord2);\n+    Thread.sleep(1000);", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM3ODMwMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r539378301", "bodyText": "okay", "author": "lw309637554", "createdAt": "2020-12-09T15:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA2MjU2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7991ef3e11afad79983ca4feecd9412e51368faf", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java\nindex 21ed771d02..c054bc4602 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java\n\n@@ -479,95 +456,4 @@ public class TestCopyOnWriteActionExecutor extends HoodieClientTestBase {\n   public void testBulkInsertRecordsWithGlobalSort(String bulkInsertMode) throws Exception {\n     testBulkInsertRecords(bulkInsertMode);\n   }\n-\n-  protected HoodieInstant createRequestedReplaceInstant(HoodieTableMetaClient metaClient, String clusterTime, List<FileSlice>[] fileSlices) throws IOException {\n-    HoodieClusteringPlan clusteringPlan =\n-        ClusteringUtils.createClusteringPlan(CLUSTERING_STRATEGY_CLASS, STRATEGY_PARAMS, fileSlices, Collections.emptyMap());\n-\n-    HoodieInstant clusteringInstant = new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, clusterTime);\n-    HoodieRequestedReplaceMetadata requestedReplaceMetadata = HoodieRequestedReplaceMetadata.newBuilder()\n-        .setClusteringPlan(clusteringPlan).setOperationType(WriteOperationType.CLUSTER.name()).build();\n-    metaClient.getActiveTimeline().saveToPendingReplaceCommit(clusteringInstant, TimelineMetadataUtils.serializeRequestedReplaceMetadata(requestedReplaceMetadata));\n-    return clusteringInstant;\n-  }\n-\n-  @Test\n-  public void testUpdateRejectForClustering() throws Exception {\n-    Properties properties = new Properties();\n-    // set max bytes small can easy generate multi file group\n-    properties.setProperty(HoodieStorageConfig.PARQUET_FILE_MAX_BYTES, \"1024\");\n-    HoodieWriteConfig config = makeHoodieClientConfig(properties);\n-    String firstCommitTime = makeNewCommitTime();\n-    SparkRDDWriteClient writeClient = getHoodieWriteClient(config);\n-    writeClient.startCommitWithTime(firstCommitTime);\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n-    String partitionPath = \"2016/01/31\";\n-\n-    // 1. insert three record with two filegroup\n-    HoodieSparkCopyOnWriteTable table = (HoodieSparkCopyOnWriteTable) HoodieSparkTable.create(config, context, metaClient);\n-    // Get some records belong to the same partition (2016/01/31)\n-    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n-\n-    List<HoodieRecord> records = new ArrayList<>();\n-    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-    records.add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-    records.add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-    records.add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n-    writeClient.insert(jsc.parallelize(records, 1), firstCommitTime);\n-    List<String> firstInsertFileGroupIds = table.getFileSystemView().getAllFileGroups(partitionPath)\n-        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n-    List<List<FileSlice>> firstInsertFileSlicesList = table.getFileSystemView().getAllFileGroups(partitionPath)\n-        .map(fileGroup -> fileGroup.getAllFileSlices().collect(Collectors.toList())).collect(Collectors.toList());\n-    assertEquals(2, firstInsertFileGroupIds.size());\n-    List<FileSlice>[] fileSlices = (List<FileSlice>[])firstInsertFileSlicesList.toArray(new List[firstInsertFileSlicesList.size()]);\n-\n-    // 2. generate clustering plan the filegroups\n-    String clusterTime1 = \"1\";\n-    createRequestedReplaceInstant(this.metaClient, clusterTime1, fileSlices);\n-\n-    // 3. insert one record with no updating reject exception\n-    String insertRecordStr4 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc40pp0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n-    RawTripTestPayload rowChange4 = new RawTripTestPayload(insertRecordStr4);\n-    HoodieRecord insertedRecord1 =\n-        new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);\n-    List<HoodieRecord> insertRecords = Arrays.asList(insertedRecord1);\n-    Thread.sleep(1000);\n-    String secondCommitTime = makeNewCommitTime();\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n-    writeClient.startCommitWithTime(secondCommitTime);\n-    List<WriteStatus> statuses = writeClient.upsert(jsc.parallelize(insertRecords), secondCommitTime).collect();\n-    assertEquals(1, statuses.size());\n-\n-    // 4. insert one record and update one record\n-    String insertRecordStr5 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n-    // We update the 1st record & add a new record\n-    String updateRecordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n-    RawTripTestPayload updateRowChanges1 = new RawTripTestPayload(updateRecordStr1);\n-    HoodieRecord updatedRecord1 = new HoodieRecord(\n-        new HoodieKey(updateRowChanges1.getRowKey(), updateRowChanges1.getPartitionPath()), updateRowChanges1);\n-    RawTripTestPayload rowChange5 = new RawTripTestPayload(insertRecordStr5);\n-    HoodieRecord insertedRecord2 =\n-        new HoodieRecord(new HoodieKey(rowChange5.getRowKey(), rowChange5.getPartitionPath()), rowChange5);\n-    List<HoodieRecord> updatedRecords = Arrays.asList(updatedRecord1, insertedRecord2);\n-    Thread.sleep(1000);\n-    String thirdCommitTime = makeNewCommitTime();\n-    metaClient = HoodieTableMetaClient.reload(metaClient);\n-    writeClient.startCommitWithTime(thirdCommitTime);\n-    String assertMsg = String.format(\"Not allowed to update the clustering files partition: %s fileID: %s. \"\n-        + \"For pending clustering operations, we are not going to support update for now.\", partitionPath, firstInsertFileGroupIds.get(0));\n-    assertThrows(HoodieUpsertException.class, () -> {\n-      writeClient.upsert(jsc.parallelize(updatedRecords), thirdCommitTime).collect(); }, assertMsg);\n-  }\n-\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNDI5Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533104292", "bodyText": "I think we need to think more about the algorithm here.  For example,\n\nf1, f2, f3 are file groups in partition.\nAssume there is pending clustering on all file groups f1, f2, f3.\nf3 is a small file. So we buildProfile would assign inserts to f3.\napplying update strategy will throw error because f3 is included.\n\nInstead, we may want to change buildProfile to exclude file groups that are in pending clustering. So, new file f4 would be created in step#3 and ingestion can continue. This way inserts can continue without errors.", "author": "satishkotha", "createdAt": "2020-12-01T06:41:23Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -103,6 +104,9 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     if (isWorkloadProfileNeeded()) {\n       profile = new WorkloadProfile(buildProfile(inputRecordsRDD));\n       LOG.info(\"Workload profile :\" + profile);\n+      // apply clustering update strategy.", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTY3NTkwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r541675903", "bodyText": "this is a good case , for generate new file f4 , can exclude small files in clustering", "author": "lw309637554", "createdAt": "2020-12-12T17:16:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNDI5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "0474898d32c45a079421bd37955c52dc86f7e406", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\nindex a48db42a56..48bd189a74 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n\n@@ -104,12 +103,11 @@ public abstract class BaseSparkCommitActionExecutor<T extends HoodieRecordPayloa\n     if (isWorkloadProfileNeeded()) {\n       profile = new WorkloadProfile(buildProfile(inputRecordsRDD));\n       LOG.info(\"Workload profile :\" + profile);\n-      // apply clustering update strategy.\n-      config.getClusteringUpdatesStrategy()\n-          .apply(table.getFileSystemView().getFileGroupsInPendingClustering().collect(Collectors.toList()), profile);\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n+    // use profile to check clustering update\n+    this.clusteringUpdateCheck(profile);\n     // partition using the insert partitioner\n     final Partitioner partitioner = getPartitioner(profile);\n     JavaRDD<HoodieRecord<T>> partitionedRecords = partition(inputRecordsRDD, partitioner);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjEzNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106135", "bodyText": "what happens if workload profile is not needed? Is there a better place to do this validation? Can we do this after tagLocation is done? If any of the records have location to files in pending clustering, we can throw error.", "author": "satishkotha", "createdAt": "2020-12-01T06:45:39Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -103,6 +104,9 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     if (isWorkloadProfileNeeded()) {", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTY3NzIwMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r541677201", "bodyText": "it is make sense. But now BaseSparkCommitActionExecutor.getUpsertPartitioner depend on profile, we can be consistent with getUpsertPartitioner.", "author": "lw309637554", "createdAt": "2020-12-12T17:18:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjEzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "0474898d32c45a079421bd37955c52dc86f7e406", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\nindex a48db42a56..48bd189a74 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n\n@@ -104,12 +103,11 @@ public abstract class BaseSparkCommitActionExecutor<T extends HoodieRecordPayloa\n     if (isWorkloadProfileNeeded()) {\n       profile = new WorkloadProfile(buildProfile(inputRecordsRDD));\n       LOG.info(\"Workload profile :\" + profile);\n-      // apply clustering update strategy.\n-      config.getClusteringUpdatesStrategy()\n-          .apply(table.getFileSystemView().getFileGroupsInPendingClustering().collect(Collectors.toList()), profile);\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n+    // use profile to check clustering update\n+    this.clusteringUpdateCheck(profile);\n     // partition using the insert partitioner\n     final Partitioner partitioner = getPartitioner(profile);\n     JavaRDD<HoodieRecord<T>> partitionedRecords = partition(inputRecordsRDD, partitioner);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjM1NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106354", "bodyText": "Good idea by adding this interface to keep it generic. Please add javadoc.", "author": "satishkotha", "createdAt": "2020-12-01T06:46:20Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.WorkloadProfile;\n+\n+import java.util.List;\n+", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5MTYzMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r540291633", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-10T16:05:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjM1NA=="}], "type": "inlineReview", "revised_code": {"commit": "0474898d32c45a079421bd37955c52dc86f7e406", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java\nindex b5d2c83b23..a29c809c79 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java\n\n@@ -25,7 +25,7 @@ import org.apache.hudi.table.WorkloadProfile;\n \n import java.util.List;\n \n-public interface UpdateStrategy {\n+public interface UpdateStrategy  {\n \n   void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjgyOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106828", "bodyText": "Since you are doing contains, is it better to change its type to Set?", "author": "satishkotha", "createdAt": "2020-12-01T06:47:39Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+\n+  @Override\n+  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n+    List<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n+        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toList());\n+    if (partitionPathAndFileIds.size() == 0) {\n+      return;\n+    }\n+\n+    Set<Map.Entry<String, WorkloadStat>> partitionStatEntries = workloadProfile.getPartitionPathStatMap().entrySet();\n+    for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n+      for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n+              partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n+        String partitionPath = partitionStat.getKey();\n+        String fileId = updateLocEntry.getKey();\n+        if (partitionPathAndFileIds.contains(Pair.of(partitionPath, fileId))) {", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5MTQ4OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r540291488", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-10T16:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjgyOA=="}], "type": "inlineReview", "revised_code": {"commit": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\nindex 8d44fe9653..1504405ed3 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\n\n@@ -37,8 +37,8 @@ public class RejectUpdateStrategy implements UpdateStrategy {\n \n   @Override\n   public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n-    List<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n-        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toList());\n+    Set<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n+        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toSet());\n     if (partitionPathAndFileIds.size() == 0) {\n       return;\n     }\n"}}, {"oid": "0474898d32c45a079421bd37955c52dc86f7e406", "url": "https://github.com/apache/hudi/commit/0474898d32c45a079421bd37955c52dc86f7e406", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-13T17:08:58Z", "type": "forcePushed"}, {"oid": "7991ef3e11afad79983ca4feecd9412e51368faf", "url": "https://github.com/apache/hudi/commit/7991ef3e11afad79983ca4feecd9412e51368faf", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:17:24Z", "type": "forcePushed"}, {"oid": "777ada0feeac54133e0050eb32ffbc25f2c836ea", "url": "https://github.com/apache/hudi/commit/777ada0feeac54133e0050eb32ffbc25f2c836ea", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:20:44Z", "type": "forcePushed"}, {"oid": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "url": "https://github.com/apache/hudi/commit/0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:46:13Z", "type": "commit"}, {"oid": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "url": "https://github.com/apache/hudi/commit/0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:46:13Z", "type": "forcePushed"}, {"oid": "f6c48629439c491fffbade1e12b73761b29746b0", "url": "https://github.com/apache/hudi/commit/f6c48629439c491fffbade1e12b73761b29746b0", "message": "Merge branch 'master' into HUDI-1354-2", "committedDate": "2020-12-22T09:26:25Z", "type": "commit"}, {"oid": "f6c48629439c491fffbade1e12b73761b29746b0", "url": "https://github.com/apache/hudi/commit/f6c48629439c491fffbade1e12b73761b29746b0", "message": "Merge branch 'master' into HUDI-1354-2", "committedDate": "2020-12-22T09:26:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA0ODg0Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r546048843", "bodyText": "can you please add javadoc for class and properties in class. Looks like we will need to resolve conflicts with clustering PR", "author": "satishkotha", "createdAt": "2020-12-18T19:30:00Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.table.action.clustering.update.RejectUpdateStrategy;\n+import org.apache.hudi.table.action.clustering.update.UpdateStrategy;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {", "originalCommit": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4Nzk2Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547987963", "bodyText": "done  have merge master", "author": "lw309637554", "createdAt": "2020-12-23T14:35:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA0ODg0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "f6c48629439c491fffbade1e12b73761b29746b0", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\nindex eb110fe02c..2825c02202 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n\n@@ -27,8 +27,57 @@ import java.io.FileReader;\n import java.io.IOException;\n import java.util.Properties;\n \n+/**\n+ * Clustering specific configs.\n+ */\n public class HoodieClusteringConfig extends DefaultHoodieConfig {\n \n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkRecentDaysClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String CLUSTERING_EXECUTION_STRATEGY_CLASS = \"hoodie.clustering.execution.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_EXECUTION_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_MAX_COMMIT_PROP = \"hoodie.clustering.inline.max.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+  \n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.plan.strategy.\";\n+\n+  // Number of partitions to list to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = CLUSTERING_STRATEGY_PARAM_PREFIX + \"daybased.lookback.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Files smaller than the size specified here are candidates for clustering.\n+  public static final String CLUSTERING_PLAN_SMALL_FILE_LIMIT = CLUSTERING_STRATEGY_PARAM_PREFIX + \"small.file.limit\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_SMALL_FILE_LIMIT = String.valueOf(600 * 1024 * 1024L); // 600MB\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_BYTES_PER_GROUP * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_BYTES_PER_GROUP = CLUSTERING_STRATEGY_PARAM_PREFIX + \"max.bytes.per.group\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  // Maximum number of groups to create as part of ClusteringPlan. Increasing groups will increase parallelism.\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = CLUSTERING_STRATEGY_PARAM_PREFIX + \"max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"30\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_MAX_BYTES = CLUSTERING_STRATEGY_PARAM_PREFIX + \"target.file.max.bytes\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_MAX_BYTES = String.valueOf(1 * 1024 * 1024 * 1024L); // 1GB\n+  \n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String CLUSTERING_SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+  \n   public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n   public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = RejectUpdateStrategy.class.getName();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA1MTU0NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r546051544", "bodyText": "Why not collect Set of HoodieFileGroupId? I'd avoid Pair as much as possible because its not that easy to read.", "author": "satishkotha", "createdAt": "2020-12-18T19:35:52Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+\n+  @Override\n+  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n+    Set<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()", "originalCommit": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODA3NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988074", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:35:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA1MTU0NA=="}], "type": "inlineReview", "revised_code": {"commit": "d2fb59a0c6193b2db03408abfc04ced18902e009", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\ndeleted file mode 100644\nindex 1504405ed3..0000000000\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\n+++ /dev/null\n\n@@ -1,61 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.table.action.clustering.update;\n-\n-import org.apache.hudi.common.model.HoodieFileGroupId;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.util.collection.Pair;\n-import org.apache.hudi.exception.HoodieClusteringUpdateException;\n-import org.apache.hudi.table.WorkloadProfile;\n-import org.apache.hudi.table.WorkloadStat;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n-\n-public class RejectUpdateStrategy implements UpdateStrategy {\n-  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n-\n-  @Override\n-  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n-    Set<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n-        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toSet());\n-    if (partitionPathAndFileIds.size() == 0) {\n-      return;\n-    }\n-\n-    Set<Map.Entry<String, WorkloadStat>> partitionStatEntries = workloadProfile.getPartitionPathStatMap().entrySet();\n-    for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n-      for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n-              partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n-        String partitionPath = partitionStat.getKey();\n-        String fileId = updateLocEntry.getKey();\n-        if (partitionPathAndFileIds.contains(Pair.of(partitionPath, fileId))) {\n-          String msg = String.format(\"Not allowed to update the clustering files partition: %s fileID: %s. \"\n-              + \"For pending clustering operations, we are not going to support update for now.\", partitionPath, fileId);\n-          LOG.error(msg);\n-          throw new HoodieClusteringUpdateException(msg);\n-        }\n-      }\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MjAyNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547482024", "bodyText": "I may have mentioned this earlier, consider removing WorkloadProfile and send fileGroupsWithUpdates.\nI think sending taggedRecords as additional parameter will also be useful. In future, we may want to update tagged records location to a different fileId.", "author": "satishkotha", "createdAt": "2020-12-22T20:01:57Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.WorkloadProfile;\n+\n+import java.util.List;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public interface UpdateStrategy  {\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param workloadProfile workloadProfile have the records update info,\n+   *                       just like BaseSparkCommitActionExecutor.getUpsertPartitioner use it.\n+   */\n+  void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile);", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODQ0MQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988441", "bodyText": "make sense , now taggedRecords as parameter , the update file groupid get from taggedRecords", "author": "lw309637554", "createdAt": "2020-12-23T14:36:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MjAyNA=="}], "type": "inlineReview", "revised_code": {"commit": "d2fb59a0c6193b2db03408abfc04ced18902e009", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java\ndeleted file mode 100644\nindex 72a354eca6..0000000000\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java\n+++ /dev/null\n\n@@ -1,41 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.table.action.clustering.update;\n-\n-import org.apache.hudi.common.model.HoodieFileGroupId;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.util.collection.Pair;\n-import org.apache.hudi.table.WorkloadProfile;\n-\n-import java.util.List;\n-\n-/**\n- * When file groups in clustering, write records to these file group need to check.\n- */\n-public interface UpdateStrategy  {\n-\n-  /**\n-   * check the update records to the file group in clustering.\n-   * @param fileGroupsInPendingClustering\n-   * @param workloadProfile workloadProfile have the records update info,\n-   *                       just like BaseSparkCommitActionExecutor.getUpsertPartitioner use it.\n-   */\n-  void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile);\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NDgwNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547484804", "bodyText": "Should we add a constructor to take engineContext (similar to clustering strategy). For some of the future strategies, we need engineContext. So it'd be helpful if we start with that.", "author": "satishkotha", "createdAt": "2020-12-22T20:08:24Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODYwOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988608", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:36:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NDgwNA=="}], "type": "inlineReview", "revised_code": {"commit": "d2fb59a0c6193b2db03408abfc04ced18902e009", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\ndeleted file mode 100644\nindex 1504405ed3..0000000000\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java\n+++ /dev/null\n\n@@ -1,61 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.table.action.clustering.update;\n-\n-import org.apache.hudi.common.model.HoodieFileGroupId;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.util.collection.Pair;\n-import org.apache.hudi.exception.HoodieClusteringUpdateException;\n-import org.apache.hudi.table.WorkloadProfile;\n-import org.apache.hudi.table.WorkloadStat;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n-\n-public class RejectUpdateStrategy implements UpdateStrategy {\n-  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n-\n-  @Override\n-  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n-    Set<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n-        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toSet());\n-    if (partitionPathAndFileIds.size() == 0) {\n-      return;\n-    }\n-\n-    Set<Map.Entry<String, WorkloadStat>> partitionStatEntries = workloadProfile.getPartitionPathStatMap().entrySet();\n-    for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n-      for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n-              partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n-        String partitionPath = partitionStat.getKey();\n-        String fileId = updateLocEntry.getKey();\n-        if (partitionPathAndFileIds.contains(Pair.of(partitionPath, fileId))) {\n-          String msg = String.format(\"Not allowed to update the clustering files partition: %s fileID: %s. \"\n-              + \"For pending clustering operations, we are not going to support update for now.\", partitionPath, fileId);\n-          LOG.error(msg);\n-          throw new HoodieClusteringUpdateException(msg);\n-        }\n-      }\n-    }\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NTY5MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547485690", "bodyText": "Maybe this can also be moved into UpdateStrategy? i.e., UpdateStrategy#filterSmallFiles(List smallFiles) removes small files  in pending clustering.", "author": "satishkotha", "createdAt": "2020-12-22T20:10:49Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +140,21 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    // get the in pending clustering fileId for each partition path", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODcwOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988708", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NTY5MA=="}], "type": "inlineReview", "revised_code": {"commit": "d2fb59a0c6193b2db03408abfc04ced18902e009", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex cb46920f86..15f984aba8 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -140,20 +151,14 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n-    // get the in pending clustering fileId for each partition path\n-    Map<String, List<String>>  partitionPathToInPendingClusteringFileId =\n-        table.getFileSystemView().getFileGroupsInPendingClustering()\n-            .map(fileGroupIdAndInstantPair ->\n-                Pair.of(fileGroupIdAndInstantPair.getKey().getPartitionPath(), fileGroupIdAndInstantPair.getKey().getFileId()))\n-            .collect(Collectors.groupingBy(Pair::getKey, Collectors.mapping(Pair::getValue, Collectors.toList())));\n+    Map<String, Set<String>> partitionPathToPendingClustering = getPartitionPathToPendingClustering();\n \n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n         // exclude the small file in pending clustering, because in pending clustering file not support update now.\n-        List<String> inPendingClusteringFileId = partitionPathToInPendingClusteringFileId.getOrDefault(partitionPath, Collections.emptyList());\n-        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath).stream()\n-            .filter(smallFile -> !inPendingClusteringFileId.contains(smallFile.location.getFileId())).collect(Collectors.toList());\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClustering.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));\n \n         this.smallFiles.addAll(smallFiles);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NjI0OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547486248", "bodyText": "looks like Map<String, Set> makes more sense because we only do contains check?", "author": "satishkotha", "createdAt": "2020-12-22T20:12:14Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +140,21 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    // get the in pending clustering fileId for each partition path\n+    Map<String, List<String>>  partitionPathToInPendingClusteringFileId =", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d2fb59a0c6193b2db03408abfc04ced18902e009", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex cb46920f86..15f984aba8 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -140,20 +151,14 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n-    // get the in pending clustering fileId for each partition path\n-    Map<String, List<String>>  partitionPathToInPendingClusteringFileId =\n-        table.getFileSystemView().getFileGroupsInPendingClustering()\n-            .map(fileGroupIdAndInstantPair ->\n-                Pair.of(fileGroupIdAndInstantPair.getKey().getPartitionPath(), fileGroupIdAndInstantPair.getKey().getFileId()))\n-            .collect(Collectors.groupingBy(Pair::getKey, Collectors.mapping(Pair::getValue, Collectors.toList())));\n+    Map<String, Set<String>> partitionPathToPendingClustering = getPartitionPathToPendingClustering();\n \n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n         // exclude the small file in pending clustering, because in pending clustering file not support update now.\n-        List<String> inPendingClusteringFileId = partitionPathToInPendingClusteringFileId.getOrDefault(partitionPath, Collections.emptyList());\n-        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath).stream()\n-            .filter(smallFile -> !inPendingClusteringFileId.contains(smallFile.location.getFileId())).collect(Collectors.toList());\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClustering.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));\n \n         this.smallFiles.addAll(smallFiles);\n \n"}}, {"oid": "d2fb59a0c6193b2db03408abfc04ced18902e009", "url": "https://github.com/apache/hudi/commit/d2fb59a0c6193b2db03408abfc04ced18902e009", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T06:17:30Z", "type": "forcePushed"}, {"oid": "7b98a2a78e6d0dd8f7a790d0918c5ff35b8f54bd", "url": "https://github.com/apache/hudi/commit/7b98a2a78e6d0dd8f7a790d0918c5ff35b8f54bd", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T07:20:32Z", "type": "forcePushed"}, {"oid": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "url": "https://github.com/apache/hudi/commit/94a20f74484458fa4bdf45f25b2612a1d75fdb35", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T14:59:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDE5Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548030192", "bodyText": "use SparkRejectUpdateStrategy.class.getName?", "author": "leesf", "createdAt": "2020-12-23T16:05:43Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -73,9 +73,13 @@\n   public static final String CLUSTERING_TARGET_FILE_MAX_BYTES = CLUSTERING_STRATEGY_PARAM_PREFIX + \"target.file.max.bytes\";\n   public static final String DEFAULT_CLUSTERING_TARGET_FILE_MAX_BYTES = String.valueOf(1 * 1024 * 1024 * 1024L); // 1GB\n   \n-  // constants related to clustering that may be used by more than 1 strategy.\n+  // Constants related to clustering that may be used by more than 1 strategy.\n   public static final String CLUSTERING_SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n \n+  // When file groups is in clustering, need to handle the update to these file groups. Default strategy just reject the update\n+  public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n+  public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0MzcyMg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548043722", "bodyText": "SparkRejectUpdateStrategy is in hudi-spark, hudi-client-common not depend hudi-spark", "author": "lw309637554", "createdAt": "2020-12-23T16:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDE5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "4f7de715e708251303691573dcd05b1a5c24c9d8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\nindex 60ce2ff2aa..12fca4eeae 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n\n@@ -80,6 +80,10 @@ public class HoodieClusteringConfig extends DefaultHoodieConfig {\n   public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n   public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";\n \n+  // Async clustering\n+  public static final String ASYNC_CLUSTERING_ENABLE_OPT_KEY = \"hoodie.clustering.async\";\n+  public static final String DEFAULT_ASYNC_CLUSTERING_ENABLE_OPT_VAL = \"false\";\n+\n   public HoodieClusteringConfig(Properties props) {\n     super(props);\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDkyNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548030925", "bodyText": "please use\n`/**\n\nexclude ....\n*/` ?", "author": "leesf", "createdAt": "2020-12-23T16:07:09Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);\n+\n+  public HoodieEngineContext getEngineContext() {\n+    return engineContext;\n+  }\n+\n+  // exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\nindex fcef4eaaf5..1cf1fd23bb 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n\n@@ -39,7 +39,7 @@ public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n   }\n \n   /**\n-   * check the update records to the file group in clustering.\n+   * Check the update records to the file group in clustering.\n    * @param fileGroupsInPendingClustering\n    * @param taggedRecordsRDD the records will write, can get the update record,\n    *                        future can update tagged records location to a different fileId.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMTgxOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548031819", "bodyText": "If some file groups have updates or inserts, just throws exception now ?", "author": "leesf", "createdAt": "2020-12-23T16:09:09Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Update trategy Strategy based on following.\n+ * if some file group have update write ,just throw exception now", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\nindex c9582005ee..b659711843 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n\n@@ -35,7 +35,7 @@ import java.util.stream.Collectors;\n \n /**\n  * Update trategy Strategy based on following.\n- * if some file group have update write ,just throw exception now\n+ * if some file group have update write, just throw exception now\n  */\n public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n   private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMjIwOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548032209", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-12-23T16:10:06Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -129,6 +130,16 @@ private int addUpdateBucket(String partitionPath, String fileIdHint) {\n     return bucket;\n   }\n \n+  // get the in pending clustering fileId for each partition path", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex 289b4a0e03..9d70348516 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -130,7 +130,10 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n     return bucket;\n   }\n \n-  // get the in pending clustering fileId for each partition path\n+  /**\n+   * Get the in pending clustering fileId for each partition path.\n+   * @return partition path to pending clustering file groups id\n+   */\n   private Map<String, Set<String>> getPartitionPathToPendingClusteringFileGroupsId() {\n     Map<String, Set<String>>  partitionPathToInPendingClusteringFileId =\n         table.getFileSystemView().getFileGroupsInPendingClustering()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMzc0MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548033740", "bodyText": "files update are not supported in pending clustering currently?", "author": "leesf", "createdAt": "2020-12-23T16:13:46Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +151,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0Njk3OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548046978", "bodyText": "insert to small files update will conflict to clustering. resolved this case:\nf1, f2, f3 are file groups in partition.\nAssume there is pending clustering on all file groups f1, f2, f3.\nf3 is a small file. So we buildProfile would assign inserts to f3.\napplying update strategy will throw error because f3 is included.\nInstead, we may want to change buildProfile to exclude file groups that are in pending clustering. So, new file f4 would be created in step#3 and ingestion can continue. This way inserts can continue without errors.", "author": "lw309637554", "createdAt": "2020-12-23T16:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMzc0MA=="}], "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex 289b4a0e03..9d70348516 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -158,7 +161,7 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n       if (pStat.getNumInserts() > 0) {\n         // exclude the small file in pending clustering, because in pending clustering file not support update now.\n         Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n-        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId, partitionSmallFilesMap.get(partitionPath));\n \n         this.smallFiles.addAll(smallFiles);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTIyNw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035227", "bodyText": "moving to ClusteringUtils would be more proper?", "author": "leesf", "createdAt": "2020-12-23T16:17:07Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);\n+\n+  public HoodieEngineContext getEngineContext() {\n+    return engineContext;\n+  }\n+\n+  // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+  public static List<SmallFile> filterSmallFiles(final Set<String> pendingClusteringFileGroupsId, final List<SmallFile> smallFiles) {", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0ODAyNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548048025", "bodyText": "filterSmallFiles is  strongly related with  clustering update strategy.  In updateStrategy.java will be better", "author": "lw309637554", "createdAt": "2020-12-23T16:45:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTIyNw=="}], "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\nindex fcef4eaaf5..1cf1fd23bb 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n\n@@ -39,7 +39,7 @@ public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n   }\n \n   /**\n-   * check the update records to the file group in clustering.\n+   * Check the update records to the file group in clustering.\n    * @param fileGroupsInPendingClustering\n    * @param taggedRecordsRDD the records will write, can get the update record,\n    *                        future can update tagged records location to a different fileId.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTYyMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035621", "bodyText": "extra space", "author": "leesf", "createdAt": "2020-12-23T16:17:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +151,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex 289b4a0e03..9d70348516 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -158,7 +161,7 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n       if (pStat.getNumInserts() > 0) {\n         // exclude the small file in pending clustering, because in pending clustering file not support update now.\n         Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n-        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId, partitionSmallFilesMap.get(partitionPath));\n \n         this.smallFiles.addAll(smallFiles);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTk0Nw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035947", "bodyText": "parameters put into two lines?", "author": "leesf", "createdAt": "2020-12-23T16:18:45Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -681,6 +697,72 @@ private void assertActualAndExpectedPartitionPathRecordKeyMatches(Set<Pair<Strin\n     }\n   }\n \n+  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime, Integer recordNum, int expectStatueSize) {", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\nindex dfbda692e6..dd91ba1680 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n\n@@ -697,7 +697,8 @@ public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n     }\n   }\n \n-  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime, Integer recordNum, int expectStatueSize) {\n+  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime,\n+                                                                         Integer recordNum, int expectStatueSize) {\n     client.startCommitWithTime(commitTime);\n     List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime, recordNum);\n     JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 1);\n"}}, {"oid": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "url": "https://github.com/apache/hudi/commit/df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T16:53:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMzNjAxMg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548336012", "bodyText": "why is this a static method? If clustering is not enabled on a table, this filtering is expensive. Should we add 'default' strategy for that case? The default strategy would not do any filtering.\nIf clustering is enabled, we would use RejectUpdateStrategy and that does filtering.", "author": "satishkotha", "createdAt": "2020-12-24T00:57:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +154,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId, partitionSmallFilesMap.get(partitionPath));", "originalCommit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM1MzMzOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548353338", "bodyText": "thanks\n1\u3001\"why is this a static method?\":   because transform updatestrategy into UpsertPartitioner  may be modify the UpsertPartitioner construct.  I will add updatestrategy param to UpsertPartitioner construct\n2\u3001\" If clustering is not enabled on a table, this filtering is expensive. Should we add 'default' strategy for that case? The default strategy would not do any filtering.\"  make sense", "author": "lw309637554", "createdAt": "2020-12-24T02:25:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMzNjAxMg=="}], "type": "inlineReview", "revised_code": {"commit": "4f7de715e708251303691573dcd05b1a5c24c9d8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex 9d70348516..d565082fdd 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -159,9 +173,10 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n-        // exclude the small file in pending clustering, because in pending clustering file not support update now.\n-        Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n-        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId, partitionSmallFilesMap.get(partitionPath));\n+\n+        List<SmallFile> smallFiles =\n+            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n+                partitionSmallFilesMap.get(partitionPath));\n \n         this.smallFiles.addAll(smallFiles);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2NjQ4OQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548366489", "bodyText": "can you change it to retrun taggedRecords? You can just simply return same taggedRecordsRDD. This will be helpful later on if we want to store updates in a different file group and later reconcile.", "author": "satishkotha", "createdAt": "2020-12-24T03:30:12Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);", "originalCommit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3MjA1Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548372053", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-24T03:59:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2NjQ4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "4f7de715e708251303691573dcd05b1a5c24c9d8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\nindex 1cf1fd23bb..ef36302671 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n\n@@ -21,45 +21,27 @@ package org.apache.hudi.table.action.cluster.strategy;\n import org.apache.hudi.client.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieFileGroupId;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.table.action.commit.SmallFile;\n \n-import java.util.List;\n import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n  * When file groups in clustering, write records to these file group need to check.\n  */\n-public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n \n-  private final HoodieEngineContext engineContext;\n+  protected final HoodieEngineContext engineContext;\n+  protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;\n \n-  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+  protected UpdateStrategy(HoodieEngineContext engineContext, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {\n     this.engineContext = engineContext;\n+    this.fileGroupsInPendingClustering = fileGroupsInPendingClustering;\n   }\n \n   /**\n    * Check the update records to the file group in clustering.\n-   * @param fileGroupsInPendingClustering\n    * @param taggedRecordsRDD the records will write, can get the update record,\n    *                        future can update tagged records location to a different fileId.\n    */\n-  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);\n-\n-  public HoodieEngineContext getEngineContext() {\n-    return engineContext;\n-  }\n-\n-  /**\n-   * Exclude the small file in pending clustering, because in pending clustering file not support update now.\n-   * @param pendingClusteringFileGroupsId  pending clustering file groups id of partition\n-   * @param smallFiles small files of partition\n-   * @return\n-   */\n-  public static List<SmallFile> filterSmallFiles(final Set<String> pendingClusteringFileGroupsId, final List<SmallFile> smallFiles) {\n-    List<SmallFile> smallFilesWithoutPendingClustering = smallFiles.stream()\n-        .filter(smallFile -> !pendingClusteringFileGroupsId.contains(smallFile.location.getFileId())).collect(Collectors.toList());\n-    return smallFilesWithoutPendingClustering;\n-  }\n+  public abstract I handleRecordUpdate(I taggedRecordsRDD);\n \n }\n"}}, {"oid": "4f7de715e708251303691573dcd05b1a5c24c9d8", "url": "https://github.com/apache/hudi/commit/4f7de715e708251303691573dcd05b1a5c24c9d8", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T07:29:36Z", "type": "forcePushed"}, {"oid": "e79787cf367edfd848df3b1b79894ae103edd910", "url": "https://github.com/apache/hudi/commit/e79787cf367edfd848df3b1b79894ae103edd910", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T07:36:36Z", "type": "forcePushed"}, {"oid": "0c55e06ac0c074314175f7bff07fc98f5bb3704a", "url": "https://github.com/apache/hudi/commit/0c55e06ac0c074314175f7bff07fc98f5bb3704a", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T08:55:56Z", "type": "forcePushed"}, {"oid": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "url": "https://github.com/apache/hudi/commit/93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T10:11:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwMzg2MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548903860", "bodyText": "Change property to hoodie.clustering.async.enabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:17:58Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -73,9 +73,17 @@\n   public static final String CLUSTERING_TARGET_FILE_MAX_BYTES = CLUSTERING_STRATEGY_PARAM_PREFIX + \"target.file.max.bytes\";\n   public static final String DEFAULT_CLUSTERING_TARGET_FILE_MAX_BYTES = String.valueOf(1 * 1024 * 1024 * 1024L); // 1GB\n   \n-  // constants related to clustering that may be used by more than 1 strategy.\n+  // Constants related to clustering that may be used by more than 1 strategy.\n   public static final String CLUSTERING_SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n \n+  // When file groups is in clustering, need to handle the update to these file groups. Default strategy just reject the update\n+  public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n+  public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";\n+\n+  // Async clustering\n+  public static final String ASYNC_CLUSTERING_ENABLE_OPT_KEY = \"hoodie.clustering.async\";", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\nindex 12fca4eeae..91acd30753 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java\n\n@@ -81,7 +81,7 @@ public class HoodieClusteringConfig extends DefaultHoodieConfig {\n   public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";\n \n   // Async clustering\n-  public static final String ASYNC_CLUSTERING_ENABLE_OPT_KEY = \"hoodie.clustering.async\";\n+  public static final String ASYNC_CLUSTERING_ENABLE_OPT_KEY = \"hoodie.clustering.async.enabled\";\n   public static final String DEFAULT_ASYNC_CLUSTERING_ENABLE_OPT_VAL = \"false\";\n \n   public HoodieClusteringConfig(Properties props) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwMzkyMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548903921", "bodyText": "nit: isClusteringEnabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:18:38Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -394,6 +395,15 @@ public boolean isInlineClustering() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n+  public boolean isAsyncClustering() {\n+    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+  }\n+\n+  public boolean isClusteringEnable() {", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 735e62211d..ff43965235 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -395,13 +395,13 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n-  public boolean isAsyncClustering() {\n+  public boolean isAsyncClusteringEnabled() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n   }\n \n-  public boolean isClusteringEnable() {\n+  public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClustering();\n+    return isInlineClustering() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDAwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904003", "bodyText": "nit: isAsyncClusteringEnabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:19:19Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -394,6 +395,15 @@ public boolean isInlineClustering() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n+  public boolean isAsyncClustering() {", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 735e62211d..ff43965235 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -395,13 +395,13 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n-  public boolean isAsyncClustering() {\n+  public boolean isAsyncClusteringEnabled() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n   }\n \n-  public boolean isClusteringEnable() {\n+  public boolean isClusteringEnabled() {\n     // TODO: future support async clustering\n-    return isInlineClustering() || isAsyncClustering();\n+    return isInlineClustering() || isAsyncClusteringEnabled();\n   }\n \n   public int getInlineClusterMaxCommits() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDI3NQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904275", "bodyText": "What is the reason for collecting only fileId? HoodieFileGroupId will be more accurate because there may be two partitions with same fileId (Unlikely, but can happen). FileGroupId supports equals/hashcode, so it should not be a big change to do this on HoodieFileGroupId.", "author": "satishkotha", "createdAt": "2020-12-25T19:23:47Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Update strategy based on following.\n+ * if some file group have update record, just throw exception now\n+ */\n+public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n+\n+  public SparkRejectUpdateStrategy(HoodieSparkEngineContext engineContext, HashSet<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    super(engineContext, fileGroupsInPendingClustering);\n+  }\n+\n+  private List<Tuple2<String, String>> getPartitionPathToFileGroupsIdWithUpdates(JavaRDD<HoodieRecord<T>> inputRecords) {\n+    List<Tuple2<String, String>> fileGroupsIdWithUpdates = inputRecords\n+        .filter(record -> record.getCurrentLocation() != null)\n+        .map(record -> new Tuple2<>(record.getPartitionPath(), record.getCurrentLocation().getFileId())).distinct().collect();\n+    return fileGroupsIdWithUpdates;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> handleRecordUpdate(JavaRDD<HoodieRecord<T>> taggedRecordsRDD) {\n+    Set<String> pendingClusteringFileGroupIds =", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkzNDIzOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548934238", "bodyText": "okay", "author": "lw309637554", "createdAt": "2020-12-26T02:37:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDI3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\nindex 917be0506f..efc1b6e0c5 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n\n@@ -27,12 +27,9 @@ import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.api.java.JavaRDD;\n-import scala.Tuple2;\n \n import java.util.HashSet;\n import java.util.List;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n  * Update strategy based on following.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDYwMA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904600", "bodyText": "the class 'org.apache.hudi.DefaultClusteringStrategy' doesnt seem to exist. could you explain what this is doing? can we use default from clustering config?", "author": "satishkotha", "createdAt": "2020-12-25T19:27:39Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -110,6 +120,13 @@\n public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private static final String CLUSTERING_STRATEGY_CLASS = \"org.apache.hudi.DefaultClusteringStrategy\";", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\nindex 74bd535aa7..c201efd709 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n\n@@ -120,7 +122,6 @@ import static org.mockito.Mockito.when;\n public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n-  private static final String CLUSTERING_STRATEGY_CLASS = \"org.apache.hudi.DefaultClusteringStrategy\";\n   private static final Map<String, String> STRATEGY_PARAMS = new HashMap<String, String>() {\n     {\n       put(\"sortColumn\", \"record_key\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDg2NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904864", "bodyText": "this looks out of place. move this to line 742?", "author": "satishkotha", "createdAt": "2020-12-25T19:31:22Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -681,6 +698,75 @@ private void assertActualAndExpectedPartitionPathRecordKeyMatches(Set<Pair<Strin\n     }\n   }\n \n+  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime,\n+                                                                         Integer recordNum, int expectStatueSize) {\n+    client.startCommitWithTime(commitTime);\n+    List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime, recordNum);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 1);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime).collect();\n+    assertNoWriteErrors(statuses);\n+    assertEquals(expectStatueSize, statuses.size(), \"check expect statue size.\");\n+    return Pair.of(statuses, inserts1);\n+  }\n+\n+  @Test\n+  public void testUpdateRejectForClustering() throws IOException {\n+    final String testPartitionPath = \"2016/09/26\";\n+    dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n+    Properties props = new Properties();\n+    props.setProperty(\"hoodie.clustering.async\", \"true\");\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(100,\n+        TRIP_EXAMPLE_SCHEMA, dataGen.getEstimatedFileSizeInBytes(150), props);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    HoodieSparkCopyOnWriteTable table = (HoodieSparkCopyOnWriteTable) HoodieSparkTable.create(config, context, metaClient);\n+\n+    //1. insert to generate 2 file group\n+    String commitTime1 = \"001\";\n+    Pair<List<WriteStatus>, List<HoodieRecord>> upsertResult = insertBatchRecords(client, commitTime1, 600, 2);\n+    List<WriteStatus> statuses = upsertResult.getKey();\n+    List<HoodieRecord> inserts1 = upsertResult.getValue();\n+    List<String> fileGroupIds1 = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    assertEquals(2, fileGroupIds1.size());\n+\n+    // 2. generate clustering plan for fileGroupIds1 file groups\n+    String commitTime2 = \"002\";\n+    List<List<FileSlice>> firstInsertFileSlicesList = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getAllFileSlices().collect(Collectors.toList())).collect(Collectors.toList());\n+    List<FileSlice>[] fileSlices = (List<FileSlice>[])firstInsertFileSlicesList.toArray(new List[firstInsertFileSlicesList.size()]);\n+    createRequestedReplaceInstant(this.metaClient, commitTime2, fileSlices);\n+\n+    // 3. insert one record with no updating reject exception, and not merge the small file, just generate a new file group\n+    String commitTime3 = \"003\";\n+    statuses = insertBatchRecords(client, commitTime3, 1, 1).getKey();\n+    List<String> fileGroupIds2 = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    assertEquals(3, fileGroupIds2.size());\n+\n+\n+    // 4. update one record for the clustering two file groups, throw reject update exception\n+    String commitTime4 = \"004\";\n+    client.startCommitWithTime(commitTime4);\n+    List<HoodieRecord> insertsAndUpdates3 = new ArrayList<>();\n+    insertsAndUpdates3.addAll(dataGen.generateUpdates(commitTime4, inserts1));\n+    assertNoWriteErrors(statuses);", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkzNDI3MQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548934271", "bodyText": "have remove it , because in insertBatchRecords have do it .", "author": "lw309637554", "createdAt": "2020-12-26T02:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDg2NA=="}], "type": "inlineReview", "revised_code": {"commit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\nindex 74bd535aa7..c201efd709 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n\n@@ -714,7 +715,7 @@ public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n     final String testPartitionPath = \"2016/09/26\";\n     dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n     Properties props = new Properties();\n-    props.setProperty(\"hoodie.clustering.async\", \"true\");\n+    props.setProperty(ASYNC_CLUSTERING_ENABLE_OPT_KEY, \"true\");\n     HoodieWriteConfig config = getSmallInsertWriteConfig(100,\n         TRIP_EXAMPLE_SCHEMA, dataGen.getEstimatedFileSizeInBytes(150), props);\n     SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n"}}, {"oid": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "url": "https://github.com/apache/hudi/commit/5372851ab6bd08c19faa7fc670fd00c8df9624b8", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-26T02:36:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NjcxMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549056711", "bodyText": "This looks like will be applicable for an RDD of records, if yes, rename this to 'handleUpdate'", "author": "n3nash", "createdAt": "2020-12-27T02:52:02Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+\n+import java.util.Set;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n+\n+  protected final HoodieEngineContext engineContext;\n+  protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    this.engineContext = engineContext;\n+    this.fileGroupsInPendingClustering = fileGroupsInPendingClustering;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                         future can update tagged records location to a different fileId.\n+   * @return the recordsRDD strategy updated\n+   */\n+  public abstract I handleRecordUpdate(I taggedRecordsRDD);", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\nindex 1340ccfeda..667a58b355 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n\n@@ -39,10 +39,10 @@ public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n \n   /**\n    * Check the update records to the file group in clustering.\n-   * @param taggedRecordsRDD the records will write, can get the update record,\n+   * @param taggedRecordsRDD the records to write, tagged with target file id,\n    *                         future can update tagged records location to a different fileId.\n    * @return the recordsRDD strategy updated\n    */\n-  public abstract I handleRecordUpdate(I taggedRecordsRDD);\n+  public abstract I handleUpdate(I taggedRecordsRDD);\n \n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1Njc0Ng==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549056746", "bodyText": "Rename to getGroupIdsWithUpdate", "author": "n3nash", "createdAt": "2020-12-27T02:52:42Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * Update strategy based on following.\n+ * if some file group have update record, just throw exception now\n+ */\n+public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n+\n+  public SparkRejectUpdateStrategy(HoodieSparkEngineContext engineContext, HashSet<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    super(engineContext, fileGroupsInPendingClustering);\n+  }\n+\n+  private List<HoodieFileGroupId> getFileGroupIdsWithRecordUpdate(JavaRDD<HoodieRecord<T>> inputRecords) {", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\nindex efc1b6e0c5..134e490246 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n\n@@ -33,7 +33,7 @@ import java.util.List;\n \n /**\n  * Update strategy based on following.\n- * if some file group have update record, just throw exception now\n+ * if some file group have update record, throw exception\n  */\n public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n   private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzIxNw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057217", "bodyText": "rename this to clusteringHandleUpdate as well, records is implied", "author": "n3nash", "createdAt": "2020-12-27T02:59:02Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -88,6 +93,18 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     super(context, config, table, instantTime, operationType, extraMetadata);\n   }\n \n+  private JavaRDD<HoodieRecord<T>> clusteringHandleRecordsUpdate(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\nindex f0ac212fc0..1fd5dad688 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n\n@@ -93,13 +93,13 @@ public abstract class BaseSparkCommitActionExecutor<T extends HoodieRecordPayloa\n     super(context, config, table, instantTime, operationType, extraMetadata);\n   }\n \n-  private JavaRDD<HoodieRecord<T>> clusteringHandleRecordsUpdate(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+  private JavaRDD<HoodieRecord<T>> clusteringHandleUpdate(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n     if (config.isClusteringEnabled()) {\n       Set<HoodieFileGroupId> fileGroupsInPendingClustering =\n           table.getFileSystemView().getFileGroupsInPendingClustering().map(entry -> entry.getKey()).collect(Collectors.toSet());\n       UpdateStrategy updateStrategy = (UpdateStrategy)ReflectionUtils\n           .loadClass(config.getClusteringUpdatesStrategyClass(), this.context, fileGroupsInPendingClustering);\n-      return (JavaRDD<HoodieRecord<T>>)updateStrategy.handleRecordUpdate(inputRecordsRDD);\n+      return (JavaRDD<HoodieRecord<T>>)updateStrategy.handleUpdate(inputRecordsRDD);\n     } else {\n       return inputRecordsRDD;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzMwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057303", "bodyText": "Nit -> remove the \"just throw exception now\" to \"throw exception\"", "author": "n3nash", "createdAt": "2020-12-27T03:00:21Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * Update strategy based on following.", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\nindex efc1b6e0c5..134e490246 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java\n\n@@ -33,7 +33,7 @@ import java.util.List;\n \n /**\n  * Update strategy based on following.\n- * if some file group have update record, just throw exception now\n+ * if some file group have update record, throw exception\n  */\n public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n   private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzQwNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057404", "bodyText": "Nit : for comments, please replace with \"the records to write, tagged with target file id\"", "author": "n3nash", "createdAt": "2020-12-27T03:02:40Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+\n+import java.util.Set;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n+\n+  protected final HoodieEngineContext engineContext;\n+  protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    this.engineContext = engineContext;\n+    this.fileGroupsInPendingClustering = fileGroupsInPendingClustering;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param taggedRecordsRDD the records will write, can get the update record,", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\nindex 1340ccfeda..667a58b355 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java\n\n@@ -39,10 +39,10 @@ public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n \n   /**\n    * Check the update records to the file group in clustering.\n-   * @param taggedRecordsRDD the records will write, can get the update record,\n+   * @param taggedRecordsRDD the records to write, tagged with target file id,\n    *                         future can update tagged records location to a different fileId.\n    * @return the recordsRDD strategy updated\n    */\n-  public abstract I handleRecordUpdate(I taggedRecordsRDD);\n+  public abstract I handleUpdate(I taggedRecordsRDD);\n \n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzUzOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057539", "bodyText": "Please Change to : \"exclude small file handling for clustering since update path is not supported\"", "author": "n3nash", "createdAt": "2020-12-27T03:04:28Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -129,6 +129,34 @@ private int addUpdateBucket(String partitionPath, String fileIdHint) {\n     return bucket;\n   }\n \n+  /**\n+   * Get the in pending clustering fileId for each partition path.\n+   * @return partition path to pending clustering file groups id\n+   */\n+  private Map<String, Set<String>> getPartitionPathToPendingClusteringFileGroupsId() {\n+    Map<String, Set<String>>  partitionPathToInPendingClusteringFileId =\n+        table.getFileSystemView().getFileGroupsInPendingClustering()\n+            .map(fileGroupIdAndInstantPair ->\n+                Pair.of(fileGroupIdAndInstantPair.getKey().getPartitionPath(), fileGroupIdAndInstantPair.getKey().getFileId()))\n+            .collect(Collectors.groupingBy(Pair::getKey, Collectors.mapping(Pair::getValue, Collectors.toSet())));\n+    return partitionPathToInPendingClusteringFileId;\n+  }\n+\n+  /**\n+   * Exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\nindex 648b6dd6ec..a84e9127e2 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java\n\n@@ -143,7 +143,7 @@ public class UpsertPartitioner<T extends HoodieRecordPayload<T>> extends Partiti\n   }\n \n   /**\n-   * Exclude the small file in pending clustering, because in pending clustering file not support update now.\n+   * Exclude small file handling for clustering since update path is not supported.\n    * @param pendingClusteringFileGroupsId  pending clustering file groups id of partition\n    * @param smallFiles small files of partition\n    * @return smallFiles not in clustering\n"}}, {"oid": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "url": "https://github.com/apache/hudi/commit/589e9b10c04593c685001874ea3ebcd066ba4bb9", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-27T11:47:49Z", "type": "commit"}, {"oid": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "url": "https://github.com/apache/hudi/commit/589e9b10c04593c685001874ea3ebcd066ba4bb9", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-27T11:47:49Z", "type": "forcePushed"}]}