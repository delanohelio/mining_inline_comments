{"pr_number": 2328, "pr_title": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0", "pr_createdAt": "2020-12-11T07:29:57Z", "pr_url": "https://github.com/apache/hudi/pull/2328", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDAzMDUwMQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544030501", "bodyText": "all other configs are named as \"bulkinsert\". can we fix this one too instead of \"bulk_insert\".", "author": "nsivabalan", "createdAt": "2020-12-16T06:37:20Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -70,6 +70,7 @@\n   public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n   public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n   public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n+  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulk_insert.schema.ddl\";", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex bb5bc36227..e573ea8734 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -70,7 +70,7 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n   public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n   public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n-  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulk_insert.schema.ddl\";\n+  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulkinsert.schema.ddl\";\n   public static final String UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\";\n   public static final String DELETE_PARALLELISM = \"hoodie.delete.shuffle.parallelism\";\n   public static final String DEFAULT_ROLLBACK_PARALLELISM = \"100\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0MDM4NA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544040384", "bodyText": "do you think we can create \"BaseDefaultSource\" and move SparkSession, Configuration and  getters there. So that spark 3 and spark2 defaultSource can extend from it.", "author": "nsivabalan", "createdAt": "2020-12-16T06:49:15Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+import java.util.Map;\n+\n+/**\n+ * DataSource V2 implementation for managing internal write logic. Only called internally.\n+ * This class is only compatible with datasource V2 API in Spark 3.\n+ */\n+public class DefaultSource implements TableProvider {\n+\n+  private SparkSession sparkSession = null;", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java\nindex 8af02d65e6..aa4335109f 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java\n\n@@ -20,9 +20,9 @@ package org.apache.hudi.spark3.internal;\n \n import org.apache.hudi.DataSourceUtils;\n import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.BaseDefaultSource;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterHelper;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.connector.catalog.Table;\n import org.apache.spark.sql.connector.catalog.TableProvider;\n import org.apache.spark.sql.connector.expressions.Transform;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544046633", "bodyText": "I assume this class is almost exact replica of HoodieDataSourceInternalWriter", "author": "nsivabalan", "createdAt": "2020-12-16T06:57:08Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.PhysicalWriteInfo;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implementation of {@link BatchWrite} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriter implements BatchWrite {", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDE0OA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054148", "bodyText": "why named it as \"...Writer\" even though interface is called Write. i.e. BatchWrite.", "author": "nsivabalan", "createdAt": "2020-12-16T07:05:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriterHelper.java\nsimilarity index 57%\nrename from hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java\nrename to hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriterHelper.java\nindex ac3c44c160..84f8e9f544 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java\n+++ b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriterHelper.java\n\n@@ -16,10 +16,9 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.spark3.internal;\n+package org.apache.hudi.internal;\n \n import org.apache.hudi.DataSourceUtils;\n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n import org.apache.hudi.client.SparkRDDWriteClient;\n import org.apache.hudi.client.common.HoodieSparkEngineContext;\n import org.apache.hudi.common.model.HoodieWriteStat;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NzU3MQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544047571", "bodyText": "I assume this is exact replica of HoodieBulkInsertDataInternalWriterFactory in spark2 datasource.", "author": "nsivabalan", "createdAt": "2020-12-16T06:58:17Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriterFactory.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Factory to assist in instantiating {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class HoodieBulkInsertDataInternalWriterFactory implements DataWriterFactory {", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk4ODg5OA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r546988898", "bodyText": "I assume this is exact replica of HoodieBulkInsertDataInternalWriterFactory in spark2 datasource.\n\nThe only difference is the import class path.", "author": "zhedoubushishi", "createdAt": "2020-12-21T23:49:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NzU3MQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0ODU2NA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544048564", "bodyText": "is there any changes in this class compared to HoodieBulkInsertDataInternalWriter in spark 2 datasource?", "author": "nsivabalan", "createdAt": "2020-12-16T06:59:22Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Hoodie's Implementation of {@link DataWriter<InternalRow>}. This is used in data source \"hudi.spark3.internal\" implementation for bulk insert.\n+ */\n+public class HoodieBulkInsertDataInternalWriter implements DataWriter<InternalRow> {", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzcwNzA5Mw==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r547707093", "bodyText": "The only difference is:\nimport org.apache.spark.sql.sources.v2.writer.DataWriter;\nimport org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n\nimport org.apache.spark.sql.connector.write.DataWriter;\nimport org.apache.spark.sql.connector.write.WriterCommitMessage;\n\nI created a class named HoodieBulkInsertDataInternalWriterHelper to avoid duplicated code.", "author": "zhedoubushishi", "createdAt": "2020-12-23T06:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0ODU2NA=="}], "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java\nindex 53d3a2f1e8..ed1ec47c65 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java\n\n@@ -18,99 +18,47 @@\n \n package org.apache.hudi.spark3.internal;\n \n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.internal.HoodieBulkInsertDataInternalWriterHelper;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.connector.write.DataWriter;\n import org.apache.spark.sql.connector.write.WriterCommitMessage;\n import org.apache.spark.sql.types.StructType;\n \n import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.UUID;\n \n /**\n  * Hoodie's Implementation of {@link DataWriter<InternalRow>}. This is used in data source \"hudi.spark3.internal\" implementation for bulk insert.\n  */\n public class HoodieBulkInsertDataInternalWriter implements DataWriter<InternalRow> {\n \n-  private static final Logger LOG = LogManager.getLogger(HoodieBulkInsertDataInternalWriter.class);\n-\n-  private final String instantTime;\n-  private final int taskPartitionId;\n-  private final long taskId;\n-  private final HoodieTable hoodieTable;\n-  private final HoodieWriteConfig writeConfig;\n-  private final StructType structType;\n-  private final List<HoodieInternalWriteStatus> writeStatusList = new ArrayList<>();\n-\n-  private HoodieRowCreateHandle handle;\n-  private String lastKnownPartitionPath = null;\n-  private String fileIdPrefix;\n-  private int numFilesWritten = 0;\n+  private final HoodieBulkInsertDataInternalWriterHelper bulkInsertWriterHelper;\n \n   public HoodieBulkInsertDataInternalWriter(HoodieTable hoodieTable, HoodieWriteConfig writeConfig,\n       String instantTime, int taskPartitionId, long taskId, StructType structType) {\n-    this.hoodieTable = hoodieTable;\n-    this.writeConfig = writeConfig;\n-    this.instantTime = instantTime;\n-    this.taskPartitionId = taskPartitionId;\n-    this.taskId = taskId;\n-    this.structType = structType;\n-    this.fileIdPrefix = UUID.randomUUID().toString();\n+    this.bulkInsertWriterHelper = new HoodieBulkInsertDataInternalWriterHelper(hoodieTable,\n+        writeConfig, instantTime, taskPartitionId, taskId, 0, structType);\n   }\n \n   @Override\n   public void write(InternalRow record) throws IOException {\n-    try {\n-      String partitionPath = record.getUTF8String(\n-          HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).toString();\n-\n-      if ((lastKnownPartitionPath == null) || !lastKnownPartitionPath.equals(partitionPath) || !handle.canWrite()) {\n-        LOG.info(\"Creating new file for partition path \" + partitionPath);\n-        createNewHandle(partitionPath);\n-        lastKnownPartitionPath = partitionPath;\n-      }\n-      handle.write(record);\n-    } catch (Throwable t) {\n-      LOG.error(\"Global error thrown while trying to write records in HoodieRowCreateHandle \", t);\n-      throw t;\n-    }\n+    bulkInsertWriterHelper.write(record);\n   }\n \n   @Override\n   public WriterCommitMessage commit() throws IOException {\n-    close();\n-    return new HoodieWriterCommitMessage(writeStatusList);\n+    return new HoodieWriterCommitMessage(bulkInsertWriterHelper.getWriteStatuses());\n   }\n \n   @Override\n   public void abort() {\n+    bulkInsertWriterHelper.abort();\n   }\n \n-  private void createNewHandle(String partitionPath) throws IOException {\n-    if (null != handle) {\n-      close();\n-    }\n-    handle = new HoodieRowCreateHandle(hoodieTable, writeConfig, partitionPath, getNextFileId(),\n-        instantTime, taskPartitionId, taskId, 0, structType);\n-  }\n-\n+  @Override\n   public void close() throws IOException {\n-    if (null != handle) {\n-      writeStatusList.add(handle.close());\n-      handle = null;\n-    }\n-  }\n-\n-  private String getNextFileId() {\n-    return String.format(\"%s-%d\", fileIdPrefix, numFilesWritten++);\n+    bulkInsertWriterHelper.close();\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1MjQ0NA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544052444", "bodyText": "java docs.", "author": "nsivabalan", "createdAt": "2020-12-16T07:03:56Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.SupportsWrite;\n+import org.apache.spark.sql.connector.catalog.TableCapability;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java\nindex b3a4574195..f1fded033d 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java\n\n@@ -19,13 +19,8 @@\n package org.apache.hudi.spark3.internal;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hudi.client.common.HoodieSparkEngineContext;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.table.HoodieSparkTable;\n-import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.connector.catalog.SupportsWrite;\n import org.apache.spark.sql.connector.catalog.TableCapability;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDQ2NA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054464", "bodyText": "why suffixed it as \"WriterBuilder\" while interface is called \"WriteBuilder\"", "author": "nsivabalan", "createdAt": "2020-12-16T07:06:12Z", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Implementation of {@link WriteBuilder} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriterBuilder implements WriteBuilder {", "originalCommit": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b01dd14366b04e57b4860b57b13891a025360fe4", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriteBuilder.java\nsimilarity index 70%\nrename from hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java\nrename to hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriteBuilder.java\nindex f483ad9cca..10e2e64f11 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriteBuilder.java\n\n@@ -18,9 +18,9 @@\n \n package org.apache.hudi.spark3.internal;\n \n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.connector.write.BatchWrite;\n import org.apache.spark.sql.connector.write.WriteBuilder;\n"}}, {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4", "url": "https://github.com/apache/hudi/commit/b01dd14366b04e57b4860b57b13891a025360fe4", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0", "committedDate": "2020-12-23T06:32:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3NzQyMQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548077421", "bodyText": "does it makes sense to print Arrays.toString(writeStatuses.toArray()) ?", "author": "nsivabalan", "createdAt": "2020-12-23T17:37:26Z", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {\n+\n+  private List<HoodieInternalWriteStatus> writeStatuses;\n+\n+  public BaseHoodieWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n+    this.writeStatuses = writeStatuses;\n+  }\n+\n+  public List<HoodieInternalWriteStatus> getWriteStatuses() {\n+    return writeStatuses;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"HoodieWriterCommitMessage{\" + \"writeStatuses=\" + writeStatuses + '}';", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\nsimilarity index 83%\nrename from hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java\nrename to hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\nindex ff3685141a..88a7921236 100644\n--- a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java\n+++ b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\n\n@@ -20,16 +20,17 @@ package org.apache.hudi.internal;\n \n import org.apache.hudi.client.HoodieInternalWriteStatus;\n \n+import java.util.Arrays;\n import java.util.List;\n \n /**\n  * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n  */\n-public class BaseHoodieWriterCommitMessage {\n+public class BaseWriterCommitMessage {\n \n   private List<HoodieInternalWriteStatus> writeStatuses;\n \n-  public BaseHoodieWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n+  public BaseWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n     this.writeStatuses = writeStatuses;\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3ODE0Mw==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548078143", "bodyText": "my bad. since this is not public class per se, we can remove \"Hoodie\" prefix. btw, can you help me understand why this needs to be public? package private should work right?", "author": "nsivabalan", "createdAt": "2020-12-23T17:38:18Z", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Helper class for HoodieBulkInsertDataInternalWriter used by Spark datasource v2.\n+ */\n+public class HoodieBulkInsertDataInternalWriterHelper {", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM0MjA3MQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548342071", "bodyText": "Sure will remove \"Hoodie\" prefix.\nThis class is under hudi-spark-common package, and it's called in HoodieBulkInsertDataInternalWriter which is located in hudi-spark2 & hudi-spark3 package. So it needs to be public.", "author": "zhedoubushishi", "createdAt": "2020-12-24T01:28:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3ODE0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BulkInsertDataInternalWriterHelper.java\nsimilarity index 93%\nrename from hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java\nrename to hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BulkInsertDataInternalWriterHelper.java\nindex 8dde92da5e..eb26c4f320 100644\n--- a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java\n+++ b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BulkInsertDataInternalWriterHelper.java\n\n@@ -37,9 +37,9 @@ import java.util.UUID;\n /**\n  * Helper class for HoodieBulkInsertDataInternalWriter used by Spark datasource v2.\n  */\n-public class HoodieBulkInsertDataInternalWriterHelper {\n+public class BulkInsertDataInternalWriterHelper {\n \n-  private static final Logger LOG = LogManager.getLogger(HoodieBulkInsertDataInternalWriterHelper.class);\n+  private static final Logger LOG = LogManager.getLogger(BulkInsertDataInternalWriterHelper.class);\n \n   private final String instantTime;\n   private final int taskPartitionId;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEwNjk3OQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548106979", "bodyText": "help me understand. do we need two separate derived classes for each datasource. Why can't we just use this class directly in both datasources?", "author": "nsivabalan", "createdAt": "2020-12-23T18:13:59Z", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM0MDQzMw==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548340433", "bodyText": "Because Spark 3 relocate this class: WriterCommitMessage.\nIn Spark 2, it's:\nimport org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n\nIn Spark 3, it's:\nimport org.apache.spark.sql.connector.write.WriterCommitMessage;\n\nThat's why we cannot use a single class to do this.", "author": "zhedoubushishi", "createdAt": "2020-12-24T01:20:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEwNjk3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\nsimilarity index 83%\nrename from hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java\nrename to hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\nindex ff3685141a..88a7921236 100644\n--- a/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java\n+++ b/hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseWriterCommitMessage.java\n\n@@ -20,16 +20,17 @@ package org.apache.hudi.internal;\n \n import org.apache.hudi.client.HoodieInternalWriteStatus;\n \n+import java.util.Arrays;\n import java.util.List;\n \n /**\n  * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n  */\n-public class BaseHoodieWriterCommitMessage {\n+public class BaseWriterCommitMessage {\n \n   private List<HoodieInternalWriteStatus> writeStatuses;\n \n-  public BaseHoodieWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n+  public BaseWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n     this.writeStatuses = writeStatuses;\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMDI3OA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548110278", "bodyText": "can we test both classes in one test class only? may be parametrized?", "author": "nsivabalan", "createdAt": "2020-12-23T18:18:13Z", "path": "hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -18,63 +18,32 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.testutils.HoodieClientTestHarness;\n \n-import org.apache.spark.package$;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n-import java.util.Random;\n \n import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n import static org.junit.jupiter.api.Assertions.fail;\n-import static org.junit.jupiter.api.Assumptions.assumeTrue;\n \n /**\n  * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n  */\n-public class TestHoodieBulkInsertDataInternalWriter extends HoodieClientTestHarness {\n-\n-  private static final Random RANDOM = new Random();\n-\n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    // this test is only compatible with spark 2\n-    assumeTrue(package$.MODULE$.SPARK_VERSION().startsWith(\"2.\"));\n-    initSparkContexts(\"TestHoodieBulkInsertDataInternalWriter\");\n-    initPath();\n-    initFileSystem();\n-    initTestDataGenerator();\n-    initMetaClient();\n-  }\n-\n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupResources();\n-  }\n+public class TestHoodieBulkInsertDataInternalWriter extends", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQyNDUyNA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548424524", "bodyText": "Currently one class mainly tests HoodieBulkInsertDataInternalWriter, one class mainly tests HoodieDataSourceInternalBatchWrite. I am not sure if we should merge them into one. But definitely we can do this.", "author": "zhedoubushishi", "createdAt": "2020-12-24T07:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMDI3OA=="}], "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java b/hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java\nindex a87f4c4794..0b021abeb3 100644\n--- a/hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java\n+++ b/hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java\n\n@@ -19,6 +19,7 @@\n package org.apache.hudi.internal;\n \n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.util.Option;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548112355", "bodyText": "again I see some opportunity for code reuse here. Do you think we can avoid duplicating the test code across two test classes. Except for few lines, rest could be made common.", "author": "nsivabalan", "createdAt": "2020-12-23T18:21:12Z", "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQyNzU4Mw==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548427583", "bodyText": "Few more comments. thanks for code reuse. now most of our core logic is in one place for both datasources.\nbtw, can you add tests to HoodieSparkSqlWriterSuite as well.\nYou can check out previous tests here.\n\nThanks for the quick review.\nI am not very sure about what kind of test do you want to add here.\nThe test test(\"test bulk insert dataset with datasource impl\") in HoodieSparkSqlWriterSuite can also be used for testing this pr. You can do this through mvn test xxx -Pspark3.", "author": "zhedoubushishi", "createdAt": "2020-12-24T07:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMjAzMg==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548432032", "bodyText": "Yea I can try to reduce the duplicated code between TestHoodieBulkInsertDataInternalWriter and TestHoodieDataSourceInternalBatchWrite.\nThe code of org.apache.hudi.internal.TestHoodieBulkInsertDataInternalWriter is almost the same as  org.apache.hudi.spark3.internal.TestHoodieBulkInsertDataInternalWriter. However it's kind of tricky to avoid this. Because one HoodieBulkInsertDataInternalWriter implements  org.apache.spark.sql.connector.write.DataWriter which is only compatible with Spark 3 but another HoodieBulkInsertDataInternalWriter implements org.apache.spark.sql.sources.v2.writer.DataWriter which is only compatible with Spark 2.\nReflection is one way to solve this problem.", "author": "zhedoubushishi", "createdAt": "2020-12-24T07:40:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODUyMjEzOA==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548522138", "bodyText": "if we can solve it using Reflection, it might be okay for a test.", "author": "vinothchandar", "createdAt": "2020-12-24T12:52:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java b/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java\nindex fc41707227..69829ec281 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java\n\n@@ -19,8 +19,9 @@\n package org.apache.hudi.spark3.internal;\n \n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.util.Option;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.internal.HoodieBulkInsertInternalWriterTestBase;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.testutils.HoodieClientTestUtils;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMzAwMg==", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548113002", "bodyText": "same comment as other tests.", "author": "nsivabalan", "createdAt": "2020-12-23T18:22:17Z", "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieBulkInsertDataInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class TestHoodieBulkInsertDataInternalWriter extends\n+    HoodieBulkInsertDataInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataInternalWriter() throws Exception {", "originalCommit": "b01dd14366b04e57b4860b57b13891a025360fe4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java b/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java\nindex 511afc9148..ffb649bd39 100644\n--- a/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java\n+++ b/hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java\n\n@@ -19,8 +19,9 @@\n package org.apache.hudi.spark3.internal;\n \n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.util.Option;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.internal.HoodieBulkInsertDataInternalWriterTestBase;\n+import org.apache.hudi.internal.HoodieBulkInsertInternalWriterTestBase;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n \n"}}, {"oid": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "url": "https://github.com/apache/hudi/commit/a7b5d803bba9bd59b0bb8974c672297c78074fa3", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0", "committedDate": "2020-12-25T13:16:03Z", "type": "commit"}, {"oid": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "url": "https://github.com/apache/hudi/commit/a7b5d803bba9bd59b0bb8974c672297c78074fa3", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0", "committedDate": "2020-12-25T13:16:03Z", "type": "forcePushed"}]}