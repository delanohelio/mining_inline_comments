{"pr_number": 1460, "pr_title": "[HUDI-679] Make io package Spark free", "pr_createdAt": "2020-03-28T06:36:29Z", "pr_url": "https://github.com/apache/hudi/pull/1460", "timeline": [{"oid": "5f65bfb02846ce61bd733b70bccb981f98348777", "url": "https://github.com/apache/hudi/commit/5f65bfb02846ce61bd733b70bccb981f98348777", "message": "[HUDI-679] Make io package Spark free", "committedDate": "2020-03-28T06:32:12Z", "type": "commit"}, {"oid": "24e68e3a8238453a603455eb9c69b438a82bc25c", "url": "https://github.com/apache/hudi/commit/24e68e3a8238453a603455eb9c69b438a82bc25c", "message": "address travis failure", "committedDate": "2020-03-28T08:31:41Z", "type": "commit"}, {"oid": "bb7268f4d20e9bffa60220487fed03a945916b99", "url": "https://github.com/apache/hudi/commit/bb7268f4d20e9bffa60220487fed03a945916b99", "message": "add travis", "committedDate": "2020-03-28T09:25:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY0OTE5Ng==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399649196", "bodyText": "Considering this interface supports some information about TaskContext, can we rename to SparkTaskContextDetailSupplier?", "author": "yanghua", "createdAt": "2020-03-28T11:01:54Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkSupplier<T> extends Supplier<T>, Serializable {", "originalCommit": "bb7268f4d20e9bffa60220487fed03a945916b99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MTQ5NQ==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399651495", "bodyText": "sounds reasonable.", "author": "leesf", "createdAt": "2020-03-28T11:28:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY0OTE5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nsimilarity index 65%\nrename from hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java\nrename to hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nindex e163da604..9a29b74e4 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n\n@@ -26,8 +26,20 @@ import java.util.function.Supplier;\n /**\n  * Spark Supplier.\n  */\n-public interface SparkSupplier<T> extends Supplier<T>, Serializable {\n-  SparkSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n-  SparkSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n-  SparkSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+\n+  /**\n+   * Supplier to get partition id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+\n+  /**\n+   * Supplier to get stage id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+\n+  /**\n+   * Supplier to get task attempt id.\n+   */\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY0OTI1OA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399649258", "bodyText": "Can we add empty line to split this definition?  Additionally, add some comments?", "author": "yanghua", "createdAt": "2020-03-28T11:02:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkSupplier<T> extends Supplier<T>, Serializable {\n+  SparkSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();", "originalCommit": "bb7268f4d20e9bffa60220487fed03a945916b99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MTUwNw==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399651507", "bodyText": "Can we add empty line to split this definition? Additionally, add some comments?\n\nsure", "author": "leesf", "createdAt": "2020-03-28T11:28:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY0OTI1OA=="}], "type": "inlineReview", "revised_code": {"commit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nsimilarity index 65%\nrename from hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java\nrename to hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nindex e163da604..9a29b74e4 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n\n@@ -26,8 +26,20 @@ import java.util.function.Supplier;\n /**\n  * Spark Supplier.\n  */\n-public interface SparkSupplier<T> extends Supplier<T>, Serializable {\n-  SparkSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n-  SparkSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n-  SparkSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+\n+  /**\n+   * Supplier to get partition id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+\n+  /**\n+   * Supplier to get stage id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+\n+  /**\n+   * Supplier to get task attempt id.\n+   */\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n }\n"}}, {"oid": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "url": "https://github.com/apache/hudi/commit/9ef6c852631c8344a77def77a9e8f8d81a67c283", "message": "address comments", "committedDate": "2020-03-28T11:31:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDA5NA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660094", "bodyText": "PARTITION_ID_SUPPLIER ?", "author": "yanghua", "createdAt": "2020-03-28T13:02:40Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+\n+  /**\n+   * Supplier to get partition id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nindex 9a29b74e4..0efd28cf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n\n@@ -31,15 +31,15 @@ public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializ\n   /**\n    * Supplier to get partition id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_ID_SUPPLIER = () -> TaskContext.getPartitionId();\n \n   /**\n    * Supplier to get stage id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+  SparkTaskContextDetailSupplier<Integer> STAGE_ID_SUPPLIER = () -> TaskContext.get().stageId();\n \n   /**\n    * Supplier to get task attempt id.\n    */\n-  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_ID_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDEzMg==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660132", "bodyText": "STAGE_ID_SUPPLIER ?", "author": "yanghua", "createdAt": "2020-03-28T13:03:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+\n+  /**\n+   * Supplier to get partition id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+\n+  /**\n+   * Supplier to get stage id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nindex 9a29b74e4..0efd28cf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n\n@@ -31,15 +31,15 @@ public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializ\n   /**\n    * Supplier to get partition id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_ID_SUPPLIER = () -> TaskContext.getPartitionId();\n \n   /**\n    * Supplier to get stage id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+  SparkTaskContextDetailSupplier<Integer> STAGE_ID_SUPPLIER = () -> TaskContext.get().stageId();\n \n   /**\n    * Supplier to get task attempt id.\n    */\n-  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_ID_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDE2Nw==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660167", "bodyText": "ATTEMPT_ID_SUPPLIER ?", "author": "yanghua", "createdAt": "2020-03-28T13:03:29Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+\n+  /**\n+   * Supplier to get partition id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+\n+  /**\n+   * Supplier to get stage id.\n+   */\n+  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+\n+  /**\n+   * Supplier to get task attempt id.\n+   */\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nindex 9a29b74e4..0efd28cf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n\n@@ -31,15 +31,15 @@ public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializ\n   /**\n    * Supplier to get partition id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> PARTITION_SUPPLIER = () -> TaskContext.getPartitionId();\n+  SparkTaskContextDetailSupplier<Integer> PARTITION_ID_SUPPLIER = () -> TaskContext.getPartitionId();\n \n   /**\n    * Supplier to get stage id.\n    */\n-  SparkTaskContextDetailSupplier<Integer> STAGE_SUPPLIER = () -> TaskContext.get().stageId();\n+  SparkTaskContextDetailSupplier<Integer> STAGE_ID_SUPPLIER = () -> TaskContext.get().stageId();\n \n   /**\n    * Supplier to get task attempt id.\n    */\n-  SparkTaskContextDetailSupplier<Long> ATTEMPT_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+  SparkTaskContextDetailSupplier<Long> ATTEMPT_ID_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDMzOA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660338", "bodyText": "hoodieTable.getIdSupplier() is not clear here. I suggest we can rename these getter to e.g. getPartitionIdSupplier, getStageId and getAttemptId?", "author": "yanghua", "createdAt": "2020-03-28T13:05:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java", "diffHunk": "@@ -51,6 +51,7 @@ public BulkInsertMapFunction(String instantTime, HoodieWriteConfig config, Hoodi\n   @Override\n   public Iterator<List<WriteStatus>> call(Integer partition, Iterator<HoodieRecord<T>> sortedRecordItr) {\n     return new CopyOnWriteLazyInsertIterable<>(sortedRecordItr, config, instantTime, hoodieTable,\n-        fileIDPrefixes.get(partition));\n+        fileIDPrefixes.get(partition), hoodieTable.getIdSupplier(), hoodieTable.getStageSupplier(),", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java b/hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java\nindex 7904a7c23..027a557ef 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/execution/BulkInsertMapFunction.java\n\n@@ -51,7 +51,6 @@ public class BulkInsertMapFunction<T extends HoodieRecordPayload>\n   @Override\n   public Iterator<List<WriteStatus>> call(Integer partition, Iterator<HoodieRecord<T>> sortedRecordItr) {\n     return new CopyOnWriteLazyInsertIterable<>(sortedRecordItr, config, instantTime, hoodieTable,\n-        fileIDPrefixes.get(partition), hoodieTable.getIdSupplier(), hoodieTable.getStageSupplier(),\n-            hoodieTable.getAttemptSupplier());\n+        fileIDPrefixes.get(partition), hoodieTable.getSuppliers());\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDM3Mg==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660372", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-03-28T13:05:33Z", "path": "hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java", "diffHunk": "@@ -50,15 +51,23 @@\n   protected final HoodieTable<T> hoodieTable;\n   protected final String idPrefix;\n   protected int numFilesWritten;\n+  protected Supplier<Integer> idSupplier;", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java b/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\nindex 53166c0e6..b38c697fe 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\n\n@@ -51,23 +52,20 @@ public class CopyOnWriteLazyInsertIterable<T extends HoodieRecordPayload>\n   protected final HoodieTable<T> hoodieTable;\n   protected final String idPrefix;\n   protected int numFilesWritten;\n-  protected Supplier<Integer> idSupplier;\n-  protected Supplier<Integer> stageSupplier;\n-  protected Supplier<Long> attemptSupplier;\n+  protected Suppliers suppliers;\n+  protected Supplier<Integer> stageIdSupplier;\n+  protected Supplier<Long> attemptIdSupplier;\n \n   public CopyOnWriteLazyInsertIterable(Iterator<HoodieRecord<T>> sortedRecordItr, HoodieWriteConfig config,\n                                        String instantTime, HoodieTable<T> hoodieTable, String idPrefix,\n-                                       Supplier<Integer> idSupplier, Supplier<Integer> stageSupplier,\n-                                       Supplier<Long> attemptSupplier) {\n+                                       Suppliers suppliers) {\n     super(sortedRecordItr);\n     this.hoodieConfig = config;\n     this.instantTime = instantTime;\n     this.hoodieTable = hoodieTable;\n     this.idPrefix = idPrefix;\n     this.numFilesWritten = 0;\n-    this.idSupplier = idSupplier;\n-    this.stageSupplier = stageSupplier;\n-    this.attemptSupplier = attemptSupplier;\n+    this.suppliers = suppliers;\n   }\n \n   // Used for caching HoodieRecord along with insertValue. We need this to offload computation work to buffering thread.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDQ2Ng==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660466", "bodyText": "Actually, I am not sure if we can package these three args into a DTO structure. Just a thought, you can ignore.", "author": "yanghua", "createdAt": "2020-03-28T13:06:49Z", "path": "hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java", "diffHunk": "@@ -50,15 +51,23 @@\n   protected final HoodieTable<T> hoodieTable;\n   protected final String idPrefix;\n   protected int numFilesWritten;\n+  protected Supplier<Integer> idSupplier;\n+  protected Supplier<Integer> stageSupplier;\n+  protected Supplier<Long> attemptSupplier;\n \n   public CopyOnWriteLazyInsertIterable(Iterator<HoodieRecord<T>> sortedRecordItr, HoodieWriteConfig config,\n-                                       String instantTime, HoodieTable<T> hoodieTable, String idPrefix) {\n+                                       String instantTime, HoodieTable<T> hoodieTable, String idPrefix,\n+                                       Supplier<Integer> idSupplier, Supplier<Integer> stageSupplier,\n+                                       Supplier<Long> attemptSupplier) {", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY3Mzg3MA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399673870", "bodyText": "Actually, I am not sure if we can package these three args into a DTO structure. Just a thought, you can ignore.\n\nYes, I think it is better.", "author": "leesf", "createdAt": "2020-03-28T15:22:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDQ2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java b/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\nindex 53166c0e6..b38c697fe 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java\n\n@@ -51,23 +52,20 @@ public class CopyOnWriteLazyInsertIterable<T extends HoodieRecordPayload>\n   protected final HoodieTable<T> hoodieTable;\n   protected final String idPrefix;\n   protected int numFilesWritten;\n-  protected Supplier<Integer> idSupplier;\n-  protected Supplier<Integer> stageSupplier;\n-  protected Supplier<Long> attemptSupplier;\n+  protected Suppliers suppliers;\n+  protected Supplier<Integer> stageIdSupplier;\n+  protected Supplier<Long> attemptIdSupplier;\n \n   public CopyOnWriteLazyInsertIterable(Iterator<HoodieRecord<T>> sortedRecordItr, HoodieWriteConfig config,\n                                        String instantTime, HoodieTable<T> hoodieTable, String idPrefix,\n-                                       Supplier<Integer> idSupplier, Supplier<Integer> stageSupplier,\n-                                       Supplier<Long> attemptSupplier) {\n+                                       Suppliers suppliers) {\n     super(sortedRecordItr);\n     this.hoodieConfig = config;\n     this.instantTime = instantTime;\n     this.hoodieTable = hoodieTable;\n     this.idPrefix = idPrefix;\n     this.numFilesWritten = 0;\n-    this.idSupplier = idSupplier;\n-    this.stageSupplier = stageSupplier;\n-    this.attemptSupplier = attemptSupplier;\n+    this.suppliers = suppliers;\n   }\n \n   // Used for caching HoodieRecord along with insertValue. We need this to offload computation work to buffering thread.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY2MDU5NQ==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399660595", "bodyText": "ditto, IMO, id here is not clear. partition id is better.", "author": "yanghua", "createdAt": "2020-03-28T13:08:31Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -55,26 +55,32 @@\n   protected final String partitionPath;\n   protected final String fileId;\n   protected final String writeToken;\n+  protected final Supplier<Integer> idSupplier;", "originalCommit": "9ef6c852631c8344a77def77a9e8f8d81a67c283", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "934c41dea3b8931288f1b339f95140f5e8f5b048", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java b/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\nindex 3acf179a9..801a9bd6d 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\n\n@@ -55,13 +55,10 @@ public abstract class HoodieWriteHandle<T extends HoodieRecordPayload> extends H\n   protected final String partitionPath;\n   protected final String fileId;\n   protected final String writeToken;\n-  protected final Supplier<Integer> idSupplier;\n-  protected final Supplier<Integer> stageSupplier;\n-  protected final Supplier<Long> attemptSupplier;\n+  protected final Suppliers suppliers;\n \n   public HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String partitionPath,\n-                           String fileId, HoodieTable<T> hoodieTable, Supplier<Integer> idSupplier,\n-                           Supplier<Integer> stageSupplier, Supplier<Long> attemptSupplier) {\n+                           String fileId, HoodieTable<T> hoodieTable, Suppliers suppliers) {\n     super(config, instantTime, hoodieTable);\n     this.partitionPath = partitionPath;\n     this.fileId = fileId;\n"}}, {"oid": "934c41dea3b8931288f1b339f95140f5e8f5b048", "url": "https://github.com/apache/hudi/commit/934c41dea3b8931288f1b339f95140f5e8f5b048", "message": "address comments", "committedDate": "2020-03-28T15:21:28Z", "type": "commit"}, {"oid": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "url": "https://github.com/apache/hudi/commit/e9a3c06b0907886d62812d9ed5255bfa47babcd6", "message": "address comments", "committedDate": "2020-03-28T15:22:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY5OTUwNg==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399699506", "bodyText": "Should be interface be something generic like WriteTaskContextSupplier  which is extended by SparkTaskContextSupplier ?", "author": "vinothchandar", "createdAt": "2020-03-28T19:24:56Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {", "originalCommit": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\nsimilarity index 63%\nrename from hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nrename to hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\nindex 0efd28cf2..601dd98a2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\n\n@@ -24,22 +24,19 @@ import java.io.Serializable;\n import java.util.function.Supplier;\n \n /**\n- * Spark Supplier.\n+ * Spark task context supplier.\n  */\n-public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+public class SparkTaskContextSupplier implements Serializable {\n \n-  /**\n-   * Supplier to get partition id.\n-   */\n-  SparkTaskContextDetailSupplier<Integer> PARTITION_ID_SUPPLIER = () -> TaskContext.getPartitionId();\n+  public Supplier<Integer> getPartitionIdSupplier() {\n+    return () -> TaskContext.getPartitionId();\n+  }\n \n-  /**\n-   * Supplier to get stage id.\n-   */\n-  SparkTaskContextDetailSupplier<Integer> STAGE_ID_SUPPLIER = () -> TaskContext.get().stageId();\n+  public Supplier<Integer> getStageIdSupplier() {\n+    return () -> TaskContext.get().stageId();\n+  }\n \n-  /**\n-   * Supplier to get task attempt id.\n-   */\n-  SparkTaskContextDetailSupplier<Long> ATTEMPT_ID_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+  public Supplier<Long> getAttemptIdSupplier() {\n+    return () -> TaskContext.get().taskAttemptId();\n+  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY5OTc5NA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399699794", "bodyText": "I am not sure this abstraction is at the right level.. Should this have to be Supplier.. I think we can just have three methods that return Supplier<Integer> and Supplier<Long> and pass just one argument through the code path i.e the SparkTaskContextSuppler instance..", "author": "vinothchandar", "createdAt": "2020-03-28T19:27:59Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.TaskContext;\n+\n+import java.io.Serializable;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Spark Supplier.\n+ */\n+public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {", "originalCommit": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTczNzk5MA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399737990", "bodyText": "Just online seeing the latest changes comes from @leesf . Yes, it seems this is a better abstraction.", "author": "yanghua", "createdAt": "2020-03-29T03:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY5OTc5NA=="}], "type": "inlineReview", "revised_code": {"commit": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\nsimilarity index 63%\nrename from hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\nrename to hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\nindex 0efd28cf2..601dd98a2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextDetailSupplier.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/SparkTaskContextSupplier.java\n\n@@ -24,22 +24,19 @@ import java.io.Serializable;\n import java.util.function.Supplier;\n \n /**\n- * Spark Supplier.\n+ * Spark task context supplier.\n  */\n-public interface SparkTaskContextDetailSupplier<T> extends Supplier<T>, Serializable {\n+public class SparkTaskContextSupplier implements Serializable {\n \n-  /**\n-   * Supplier to get partition id.\n-   */\n-  SparkTaskContextDetailSupplier<Integer> PARTITION_ID_SUPPLIER = () -> TaskContext.getPartitionId();\n+  public Supplier<Integer> getPartitionIdSupplier() {\n+    return () -> TaskContext.getPartitionId();\n+  }\n \n-  /**\n-   * Supplier to get stage id.\n-   */\n-  SparkTaskContextDetailSupplier<Integer> STAGE_ID_SUPPLIER = () -> TaskContext.get().stageId();\n+  public Supplier<Integer> getStageIdSupplier() {\n+    return () -> TaskContext.get().stageId();\n+  }\n \n-  /**\n-   * Supplier to get task attempt id.\n-   */\n-  SparkTaskContextDetailSupplier<Long> ATTEMPT_ID_SUPPLIER = () -> TaskContext.get().taskAttemptId();\n+  public Supplier<Long> getAttemptIdSupplier() {\n+    return () -> TaskContext.get().taskAttemptId();\n+  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY5OTk0MQ==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399699941", "bodyText": "then this can go away.. I feel this is additional abstraction, that we may not need..", "author": "vinothchandar", "createdAt": "2020-03-28T19:29:06Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/Suppliers.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * A bundle of Suppliers.\n+ */\n+public class Suppliers implements Serializable {", "originalCommit": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/Suppliers.java b/hudi-client/src/main/java/org/apache/hudi/client/Suppliers.java\ndeleted file mode 100644\nindex 871b0ccf9..000000000\n--- a/hudi-client/src/main/java/org/apache/hudi/client/Suppliers.java\n+++ /dev/null\n\n@@ -1,48 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.client;\n-\n-import java.io.Serializable;\n-\n-/**\n- * A bundle of Suppliers.\n- */\n-public class Suppliers implements Serializable {\n-\n-  private SparkTaskContextDetailSupplier<Integer> partitionIdSupplier =\n-          SparkTaskContextDetailSupplier.PARTITION_ID_SUPPLIER;\n-\n-  private SparkTaskContextDetailSupplier<Integer> stageIdSupplier =\n-          SparkTaskContextDetailSupplier.STAGE_ID_SUPPLIER;\n-\n-  private SparkTaskContextDetailSupplier<Long> attemptIdSupplier =\n-          SparkTaskContextDetailSupplier.ATTEMPT_ID_SUPPLIER;\n-\n-  public SparkTaskContextDetailSupplier<Integer> getPartitionIdSupplier() {\n-    return partitionIdSupplier;\n-  }\n-\n-  public SparkTaskContextDetailSupplier<Integer> getStageIdSupplier() {\n-    return stageIdSupplier;\n-  }\n-\n-  public SparkTaskContextDetailSupplier<Long> getAttemptIdSupplier() {\n-    return attemptIdSupplier;\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcwMDA1OA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399700058", "bodyText": "side point: we should fix method arg formatting  consistently between intellij and checkstyle. Keep seeing these sort of whitespace changes in PRs.", "author": "vinothchandar", "createdAt": "2020-03-28T19:30:23Z", "path": "hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java", "diffHunk": "@@ -35,8 +36,9 @@\n public class MergeOnReadLazyInsertIterable<T extends HoodieRecordPayload> extends CopyOnWriteLazyInsertIterable<T> {\n \n   public MergeOnReadLazyInsertIterable(Iterator<HoodieRecord<T>> sortedRecordItr, HoodieWriteConfig config,\n-      String instantTime, HoodieTable<T> hoodieTable, String idPfx) {\n-    super(sortedRecordItr, config, instantTime, hoodieTable, idPfx);\n+                                       String instantTime, HoodieTable<T> hoodieTable, String idPfx,", "originalCommit": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java b/hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java\nindex 2b2ea0e6e..02a9eada8 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/execution/MergeOnReadLazyInsertIterable.java\n\n@@ -36,9 +36,8 @@ import java.util.List;\n public class MergeOnReadLazyInsertIterable<T extends HoodieRecordPayload> extends CopyOnWriteLazyInsertIterable<T> {\n \n   public MergeOnReadLazyInsertIterable(Iterator<HoodieRecord<T>> sortedRecordItr, HoodieWriteConfig config,\n-                                       String instantTime, HoodieTable<T> hoodieTable, String idPfx,\n-                                       Suppliers suppliers) {\n-    super(sortedRecordItr, config, instantTime, hoodieTable, idPfx, suppliers);\n+      String instantTime, HoodieTable<T> hoodieTable, String idPfx, SparkTaskContextSupplier sparkTaskContextSupplier) {\n+    super(sortedRecordItr, config, instantTime, hoodieTable, idPfx, sparkTaskContextSupplier);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTcwMDE4NA==", "url": "https://github.com/apache/hudi/pull/1460#discussion_r399700184", "bodyText": "rename to just makeWriteToken()?", "author": "vinothchandar", "createdAt": "2020-03-28T19:31:33Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -55,26 +55,27 @@\n   protected final String partitionPath;\n   protected final String fileId;\n   protected final String writeToken;\n+  protected final Suppliers suppliers;\n \n   public HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String partitionPath,\n-                           String fileId, HoodieTable<T> hoodieTable) {\n+                           String fileId, HoodieTable<T> hoodieTable, Suppliers suppliers) {\n     super(config, instantTime, hoodieTable);\n     this.partitionPath = partitionPath;\n     this.fileId = fileId;\n-    this.writeToken = makeSparkWriteToken();\n     this.originalSchema = new Schema.Parser().parse(config.getSchema());\n     this.writerSchema = createHoodieWriteSchema(originalSchema);\n     this.timer = new HoodieTimer().startTimer();\n     this.writeStatus = (WriteStatus) ReflectionUtils.loadClass(config.getWriteStatusClassName(),\n         !hoodieTable.getIndex().isImplicitWithStorage(), config.getWriteStatusFailureFraction());\n+    this.suppliers = suppliers;\n+    this.writeToken = makeSparkWriteToken();", "originalCommit": "e9a3c06b0907886d62812d9ed5255bfa47babcd6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java b/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\nindex 801a9bd6d..dd67a6ac1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java\n\n@@ -55,10 +55,10 @@ public abstract class HoodieWriteHandle<T extends HoodieRecordPayload> extends H\n   protected final String partitionPath;\n   protected final String fileId;\n   protected final String writeToken;\n-  protected final Suppliers suppliers;\n+  protected final SparkTaskContextSupplier sparkTaskContextSupplier;\n \n   public HoodieWriteHandle(HoodieWriteConfig config, String instantTime, String partitionPath,\n-                           String fileId, HoodieTable<T> hoodieTable, Suppliers suppliers) {\n+                           String fileId, HoodieTable<T> hoodieTable, SparkTaskContextSupplier sparkTaskContextSupplier) {\n     super(config, instantTime, hoodieTable);\n     this.partitionPath = partitionPath;\n     this.fileId = fileId;\n"}}, {"oid": "3d215b83c25e3df548d44d8a8c4d20a16efbf863", "url": "https://github.com/apache/hudi/commit/3d215b83c25e3df548d44d8a8c4d20a16efbf863", "message": "address comments", "committedDate": "2020-03-29T05:09:06Z", "type": "commit"}, {"oid": "07f073a7b0c5e326524961105fed84a2b92e2c18", "url": "https://github.com/apache/hudi/commit/07f073a7b0c5e326524961105fed84a2b92e2c18", "message": "format", "committedDate": "2020-03-29T05:12:15Z", "type": "commit"}]}