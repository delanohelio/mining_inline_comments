{"pr_number": 2014, "pr_title": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured", "pr_createdAt": "2020-08-24T02:07:54Z", "pr_url": "https://github.com/apache/hudi/pull/2014", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwNjU1MQ==", "url": "https://github.com/apache/hudi/pull/2014#discussion_r475306551", "bodyText": "Not throwing an explicit error here, the only other value it can potentially have is Bootstrap based on the enum, the issue which exposed this issue would have thrown an exception on WriteOperationType.fromValue itself.\nCan change to throw a HoodieException, if the reviewer feels that is necessary", "author": "sreeram26", "createdAt": "2020-08-24T02:10:29Z", "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {\n+    if (operation == WriteOperationType.BULK_INSERT) {\n       Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n           createUserDefinedBulkInsertPartitioner(client.getConfig());\n       return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) {\n+    } else if (operation == WriteOperationType.INSERT) {\n       return client.insert(hoodieRecords, instantTime);\n     } else {\n       // default is upsert\n+      if (operation != WriteOperationType.UPSERT) {", "originalCommit": "7f81b53266a38e931650b31075e919f2a180d0e5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzMjIxNQ==", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477432215", "bodyText": "@sreeram26 : I think it is better to throw an exception in this case. We saw some cases where uses intended to use bulk insert but had a typo in the operation type.", "author": "bvaradar", "createdAt": "2020-08-26T16:30:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwNjU1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "3183b5ce0acc13603846b015102380a65df1bd19", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\nindex 53799fb50..0115f226f 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n+++ b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n\n@@ -250,18 +250,17 @@ public class DataSourceUtils {\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n       String instantTime, WriteOperationType operation) throws HoodieException {\n-    if (operation == WriteOperationType.BULK_INSERT) {\n-      Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n-          createUserDefinedBulkInsertPartitioner(client.getConfig());\n-      return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation == WriteOperationType.INSERT) {\n-      return client.insert(hoodieRecords, instantTime);\n-    } else {\n-      // default is upsert\n-      if (operation != WriteOperationType.UPSERT) {\n-        LOG.warn(\"Not a valid operation type for doWriteOperation: \" + operation.toString() + \" using default operation: UPSERT instead\");\n-      }\n-      return client.upsert(hoodieRecords, instantTime);\n+    switch (operation) {\n+      case BULK_INSERT:\n+        Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n+                createUserDefinedBulkInsertPartitioner(client.getConfig());\n+        return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n+      case INSERT:\n+        return client.insert(hoodieRecords, instantTime);\n+      case UPSERT:\n+        return client.upsert(hoodieRecords, instantTime);\n+      default:\n+        throw new HoodieException(\"Not a valid operation type for doWriteOperation: \" + operation.toString());\n     }\n   }\n \n"}}, {"oid": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "url": "https://github.com/apache/hudi/commit/24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "message": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured", "committedDate": "2020-08-24T02:10:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzNTM4OA==", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477435388", "bodyText": "Also, since we have changed this to enum, can you also update DataSourceWriteOptions to reference this enum. It is good that both the enum values and those defined in DataSourceWriteOptions are consistent. So, we can do this without any backwards compatibility issues.\n  val OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n  val BULK_INSERT_OPERATION_OPT_VAL = WriteOperationType.BULK_INSERT.name\n  val INSERT_OPERATION_OPT_VAL = WriteOperationType.INSERT.name\n  val UPSERT_OPERATION_OPT_VAL = WriteOperationType.UPSERT.name\n  val DELETE_OPERATION_OPT_VAL = WriteOperationType.DELETE.name\n  val BOOTSTRAP_OPERATION_OPT_VAL = WriteOperationType.BOOTSTRAP.name\n  val DEFAULT_OPERATION_OPT_VAL = UPSERT_OPERATION_OPT_VAL", "author": "bvaradar", "createdAt": "2020-08-26T16:35:16Z", "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {", "originalCommit": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3183b5ce0acc13603846b015102380a65df1bd19", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\nindex 53799fb50..0115f226f 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n+++ b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n\n@@ -250,18 +250,17 @@ public class DataSourceUtils {\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n       String instantTime, WriteOperationType operation) throws HoodieException {\n-    if (operation == WriteOperationType.BULK_INSERT) {\n-      Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n-          createUserDefinedBulkInsertPartitioner(client.getConfig());\n-      return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation == WriteOperationType.INSERT) {\n-      return client.insert(hoodieRecords, instantTime);\n-    } else {\n-      // default is upsert\n-      if (operation != WriteOperationType.UPSERT) {\n-        LOG.warn(\"Not a valid operation type for doWriteOperation: \" + operation.toString() + \" using default operation: UPSERT instead\");\n-      }\n-      return client.upsert(hoodieRecords, instantTime);\n+    switch (operation) {\n+      case BULK_INSERT:\n+        Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n+                createUserDefinedBulkInsertPartitioner(client.getConfig());\n+        return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n+      case INSERT:\n+        return client.insert(hoodieRecords, instantTime);\n+      case UPSERT:\n+        return client.upsert(hoodieRecords, instantTime);\n+      default:\n+        throw new HoodieException(\"Not a valid operation type for doWriteOperation: \" + operation.toString());\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzOTE3Ng==", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477439176", "bodyText": "Suggestion : Instead of if-else, by using switch-case, we can take advantage of checkstyle to ensure we are not missing handling of any operation types .", "author": "bvaradar", "createdAt": "2020-08-26T16:40:43Z", "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {\n+    if (operation == WriteOperationType.BULK_INSERT) {", "originalCommit": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3183b5ce0acc13603846b015102380a65df1bd19", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\nindex 53799fb50..0115f226f 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n+++ b/hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\n\n@@ -250,18 +250,17 @@ public class DataSourceUtils {\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n       String instantTime, WriteOperationType operation) throws HoodieException {\n-    if (operation == WriteOperationType.BULK_INSERT) {\n-      Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n-          createUserDefinedBulkInsertPartitioner(client.getConfig());\n-      return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation == WriteOperationType.INSERT) {\n-      return client.insert(hoodieRecords, instantTime);\n-    } else {\n-      // default is upsert\n-      if (operation != WriteOperationType.UPSERT) {\n-        LOG.warn(\"Not a valid operation type for doWriteOperation: \" + operation.toString() + \" using default operation: UPSERT instead\");\n-      }\n-      return client.upsert(hoodieRecords, instantTime);\n+    switch (operation) {\n+      case BULK_INSERT:\n+        Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n+                createUserDefinedBulkInsertPartitioner(client.getConfig());\n+        return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n+      case INSERT:\n+        return client.insert(hoodieRecords, instantTime);\n+      case UPSERT:\n+        return client.upsert(hoodieRecords, instantTime);\n+      default:\n+        throw new HoodieException(\"Not a valid operation type for doWriteOperation: \" + operation.toString());\n     }\n   }\n \n"}}, {"oid": "3183b5ce0acc13603846b015102380a65df1bd19", "url": "https://github.com/apache/hudi/commit/3183b5ce0acc13603846b015102380a65df1bd19", "message": "Address Balaji comments", "committedDate": "2020-08-27T17:30:04Z", "type": "forcePushed"}, {"oid": "764d5eedbb8b14874881885b874b99012c744b35", "url": "https://github.com/apache/hudi/commit/764d5eedbb8b14874881885b874b99012c744b35", "message": "Address Balaji comments", "committedDate": "2020-08-27T20:53:05Z", "type": "forcePushed"}, {"oid": "6692b6a99becf955c1154778aa83a48a7d9758f5", "url": "https://github.com/apache/hudi/commit/6692b6a99becf955c1154778aa83a48a7d9758f5", "message": "Move from name to value", "committedDate": "2020-08-31T00:28:22Z", "type": "forcePushed"}, {"oid": "cc35f0db20e882b7c26d3296ec161dcb935e2ed7", "url": "https://github.com/apache/hudi/commit/cc35f0db20e882b7c26d3296ec161dcb935e2ed7", "message": "Move from name to value", "committedDate": "2020-08-31T00:36:16Z", "type": "forcePushed"}, {"oid": "fa63b1edbc2409c00b41de016c4d486e93d35e77", "url": "https://github.com/apache/hudi/commit/fa63b1edbc2409c00b41de016c4d486e93d35e77", "message": "Add back value", "committedDate": "2020-08-31T16:06:07Z", "type": "forcePushed"}, {"oid": "9e1c006597804a92d5cf5bbf7698ab4dcc75b3e5", "url": "https://github.com/apache/hudi/commit/9e1c006597804a92d5cf5bbf7698ab4dcc75b3e5", "message": "Add back value", "committedDate": "2020-08-31T16:08:32Z", "type": "forcePushed"}, {"oid": "48fe545f43fe5f5c17febfaef444b868bc456014", "url": "https://github.com/apache/hudi/commit/48fe545f43fe5f5c17febfaef444b868bc456014", "message": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured", "committedDate": "2020-09-02T01:12:29Z", "type": "commit"}, {"oid": "0ce15085a16acc58519c2d0576dacc1ec5056a6a", "url": "https://github.com/apache/hudi/commit/0ce15085a16acc58519c2d0576dacc1ec5056a6a", "message": "Address Balaji comments", "committedDate": "2020-09-02T01:12:29Z", "type": "commit"}, {"oid": "0a54d48a1981e38fef599ca9d40565af47b9bbc2", "url": "https://github.com/apache/hudi/commit/0a54d48a1981e38fef599ca9d40565af47b9bbc2", "message": "Move from name to value", "committedDate": "2020-09-02T01:12:30Z", "type": "commit"}, {"oid": "1871d6f5e520f4776d60394dc585e33dfa2ac7d7", "url": "https://github.com/apache/hudi/commit/1871d6f5e520f4776d60394dc585e33dfa2ac7d7", "message": "move to toString instead", "committedDate": "2020-09-02T01:12:30Z", "type": "commit"}, {"oid": "f8fccb0ff2fbc4e374774f178474b04571626b97", "url": "https://github.com/apache/hudi/commit/f8fccb0ff2fbc4e374774f178474b04571626b97", "message": "Add back value", "committedDate": "2020-09-02T01:12:30Z", "type": "commit"}, {"oid": "20d18830fefcbc66c19e3c7484ea66f93f6d6974", "url": "https://github.com/apache/hudi/commit/20d18830fefcbc66c19e3c7484ea66f93f6d6974", "message": "fix linter", "committedDate": "2020-09-02T01:12:30Z", "type": "commit"}, {"oid": "20d18830fefcbc66c19e3c7484ea66f93f6d6974", "url": "https://github.com/apache/hudi/commit/20d18830fefcbc66c19e3c7484ea66f93f6d6974", "message": "fix linter", "committedDate": "2020-09-02T01:12:30Z", "type": "forcePushed"}, {"oid": "811fdc7b8c5352259259ccf36187be69f5c4ed10", "url": "https://github.com/apache/hudi/commit/811fdc7b8c5352259259ccf36187be69f5c4ed10", "message": "Removing toString() as the avro representative has it as string with a weird conversion logic", "committedDate": "2020-09-02T17:16:17Z", "type": "commit"}, {"oid": "e2f7564821b9ea026a460dd5f0693be2a44c19dd", "url": "https://github.com/apache/hudi/commit/e2f7564821b9ea026a460dd5f0693be2a44c19dd", "message": "Trigger build", "committedDate": "2020-09-03T00:31:30Z", "type": "commit"}, {"oid": "cf330f47888857a03e0b26f20b75606aa0f69c85", "url": "https://github.com/apache/hudi/commit/cf330f47888857a03e0b26f20b75606aa0f69c85", "message": "Trigger notification", "committedDate": "2020-09-03T03:52:06Z", "type": "commit"}]}