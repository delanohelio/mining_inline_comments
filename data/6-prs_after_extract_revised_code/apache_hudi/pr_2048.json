{"pr_number": 2048, "pr_title": "[HUDI-1072] Introduce REPLACE top level action", "pr_createdAt": "2020-08-27T23:48:41Z", "pr_url": "https://github.com/apache/hudi/pull/2048", "timeline": [{"oid": "d259d1f041753ff2b88c13870b03979734942363", "url": "https://github.com/apache/hudi/commit/d259d1f041753ff2b88c13870b03979734942363", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-28T04:05:31Z", "type": "forcePushed"}, {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "url": "https://github.com/apache/hudi/commit/c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-28T05:24:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyNjUwMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479626501", "bodyText": "As discussed, lets retain all the file-groups but perform filtering in the get APIs. THis would avoid correctness issues in filtering and also makes handling incremental file system view easier.", "author": "bvaradar", "createdAt": "2020-08-29T08:40:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -173,29 +180,59 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     List<HoodieFileGroup> fileGroups = new ArrayList<>();\n     fileIdSet.forEach(pair -> {\n       String fileId = pair.getValue();\n-      HoodieFileGroup group = new HoodieFileGroup(pair.getKey(), fileId, timeline);\n-      if (baseFiles.containsKey(pair)) {\n-        baseFiles.get(pair).forEach(group::addBaseFile);\n-      }\n-      if (logFiles.containsKey(pair)) {\n-        logFiles.get(pair).forEach(group::addLogFile);\n-      }\n+      String partitionPath = pair.getKey();\n+      if (isExcludeFileGroup(partitionPath, fileId)) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDE3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364171", "bodyText": "Modified. Please take a look. RocksDB needs to be implemented.", "author": "satishkotha", "createdAt": "2020-09-01T18:56:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyNjUwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex f8f80278d..8fa9eebd6 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -174,6 +172,8 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n       return Pair.of(partitionPathStr, logFile.getFileId());\n     }));\n \n+    Map<HoodieFileGroupId, HoodieInstant> fileGroupToReplaceInstant = getFileGroupToReplaceInstantMapping(visibleCommitsAndCompactionTimeline);\n+\n     Set<Pair<String, String>> fileIdSet = new HashSet<>(baseFiles.keySet());\n     fileIdSet.addAll(logFiles.keySet());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyOTcwOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479629709", "bodyText": "Why are we not tracking dropped fileIds ?", "author": "bvaradar", "createdAt": "2020-08-29T09:18:25Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a single Hoodie replace operation.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceStat extends HoodieWriteStat {\n+\n+  // records from the 'getFileId()' can be written to multiple new file groups. This list tracks all new fileIds\n+  private List<String> newFileIds;", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDQ2Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364462", "bodyText": "its part of HoodieWriteStat#fileId", "author": "satishkotha", "createdAt": "2020-09-01T18:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyOTcwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java\nindex 815ea7b78..e88166394 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java\n\n@@ -20,29 +20,14 @@ package org.apache.hudi.common.model;\n \n import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n \n-import java.util.Collections;\n-import java.util.List;\n-\n /**\n  * Statistics about a single Hoodie replace operation.\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class HoodieReplaceStat extends HoodieWriteStat {\n \n-  // records from the 'getFileId()' can be written to multiple new file groups. This list tracks all new fileIds\n-  private List<String> newFileIds;\n-\n   public HoodieReplaceStat() {\n     // called by jackson json lib\n-    newFileIds = Collections.emptyList();\n-  }\n-\n-  public void setNewFileIds(List<String> fileIds) {\n-    this.newFileIds = fileIds;\n-  }\n-\n-  public List<String> getNewFileIds() {\n-    return Collections.unmodifiableList(this.newFileIds);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDE0MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630141", "bodyText": "getAllExcludeFileGroups -> getReplacedFileGroups ?", "author": "bvaradar", "createdAt": "2020-08-29T09:23:53Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java", "diffHunk": "@@ -355,6 +357,18 @@ public RemoteHoodieTableFileSystemView(String server, int port, HoodieTableMetaC\n     }\n   }\n \n+  @Override\n+  public Stream<String> getAllExcludeFileGroups(final String partitionPath) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDUzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364537", "bodyText": "changed", "author": "satishkotha", "createdAt": "2020-09-01T18:57:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDE0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java\nindex 160745226..5bc371ffa 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java\n\n@@ -357,18 +357,6 @@ public class RemoteHoodieTableFileSystemView implements SyncableFileSystemView,\n     }\n   }\n \n-  @Override\n-  public Stream<String> getAllExcludeFileGroups(final String partitionPath) {\n-    Map<String, String> paramsMap = getParamsWithPartitionPath(partitionPath);\n-    try {\n-      List<String> fileGroups = executeRequest(ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL, paramsMap,\n-          new TypeReference<List<String>>() {}, RequestMethod.GET);\n-      return fileGroups.stream();\n-    } catch (IOException e) {\n-      throw new HoodieRemoteException(e);\n-    }\n-  }\n-\n   public boolean refresh() {\n     Map<String, String> paramsMap = getParams();\n     try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDIxMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630211", "bodyText": "ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL -> REPLACED_FILEGROUPS_FOR_PARTITION_URL\nLets use a single consistent name \"replaced\" instead of exclude everywhere.", "author": "bvaradar", "createdAt": "2020-08-29T09:24:45Z", "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java", "diffHunk": "@@ -284,6 +284,13 @@ private void registerFileSlicesAPI() {\n       writeValueAsString(ctx, dtos);\n     }, true));\n \n+    app.get(RemoteHoodieTableFileSystemView.ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL, new ViewHandler(ctx -> {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDU3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364575", "bodyText": "changed", "author": "satishkotha", "createdAt": "2020-09-01T18:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDIxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java\nindex 1b5a12e64..683eb0658 100644\n--- a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java\n+++ b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java\n\n@@ -284,13 +284,6 @@ public class FileSystemViewHandler {\n       writeValueAsString(ctx, dtos);\n     }, true));\n \n-    app.get(RemoteHoodieTableFileSystemView.ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL, new ViewHandler(ctx -> {\n-      List<String> excludeFileGroups = sliceHandler.getExcludeFileGroups(\n-          ctx.validatedQueryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM).getOrThrow(),\n-          ctx.queryParam(RemoteHoodieTableFileSystemView.PARTITION_PARAM,\"\"));\n-      writeValueAsString(ctx, excludeFileGroups);\n-    }, true));\n-\n     app.post(RemoteHoodieTableFileSystemView.REFRESH_TABLE, new ViewHandler(ctx -> {\n       boolean success = sliceHandler\n           .refreshTable(ctx.validatedQueryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM).getOrThrow());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDMxMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630310", "bodyText": "why is this needed ?", "author": "bvaradar", "createdAt": "2020-08-29T09:25:46Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -104,4 +104,8 @@ protected SyncableFileSystemView getFileSystemViewWithUnCommittedSlices(HoodieTa\n   protected HoodieTableType getTableType() {\n     return HoodieTableType.COPY_ON_WRITE;\n   }\n+\n+  protected boolean areTimeTravelTestsEnabled() {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NDk5NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481364995", "bodyText": "RocksDB and Remote FileSystemViews are not implemented yet. So i temporarily disabled those tests. Moved this method to view tests instead of common", "author": "satishkotha", "createdAt": "2020-09-01T18:57:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDMxMA=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java\nindex f0b2f6d24..0aad0c2b3 100644\n--- a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java\n+++ b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java\n\n@@ -104,8 +104,4 @@ public class HoodieCommonTestHarness {\n   protected HoodieTableType getTableType() {\n     return HoodieTableType.COPY_ON_WRITE;\n   }\n-\n-  protected boolean areTimeTravelTestsEnabled() {\n-    return true;\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTExNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631115", "bodyText": "Are you planning to address this TODO as part of this PR ?", "author": "bvaradar", "createdAt": "2020-08-29T09:36:10Z", "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +102,7 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(), HoodieTimeline.COMMIT_ACTION); //TODO get action type from HoodieWriterCommitMessage", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTA4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365080", "bodyText": "Fixed", "author": "satishkotha", "createdAt": "2020-09-01T18:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTExNQ=="}], "type": "inlineReview", "revised_code": {"commit": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java b/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\nindex dff7be7e3..4012ff1dd 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\n+++ b/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\n\n@@ -102,7 +103,8 @@ public class HoodieDataSourceInternalWriter implements DataSourceWriter {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty(), HoodieTimeline.COMMIT_ACTION); //TODO get action type from HoodieWriterCommitMessage\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(),\n+          DataSourceUtils.getCommitActionType(operationType.toString(), metaClient));\n     } catch (Exception ioe) {\n       throw new HoodieException(ioe.getMessage(), ioe);\n     } finally {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTcyNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631725", "bodyText": "Rename to getReplacedFileGroups", "author": "bvaradar", "createdAt": "2020-08-29T09:43:46Z", "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java", "diffHunk": "@@ -89,6 +89,10 @@ public FileSliceHandler(Configuration conf, FileSystemViewManager viewManager) t\n         .collect(Collectors.toList());\n   }\n \n+  public List<String> getExcludeFileGroups(String basePath, String partitionPath) {", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTE0NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365144", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-09-01T18:58:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTcyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java\nindex e8e05818e..4692dbcd9 100644\n--- a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java\n+++ b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java\n\n@@ -89,10 +88,6 @@ public class FileSliceHandler extends Handler {\n         .collect(Collectors.toList());\n   }\n \n-  public List<String> getExcludeFileGroups(String basePath, String partitionPath) {\n-    return viewManager.getFileSystemView(basePath).getAllExcludeFileGroups(partitionPath).collect(Collectors.toList());\n-  }\n-\n   public boolean refreshTable(String basePath) {\n     viewManager.clearFileSystemView(basePath);\n     return true;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjMxMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632310", "bodyText": "Please follow the same structure like the one we did for compaction.", "author": "bvaradar", "createdAt": "2020-08-29T09:51:09Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java", "diffHunk": "@@ -77,6 +79,13 @@ public SpillableMapBasedFileSystemView(HoodieTableMetaClient metaClient, HoodieT\n     }\n   }\n \n+  @Override\n+  protected Map<String, Set<String>> createPartitionToExcludeFileGroups() {\n+    // TODO should we create another spillable directory under baseStoreDir?\n+    // the exclude file group is expected to be small, so use parent class in-memory representation\n+    return super.createPartitionToExcludeFileGroups();", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTc0Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365742", "bodyText": "Please take a look if i did it correctly. (To be honest, dont fully understand compaction implementation in great detail)", "author": "satishkotha", "createdAt": "2020-09-01T18:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjMxMA=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java\nindex e6052d58e..f6c901e20 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java\n\n@@ -79,13 +78,6 @@ public class SpillableMapBasedFileSystemView extends HoodieTableFileSystemView {\n     }\n   }\n \n-  @Override\n-  protected Map<String, Set<String>> createPartitionToExcludeFileGroups() {\n-    // TODO should we create another spillable directory under baseStoreDir?\n-    // the exclude file group is expected to be small, so use parent class in-memory representation\n-    return super.createPartitionToExcludeFileGroups();\n-  }\n-\n   @Override\n   protected Map<HoodieFileGroupId, Pair<String, CompactionOperation>> createFileIdToPendingCompactionMap(\n       Map<HoodieFileGroupId, Pair<String, CompactionOperation>> fgIdToPendingCompaction) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjM4MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632381", "bodyText": "I think I have mentioned it somewhere else. Lets denote this feature with the consistent term \"Replace\" everywhere.", "author": "bvaradar", "createdAt": "2020-08-29T09:52:04Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java", "diffHunk": "@@ -148,6 +148,9 @@\n    */\n   Stream<HoodieFileGroup> getAllFileGroups(String partitionPath);\n \n+  Stream<String> getAllExcludeFileGroups(String partitionPath);", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTgyNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365825", "bodyText": "Done", "author": "satishkotha", "createdAt": "2020-09-01T18:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjM4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1f5f737ee065b08979dca06d3153082806710057", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java\nindex 0719f52f6..5b12416a8 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java\n\n@@ -148,8 +148,6 @@ public interface TableFileSystemView {\n    */\n   Stream<HoodieFileGroup> getAllFileGroups(String partitionPath);\n \n-  Stream<String> getAllExcludeFileGroups(String partitionPath);\n-\n \n   /**\n    * Return Pending Compaction Operations.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzNDgxMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479634813", "bodyText": "Instead of direct listStatus, can you use FileSystemViewAbstraction to get the file-group and then delete each files in it ? THis way, once consolidated metadata becomes available, you can take advantage of that. cc @prashantwason", "author": "bvaradar", "createdAt": "2020-08-29T10:22:24Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,44 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieCommitMetadata metadata = HoodieCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().entrySet().stream().forEach(entry ->\n+            deleteFileGroups(entry.getKey(), entry.getValue().stream().map(e -> e.getFileId()).collect(Collectors.toSet()), instant)\n+        );\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private void deleteFileGroups(String partitionPath, Set<String> fileIdsToDelete, HoodieInstant instant) {\n+    try {\n+      FileStatus[] statuses = metaClient.getFs().listStatus(FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPath));", "originalCommit": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM2NTk4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r481365985", "bodyText": "Modified to use FileSystemViews", "author": "satishkotha", "createdAt": "2020-09-01T18:59:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzNDgxMw=="}], "type": "inlineReview", "revised_code": {"commit": "aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 6c4a0cd0c..77dace426 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -314,9 +315,9 @@ public class HoodieTimelineArchiveLog {\n \n     replaceInstantOption.ifPresent(replaceInstant -> {\n       try {\n-        HoodieCommitMetadata metadata = HoodieCommitMetadata.fromBytes(\n+        HoodieReplaceMetadata metadata = HoodieReplaceMetadata.fromBytes(\n             metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n-            HoodieCommitMetadata.class);\n+            HoodieReplaceMetadata.class);\n \n         metadata.getPartitionToReplaceStats().entrySet().stream().forEach(entry ->\n             deleteFileGroups(entry.getKey(), entry.getValue().stream().map(e -> e.getFileId()).collect(Collectors.toSet()), instant)\n"}}, {"oid": "1f5f737ee065b08979dca06d3153082806710057", "url": "https://github.com/apache/hudi/commit/1f5f737ee065b08979dca06d3153082806710057", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-08-31T07:33:49Z", "type": "forcePushed"}, {"oid": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "url": "https://github.com/apache/hudi/commit/849d7880b8b3dd7b32abb8ab313f58b7793c604f", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T00:40:04Z", "type": "forcePushed"}, {"oid": "aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "url": "https://github.com/apache/hudi/commit/aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T00:47:33Z", "type": "forcePushed"}, {"oid": "28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "url": "https://github.com/apache/hudi/commit/28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T17:15:17Z", "type": "forcePushed"}, {"oid": "f04fbc6bc375ec29271a449dd870cb2652f85626", "url": "https://github.com/apache/hudi/commit/f04fbc6bc375ec29271a449dd870cb2652f85626", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-01T20:22:50Z", "type": "forcePushed"}, {"oid": "087909fe7d2000171a551be12c8ffa79b6ec6250", "url": "https://github.com/apache/hudi/commit/087909fe7d2000171a551be12c8ffa79b6ec6250", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T00:59:47Z", "type": "forcePushed"}, {"oid": "f5b19ed6194364a9125f7b7110ed49ae5090b66c", "url": "https://github.com/apache/hudi/commit/f5b19ed6194364a9125f7b7110ed49ae5090b66c", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T18:06:39Z", "type": "forcePushed"}, {"oid": "6bd5bce92282f4131c0349083174e3e0b68a2f1d", "url": "https://github.com/apache/hudi/commit/6bd5bce92282f4131c0349083174e3e0b68a2f1d", "message": "[HUDI-1072] Introduce REPLACE top level action", "committedDate": "2020-09-02T18:59:17Z", "type": "forcePushed"}, {"oid": "00b151b7ed0b419146fabb9b85122903aaada57f", "url": "https://github.com/apache/hudi/commit/00b151b7ed0b419146fabb9b85122903aaada57f", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-04T19:41:30Z", "type": "forcePushed"}, {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b", "url": "https://github.com/apache/hudi/commit/6c9793056d885a74c17e3d0275d348d28684e63b", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-04T20:23:41Z", "type": "forcePushed"}, {"oid": "f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "url": "https://github.com/apache/hudi/commit/f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-10T18:58:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjM2OTQ0MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r486369440", "bodyText": "can't we just call commit(String, JavaRDD, Option.empty()) without having to implement this?", "author": "vinothchandar", "createdAt": "2020-09-10T14:03:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,55 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);", "originalCommit": "6c9793056d885a74c17e3d0275d348d28684e63b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\nindex 4719bca99..75369035b 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n\n@@ -88,9 +89,7 @@ public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload> e\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n-    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+    return commit(instantTime, writeStatuses, Option.empty());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTU3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541578", "bodyText": "rename: to just buildMetdata() , Commit is already  implicit from context.", "author": "vinothchandar", "createdAt": "2020-09-13T15:14:50Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replaceStats = writeStatuses.filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, replaceStats);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, List<HoodieWriteStat> replaceStats) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n     HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-\n+    HoodieCommitMetadata metadata = CommitUtils.buildWriteActionMetadata(stats, replaceStats, extraMetadata, operationType, config.getSchema(), commitActionType);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\nindex f89a5515d..75369035b 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n\n@@ -88,9 +89,7 @@ public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload> e\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n-    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+    return commit(instantTime, writeStatuses, Option.empty());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTY2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541663", "bodyText": "note to self: lets see if we can simply the commit() overloaded methods.", "author": "vinothchandar", "createdAt": "2020-09-13T15:15:30Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\nindex f89a5515d..75369035b 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n\n@@ -88,9 +89,7 @@ public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload> e\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n-    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+    return commit(instantTime, writeStatuses, Option.empty());\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTg2NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541865", "bodyText": "rename: writeInputRecords()", "author": "vinothchandar", "createdAt": "2020-09-13T15:17:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -95,6 +93,13 @@ public HoodieWriteMetadata execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n+    JavaRDD<WriteStatus> writeStatusRDD = processInputRecords(inputRecordsRDD, profile);\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, result);\n+    return result;\n+  }\n+\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\nindex 9c0bd6d3a..e8bf0a0d1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n\n@@ -93,13 +93,13 @@ public abstract class BaseCommitActionExecutor<T extends HoodieRecordPayload<T>,\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n-    JavaRDD<WriteStatus> writeStatusRDD = processInputRecords(inputRecordsRDD, profile);\n+    JavaRDD<WriteStatus> writeStatusRDD = writeInputRecords(inputRecordsRDD, profile);\n     HoodieWriteMetadata result = new HoodieWriteMetadata();\n     updateIndexAndCommitIfNeeded(writeStatusRDD, result);\n     return result;\n   }\n \n-  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+  protected JavaRDD<WriteStatus> writeInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n     // partition using the insert partitioner\n     final Partitioner partitioner = getPartitioner(profile);\n     JavaRDD<HoodieRecord<T>> partitionedRecords = partition(inputRecordsRDD, partitioner);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjA3Mg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542072", "bodyText": "note to self: see if there is a way to avoid repeating this filtering here again", "author": "vinothchandar", "createdAt": "2020-09-13T15:19:03Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\nindex 9c0bd6d3a..e8bf0a0d1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n\n@@ -212,27 +212,21 @@ public abstract class BaseCommitActionExecutor<T extends HoodieRecordPayload<T>,\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n-    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplaced()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplaced()).map(WriteStatus::getStat).collect();\n     commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n     String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n-    // Create a Hoodie table which encapsulated the commits and files visible\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-\n-\n     result.setCommitted(true);\n     result.setWriteStats(writeStats);\n-    result.setReplaceStats(replaceStats);\n-\n     // Finalize write\n     finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      HoodieCommitMetadata metadata = completeInstant(writeStats, replaceStats, extraMetadata);\n       result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjI0MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542241", "bodyText": "rename: completeInstant(), its better to stay close to what the method is doing; using just one terminology", "author": "vinothchandar", "createdAt": "2020-09-13T15:20:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);\n+    finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),\n-          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-      LOG.info(\"Committed \" + instantTime);\n+      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n           e);\n     }\n-    result.setCommitMetadata(Option.of(metadata));\n+  }\n+\n+  private HoodieCommitMetadata writeInstant(List<HoodieWriteStat>  writeStats, List<HoodieWriteStat> replaceStats, Option<Map<String, String>> extraMetadata) throws IOException {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\nindex 9c0bd6d3a..e8bf0a0d1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n\n@@ -212,27 +212,21 @@ public abstract class BaseCommitActionExecutor<T extends HoodieRecordPayload<T>,\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n-    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplaced()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplaced()).map(WriteStatus::getStat).collect();\n     commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n     String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n-    // Create a Hoodie table which encapsulated the commits and files visible\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-\n-\n     result.setCommitted(true);\n     result.setWriteStats(writeStats);\n-    result.setReplaceStats(replaceStats);\n-\n     // Finalize write\n     finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      HoodieCommitMetadata metadata = completeInstant(writeStats, replaceStats, extraMetadata);\n       result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Mjc2OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542769", "bodyText": "nts: need to ensure the operation type  is properly set", "author": "vinothchandar", "createdAt": "2020-09-13T15:25:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\nindex 9c0bd6d3a..e8bf0a0d1 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java\n\n@@ -212,27 +212,21 @@ public abstract class BaseCommitActionExecutor<T extends HoodieRecordPayload<T>,\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n-    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplaced()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplaced()).map(WriteStatus::getStat).collect();\n     commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n     String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n-    // Create a Hoodie table which encapsulated the commits and files visible\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-\n-\n     result.setCommitted(true);\n     result.setWriteStats(writeStats);\n-    result.setReplaceStats(replaceStats);\n-\n     // Finalize write\n     finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      HoodieCommitMetadata metadata = completeInstant(writeStats, replaceStats, extraMetadata);\n       result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543194", "bodyText": "I think at the HoodieTable level, the API has to be about replacing file groups and not insertOverwrite (which can be limited to the WriteClient level). This way clustering can also use the same method, to build on top.", "author": "vinothchandar", "createdAt": "2020-09-13T15:29:36Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -213,6 +213,12 @@ public abstract HoodieWriteMetadata insertPrepped(JavaSparkContext jsc, String i\n   public abstract HoodieWriteMetadata bulkInsertPrepped(JavaSparkContext jsc, String instantTime,\n       JavaRDD<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> bulkInsertPartitioner);\n \n+  /**\n+   * Logically delete all existing records and Insert a batch of new records into Hoodie table at the supplied instantTime.\n+   */\n+  public abstract HoodieWriteMetadata insertOverwrite(JavaSparkContext jsc, String instantTime,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDIxNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564215", "bodyText": "On second thoughts, I am okay leaving this as-is for now as well. and reeval when acutally implementing clustering", "author": "vinothchandar", "createdAt": "2020-09-13T18:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzMxNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543317", "bodyText": "rename: deleteReplacedFileGroups() , to be consistent with our terminology", "author": "vinothchandar", "createdAt": "2020-09-13T15:30:29Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487544876", "bodyText": "you probably dont want to fetch the fs object each time? Also lets delete in parallel, from the get go?  we will invariably need to do this, much like parallelizing cleaning.", "author": "vinothchandar", "createdAt": "2020-09-13T15:45:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieReplaceCommitMetadata metadata = HoodieReplaceCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieReplaceCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().keySet().forEach(partition -> fileSystemView.getAllFileGroups(partition));\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private boolean deletePath(Path path, HoodieInstant instant) {\n+    try {\n+      LOG.info(\"Deleting \" + path + \" before archiving \" + instant);\n+      metaClient.getFs().delete(path);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODE4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208180", "bodyText": "metaclient.getFs() seems to be following singleton pattern, so it doesnt seem expensive to get this. am i reading it incorrectly?\nI can work to setup parallel deletes", "author": "satishkotha", "createdAt": "2020-09-14T20:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk5MDc5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488990799", "bodyText": "for parallel deletes, JavaSparkContext is not exposed to Archive process. Since we anyway want to move this to be part of clean, is it ok if  I defer this to https://issues.apache.org/jira/browse/HUDI-1276?", "author": "satishkotha", "createdAt": "2020-09-15T21:40:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0Nzk4Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489247987", "bodyText": "This could become a performance issue when we are deleting lot of replaced files. HoodieTimelineArchiveLog.archive() method is taking JavaSparkContext. right ?", "author": "bvaradar", "createdAt": "2020-09-16T08:12:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2MTE3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490461178", "bodyText": "i missed that we already have JavaSparkContext. Implemented parallel clean up. PTAL", "author": "satishkotha", "createdAt": "2020-09-17T18:13:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545022", "bodyText": "we need to do the deletion in parallel.", "author": "vinothchandar", "createdAt": "2020-09-13T15:46:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NjY2Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489246667", "bodyText": "+1", "author": "bvaradar", "createdAt": "2020-09-16T08:10:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ3NjY4OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490476688", "bodyText": "Implemented parallel execution. PTAL", "author": "satishkotha", "createdAt": "2020-09-17T18:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545115", "bodyText": "probably a check that this is a replace instant as well?", "author": "vinothchandar", "createdAt": "2020-09-13T15:47:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODYzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208637", "bodyText": "correct. will add it.", "author": "satishkotha", "createdAt": "2020-09-14T20:44:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2MTMzMQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490461331", "bodyText": "Implemented parallel deletion. PTAL.", "author": "satishkotha", "createdAt": "2020-09-17T18:14:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NzUwNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547505", "bodyText": "This seems like a check for whether the instant is a replacecommit or not. if the instant time is a completed instant and replacecommit type, then we must find the instant here, right?", "author": "vinothchandar", "createdAt": "2020-09-13T16:10:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwODU3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488208571", "bodyText": "correct, i'll restructure this code", "author": "satishkotha", "createdAt": "2020-09-14T20:43:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NzUwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547971", "bodyText": "Do we need to ask the file system view for all the replace file groups? this must be in the metadata already right? As long as we can get the HoodieFileGroup objects corresponding to the filegroup ids in the metadata, we can go ahead? What I am suggesting in an alternative and subjectively cleaner replacement for ensureReplaced... above, which seems to make a dummy read to warm up the datastuctures. I prefer to let that happen naturally on its own as opposed to having this \"special\" call", "author": "vinothchandar", "createdAt": "2020-09-13T16:14:41Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjMxOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552318", "bodyText": "As an after thought, I also realize that if we just encoded the entire file group being replaced into the metadata, (as opposed to just encoding the file ids), we can simply delete the file groups without any interaction with tableFileSytemView at all. Probably a simpler solution even?", "author": "vinothchandar", "createdAt": "2020-09-13T16:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDQwMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564403", "bodyText": "One more consideration, as I went through the remainder of the PR. if there was an pending compaction for the replaced file group, then the file group metadata we encode may miss new base files produced as a result of the compaction. This scenario needs to be thought thru.", "author": "vinothchandar", "createdAt": "2020-09-13T18:58:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwNzI3OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488207278", "bodyText": "Yes, compaction is the primary reason I only recorded fileId in the replace metadata. When deleting, we can get all file paths (through view or by listing using consolidated metadata) that have same fileId and delete these files.\nThere can be race conditions that compaction might create a new file with replaced fileId after we queried for existing files though. But because FileSystemView#get methods do not include replaced file groups, I think this is unlikely to happen. I'm not sure if there are edge cases with long running compactions.\nPlease suggest any other improvements.", "author": "satishkotha", "createdAt": "2020-09-14T20:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NTc1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489245751", "bodyText": "Only commit instants older than oldest pending compaction is allowed to be archived. But, if we encode the entire file-group in the replace metadata, we will have race conditions with pending compactions. So, I guess it is safer to figure out the file-group during the time of archiving when it is guaranteed pending compaction is done.\nRegarding the requirement for ensureReplacedPartitionsLoadedCorrectly, If you look at pending compaction handling in filesystem-view, pending compactions are eagerly loaded whenever we construct the filesystem view. This seems to be the case also for replace metadata. Then, why do we need to trigger loading from outside ?", "author": "bvaradar", "createdAt": "2020-09-16T08:08:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5MTc0NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490491745", "bodyText": "So, this is tricky to explain. In FileSystemView, only metadata seems to be eagerly loaded. file groups are not eagerly loaded. i.e., fetchAllStoredFileGroups() returns empty.  For replace instants, we need to get List for all replaced fileId. Because fetchAllStoredFileGroups() is empty, its also returning empty list of FileSlices. So we dont delete replaced files.\nI think instead of creating new HoodieTable in constructor. passing that from callers would help workaround this problem. But that is somewhat involved change because of test dependencies. Also, it might be better to refresh partition content in case new files are created by compaction or other process and somehow that is not reflected in table views. This might be safer option.\nLet me know if you want me to work on passing in HoodieTable to HoodieTimelineArchiveLog constructor.", "author": "satishkotha", "createdAt": "2020-09-17T19:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4OTkwOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490589908", "bodyText": "@satishkotha Here is the plan as we discussed.\n\nChange the signature of fileSystemView.getReplacedFileGroupsBeforeOrOn to also take in partitionId\nIn HoodieTimelineArchiveLog.deleteReplacedFileGroups, read the replace metadata (which we are already doing) and for each partition, call fileSystemView.getReplacedFileGroupsBeforeOrOn().\n(2) must be done in such a way that we are calling the fileSystemView.getReplacedFileGroupsBeforeOrOn in parallel.\n\nThis should allow for lazy loading semantics to be retained at file-system view.", "author": "bvaradar", "createdAt": "2020-09-17T22:09:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDYyMTM0MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r490621340", "bodyText": "Done. PTAL.", "author": "satishkotha", "createdAt": "2020-09-17T23:44:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 2ebc9b07c..9cd8ceaf2 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -304,7 +304,7 @@ public class HoodieTimelineArchiveLog {\n     }\n   }\n \n-  private void deleteReplacedFiles(HoodieInstant instant) {\n+  private void deleteReplacedFileGroups(HoodieInstant instant) {\n     if (!instant.isCompleted()) {\n       // only delete files for completed instants\n       return;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODI4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548280", "bodyText": "you can just call startCommitWithTime(instantTime) from here?", "author": "vinothchandar", "createdAt": "2020-09-13T16:17:26Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -576,7 +592,8 @@ public String startCommit() {\n       rollbackPendingCommits();\n     }\n     String instantTime = HoodieActiveTimeline.createNewInstantTime();\n-    startCommit(instantTime);\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIwOTY4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488209685", "bodyText": "sure", "author": "satishkotha", "createdAt": "2020-09-14T20:46:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODI4MA=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\nindex 33b62f73e..37574f838 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\n\n@@ -592,8 +592,7 @@ public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHo\n       rollbackPendingCommits();\n     }\n     String instantTime = HoodieActiveTimeline.createNewInstantTime();\n-    HoodieTableMetaClient metaClient = createMetaClient(true);\n-    startCommit(instantTime, metaClient.getCommitActionType());\n+    startCommitWithTime(instantTime);\n     return instantTime;\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODM1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548355", "bodyText": "can we pass in the metaClient from caller. This seems to introduce additional creations, which all list .hoodie again", "author": "vinothchandar", "createdAt": "2020-09-13T16:18:17Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,15 +603,23 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType());\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType);\n   }\n \n-  private void startCommit(String instantTime) {\n+  private void startCommit(String instantTime, String actionType) {\n     LOG.info(\"Generate a new instant time \" + instantTime);\n     HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxMjkyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488212928", "bodyText": "Good point. Will do.", "author": "satishkotha", "createdAt": "2020-09-14T20:52:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODM1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\nindex 33b62f73e..37574f838 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java\n\n@@ -604,24 +603,31 @@ public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHo\n    */\n   public void startCommitWithTime(String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    startCommitWithTime(instantTime, metaClient.getCommitActionType());\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType(), metaClient);\n   }\n \n   /**\n    * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n    */\n   public void startCommitWithTime(String instantTime, String actionType) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, actionType, metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  private void startCommitWithTime(String instantTime, String actionType, HoodieTableMetaClient metaClient) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime, actionType);\n+    startCommit(instantTime, actionType, metaClient);\n   }\n \n-  private void startCommit(String instantTime, String actionType) {\n-    LOG.info(\"Generate a new instant time \" + instantTime);\n-    HoodieTableMetaClient metaClient = createMetaClient(true);\n+  private void startCommit(String instantTime, String actionType, HoodieTableMetaClient metaClient) {\n+    LOG.info(\"Generate a new instant time: \" + instantTime + \" action: \" + actionType);\n     // if there are pending compactions, their instantTime must not be greater than that of this instant time\n     metaClient.getActiveTimeline().filterPendingCompactionTimeline().lastInstant().ifPresent(latestPending ->\n         ValidationUtils.checkArgument(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0OTc3Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487549773", "bodyText": "typo: getReplaceStats", "author": "vinothchandar", "createdAt": "2020-09-13T16:32:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java", "diffHunk": "@@ -94,6 +94,14 @@ public void setWriteStats(List<HoodieWriteStat> writeStats) {\n     this.writeStats = Option.of(writeStats);\n   }\n \n+  public Option<List<HoodieWriteStat>> getReplacetats() {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxNTgzNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488215834", "bodyText": "This is not needed anymore given we are tracking replaced files as boolean in WriteStatus. I removed this.", "author": "satishkotha", "createdAt": "2020-09-14T20:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0OTc3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java b/hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java\nindex 25b79a68f..53e67f902 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java\n\n@@ -94,14 +93,6 @@ public class HoodieWriteMetadata {\n     this.writeStats = Option.of(writeStats);\n   }\n \n-  public Option<List<HoodieWriteStat>> getReplacetats() {\n-    return replaceStats;\n-  }\n-\n-  public void setReplaceStats(List<HoodieWriteStat> replaceStats) {\n-    this.replaceStats = Option.of(replaceStats);\n-  }\n-\n   public Option<Duration> getIndexLookupDuration() {\n     return indexLookupDuration;\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTQ2Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551467", "bodyText": "why limit to just base files. we may have log files without base files. i.e insert to log files code path", "author": "vinothchandar", "createdAt": "2020-09-13T16:49:36Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODIxNjE5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488216199", "bodyText": "My bad, missed that insert into log files case. I fixed it now. thanks for finding this bug.", "author": "satishkotha", "createdAt": "2020-09-14T20:58:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTQ2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\nindex 523adf9be..a1de71d0c 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n\n@@ -68,11 +68,11 @@ public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T\n   }\n \n   @Override\n-  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+  protected JavaRDD<WriteStatus> writeInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n     // get all existing fileIds to mark them as replaced\n     JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n     // do necessary inserts into new file groups\n-    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    JavaRDD<WriteStatus> writeStatuses = super.writeInputRecords(inputRecordsRDD, profile);\n     return writeStatuses.union(replaceStatuses);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTY4MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551680", "bodyText": "Seeing such large values in the metadata can be bit confusing. can we set it to -1 instead for now", "author": "vinothchandar", "createdAt": "2020-09-13T16:51:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());\n+  }\n+\n+  private WriteStatus getReplaceWriteStatus(String partitionPath, String fileId) {\n+    // mark file as 'replaced' in metadata. the actual file will be removed later by cleaner to provide snapshot isolation\n+    WriteStatus status = new WriteStatus(false, 0.0);\n+    status.setReplacedFileId(true);\n+    status.setFileId(fileId);\n+    status.setTotalErrorRecords(0);\n+    status.setPartitionPath(partitionPath);\n+    HoodieWriteStat replaceStat = new HoodieWriteStat();\n+    status.setStat(replaceStat);\n+    replaceStat.setPartitionPath(partitionPath);\n+    replaceStat.setFileId(fileId);\n+    replaceStat.setPath(table.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get().getPath());\n+    status.getStat().setNumDeletes(Integer.MAX_VALUE);//token to indicate all rows are deleted", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\nindex 523adf9be..a1de71d0c 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n\n@@ -68,11 +68,11 @@ public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T\n   }\n \n   @Override\n-  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+  protected JavaRDD<WriteStatus> writeInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n     // get all existing fileIds to mark them as replaced\n     JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n     // do necessary inserts into new file groups\n-    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    JavaRDD<WriteStatus> writeStatuses = super.writeInputRecords(inputRecordsRDD, profile);\n     return writeStatuses.union(replaceStatuses);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjE3MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552170", "bodyText": "So, this creates a dependency on the workloadProfile for doing insert overwrite. While we always provide a WorkloadProfile for now, in the future we would like to remove this need for caching data in memory and building the profile.\nCan we try to reimplement this such that\n\nprocessInputRecords(..) just writes the new records and returns WriteStatus for the new file groups alone.\nDuring commit time, after we collect the WriteStatus, we can obtain the replaceStatuses based on the partitions that were actually written to during step above.\n\nThis also gives us a cleaner solution for avoiding the boolean flag we discussed. API is also consistent now, that writeClient.insertOverwrite() only returns the WriteStatus for the new file group IDs.", "author": "vinothchandar", "createdAt": "2020-09-13T16:56:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4NzkzNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488987937", "bodyText": "I refactored it and removed boolean from WriteStatus. PTAL", "author": "satishkotha", "createdAt": "2020-09-15T21:36:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjE3MA=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\nindex 523adf9be..a1de71d0c 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java\n\n@@ -68,11 +68,11 @@ public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T\n   }\n \n   @Override\n-  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+  protected JavaRDD<WriteStatus> writeInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n     // get all existing fileIds to mark them as replaced\n     JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n     // do necessary inserts into new file groups\n-    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    JavaRDD<WriteStatus> writeStatuses = super.writeInputRecords(inputRecordsRDD, profile);\n     return writeStatuses.union(replaceStatuses);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjYwOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552608", "bodyText": "please keep test/naming to base and log files. and not leak parquet to the test? Also can you please see if this test can be authored by reusing existing helpers. Its often bit hard to read and reuse the exisiting helpers, but hte more one-offs we introduce, the worse this situation becomes.", "author": "vinothchandar", "createdAt": "2020-09-13T17:00:59Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -880,6 +880,89 @@ public void testDeletesWithDeleteApi() throws Exception {\n     testDeletes(client, updateBatch3.getRight(), 10, file1, \"007\", 140, keysSoFar);\n   }\n \n+  /**\n+   * Test scenario of writing more file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithMoreRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(1000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing fewer file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithFewerRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 1000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlinWithSimilarNumberOfRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 3000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records.\n+   *  2) Do write2 (insert overwrite) with 'batch2RecordsCount' number of records.\n+   *\n+   *  Verify that all records in step1 are overwritten\n+   */\n+  private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int batch2RecordsCount) throws Exception {\n+    final String testPartitionPath = \"americas\";\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000);\n+    HoodieWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n+\n+    // Do Inserts\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime1, batch1RecordsCount);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyParquetFileData(commitTime1, inserts1, statuses);\n+\n+    // Do Insert Overwrite\n+    String commitTime2 = \"002\";\n+    client.startCommitWithTime(commitTime2, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    List<HoodieRecord> inserts2 = dataGen.generateInserts(commitTime2, batch2RecordsCount);\n+    List<HoodieRecord> insertsAndUpdates2 = new ArrayList<>();\n+    insertsAndUpdates2.addAll(inserts2);\n+    JavaRDD<HoodieRecord> insertAndUpdatesRDD2 = jsc.parallelize(insertsAndUpdates2, 2);\n+    statuses = client.insertOverwrite(insertAndUpdatesRDD2, commitTime2).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> replacedBuckets = statuses.stream().filter(s -> s.isReplacedFileId())\n+        .map(s -> s.getFileId()).collect(Collectors.toSet());\n+    assertEquals(batch1Buckets, replacedBuckets);\n+    List<WriteStatus> newBuckets = statuses.stream().filter(s -> !(s.isReplacedFileId()))\n+        .collect(Collectors.toList());\n+    verifyParquetFileData(commitTime2, inserts2, newBuckets);\n+  }\n+\n+  /**\n+   * Verify data in parquet files matches expected records and commit time.\n+   */\n+  private void verifyParquetFileData(String commitTime, List<HoodieRecord> expectedRecords, List<WriteStatus> allStatus) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NjE1Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488346156", "bodyText": "renamed it for now. I'll look into if there are any other helpers for doing this.", "author": "satishkotha", "createdAt": "2020-09-15T02:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjYwOA=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java b/hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\nindex c450d7471..b47c15191 100644\n--- a/hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n+++ b/hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java\n\n@@ -924,7 +924,7 @@ public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n     List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n     assertNoWriteErrors(statuses);\n     Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n-    verifyParquetFileData(commitTime1, inserts1, statuses);\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n \n     // Do Insert Overwrite\n     String commitTime2 = \"002\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjY4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552685", "bodyText": "rename: isReplaced", "author": "vinothchandar", "createdAt": "2020-09-13T17:01:51Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java", "diffHunk": "@@ -52,6 +52,9 @@\n \n   private HoodieWriteStat stat = null;\n \n+  // if true, indicates the fileId in this WriteStatus is being replaced\n+  private boolean isReplacedFileId;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "067c285f5dc762a3753ad4162d02957662daa1d9", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java b/hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java\nindex 99b390400..a93f2682b 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java\n\n@@ -52,9 +52,6 @@ public class WriteStatus implements Serializable {\n \n   private HoodieWriteStat stat = null;\n \n-  // if true, indicates the fileId in this WriteStatus is being replaced\n-  private boolean isReplacedFileId;\n-\n   private long totalRecords = 0;\n   private long totalErrorRecords = 0;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Mjc4MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552781", "bodyText": "now that replace also is replacecommit. should we just leave the getCommitsAndCompactionTimeline() be?", "author": "vinothchandar", "createdAt": "2020-09-13T17:03:07Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -116,8 +115,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n-    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjgyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552828", "bodyText": "nit: remove extra line?", "author": "vinothchandar", "createdAt": "2020-09-13T17:03:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n   }\n \n   /**\n    * Adds the provided statuses into the file system view, and also caches it inside this object.\n    */\n   protected List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {\n     HoodieTimer timer = new HoodieTimer().startTimer();\n+", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -116,8 +115,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n-    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NTEzOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487555139", "bodyText": "please use HoodieTimer to time code segments.", "author": "vinothchandar", "createdAt": "2020-09-13T17:27:05Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -196,6 +205,32 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     return fileGroups;\n   }\n \n+  /**\n+   * Get replaced instant for each file group by looking at all commit instants.\n+   */\n+  private void resetFileGroupsReplaced(HoodieTimeline timeline) {\n+    Instant indexStartTime = Instant.now();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -209,17 +206,18 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n    * Get replaced instant for each file group by looking at all commit instants.\n    */\n   private void resetFileGroupsReplaced(HoodieTimeline timeline) {\n-    Instant indexStartTime = Instant.now();\n+    HoodieTimer hoodieTimer = new HoodieTimer();\n+    hoodieTimer.startTimer();\n     // for each REPLACE instant, get map of (partitionPath -> deleteFileGroup)\n-    HoodieTimeline replacedTimeline = timeline.getCompletedAndReplaceTimeline();\n+    HoodieTimeline replacedTimeline = timeline.getCompletedReplaceTimeline();\n     Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = replacedTimeline.getInstants().flatMap(instant -> {\n       try {\n         HoodieReplaceCommitMetadata replaceMetadata = HoodieReplaceCommitMetadata.fromBytes(metaClient.getActiveTimeline().getInstantDetails(instant).get(),\n             HoodieReplaceCommitMetadata.class);\n \n         // get replace instant mapping for each partition, fileId\n-        return replaceMetadata.getPartitionToReplaceStats().entrySet().stream().flatMap(entry -> entry.getValue().stream().map(e ->\n-                new AbstractMap.SimpleEntry<>(new HoodieFileGroupId(entry.getKey(), e.getFileId()), instant)));\n+        return replaceMetadata.getPartitionToReplaceFileIds().entrySet().stream().flatMap(entry -> entry.getValue().stream().map(e ->\n+                new AbstractMap.SimpleEntry<>(new HoodieFileGroupId(entry.getKey(), e), instant)));\n       } catch (IOException e) {\n         throw new HoodieIOException(\"error reading commit metadata for \" + instant);\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjEzMA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556130", "bodyText": "I think its sufficient todo this reset in the init() method above, much like pendingCompaction and bootstreap handling. This method is simply used to refresh the timeline i.e the instants that are visible.", "author": "vinothchandar", "createdAt": "2020-09-13T17:36:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -116,8 +115,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n-    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjYyNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556626", "bodyText": "rename: getReplaceInstant()", "author": "vinothchandar", "createdAt": "2020-09-13T17:41:51Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +775,21 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract Option<HoodieInstant> getReplacedInstant(final HoodieFileGroupId fileGroupId);", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -788,7 +786,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n   /**\n    * Track instant time for file groups replaced.\n    */\n-  protected abstract Option<HoodieInstant> getReplacedInstant(final HoodieFileGroupId fileGroupId);\n+  protected abstract Option<HoodieInstant> getReplaceInstant(final HoodieFileGroupId fileGroupId);\n \n   /**\n    * Check if the view is already closed.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Njg5OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556898", "bodyText": "can you just call isFileGroupReplacedBeforeOrOn(fileGroup, max(instants)) ? without having to implement this again", "author": "vinothchandar", "createdAt": "2020-09-13T17:44:44Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -880,6 +957,30 @@ private FileSlice fetchMergedFileSlice(HoodieFileGroup fileGroup, FileSlice file\n         .fromJavaOptional(fetchLatestFileSlices(partitionPath).filter(fs -> fs.getFileId().equals(fileId)).findFirst());\n   }\n \n+  private boolean isFileGroupReplaced(HoodieFileGroup fileGroup) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    return hoodieInstantOption.isPresent();\n+  }\n+\n+  private boolean isFileGroupReplacedBeforeAny(HoodieFileGroup fileGroup, List<String> instants) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    if (!hoodieInstantOption.isPresent()) {\n+      return false;\n+    }\n+\n+    return HoodieTimeline.compareTimestamps(instants.stream().max(Comparator.naturalOrder()).get(),", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 8b222945c..bdbb660a2 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -958,22 +956,16 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n   }\n \n   private boolean isFileGroupReplaced(HoodieFileGroup fileGroup) {\n-    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    Option<HoodieInstant> hoodieInstantOption = getReplaceInstant(fileGroup.getFileGroupId());\n     return hoodieInstantOption.isPresent();\n   }\n \n   private boolean isFileGroupReplacedBeforeAny(HoodieFileGroup fileGroup, List<String> instants) {\n-    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n-    if (!hoodieInstantOption.isPresent()) {\n-      return false;\n-    }\n-\n-    return HoodieTimeline.compareTimestamps(instants.stream().max(Comparator.naturalOrder()).get(),\n-        GREATER_THAN_OR_EQUALS, hoodieInstantOption.get().getTimestamp());\n+    return isFileGroupReplacedBeforeOrOn(fileGroup, instants.stream().max(Comparator.naturalOrder()).get());\n   }\n \n   private boolean isFileGroupReplacedBeforeOrOn(HoodieFileGroup fileGroup, String instant) {\n-    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    Option<HoodieInstant> hoodieInstantOption = getReplaceInstant(fileGroup.getFileGroupId());\n     if (!hoodieInstantOption.isPresent()) {\n       return false;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzA0OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557049", "bodyText": "deltacommit is not here. huh. file a \"code cleanup\" JIRA for later?", "author": "vinothchandar", "createdAt": "2020-09-13T17:45:40Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java", "diffHunk": "@@ -22,5 +22,5 @@\n  * The supported action types.\n  */\n public enum ActionType {\n-  commit, savepoint, compaction, clean, rollback\n+  commit, savepoint, compaction, clean, rollback, replacecommit", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI2ODc3Nw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488268777", "bodyText": "Filed HUDI-1281", "author": "satishkotha", "createdAt": "2020-09-14T22:31:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzA0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java b/hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java\nindex acef137c4..6be321c98 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java\n\n@@ -22,5 +22,6 @@ package org.apache.hudi.common.model;\n  * The supported action types.\n  */\n public enum ActionType {\n+  //TODO HUDI-1281 make deltacommit part of this\n   commit, savepoint, compaction, clean, rollback, replacecommit\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzE5MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557190", "bodyText": "please add a simple unit tests for this . testing for e.g that the schema is set, op type is set etc", "author": "vinothchandar", "createdAt": "2020-09-13T17:46:52Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.\n+ */\n+public class CommitUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(CommitUtils.class);\n+\n+  public static HoodieCommitMetadata buildWriteActionMetadata(List<HoodieWriteStat> writeStats,", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0MzM2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488343363", "bodyText": "Added", "author": "satishkotha", "createdAt": "2020-09-15T02:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzE5MA=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\nindex 6991404f8..1a7c6d374 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\n\n@@ -20,9 +20,12 @@ package org.apache.hudi.common.util;\n \n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.exception.HoodieException;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzM3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557376", "bodyText": "nit: extra line", "author": "vinothchandar", "createdAt": "2020-09-13T17:48:56Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "diffHunk": "@@ -46,11 +46,12 @@\n   public static final String SCHEMA_KEY = \"schema\";\n   private static final Logger LOG = LogManager.getLogger(HoodieCommitMetadata.class);\n   protected Map<String, List<HoodieWriteStat>> partitionToWriteStats;\n+", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java\nindex c5e49d254..3e760f6bd 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java\n\n@@ -46,7 +46,6 @@ public class HoodieCommitMetadata implements Serializable {\n   public static final String SCHEMA_KEY = \"schema\";\n   private static final Logger LOG = LogManager.getLogger(HoodieCommitMetadata.class);\n   protected Map<String, List<HoodieWriteStat>> partitionToWriteStats;\n-\n   protected Boolean compacted;\n \n   protected Map<String, String> extraMetadata;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzQzMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557432", "bodyText": "rename: getCompletedReplaceTimeline()  current naming gives the impression that its either completed or replacecommit", "author": "vinothchandar", "createdAt": "2020-09-13T17:49:49Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -113,6 +112,18 @@ public HoodieDefaultTimeline getCommitsAndCompactionTimeline() {\n     return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n   }\n \n+  @Override\n+  public HoodieDefaultTimeline getWriteActionTimeline() {\n+    Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION, REPLACE_COMMIT_ACTION);\n+    return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n+  }\n+\n+  @Override\n+  public HoodieTimeline getCompletedAndReplaceTimeline() {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java\nindex 23309ee27..8ced025af 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java\n\n@@ -108,18 +108,12 @@ public class HoodieDefaultTimeline implements HoodieTimeline {\n \n   @Override\n   public HoodieDefaultTimeline getCommitsAndCompactionTimeline() {\n-    Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION);\n-    return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n-  }\n-\n-  @Override\n-  public HoodieDefaultTimeline getWriteActionTimeline() {\n     Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION, REPLACE_COMMIT_ACTION);\n     return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n   }\n \n   @Override\n-  public HoodieTimeline getCompletedAndReplaceTimeline() {\n+  public HoodieTimeline getCompletedReplaceTimeline() {\n     return new HoodieDefaultTimeline(\n         instants.stream().filter(s -> s.getAction().equals(REPLACE_COMMIT_ACTION)).filter(s -> s.isCompleted()), details);\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557894", "bodyText": "these are the file groups being replaced? I thought we were going to just track the file ids?", "author": "vinothchandar", "createdAt": "2020-09-13T17:54:22Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<HoodieWriteStat>> partitionToReplaceStats;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODAwNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558004", "bodyText": "if these are the file groups being replaced, then does this contain all the file slices (see my comment around deleting the replaced file groups in timeline archive log)", "author": "vinothchandar", "createdAt": "2020-09-13T17:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NDg1OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488344859", "bodyText": "I changed it to List to include only fileIds. I'm inclined against storing all file slices because they can evolve between metadata creation and archival/clean. Let me know if this understanding is incorrect", "author": "satishkotha", "createdAt": "2020-09-15T02:28:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\nindex 7ac8be5c3..477e2968b 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\n\n@@ -39,7 +39,7 @@ import java.util.Map;\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n   private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n-  protected Map<String, List<HoodieWriteStat>> partitionToReplaceStats;\n+  protected Map<String, List<String>> partitionToReplaceFileIds;\n \n   // for ser/deser\n   public HoodieReplaceCommitMetadata() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODA1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558055", "bodyText": "rename: fgIdToReplaceInstants", "author": "vinothchandar", "createdAt": "2020-09-13T17:55:59Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java", "diffHunk": "@@ -64,6 +64,11 @@\n    */\n   protected Map<HoodieFileGroupId, BootstrapBaseFileMapping> fgIdToBootstrapBaseFile;\n \n+  /**\n+   * Track replace time for replaced file groups.\n+   */\n+  protected Map<HoodieFileGroupId, HoodieInstant> fgIdToReplaceInstant;", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java\nindex d99a6fe15..ee53fb75e 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java\n\n@@ -67,7 +67,7 @@ public class HoodieTableFileSystemView extends IncrementalTimelineSyncFileSystem\n   /**\n    * Track replace time for replaced file groups.\n    */\n-  protected Map<HoodieFileGroupId, HoodieInstant> fgIdToReplaceInstant;\n+  protected Map<HoodieFileGroupId, HoodieInstant> fgIdToReplaceInstants;\n \n   /**\n    * Flag to determine if closed.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODI3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558275", "bodyText": "for tests, I suggest using the HoodieWritableTestTable etc instead of introducing new methods. Also please check other utilities to avoid writing a new method here", "author": "vinothchandar", "createdAt": "2020-09-13T17:58:44Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -366,6 +378,14 @@ public static void createCompactionRequestedFile(String basePath, String instant\n     createEmptyFile(basePath, commitFile, configuration);\n   }\n \n+  public static void createDataFile(String basePath, String partitionPath, String instantTime, String fileID, Configuration configuration)", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "chunk": "diff --git a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java\nindex 63197c6fd..bf9728045 100644\n--- a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java\n+++ b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java\n\n@@ -378,14 +382,6 @@ public class HoodieTestDataGenerator {\n     createEmptyFile(basePath, commitFile, configuration);\n   }\n \n-  public static void createDataFile(String basePath, String partitionPath, String instantTime, String fileID, Configuration configuration)\n-      throws IOException {\n-\n-    Path dataFilePath = new Path(basePath + \"/\" + partitionPath + \"/\"\n-        + FSUtils.makeDataFileName(instantTime, \"1_0_1\", fileID));\n-    createEmptyFile(basePath, dataFilePath, configuration);\n-  }\n-\n   private static void createEmptyFile(String basePath, Path filePath, Configuration configuration) throws IOException {\n     FileSystem fs = FSUtils.getFs(basePath, configuration);\n     FSDataOutputStream os = fs.create(filePath, true);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODMzNQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558335", "bodyText": "why is this change necessayr?", "author": "vinothchandar", "createdAt": "2020-09-13T17:59:21Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -378,9 +398,9 @@ public static void createCompactionAuxiliaryMetadata(String basePath, HoodieInst\n         new Path(basePath + \"/\" + HoodieTableMetaClient.AUXILIARYFOLDER_NAME + \"/\" + instant.getFileName());\n     FileSystem fs = FSUtils.getFs(basePath, configuration);\n     try (FSDataOutputStream os = fs.create(commitFile, true)) {\n-      HoodieCompactionPlan workload = new HoodieCompactionPlan();\n+      HoodieCompactionPlan workload = HoodieCompactionPlan.newBuilder().setVersion(1).build();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4Mzg5Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488283893", "bodyText": "Version is not being set by default, so reading test plan generated here is failing. So I explicitly set version.", "author": "satishkotha", "createdAt": "2020-09-14T22:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODMzNQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODQ1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558451", "bodyText": "there is nothing specific about replace in this method? should we move this to the test class itself. inline?", "author": "vinothchandar", "createdAt": "2020-09-13T18:00:24Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -200,6 +199,14 @@ public static void createInflightCommitFiles(String basePath, String... instantT\n     }\n   }\n \n+  public static HoodieWriteStat createReplaceStat(final String partitionPath, final String fileId1) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NDY1NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488284655", "bodyText": "Done.", "author": "satishkotha", "createdAt": "2020-09-14T23:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODQ1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\nindex 61fb75929..e004ec861 100644\n--- a/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\n+++ b/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java\n\n@@ -199,14 +199,6 @@ public class HoodieTestUtils {\n     }\n   }\n \n-  public static HoodieWriteStat createReplaceStat(final String partitionPath, final String fileId1) {\n-    HoodieWriteStat replaceStat = new HoodieWriteStat();\n-    replaceStat.setPath(\"\");\n-    replaceStat.setPartitionPath(partitionPath);\n-    replaceStat.setFileId(fileId1);\n-    return replaceStat;\n-  }\n-\n   public static void createPendingCleanFiles(HoodieTableMetaClient metaClient, String... instantTimes) {\n     for (String instantTime : instantTimes) {\n       Arrays.asList(HoodieTimeline.makeRequestedCleanerFileName(instantTime),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558507", "bodyText": "see earlier comment on whether we need this method.", "author": "vinothchandar", "createdAt": "2020-09-13T18:01:05Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java", "diffHunk": "@@ -133,6 +137,21 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+  /**\n+   * Timeline to just include commits (commit/deltacommit), replace and compaction actions.\n+   *\n+   * @return\n+   */\n+  HoodieDefaultTimeline getWriteActionTimeline();", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2MzM4NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487563384", "bodyText": "To expand, I am wondering if we should just include replacecommit within getCommitsAndCompactionTimeline(). Most of its callers are around compaction/savepoint/restore etc. So we may not be seeing some cases here.\nThings work for now, since filterCompletedInstants() etc are including replace commit in the timeline when filtering for queries. Semantically, if replace is a commit level action that can add new data to the timeline, then we should just treat it like delta commit IMO.\nWould anything break if we did do that?", "author": "vinothchandar", "createdAt": "2020-09-13T18:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NTYwNA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488345604", "bodyText": "I removed and included getCommitsAndCompactionTimeline. I think we will run into some edge cases for MOR tables where something would break. But don't have concrete examples. We can run it for a while on MOR table and see if works.", "author": "satishkotha", "createdAt": "2020-09-15T02:31:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java\nindex c97dd658e..03ba43c11 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java\n\n@@ -137,20 +137,12 @@ public interface HoodieTimeline extends Serializable {\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n-  /**\n-   * Timeline to just include commits (commit/deltacommit), replace and compaction actions.\n-   *\n-   * @return\n-   */\n-  HoodieDefaultTimeline getWriteActionTimeline();\n-\n-\n   /**\n    * Timeline to just include replace instants that have valid (commit/deltacommit) actions.\n    *\n    * @return\n    */\n-  HoodieTimeline getCompletedAndReplaceTimeline();\n+  HoodieTimeline getCompletedReplaceTimeline();\n \n   /**\n    * Filter this timeline to just include requested and inflight compaction instants.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558663", "bodyText": "@bvaradar if you can take a pass at these, that would be great", "author": "vinothchandar", "createdAt": "2020-09-13T18:02:44Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java", "diffHunk": "@@ -371,6 +371,47 @@ void removeBootstrapBaseFileMapping(Stream<BootstrapBaseFileMapping> bootstrapBa\n         schemaHelper.getPrefixForSliceViewByPartitionFile(partitionPath, fileId)).map(Pair::getValue)).findFirst());\n   }\n \n+  @Override\n+  protected void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODkyNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558926", "bodyText": "General question I have around incremental file system view and rocksDB like persistent file system view storage is whether we will keep this list updated. i.e when the archival/cleaning runs, how do we ensure the deleted replaced file groups are no longer tracked inside rocksdb.\nI guess the lines below, are doing a bulk delete and insert to achieve the same?", "author": "vinothchandar", "createdAt": "2020-09-13T18:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM0NTg0OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488345849", "bodyText": "Yes,  we do delete and insert. Definitely, would be helpful to have someone with more experience review this.", "author": "satishkotha", "createdAt": "2020-09-15T02:32:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTA5OTk5OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489099999", "bodyText": "This part looks good. Going through rest of the code", "author": "bvaradar", "createdAt": "2020-09-16T01:00:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java\nindex ba11d8b1e..dab7baa4c 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java\n\n@@ -405,7 +405,7 @@ public class RocksDbBasedFileSystemView extends IncrementalTimelineSyncFileSyste\n   }\n \n   @Override\n-  protected Option<HoodieInstant> getReplacedInstant(final HoodieFileGroupId fileGroupId) {\n+  protected Option<HoodieInstant> getReplaceInstant(final HoodieFileGroupId fileGroupId) {\n     String lookupKey = schemaHelper.getKeyForReplacedFileGroup(fileGroupId);\n     HoodieInstant replacedInstant =\n         rocksDB.get(schemaHelper.getColFamilyForReplacedFileGroups(), lookupKey);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTI3NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487559275", "bodyText": "better approach for these situations generally is to introduce a commitStats(.) that does not take the last argument and deal with it internally inside WriteClient.\nThis way the code will remain more readable, without having to reference replace stats in bulk_insert, which have nothing to do with each other. hope that makes sense", "author": "vinothchandar", "createdAt": "2020-09-13T18:09:20Z", "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +104,9 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(),", "originalCommit": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODI4NDk2NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r488284965", "bodyText": "Sure. Added new method", "author": "satishkotha", "createdAt": "2020-09-14T23:01:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTI3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java b/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\nindex 596babebb..a4bfd5910 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\n+++ b/hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java\n\n@@ -105,8 +104,7 @@ public class HoodieDataSourceInternalWriter implements DataSourceWriter {\n \n     try {\n       writeClient.commitStats(instantTime, writeStatList, Option.empty(),\n-          DataSourceUtils.getCommitActionType(operationType, metaClient),\n-          Collections.emptyList() /* No replace stats for bulk_insert */);\n+          DataSourceUtils.getCommitActionType(operationType, metaClient.getTableType()));\n     } catch (Exception ioe) {\n       throw new HoodieException(ioe.getMessage(), ioe);\n     } finally {\n"}}, {"oid": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "url": "https://github.com/apache/hudi/commit/4ac518737b4388ab4c510f517312072a4b3b6dc0", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T01:22:25Z", "type": "forcePushed"}, {"oid": "6c6f370bd9800636aee60d02cb8ceaa3776a6715", "url": "https://github.com/apache/hudi/commit/6c6f370bd9800636aee60d02cb8ceaa3776a6715", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T02:20:22Z", "type": "forcePushed"}, {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9", "url": "https://github.com/apache/hudi/commit/067c285f5dc762a3753ad4162d02957662daa1d9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-15T21:35:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIyNTM1MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489225350", "bodyText": "Is this needed ?", "author": "bvaradar", "createdAt": "2020-09-16T07:34:38Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Result of a write operation.\n+ */\n+public class HoodieWriteResult implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long RANDOM_SEED = 9038412832L;", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java\nindex ef9101a8a..69421a91d 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java\n\n@@ -30,8 +30,6 @@ import java.util.Map;\n  */\n public class HoodieWriteResult implements Serializable {\n \n-  private static final long serialVersionUID = 1L;\n-  private static final long RANDOM_SEED = 9038412832L;\n   private JavaRDD<WriteStatus> writeStatuses;\n   private Map<String, List<String>> partitionToReplaceFileIds;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI1NDI5MA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489254290", "bodyText": "Move this ReplaceHelper class ?", "author": "bvaradar", "createdAt": "2020-09-16T08:23:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -370,4 +430,17 @@ private IndexedRecord convertToAvroRecord(HoodieTimeline commitTimeline, HoodieI\n     avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n     return avroMetaData;\n   }\n+\n+  public static org.apache.hudi.avro.model.HoodieReplaceCommitMetadata convertReplaceCommitMetadata(", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\nindex 7478f0a83..457d5aa29 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java\n\n@@ -430,17 +418,4 @@ public class HoodieTimelineArchiveLog {\n     avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n     return avroMetaData;\n   }\n-\n-  public static org.apache.hudi.avro.model.HoodieReplaceCommitMetadata convertReplaceCommitMetadata(\n-      HoodieReplaceCommitMetadata hoodieReplaceCommitMetadata) {\n-    ObjectMapper mapper = new ObjectMapper();\n-    // Need this to ignore other public get() methods\n-    mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n-    org.apache.hudi.avro.model.HoodieReplaceCommitMetadata avroMetaData =\n-        mapper.convertValue(hoodieReplaceCommitMetadata, org.apache.hudi.avro.model.HoodieReplaceCommitMetadata.class);\n-\n-    // Do not archive Rolling Stats, cannot set to null since AVRO will throw null pointer\n-    avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n-    return avroMetaData;\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2MDk1Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489260956", "bodyText": "nit: Can you add @OverRide annotation ?", "author": "bvaradar", "createdAt": "2020-09-16T08:33:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<String>> partitionToReplaceFileIds;\n+\n+  // for ser/deser\n+  public HoodieReplaceCommitMetadata() {\n+    this(false);\n+  }\n+\n+  public HoodieReplaceCommitMetadata(boolean compacted) {\n+    super(compacted);\n+    partitionToReplaceFileIds = new HashMap<>();\n+  }\n+\n+  public void setPartitionToReplaceFileIds(Map<String, List<String>> partitionToReplaceFileIds) {\n+    this.partitionToReplaceFileIds = partitionToReplaceFileIds;\n+  }\n+\n+  public void addReplaceFileId(String partitionPath, String fileId) {\n+    if (!partitionToReplaceFileIds.containsKey(partitionPath)) {\n+      partitionToReplaceFileIds.put(partitionPath, new ArrayList<>());\n+    }\n+    partitionToReplaceFileIds.get(partitionPath).add(fileId);\n+  }\n+\n+  public List<String> getReplaceFileIds(String partitionPath) {\n+    return partitionToReplaceFileIds.get(partitionPath);\n+  }\n+\n+  public Map<String, List<String>> getPartitionToReplaceFileIds() {\n+    return partitionToReplaceFileIds;\n+  }\n+\n+  public String toJsonString() throws IOException {", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\nindex 4022e6647..7cc9ee3a0 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java\n\n@@ -70,6 +70,7 @@ public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n     return partitionToReplaceFileIds;\n   }\n \n+  @Override\n   public String toJsonString() throws IOException {\n     if (partitionToWriteStats.containsKey(null)) {\n       LOG.info(\"partition path is null for \" + partitionToWriteStats.get(null));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489278233", "bodyText": "@satishkotha : We need to handle case when we restore a .replace instant for incremental timeline file system view.\nAbout the implementation -\nIn addRestoreInstant(), we need to look at HoodieRestoreMetadata.instantsToRollback and for each instants which are .replace types, we need to remove the replace file-group mapping kept in the file-system view.  We would need a reverse mapping of instant to file-group-id and also a way to identify which of the entries in HoodieRestoreMetadata.instantsToRollback is replace metadata. Currently, we only store commit timestamps in HoodieRestoreMetadata.instantsToRollback.\nI think it would be useful if we add an additional field in HoodieRestoreCommitMetadata and HoodieRollbackCommitMetadata to store both the timestamp and commit-action-type and use it here.\nSince, we only read committed replace actions, rollback is fine though.", "author": "bvaradar", "createdAt": "2020-09-16T08:59:08Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java", "diffHunk": "@@ -251,6 +262,28 @@ private void addRollbackInstant(HoodieTimeline timeline, HoodieInstant instant)\n     LOG.info(\"Done Syncing rollback instant (\" + instant + \")\");\n   }\n \n+  /**\n+   * Add newly found REPLACE instant.\n+   *\n+   * @param timeline Hoodie Timeline\n+   * @param instant REPLACE Instant\n+   */\n+  private void addReplaceInstant(HoodieTimeline timeline, HoodieInstant instant) throws IOException {", "originalCommit": "067c285f5dc762a3753ad4162d02957662daa1d9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI4Mzk2Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489283963", "bodyText": "Also, can you also add test cases for incremental file-system view for both addReplaceInstant and removeReplaceInstant in TestIncrementalFSViewSync ?", "author": "bvaradar", "createdAt": "2020-09-16T09:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjMwMzEyMg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r492303122", "bodyText": "I need to understand this flow a bit more. But, have a question on why we need to track commit-action-type and timestamp. Today, HoodieRollbackMetadata tracks successFiles, deletedFiles etc.  Do you think we can add replacedFileIds also there? This will be empty for regular commits. But for replace commits, it will have some content.  If this value is present, we can remove corresponding fileIds from View#replacedFileGroups. Let me know if i'm missing anything with this approach.", "author": "satishkotha", "createdAt": "2020-09-21T19:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMjE1OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493002158", "bodyText": "Discussed with @satishkotha. We will track commit action type along with instant since we have introduced one another type of \"commit\" -> replace. Restore can perform custom handling only for this action type. It is safer to let Incremental file-system view to revert replace mappings (in memory or rocksdb) by providing the replace instant time as it  isrobust in case of partial restore failures.", "author": "bvaradar", "createdAt": "2020-09-22T20:07:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg4MjI5MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493882291", "bodyText": "@bvaradar Made the change and added basic test. Please take a look. If the general approach looks good. I'll add more complex tests.", "author": "satishkotha", "createdAt": "2020-09-23T20:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}], "type": "inlineReview", "revised_code": null}, {"oid": "694656aa63d93a18fc9aa719b5b65bf2a082f455", "url": "https://github.com/apache/hudi/commit/694656aa63d93a18fc9aa719b5b65bf2a082f455", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T17:18:04Z", "type": "forcePushed"}, {"oid": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "url": "https://github.com/apache/hudi/commit/33a5a878d7a07f1797b33d32c336b33a66e4d727", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T17:37:52Z", "type": "forcePushed"}, {"oid": "9de32eef15c6fff7296ea0683a5143dc954c0d90", "url": "https://github.com/apache/hudi/commit/9de32eef15c6fff7296ea0683a5143dc954c0d90", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-17T23:01:04Z", "type": "forcePushed"}, {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "url": "https://github.com/apache/hudi/commit/a546978c03f71ee3beadd5ea7f65abb7635b407f", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-18T17:08:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzUxMw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493003513", "bodyText": "@satishkotha  : As discussed, All the replace filtering needs to move to getXXX() apis as the fetch APIs are only responsible for fetching file slices/base-files from different types of storage.", "author": "bvaradar", "createdAt": "2020-09-22T20:09:39Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -738,7 +799,9 @@ private String formatPartitionKey(String partitionStr) {\n    * @param commitsToReturn Commits\n    */\n   Stream<FileSlice> fetchLatestFileSliceInRange(List<String> commitsToReturn) {\n-    return fetchAllStoredFileGroups().map(fileGroup -> fileGroup.getLatestFileSliceInRange(commitsToReturn))\n+    return fetchAllStoredFileGroups()\n+        .filter(fileGroup -> !isFileGroupReplacedBeforeAny(fileGroup, commitsToReturn))", "originalCommit": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg4MTE1MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493881151", "bodyText": "Changed. Please take a look.", "author": "satishkotha", "createdAt": "2020-09-23T20:39:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzUxMw=="}], "type": "inlineReview", "revised_code": {"commit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 2b3cf6fd6..9fef00a8c 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -799,9 +826,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n    * @param commitsToReturn Commits\n    */\n   Stream<FileSlice> fetchLatestFileSliceInRange(List<String> commitsToReturn) {\n-    return fetchAllStoredFileGroups()\n-        .filter(fileGroup -> !isFileGroupReplacedBeforeAny(fileGroup, commitsToReturn))\n-        .map(fileGroup -> fileGroup.getLatestFileSliceInRange(commitsToReturn))\n+    return fetchAllStoredFileGroups().map(fileGroup -> fileGroup.getLatestFileSliceInRange(commitsToReturn))\n         .map(Option::get).map(this::addBootstrapBaseFileIfPresent);\n   }\n \n"}}, {"oid": "1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "url": "https://github.com/apache/hudi/commit/1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T18:43:57Z", "type": "forcePushed"}, {"oid": "7f3de1b7355303d56aa8f50809cbb4afc256472e", "url": "https://github.com/apache/hudi/commit/7f3de1b7355303d56aa8f50809cbb4afc256472e", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T19:11:44Z", "type": "forcePushed"}, {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "url": "https://github.com/apache/hudi/commit/980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-23T20:39:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NjI1Mw==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493956253", "bodyText": "@satishkotha : Dont we need to use instant time when checking for replaced-file here ?", "author": "bvaradar", "createdAt": "2020-09-23T23:41:53Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -425,10 +459,14 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partitionPath = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partitionPath);\n-      return fetchHoodieFileGroup(partitionPath, fileId).map(fileGroup -> fileGroup.getAllBaseFiles()\n-          .filter(baseFile -> HoodieTimeline.compareTimestamps(baseFile.getCommitTime(), HoodieTimeline.EQUALS,\n-              instantTime)).filter(df -> !isBaseFileDueToPendingCompaction(df)).findFirst().orElse(null))\n-          .map(df -> addBootstrapBaseFileIfPresent(new HoodieFileGroupId(partitionPath, fileId), df));\n+      if (isFileGroupReplaced(partitionPath, fileId)) {", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 9fef00a8c..7ea9b3784 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -459,7 +459,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n       readLock.lock();\n       String partitionPath = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partitionPath);\n-      if (isFileGroupReplaced(partitionPath, fileId)) {\n+      if (isFileGroupReplacedBeforeOrOn(new HoodieFileGroupId(partitionPath, fileId), instantTime)) {\n         return Option.empty();\n       } else {\n         return fetchHoodieFileGroup(partitionPath, fileId).map(fileGroup -> fileGroup.getAllBaseFiles()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NzU2MQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493957561", "bodyText": "same case here,, we need to use the maxInstantTime passed here instead of the timeline's maxInstant.", "author": "bvaradar", "createdAt": "2020-09-23T23:46:26Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -554,14 +608,16 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partition = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partition);\n-      return fetchAllStoredFileGroups(partition).map(fileGroup -> {\n-        Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\n-        // if the file-group is under construction, pick the latest before compaction instant time.\n-        if (fileSlice.isPresent()) {\n-          fileSlice = Option.of(fetchMergedFileSlice(fileGroup, fileSlice.get()));\n-        }\n-        return fileSlice;\n-      }).filter(Option::isPresent).map(Option::get).map(this::addBootstrapBaseFileIfPresent);\n+      return fetchAllStoredFileGroups(partition)\n+          .filter(fg -> !isFileGroupReplaced(fg.getFileGroupId()))", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 9fef00a8c..7ea9b3784 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -609,7 +609,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n       String partition = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partition);\n       return fetchAllStoredFileGroups(partition)\n-          .filter(fg -> !isFileGroupReplaced(fg.getFileGroupId()))\n+          .filter(fg -> !isFileGroupReplacedBeforeOrOn(fg.getFileGroupId(), maxInstantTime))\n           .map(fileGroup -> {\n             Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\n             // if the file-group is under construction, pick the latest before compaction instant time.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4NzgyOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493987828", "bodyText": "Doc needs fixing.", "author": "bvaradar", "createdAt": "2020-09-24T01:30:57Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\nindex c2d19f233..7b4c7c5ca 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java\n\n@@ -33,7 +33,7 @@ import java.util.List;\n import java.util.Map;\n \n /**\n- * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.\n+ * Helper class to generate commit metadata.\n  */\n public class CommitUtils {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MTg3Ng==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493991876", "bodyText": "rename removeReplacedFileIdsAtInstants ?", "author": "bvaradar", "createdAt": "2020-09-24T01:47:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +795,26 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Remove file groups that are replaced in any of the specified instants.\n+   */\n+  protected abstract void removeReplacedFileIds(Set<String> instants);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\nindex 9fef00a8c..7ea9b3784 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java\n\n@@ -808,7 +808,7 @@ public abstract class AbstractTableFileSystemView implements SyncableFileSystemV\n   /**\n    * Remove file groups that are replaced in any of the specified instants.\n    */\n-  protected abstract void removeReplacedFileIds(Set<String> instants);\n+  protected abstract void removeReplacedFileIdsAtInstants(Set<String> instants);\n \n   /**\n    * Track instant time for file groups replaced.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MzUzNg==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493993536", "bodyText": "this is no longer needed and can be removed ?", "author": "bvaradar", "createdAt": "2020-09-24T01:54:16Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -91,40 +92,47 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType, Collections.emptyMap());\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType) {\n+    return commitStats(instantTime, stats, extraMetadata, commitActionType, Collections.emptyMap());\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\nindex fa6376892..06e980db3 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n\n@@ -119,7 +119,6 @@ public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload> e\n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n     LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494007285", "bodyText": "We are creating metaclient and loading timeline once here and in the function called in the next line. Can you make sure you create metaclient only once without loading timeline.", "author": "bvaradar", "createdAt": "2020-09-24T02:49:20Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "originalCommit": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDcyNzQ4NA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494727484", "bodyText": "This is not calling function in the next line (calling method after that). So we only create meta client once. Please double check and let me know if i'm misinterpreting your suggestion.", "author": "satishkotha", "createdAt": "2020-09-25T03:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTMyOQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494731329", "bodyText": "My bad. Got confused with the method naming :)", "author": "bvaradar", "createdAt": "2020-09-25T03:48:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "url": "https://github.com/apache/hudi/commit/c281f2eb0d311c7b54ac850b831cea6d48a47502", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-25T03:27:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494731768", "bodyText": "Does this have to be non-static ? Can it be moved to CommitUtils ?", "author": "bvaradar", "createdAt": "2020-09-25T03:50:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType(), metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, actionType, metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  private void startCommitWithTime(String instantTime, String actionType, HoodieTableMetaClient metaClient) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType, metaClient);\n   }\n \n-  private void startCommit(String instantTime) {\n-    LOG.info(\"Generate a new instant time \" + instantTime);\n-    HoodieTableMetaClient metaClient = createMetaClient(true);\n+  private void startCommit(String instantTime, String actionType, HoodieTableMetaClient metaClient) {", "originalCommit": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzNzk4OQ==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r495137989", "bodyText": "This is a private method. Do you want to make this public static?  Personally, I think having all startCommit methods in HoodieWriteClient makes more sense because user workflow is\n\nwriteClient#startCommit\nwriteClient#upsert\nwriteClient#commit\n\nBut if you have a strong preference to make this part of CommitUtils, I can move it. let me know.", "author": "satishkotha", "createdAt": "2020-09-25T17:40:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcwMDQzOA==", "url": "https://github.com/apache/hudi/pull/2048#discussion_r495700438", "bodyText": "Lets leave it for now If needed, we can refactor later.", "author": "bvaradar", "createdAt": "2020-09-28T05:44:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "url": "https://github.com/apache/hudi/commit/d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T17:23:44Z", "type": "forcePushed"}, {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T20:11:48Z", "type": "commit"}, {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action", "committedDate": "2020-09-28T20:11:48Z", "type": "forcePushed"}]}