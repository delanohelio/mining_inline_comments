{"pr_number": 2190, "pr_title": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "pr_createdAt": "2020-10-20T15:04:27Z", "pr_url": "https://github.com/apache/hudi/pull/2190", "timeline": [{"oid": "5942ceea24442cc7964de848b2bdf74de18883f0", "url": "https://github.com/apache/hudi/commit/5942ceea24442cc7964de848b2bdf74de18883f0", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-21T14:58:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUzMDA5NQ==", "url": "https://github.com/apache/hudi/pull/2190#discussion_r511530095", "bodyText": "I think we can structure this as a if block without need for the else? since the if above anyway returns out.", "author": "vinothchandar", "createdAt": "2020-10-25T00:39:13Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java", "diffHunk": "@@ -85,53 +85,55 @@ public boolean next(NullWritable aVoid, ArrayWritable arrayWritable) throws IOEx\n       // if the result is false, then there are no more records\n       return false;\n     } else {\n-      // TODO(VC): Right now, we assume all records in log, have a matching base record. (which\n-      // would be true until we have a way to index logs too)\n-      // return from delta records map if we have some match.\n-      String key = arrayWritable.get()[HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS].toString();\n-      if (deltaRecordMap.containsKey(key)) {\n-        // TODO(NA): Invoke preCombine here by converting arrayWritable to Avro. This is required since the\n-        // deltaRecord may not be a full record and needs values of columns from the parquet\n-        Option<GenericRecord> rec;\n-        if (usesCustomPayload) {\n-          rec = deltaRecordMap.get(key).getData().getInsertValue(getWriterSchema());\n-        } else {\n-          rec = deltaRecordMap.get(key).getData().getInsertValue(getReaderSchema());\n-        }\n-        if (!rec.isPresent()) {\n-          // If the record is not present, this is a delete record using an empty payload so skip this base record\n-          // and move to the next record\n-          return next(aVoid, arrayWritable);\n-        }\n-        GenericRecord recordToReturn = rec.get();\n-        if (usesCustomPayload) {\n-          // If using a custom payload, return only the projection fields. The readerSchema is a schema derived from\n-          // the writerSchema with only the projection fields\n-          recordToReturn = HoodieAvroUtils.rewriteRecordWithOnlyNewSchemaFields(rec.get(), getReaderSchema());\n-        }\n-        // we assume, a later safe record in the log, is newer than what we have in the map &\n-        // replace it. Since we want to return an arrayWritable which is the same length as the elements in the latest\n-        // schema, we use writerSchema to create the arrayWritable from the latest generic record\n-        ArrayWritable aWritable = (ArrayWritable) HoodieRealtimeRecordReaderUtils.avroToArrayWritable(recordToReturn, getHiveSchema());\n-        Writable[] replaceValue = aWritable.get();\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(String.format(\"key %s, base values: %s, log values: %s\", key, HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable),\n-              HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable)));\n-        }\n-        Writable[] originalValue = arrayWritable.get();\n-        try {\n-          // Sometime originalValue.length > replaceValue.length.\n-          // This can happen when hive query is looking for pseudo parquet columns like BLOCK_OFFSET_INSIDE_FILE\n-          System.arraycopy(replaceValue, 0, originalValue, 0,\n-              Math.min(originalValue.length, replaceValue.length));\n-          arrayWritable.set(originalValue);\n-        } catch (RuntimeException re) {\n-          LOG.error(\"Got exception when doing array copy\", re);\n-          LOG.error(\"Base record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable));\n-          LOG.error(\"Log record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable));\n-          String errMsg = \"Base-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable)\n-              + \" ,Log-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable) + \" ,Error :\" + re.getMessage();\n-          throw new RuntimeException(errMsg, re);\n+      if (!deltaRecordMap.isEmpty()) {", "originalCommit": "5942ceea24442cc7964de848b2bdf74de18883f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0b25273ca3cb8c67dd61fa37458cdf27e90b8d6b", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java\nindex 5fb6bc329..b710b599b 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java\n\n@@ -84,60 +84,59 @@ class RealtimeCompactedRecordReader extends AbstractRealtimeRecordReader\n     if (!result) {\n       // if the result is false, then there are no more records\n       return false;\n-    } else {\n-      if (!deltaRecordMap.isEmpty()) {\n-        // TODO(VC): Right now, we assume all records in log, have a matching base record. (which\n-        // would be true until we have a way to index logs too)\n-        // return from delta records map if we have some match.\n-        String key = arrayWritable.get()[HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS].toString();\n-        if (deltaRecordMap.containsKey(key)) {\n-          // TODO(NA): Invoke preCombine here by converting arrayWritable to Avro. This is required since the\n-          // deltaRecord may not be a full record and needs values of columns from the parquet\n-          Option<GenericRecord> rec;\n-          if (usesCustomPayload) {\n-            rec = deltaRecordMap.get(key).getData().getInsertValue(getWriterSchema());\n-          } else {\n-            rec = deltaRecordMap.get(key).getData().getInsertValue(getReaderSchema());\n-          }\n-          if (!rec.isPresent()) {\n-            // If the record is not present, this is a delete record using an empty payload so skip this base record\n-            // and move to the next record\n-            return next(aVoid, arrayWritable);\n-          }\n-          GenericRecord recordToReturn = rec.get();\n-          if (usesCustomPayload) {\n-            // If using a custom payload, return only the projection fields. The readerSchema is a schema derived from\n-            // the writerSchema with only the projection fields\n-            recordToReturn = HoodieAvroUtils.rewriteRecordWithOnlyNewSchemaFields(rec.get(), getReaderSchema());\n-          }\n-          // we assume, a later safe record in the log, is newer than what we have in the map &\n-          // replace it. Since we want to return an arrayWritable which is the same length as the elements in the latest\n-          // schema, we use writerSchema to create the arrayWritable from the latest generic record\n-          ArrayWritable aWritable = (ArrayWritable) HoodieRealtimeRecordReaderUtils.avroToArrayWritable(recordToReturn, getHiveSchema());\n-          Writable[] replaceValue = aWritable.get();\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(String.format(\"key %s, base values: %s, log values: %s\", key, HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable),\n-                HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable)));\n-          }\n-          Writable[] originalValue = arrayWritable.get();\n-          try {\n-            // Sometime originalValue.length > replaceValue.length.\n-            // This can happen when hive query is looking for pseudo parquet columns like BLOCK_OFFSET_INSIDE_FILE\n-            System.arraycopy(replaceValue, 0, originalValue, 0,\n-                Math.min(originalValue.length, replaceValue.length));\n-            arrayWritable.set(originalValue);\n-          } catch (RuntimeException re) {\n-            LOG.error(\"Got exception when doing array copy\", re);\n-            LOG.error(\"Base record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable));\n-            LOG.error(\"Log record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable));\n-            String errMsg = \"Base-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable)\n-                + \" ,Log-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable) + \" ,Error :\" + re.getMessage();\n-            throw new RuntimeException(errMsg, re);\n-          }\n+    }\n+    if (!deltaRecordMap.isEmpty()) {\n+      // TODO(VC): Right now, we assume all records in log, have a matching base record. (which\n+      // would be true until we have a way to index logs too)\n+      // return from delta records map if we have some match.\n+      String key = arrayWritable.get()[HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS].toString();\n+      if (deltaRecordMap.containsKey(key)) {\n+        // TODO(NA): Invoke preCombine here by converting arrayWritable to Avro. This is required since the\n+        // deltaRecord may not be a full record and needs values of columns from the parquet\n+        Option<GenericRecord> rec;\n+        if (usesCustomPayload) {\n+          rec = deltaRecordMap.get(key).getData().getInsertValue(getWriterSchema());\n+        } else {\n+          rec = deltaRecordMap.get(key).getData().getInsertValue(getReaderSchema());\n+        }\n+        if (!rec.isPresent()) {\n+          // If the record is not present, this is a delete record using an empty payload so skip this base record\n+          // and move to the next record\n+          return next(aVoid, arrayWritable);\n+        }\n+        GenericRecord recordToReturn = rec.get();\n+        if (usesCustomPayload) {\n+          // If using a custom payload, return only the projection fields. The readerSchema is a schema derived from\n+          // the writerSchema with only the projection fields\n+          recordToReturn = HoodieAvroUtils.rewriteRecordWithOnlyNewSchemaFields(rec.get(), getReaderSchema());\n+        }\n+        // we assume, a later safe record in the log, is newer than what we have in the map &\n+        // replace it. Since we want to return an arrayWritable which is the same length as the elements in the latest\n+        // schema, we use writerSchema to create the arrayWritable from the latest generic record\n+        ArrayWritable aWritable = (ArrayWritable) HoodieRealtimeRecordReaderUtils.avroToArrayWritable(recordToReturn, getHiveSchema());\n+        Writable[] replaceValue = aWritable.get();\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(String.format(\"key %s, base values: %s, log values: %s\", key, HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable),\n+              HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable)));\n+        }\n+        Writable[] originalValue = arrayWritable.get();\n+        try {\n+          // Sometime originalValue.length > replaceValue.length.\n+          // This can happen when hive query is looking for pseudo parquet columns like BLOCK_OFFSET_INSIDE_FILE\n+          System.arraycopy(replaceValue, 0, originalValue, 0,\n+              Math.min(originalValue.length, replaceValue.length));\n+          arrayWritable.set(originalValue);\n+        } catch (RuntimeException re) {\n+          LOG.error(\"Got exception when doing array copy\", re);\n+          LOG.error(\"Base record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable));\n+          LOG.error(\"Log record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable));\n+          String errMsg = \"Base-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable)\n+              + \" ,Log-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable) + \" ,Error :\" + re.getMessage();\n+          throw new RuntimeException(errMsg, re);\n         }\n       }\n-      return true;\n     }\n+    return true;\n   }\n \n   @Override\n"}}, {"oid": "3b1d7f9e670b2c75f32fe618100279d00d4dccab", "url": "https://github.com/apache/hudi/commit/3b1d7f9e670b2c75f32fe618100279d00d4dccab", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-29T16:31:28Z", "type": "forcePushed"}, {"oid": "887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "url": "https://github.com/apache/hudi/commit/887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-30T14:45:22Z", "type": "commit"}, {"oid": "887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "url": "https://github.com/apache/hudi/commit/887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-30T14:45:22Z", "type": "forcePushed"}, {"oid": "0b25273ca3cb8c67dd61fa37458cdf27e90b8d6b", "url": "https://github.com/apache/hudi/commit/0b25273ca3cb8c67dd61fa37458cdf27e90b8d6b", "message": "[HUDI-892]  for test", "committedDate": "2020-10-31T16:19:26Z", "type": "forcePushed"}, {"oid": "6555b66894ae3af625dc00657a6de52d1b6c34dd", "url": "https://github.com/apache/hudi/commit/6555b66894ae3af625dc00657a6de52d1b6c34dd", "message": "[HUDI-892]  for test", "committedDate": "2020-11-01T05:51:11Z", "type": "commit"}, {"oid": "b3a21af65c6717596ccaff03dedb15311c099f33", "url": "https://github.com/apache/hudi/commit/b3a21af65c6717596ccaff03dedb15311c099f33", "message": " [HUDI-892] fix bug generate array from split", "committedDate": "2020-11-01T05:52:03Z", "type": "forcePushed"}, {"oid": "23a52970b597dae4d198063a81e4c36cfe6e607e", "url": "https://github.com/apache/hudi/commit/23a52970b597dae4d198063a81e4c36cfe6e607e", "message": " [HUDI-892] fix bug generate array from split", "committedDate": "2020-11-01T08:47:18Z", "type": "forcePushed"}, {"oid": "b76a393c761b4b8f6e27d387caf99185ad008896", "url": "https://github.com/apache/hudi/commit/b76a393c761b4b8f6e27d387caf99185ad008896", "message": " [HUDI-892]  fix bug generate array from split", "committedDate": "2020-11-01T10:30:47Z", "type": "commit"}, {"oid": "b76a393c761b4b8f6e27d387caf99185ad008896", "url": "https://github.com/apache/hudi/commit/b76a393c761b4b8f6e27d387caf99185ad008896", "message": " [HUDI-892]  fix bug generate array from split", "committedDate": "2020-11-01T10:30:47Z", "type": "forcePushed"}, {"oid": "0902d583419fd907db649de5684cfd9ed1012b43", "url": "https://github.com/apache/hudi/commit/0902d583419fd907db649de5684cfd9ed1012b43", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T06:00:29Z", "type": "forcePushed"}, {"oid": "3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "url": "https://github.com/apache/hudi/commit/3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T10:21:10Z", "type": "commit"}, {"oid": "3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "url": "https://github.com/apache/hudi/commit/3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T10:21:10Z", "type": "forcePushed"}]}