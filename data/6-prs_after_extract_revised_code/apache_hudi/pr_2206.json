{"pr_number": 2206, "pr_title": "[HUDI-1105] Adding dedup support for Bulk Insert w/ Rows", "pr_createdAt": "2020-10-25T17:09:37Z", "pr_url": "https://github.com/apache/hudi/pull/2206", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r511623934", "bodyText": "@bvaradar : Is there some other option to go about deduping multiple Rows. Bcoz, in Bulksert with Rows, we don't have any HoodiePayload. Hence we have to operate on \"Row\"s only. So, have to group by keys and reduce by a function. In this patch, I have designed the function as this interface.\nSo, two questions.\na. Is there a better option.\nb. Even if we go with this option, I am getting task not serializable when executing this, since avro Schema is also sent along with Row. Also, wondering this might have any performance complications.\nCaused by: java.io.NotSerializableException: org.apache.avro.Schema$RecordSchema\nSerialization stack:\n\t- object not serializable (class: org.apache.avro.Schema$RecordSchema, value: {\"type\":\"record\",\"name\":\"trip\",\"namespace\":\"example.schema\",\"fields\":[{\"name\":\"_row_key\",\"type\":\"string\"},{\"name\":\"partition\",\"type\":\"string\"},{\"name\":\"ts\",\"type\":[\"long\",\"null\"]}]})\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper, name: schema, type: class org.apache.avro.Schema)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper, org.apache.hudi.TestHoodieDatasetBulkInsertHelper@8d7718e)\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, name: this$0, type: class org.apache.hudi.TestHoodieDatasetBulkInsertHelper)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow@3436d3d7)\n\t- element of array (index: 0)\n\t- array (class [Ljava.lang.Object;, size 1)\n\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)\n\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.hudi.SparkRowWriteHelper, functionalInterfaceMethod=org/apache/spark/api/java/function/ReduceFunction.call:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/hudi/SparkRowWriteHelper.lambda$deduplicateRows$14bf715c$1:(Lorg/apache/hudi/PreCombineRow;Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, instantiatedMethodType=(Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, numCaptured=1])\n\t- writeReplace data (class: java.lang.invoke.SerializedLambda)\n\t- object (class org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618, org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618@19d9ba89)\n\t- field (class: org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, name: f$4, type: interface org.apache.spark.api.java.function.ReduceFunction)\n\t- object (class org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, <function2>)\n\t- field (class: org.apache.spark.sql.expressions.ReduceAggregator, name: func, type: interface scala.Function2)\n\t- object (class org.apache.spark.sql.expressions.ReduceAggregator, org.apache.spark.sql.expressions.ReduceAggregator@14af73e1)", "author": "nsivabalan", "createdAt": "2020-10-25T17:15:13Z", "path": "hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.spark.sql.Row;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface used to preCombine two Spark sql Rows.\n+ */\n+public interface PreCombineRow extends Serializable {", "originalCommit": "d46a16f158358ed35528cd916b7e33ce3127904c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTQ5NzU5Nw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519497597", "bodyText": "What is holding the avro schema? this seems like a member held in the class you wrote?", "author": "vinothchandar", "createdAt": "2020-11-09T00:05:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMTAyNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519501024", "bodyText": "nope. Thats is what is confusing me.\n  class TestPreCombineRow implements PreCombineRow {\n\n    @Override\n    public Row combineTwoRows(Row v1, Row v2) {\n      long tsV1 = v1.getAs(\"ts\");\n      long tsV2 = v2.getAs(\"ts\");\n      return (tsV1 >= tsV2) ? v1 : v2;\n    }\n  }", "author": "nsivabalan", "createdAt": "2020-11-09T00:30:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMTYyMg==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519501622", "bodyText": "let me investigate this. might be in my dedup logic.\npublic Dataset<Row> deduplicateRows(Dataset<Row> inputDf, PreCombineRow preCombineRow) {\n    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n    return inputDf.groupByKey(\n        (MapFunction<Row, String>) value -> value.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD) + \"+\" + value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD), Encoders.STRING())\n        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> preCombineRow.combineTwoRows(v1, v2)).map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n  }", "author": "nsivabalan", "createdAt": "2020-11-09T00:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUwMzY2NA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r519503664", "bodyText": "yeah, no idea why the reduce fn is giving an issue. Casting to comprable works, but this may not be good enough right. We need to think about complex fields and not just simple fields. Also, we might have to support user defined preCombine.\ngroupedDataset.reduceGroups((ReduceFunction<Row>) (v1, v2) ->\n    {\n      int compareVal = ((Comparable) v1.get(fieldIndex)).compareTo(v2.get(fieldIndex));\n      if (compareVal >= 0) {\n        return v1;\n      } else {\n        return v2;\n      }\n    });", "author": "nsivabalan", "createdAt": "2020-11-09T00:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI2NzU4OQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r520267589", "bodyText": "@nsivabalan could we take the same approach as key generators, extend HoodieRecordPayload with new methods for specifying how two rows should combine, but still honor the avro based impl, by converting row -> avro as needed?", "author": "vinothchandar", "createdAt": "2020-11-10T03:45:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA=="}], "type": "inlineReview", "revised_code": {"commit": "5d68e035eef1ef37b2121bafc67e1f81d85aea74", "chunk": "diff --git a/hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/PreCombineRow.java\nsimilarity index 96%\nrename from hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java\nrename to hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/PreCombineRow.java\nindex 8583611eda..0d38ac5896 100644\n--- a/hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/PreCombineRow.java\n\n@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi;\n+package org.apache.hudi.execution.bulkinsert;\n \n import org.apache.spark.sql.Row;\n \n"}}, {"oid": "15d0db55fb82ef35633ab1807e0327954ec61233", "url": "https://github.com/apache/hudi/commit/15d0db55fb82ef35633ab1807e0327954ec61233", "message": "Fixing dedup support", "committedDate": "2020-12-01T17:06:46Z", "type": "forcePushed"}, {"oid": "01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "url": "https://github.com/apache/hudi/commit/01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "message": "Fixing dedup support", "committedDate": "2021-05-23T04:27:32Z", "type": "forcePushed"}, {"oid": "63ca76bbba456b67e5ce764907935731ff35b146", "url": "https://github.com/apache/hudi/commit/63ca76bbba456b67e5ce764907935731ff35b146", "message": "Fixing build failure", "committedDate": "2021-05-24T06:16:17Z", "type": "forcePushed"}, {"oid": "39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "url": "https://github.com/apache/hudi/commit/39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "message": "Fixing build failure", "committedDate": "2021-05-24T06:33:08Z", "type": "forcePushed"}, {"oid": "5d68e035eef1ef37b2121bafc67e1f81d85aea74", "url": "https://github.com/apache/hudi/commit/5d68e035eef1ef37b2121bafc67e1f81d85aea74", "message": "Some refactoring and fixing tests", "committedDate": "2021-06-07T20:00:50Z", "type": "forcePushed"}, {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "url": "https://github.com/apache/hudi/commit/031b8fa2a947f69815bce9fa181dc98dd972d07e", "message": "Fixing tests", "committedDate": "2021-06-08T02:31:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NDIwNQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664284205", "bodyText": "all these configs need to be redone based ConfigProperty/HoodieConfig", "author": "vinothchandar", "createdAt": "2021-07-06T06:56:33Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -98,6 +98,8 @@\n   public static final String DEFAULT_COMBINE_BEFORE_UPSERT = \"true\";\n   public static final String COMBINE_BEFORE_DELETE_PROP = \"hoodie.combine.before.delete\";\n   public static final String DEFAULT_COMBINE_BEFORE_DELETE = \"true\";\n+  public static final String COMBINE_BEFORE_BULK_INSERT_PROP = \"hoodie.combine.before.bulk.insert\";", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDY5NjEzNw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664696137", "bodyText": "yes.", "author": "nsivabalan", "createdAt": "2021-07-06T16:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NDIwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "80ae3446670e4012ef3896477964bb916b71a864", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 9b71b95b49..4a2f2c2fb2 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -51,120 +53,275 @@ import java.io.IOException;\n import java.io.InputStream;\n import java.util.Arrays;\n import java.util.List;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Properties;\n import java.util.function.Supplier;\n import java.util.stream.Collectors;\n \n-import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\n-import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\n-\n /**\n  * Class storing configs for the HoodieWriteClient.\n  */\n @Immutable\n-public class HoodieWriteConfig extends DefaultHoodieConfig {\n+public class HoodieWriteConfig extends HoodieConfig {\n \n   private static final long serialVersionUID = 0L;\n \n-  public static final String TABLE_NAME = \"hoodie.table.name\";\n-  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n-  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n-  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n-  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n-  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();\n-  public static final String DEFAULT_ROLLBACK_USING_MARKERS = \"false\";\n-  public static final String ROLLBACK_USING_MARKERS = \"hoodie.rollback.using.markers\";\n-  public static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n-  public static final String BASE_PATH_PROP = \"hoodie.base.path\";\n-  public static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n-  public static final String AVRO_SCHEMA_VALIDATE = \"hoodie.avro.schema.validate\";\n-  public static final String DEFAULT_AVRO_SCHEMA_VALIDATE = \"false\";\n-  public static final String DEFAULT_PARALLELISM = \"1500\";\n-  public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n-  public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n-  public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n-  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulkinsert.schema.ddl\";\n-  public static final String UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\";\n-  public static final String DELETE_PARALLELISM = \"hoodie.delete.shuffle.parallelism\";\n-  public static final String DEFAULT_ROLLBACK_PARALLELISM = \"100\";\n-  public static final String ROLLBACK_PARALLELISM = \"hoodie.rollback.parallelism\";\n-  public static final String WRITE_BUFFER_LIMIT_BYTES = \"hoodie.write.buffer.limit.bytes\";\n-  public static final String DEFAULT_WRITE_BUFFER_LIMIT_BYTES = String.valueOf(4 * 1024 * 1024);\n-  public static final String COMBINE_BEFORE_INSERT_PROP = \"hoodie.combine.before.insert\";\n-  public static final String DEFAULT_COMBINE_BEFORE_INSERT = \"false\";\n-  public static final String COMBINE_BEFORE_UPSERT_PROP = \"hoodie.combine.before.upsert\";\n-  public static final String DEFAULT_COMBINE_BEFORE_UPSERT = \"true\";\n-  public static final String COMBINE_BEFORE_DELETE_PROP = \"hoodie.combine.before.delete\";\n-  public static final String DEFAULT_COMBINE_BEFORE_DELETE = \"true\";\n-  public static final String COMBINE_BEFORE_BULK_INSERT_PROP = \"hoodie.combine.before.bulk.insert\";\n-  public static final String DEFAULT_COMBINE_BEFORE_BULK_INSERT = \"false\";\n-  public static final String WRITE_STATUS_STORAGE_LEVEL = \"hoodie.write.status.storage.level\";\n-  public static final String DEFAULT_WRITE_STATUS_STORAGE_LEVEL = \"MEMORY_AND_DISK_SER\";\n-  public static final String HOODIE_AUTO_COMMIT_PROP = \"hoodie.auto.commit\";\n-  public static final String DEFAULT_HOODIE_AUTO_COMMIT = \"true\";\n-\n-  public static final String HOODIE_WRITE_STATUS_CLASS_PROP = \"hoodie.writestatus.class\";\n-  public static final String DEFAULT_HOODIE_WRITE_STATUS_CLASS = WriteStatus.class.getName();\n-  public static final String FINALIZE_WRITE_PARALLELISM = \"hoodie.finalize.write.parallelism\";\n-  public static final String DEFAULT_FINALIZE_WRITE_PARALLELISM = DEFAULT_PARALLELISM;\n-  public static final String MARKERS_DELETE_PARALLELISM = \"hoodie.markers.delete.parallelism\";\n-  public static final String DEFAULT_MARKERS_DELETE_PARALLELISM = \"100\";\n-  public static final String BULKINSERT_SORT_MODE = \"hoodie.bulkinsert.sort.mode\";\n-  public static final String DEFAULT_BULKINSERT_SORT_MODE = BulkInsertSortMode.GLOBAL_SORT\n-      .toString();\n-\n-  public static final String EMBEDDED_TIMELINE_SERVER_ENABLED = \"hoodie.embed.timeline.server\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ENABLED = \"true\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = \"hoodie.embed.timeline.server.reuse.enabled\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = \"false\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_PORT = \"hoodie.embed.timeline.server.port\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_PORT = \"0\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_THREADS = \"hoodie.embed.timeline.server.threads\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_THREADS = \"-1\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT = \"hoodie.embed.timeline.server.gzip\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_COMPRESS_OUTPUT = \"true\";\n-  public static final String EMBEDDED_TIMELINE_SERVER_USE_ASYNC = \"hoodie.embed.timeline.server.async\";\n-  public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ASYNC = \"false\";\n-\n-  public static final String FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP = \"hoodie.fail.on.timeline.archiving\";\n-  public static final String DEFAULT_FAIL_ON_TIMELINE_ARCHIVING_ENABLED = \"true\";\n-  // time between successive attempts to ensure written data's metadata is consistent on storage\n-  public static final String INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP =\n-      \"hoodie.consistency.check.initial_interval_ms\";\n-  public static long DEFAULT_INITIAL_CONSISTENCY_CHECK_INTERVAL_MS = 2000L;\n-\n-  // max interval time\n-  public static final String MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP = \"hoodie.consistency.check.max_interval_ms\";\n-  public static long DEFAULT_MAX_CONSISTENCY_CHECK_INTERVAL_MS = 300000L;\n-\n-  // maximum number of checks, for consistency of written data. Will wait upto 256 Secs\n-  public static final String MAX_CONSISTENCY_CHECKS_PROP = \"hoodie.consistency.check.max_checks\";\n-  public static int DEFAULT_MAX_CONSISTENCY_CHECKS = 7;\n-\n-  // Data validation check performed during merges before actual commits\n-  private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n-  private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n-\n-  // Allow duplicates with inserts while merging with existing records\n-  private static final String MERGE_ALLOW_DUPLICATE_ON_INSERTS = \"hoodie.merge.allow.duplicate.on.inserts\";\n-  private static final String DEFAULT_MERGE_ALLOW_DUPLICATE_ON_INSERTS = \"false\";\n-\n-  public static final String CLIENT_HEARTBEAT_INTERVAL_IN_MS_PROP = \"hoodie.client.heartbeat.interval_in_ms\";\n-  public static final Integer DEFAULT_CLIENT_HEARTBEAT_INTERVAL_IN_MS = 60 * 1000;\n-\n-  public static final String CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES_PROP = \"hoodie.client.heartbeat.tolerable.misses\";\n-  public static final Integer DEFAULT_CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES = 2;\n-  // Enable different concurrency support\n-  public static final String WRITE_CONCURRENCY_MODE_PROP =\n-      \"hoodie.write.concurrency.mode\";\n-  public static final String DEFAULT_WRITE_CONCURRENCY_MODE = WriteConcurrencyMode.SINGLE_WRITER.name();\n-\n-  // Comma separated metadata key prefixes to override from latest commit during overlapping commits via multi writing\n-  public static final String WRITE_META_KEY_PREFIXES_PROP =\n-      \"hoodie.write.meta.key.prefixes\";\n-  public static final String DEFAULT_WRITE_META_KEY_PREFIXES = \"\";\n+  public static final ConfigProperty<String> TABLE_NAME = ConfigProperty\n+      .key(\"hoodie.table.name\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Table name that will be used for registering with Hive. Needs to be same across runs.\");\n+\n+  public static final ConfigProperty<String> PRECOMBINE_FIELD_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.precombine.field\")\n+      .defaultValue(\"ts\")\n+      .withDocumentation(\"Field used in preCombining before actual write. When two records have the same key value, \"\n+          + \"we will pick the one with the largest value for the precombine field, determined by Object.compareTo(..)\");\n+\n+  public static final ConfigProperty<String> WRITE_PAYLOAD_CLASS = ConfigProperty\n+      .key(\"hoodie.datasource.write.payload.class\")\n+      .defaultValue(OverwriteWithLatestAvroPayload.class.getName())\n+      .withDocumentation(\"Payload class used. Override this, if you like to roll your own merge logic, when upserting/inserting. \"\n+          + \"This will render any value set for PRECOMBINE_FIELD_OPT_VAL in-effective\");\n+\n+  public static final ConfigProperty<String> KEYGENERATOR_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.keygenerator.class\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Key generator class, that implements will extract the key out of incoming Row object\");\n+\n+  public static final ConfigProperty<String> KEYGENERATOR_TYPE_PROP = ConfigProperty\n+      .key(\"hoodie.datasource.write.keygenerator.type\")\n+      .defaultValue(KeyGeneratorType.SIMPLE.name())\n+      .withDocumentation(\"Type of build-in key generator, currently support SIMPLE, COMPLEX, TIMESTAMP, CUSTOM, NON_PARTITION, GLOBAL_DELETE\");\n+\n+  public static final ConfigProperty<String> ROLLBACK_USING_MARKERS = ConfigProperty\n+      .key(\"hoodie.rollback.using.markers\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Enables a more efficient mechanism for rollbacks based on the marker files generated \"\n+          + \"during the writes. Turned off by default.\");\n+\n+  public static final ConfigProperty<String> TIMELINE_LAYOUT_VERSION = ConfigProperty\n+      .key(\"hoodie.timeline.layout.version\")\n+      .noDefaultValue()\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> BASE_PATH_PROP = ConfigProperty\n+      .key(\"hoodie.base.path\")\n+      .noDefaultValue()\n+      .withDocumentation(\"Base DFS path under which all the data partitions are created. \"\n+          + \"Always prefix it explicitly with the storage scheme (e.g hdfs://, s3:// etc). \"\n+          + \"Hudi stores all the main meta-data about commits, savepoints, cleaning audit logs \"\n+          + \"etc in .hoodie directory under the base directory.\");\n+\n+  public static final ConfigProperty<String> AVRO_SCHEMA = ConfigProperty\n+      .key(\"hoodie.avro.schema\")\n+      .noDefaultValue()\n+      .withDocumentation(\"This is the current reader avro schema for the table. This is a string of the entire schema. \"\n+          + \"HoodieWriteClient uses this schema to pass on to implementations of HoodieRecordPayload to convert \"\n+          + \"from the source format to avro record. This is also used when re-writing records during an update.\");\n+\n+  public static final ConfigProperty<String> AVRO_SCHEMA_VALIDATE = ConfigProperty\n+      .key(\"hoodie.avro.schema.validate\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> INSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.insert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Once data has been initially imported, this parallelism controls initial parallelism for reading input records. \"\n+          + \"Ensure this value is high enough say: 1 partition for 1 GB of input data\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.bulkinsert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Bulk insert is meant to be used for large initial imports and this parallelism determines \"\n+          + \"the initial number of files in your table. Tune this to achieve a desired optimal size during initial import.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = ConfigProperty\n+      .key(\"hoodie.bulkinsert.user.defined.partitioner.class\")\n+      .noDefaultValue()\n+      .withDocumentation(\"If specified, this class will be used to re-partition input records before they are inserted.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_INPUT_DATA_SCHEMA_DDL = ConfigProperty\n+      .key(\"hoodie.bulkinsert.schema.ddl\")\n+      .noDefaultValue()\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> UPSERT_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.upsert.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"Once data has been initially imported, this parallelism controls initial parallelism for reading input records. \"\n+          + \"Ensure this value is high enough say: 1 partition for 1 GB of input data\");\n+\n+  public static final ConfigProperty<String> DELETE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.delete.shuffle.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"This parallelism is Used for \u201cdelete\u201d operation while deduping or repartioning.\");\n+\n+  public static final ConfigProperty<String> ROLLBACK_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.rollback.parallelism\")\n+      .defaultValue(\"100\")\n+      .withDocumentation(\"Determines the parallelism for rollback of commits.\");\n+\n+  public static final ConfigProperty<String> WRITE_BUFFER_LIMIT_BYTES = ConfigProperty\n+      .key(\"hoodie.write.buffer.limit.bytes\")\n+      .defaultValue(String.valueOf(4 * 1024 * 1024))\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_INSERT_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.insert\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Flag which first combines the input RDD and merges multiple partial records into a single record \"\n+          + \"before inserting or updating in DFS\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_UPSERT_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.upsert\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Flag which first combines the input RDD and merges multiple partial records into a single record \"\n+          + \"before inserting or updating in DFS\");\n+\n+  public static final ConfigProperty<String> COMBINE_BEFORE_DELETE_PROP = ConfigProperty\n+      .key(\"hoodie.combine.before.delete\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Flag which first combines the input RDD and merges multiple partial records into a single record \"\n+          + \"before deleting in DFS\");\n+\n+  public static final ConfigProperty<String> WRITE_STATUS_STORAGE_LEVEL = ConfigProperty\n+      .key(\"hoodie.write.status.storage.level\")\n+      .defaultValue(\"MEMORY_AND_DISK_SER\")\n+      .withDocumentation(\"HoodieWriteClient.insert and HoodieWriteClient.upsert returns a persisted RDD[WriteStatus], \"\n+          + \"this is because the Client can choose to inspect the WriteStatus and choose and commit or not based on the failures. \"\n+          + \"This is a configuration for the storage level for this RDD\");\n+\n+  public static final ConfigProperty<String> HOODIE_AUTO_COMMIT_PROP = ConfigProperty\n+      .key(\"hoodie.auto.commit\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"Should HoodieWriteClient autoCommit after insert and upsert. \"\n+          + \"The client can choose to turn off auto-commit and commit on a \u201cdefined success condition\u201d\");\n+\n+  public static final ConfigProperty<String> HOODIE_WRITE_STATUS_CLASS_PROP = ConfigProperty\n+      .key(\"hoodie.writestatus.class\")\n+      .defaultValue(WriteStatus.class.getName())\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> FINALIZE_WRITE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.finalize.write.parallelism\")\n+      .defaultValue(\"1500\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> MARKERS_DELETE_PARALLELISM = ConfigProperty\n+      .key(\"hoodie.markers.delete.parallelism\")\n+      .defaultValue(\"100\")\n+      .withDocumentation(\"Determines the parallelism for deleting marker files.\");\n+\n+  public static final ConfigProperty<String> BULKINSERT_SORT_MODE = ConfigProperty\n+      .key(\"hoodie.bulkinsert.sort.mode\")\n+      .defaultValue(BulkInsertSortMode.GLOBAL_SORT.toString())\n+      .withDocumentation(\"Sorting modes to use for sorting records for bulk insert. This is leveraged when user \"\n+          + \"defined partitioner is not configured. Default is GLOBAL_SORT. Available values are - GLOBAL_SORT: \"\n+          + \"this ensures best file sizes, with lowest memory overhead at cost of sorting. PARTITION_SORT: \"\n+          + \"Strikes a balance by only sorting within a partition, still keeping the memory overhead of writing \"\n+          + \"lowest and best effort file sizing. NONE: No sorting. Fastest and matches spark.write.parquet() \"\n+          + \"in terms of number of files, overheads\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_ENABLED = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.reuse.enabled\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_PORT = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.port\")\n+      .defaultValue(\"0\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_THREADS = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.threads\")\n+      .defaultValue(\"-1\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.gzip\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> EMBEDDED_TIMELINE_SERVER_USE_ASYNC = ConfigProperty\n+      .key(\"hoodie.embed.timeline.server.async\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP = ConfigProperty\n+      .key(\"hoodie.fail.on.timeline.archiving\")\n+      .defaultValue(\"true\")\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<Long> INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.initial_interval_ms\")\n+      .defaultValue(2000L)\n+      .withDocumentation(\"Time between successive attempts to ensure written data's metadata is consistent on storage\");\n+\n+  public static final ConfigProperty<Long> MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.max_interval_ms\")\n+      .defaultValue(300000L)\n+      .withDocumentation(\"Max interval time for consistency check\");\n+\n+  public static final ConfigProperty<Integer> MAX_CONSISTENCY_CHECKS_PROP = ConfigProperty\n+      .key(\"hoodie.consistency.check.max_checks\")\n+      .defaultValue(7)\n+      .withDocumentation(\"Maximum number of checks, for consistency of written data. Will wait upto 256 Secs\");\n+\n+  public static final ConfigProperty<String> MERGE_DATA_VALIDATION_CHECK_ENABLED = ConfigProperty\n+      .key(\"hoodie.merge.data.validation.enabled\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Data validation check performed during merges before actual commits\");\n+\n+  public static final ConfigProperty<String> MERGE_ALLOW_DUPLICATE_ON_INSERTS = ConfigProperty\n+      .key(\"hoodie.merge.allow.duplicate.on.inserts\")\n+      .defaultValue(\"false\")\n+      .withDocumentation(\"Allow duplicates with inserts while merging with existing records\");\n+\n+  public static final ConfigProperty<ExternalSpillableMap.DiskMapType> SPILLABLE_DISK_MAP_TYPE = ConfigProperty\n+      .key(\"hoodie.spillable.diskmap.type\")\n+      .defaultValue(ExternalSpillableMap.DiskMapType.BITCASK)\n+      .withDocumentation(\"Enable usage of either BITCASK or ROCKS_DB as disk map for External Spillable Map\");\n+\n+  public static final ConfigProperty<Integer> CLIENT_HEARTBEAT_INTERVAL_IN_MS_PROP = ConfigProperty\n+      .key(\"hoodie.client.heartbeat.interval_in_ms\")\n+      .defaultValue(60 * 1000)\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<Integer> CLIENT_HEARTBEAT_NUM_TOLERABLE_MISSES_PROP = ConfigProperty\n+      .key(\"hoodie.client.heartbeat.tolerable.misses\")\n+      .defaultValue(2)\n+      .withDocumentation(\"\");\n+\n+  public static final ConfigProperty<String> WRITE_CONCURRENCY_MODE_PROP = ConfigProperty\n+      .key(\"hoodie.write.concurrency.mode\")\n+      .defaultValue(WriteConcurrencyMode.SINGLE_WRITER.name())\n+      .withDocumentation(\"Enable different concurrency support\");\n+\n+  public static final ConfigProperty<String> WRITE_META_KEY_PREFIXES_PROP = ConfigProperty\n+      .key(\"hoodie.write.meta.key.prefixes\")\n+      .defaultValue(\"\")\n+      .withDocumentation(\"Comma separated metadata key prefixes to override from latest commit \"\n+          + \"during overlapping commits via multi writing\");\n+\n+  /**\n+   * The specified write schema. In most case, we do not need set this parameter,\n+   * but for the case the write schema is not equal to the specified table schema, we can\n+   * specify the write schema by this parameter.\n+   *\n+   * Currently the MergeIntoHoodieTableCommand use this to specify the write schema.\n+   */\n+  public static final ConfigProperty<String> WRITE_SCHEMA_PROP = ConfigProperty\n+      .key(\"hoodie.write.schema\")\n+      .noDefaultValue()\n+      .withDocumentation(\"\");\n \n   /**\n    * HUDI-858 : There are users who had been directly using RDD APIs and have relied on a behavior in 0.4.x to allow\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NTIyOQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664285229", "bodyText": "So far, we have used one config combine.before.insert to control it for both insert and bulk_insert. Can we keep it the same way? Otherwise, wont it be backwards incompatible, ie a user can be expecting the combine.before.insert continue to take effect for bulk_insert as well and it won't be the case?", "author": "vinothchandar", "createdAt": "2021-07-06T06:58:25Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -306,6 +308,10 @@ public boolean shouldCombineBeforeInsert() {\n     return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_INSERT_PROP));\n   }\n \n+  public boolean shouldCombineBeforeBulkInsert() {", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDY5NjQwMw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664696403", "bodyText": "yeah, makes sense to use combine.before.insert only. will remove the new config.", "author": "nsivabalan", "createdAt": "2021-07-06T16:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NTIyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "80ae3446670e4012ef3896477964bb916b71a864", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex 9b71b95b49..4a2f2c2fb2 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -297,96 +473,96 @@ public class HoodieWriteConfig extends DefaultHoodieConfig {\n   }\n \n   public boolean shouldRollbackUsingMarkers() {\n-    return Boolean.parseBoolean(props.getProperty(ROLLBACK_USING_MARKERS));\n+    return getBoolean(ROLLBACK_USING_MARKERS);\n   }\n \n   public int getWriteBufferLimitBytes() {\n-    return Integer.parseInt(props.getProperty(WRITE_BUFFER_LIMIT_BYTES, DEFAULT_WRITE_BUFFER_LIMIT_BYTES));\n+    return Integer.parseInt(getStringOrDefault(WRITE_BUFFER_LIMIT_BYTES));\n   }\n \n   public boolean shouldCombineBeforeInsert() {\n-    return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_INSERT_PROP));\n-  }\n-\n-  public boolean shouldCombineBeforeBulkInsert() {\n-    return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_BULK_INSERT_PROP));\n+    return getBoolean(COMBINE_BEFORE_INSERT_PROP);\n   }\n \n   public boolean shouldCombineBeforeUpsert() {\n-    return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_UPSERT_PROP));\n+    return getBoolean(COMBINE_BEFORE_UPSERT_PROP);\n   }\n \n   public boolean shouldCombineBeforeDelete() {\n-    return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_DELETE_PROP));\n+    return getBoolean(COMBINE_BEFORE_DELETE_PROP);\n   }\n \n   public boolean shouldAllowMultiWriteOnSameInstant() {\n-    return Boolean.parseBoolean(props.getProperty(ALLOW_MULTI_WRITE_ON_SAME_INSTANT));\n+    return getBoolean(ALLOW_MULTI_WRITE_ON_SAME_INSTANT);\n   }\n \n   public String getWriteStatusClassName() {\n-    return props.getProperty(HOODIE_WRITE_STATUS_CLASS_PROP);\n+    return getString(HOODIE_WRITE_STATUS_CLASS_PROP);\n   }\n \n   public int getFinalizeWriteParallelism() {\n-    return Integer.parseInt(props.getProperty(FINALIZE_WRITE_PARALLELISM));\n+    return getInt(FINALIZE_WRITE_PARALLELISM);\n   }\n \n   public int getMarkersDeleteParallelism() {\n-    return Integer.parseInt(props.getProperty(MARKERS_DELETE_PARALLELISM));\n+    return getInt(MARKERS_DELETE_PARALLELISM);\n   }\n \n   public boolean isEmbeddedTimelineServerEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(EMBEDDED_TIMELINE_SERVER_ENABLED));\n+    return getBoolean(EMBEDDED_TIMELINE_SERVER_ENABLED);\n   }\n \n   public boolean isEmbeddedTimelineServerReuseEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED));\n+    return Boolean.parseBoolean(getStringOrDefault(EMBEDDED_TIMELINE_SERVER_REUSE_ENABLED));\n   }\n \n   public int getEmbeddedTimelineServerPort() {\n-    return Integer.parseInt(props.getProperty(EMBEDDED_TIMELINE_SERVER_PORT, DEFAULT_EMBEDDED_TIMELINE_SERVER_PORT));\n+    return Integer.parseInt(getStringOrDefault(EMBEDDED_TIMELINE_SERVER_PORT));\n   }\n \n   public int getEmbeddedTimelineServerThreads() {\n-    return Integer.parseInt(props.getProperty(EMBEDDED_TIMELINE_SERVER_THREADS, DEFAULT_EMBEDDED_TIMELINE_SERVER_THREADS));\n+    return Integer.parseInt(getStringOrDefault(EMBEDDED_TIMELINE_SERVER_THREADS));\n   }\n \n   public boolean getEmbeddedTimelineServerCompressOutput() {\n-    return Boolean.parseBoolean(props.getProperty(EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT, DEFAULT_EMBEDDED_TIMELINE_COMPRESS_OUTPUT));\n+    return Boolean.parseBoolean(getStringOrDefault(EMBEDDED_TIMELINE_SERVER_COMPRESS_OUTPUT));\n   }\n \n   public boolean getEmbeddedTimelineServerUseAsync() {\n-    return Boolean.parseBoolean(props.getProperty(EMBEDDED_TIMELINE_SERVER_USE_ASYNC, DEFAULT_EMBEDDED_TIMELINE_SERVER_ASYNC));\n+    return Boolean.parseBoolean(getStringOrDefault(EMBEDDED_TIMELINE_SERVER_USE_ASYNC));\n   }\n \n   public boolean isFailOnTimelineArchivingEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP));\n+    return getBoolean(FAIL_ON_TIMELINE_ARCHIVING_ENABLED_PROP);\n   }\n \n   public int getMaxConsistencyChecks() {\n-    return Integer.parseInt(props.getProperty(MAX_CONSISTENCY_CHECKS_PROP));\n+    return getInt(MAX_CONSISTENCY_CHECKS_PROP);\n   }\n \n   public int getInitialConsistencyCheckIntervalMs() {\n-    return Integer.parseInt(props.getProperty(INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP));\n+    return getInt(INITIAL_CONSISTENCY_CHECK_INTERVAL_MS_PROP);\n   }\n \n   public int getMaxConsistencyCheckIntervalMs() {\n-    return Integer.parseInt(props.getProperty(MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP));\n+    return getInt(MAX_CONSISTENCY_CHECK_INTERVAL_MS_PROP);\n   }\n \n   public BulkInsertSortMode getBulkInsertSortMode() {\n-    String sortMode = props.getProperty(BULKINSERT_SORT_MODE);\n+    String sortMode = getString(BULKINSERT_SORT_MODE);\n     return BulkInsertSortMode.valueOf(sortMode.toUpperCase());\n   }\n \n   public boolean isMergeDataValidationCheckEnabled() {\n-    return Boolean.parseBoolean(props.getProperty(MERGE_DATA_VALIDATION_CHECK_ENABLED));\n+    return getBoolean(MERGE_DATA_VALIDATION_CHECK_ENABLED);\n   }\n \n   public boolean allowDuplicateInserts() {\n-    return Boolean.parseBoolean(props.getProperty(MERGE_ALLOW_DUPLICATE_ON_INSERTS));\n+    return getBoolean(MERGE_ALLOW_DUPLICATE_ON_INSERTS);\n+  }\n+\n+  public ExternalSpillableMap.DiskMapType getSpillableDiskMapType() {\n+    return ExternalSpillableMap.DiskMapType.valueOf(getString(SPILLABLE_DISK_MAP_TYPE).toUpperCase(Locale.ROOT));\n   }\n \n   public EngineType getEngineType() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjUzNA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286534", "bodyText": "I understand that the new config is just used here as of this PR. but from an user standpoint, on the non-row writer path, combine.before.insert was controlling this already. We should just make it consistent.", "author": "vinothchandar", "createdAt": "2021-07-06T07:00:37Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java", "diffHunk": "@@ -96,9 +97,15 @@\n                 functions.lit(\"\").cast(DataTypes.StringType))\n             .withColumn(HoodieRecord.FILENAME_METADATA_FIELD,\n                 functions.lit(\"\").cast(DataTypes.StringType));\n+\n+    Dataset<Row> dedupedDf = rowDatasetWithHoodieColumns;\n+    if (config.shouldCombineBeforeBulkInsert()) {", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "80ae3446670e4012ef3896477964bb916b71a864", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java\nindex e03faaaedd..5019900236 100644\n--- a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java\n+++ b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java\n\n@@ -99,7 +99,7 @@ public class HoodieDatasetBulkInsertHelper {\n                 functions.lit(\"\").cast(DataTypes.StringType));\n \n     Dataset<Row> dedupedDf = rowDatasetWithHoodieColumns;\n-    if (config.shouldCombineBeforeBulkInsert()) {\n+    if (config.shouldCombineBeforeInsert()) {\n       dedupedDf = SparkRowWriteHelper.newInstance().deduplicateRows(rowDatasetWithHoodieColumns, config.getPreCombineField(), isGlobalIndex);\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjkwOA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286908", "bodyText": "nit: extra line?", "author": "vinothchandar", "createdAt": "2021-07-06T07:01:14Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\nindex fecf64d383..6f5dd3713d 100644\n--- a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n+++ b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n\n@@ -47,7 +47,6 @@ public class SparkRowWriteHelper {\n   }\n \n   private static class WriteHelperHolder {\n-\n     private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzI0MQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287241", "bodyText": "why the singleton etc? Can't we just use a static method?", "author": "vinothchandar", "createdAt": "2021-07-06T07:01:52Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwNDU2Ng==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664704566", "bodyText": "I took inspiration from existing code.\nhttps://github.com/apache/hudi/blob/master/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "author": "nsivabalan", "createdAt": "2021-07-06T16:23:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzI0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\nindex fecf64d383..6f5dd3713d 100644\n--- a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n+++ b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n\n@@ -47,7 +47,6 @@ public class SparkRowWriteHelper {\n   }\n \n   private static class WriteHelperHolder {\n-\n     private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzczMA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287730", "bodyText": "lets use reduceByKey(), which we use for RDD path? groupByKey() can hog memory.", "author": "vinothchandar", "createdAt": "2021-07-06T07:02:41Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwODEwNQ==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664708105", "bodyText": "I too badly wanted to use one, but unfortunately there is none :(\nhttps://stackoverflow.com/questions/38383207/rolling-your-own-reducebykey-in-spark-dataset\nhttps://stackoverflow.com/questions/57359260/why-there-is-no-reducebykey-in-sparks-dataset\nhence, had to go with groupByKey and then do reduceBy.", "author": "nsivabalan", "createdAt": "2021-07-06T16:28:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzczMA=="}], "type": "inlineReview", "revised_code": {"commit": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\nindex fecf64d383..6f5dd3713d 100644\n--- a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n+++ b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n\n@@ -47,7 +47,6 @@ public class SparkRowWriteHelper {\n   }\n \n   private static class WriteHelperHolder {\n-\n     private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664288358", "bodyText": "have you tested with both Spark 2 and 3?  Some of these classes can be different and actually fail?", "author": "vinothchandar", "createdAt": "2021-07-06T07:03:54Z", "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(\n+        (MapFunction<Row, String>) value ->\n+            isGlobalIndex ? (value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)) :\n+                (value.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD) + \"+\" + value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)), Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          if (((Comparable) v1.getAs(preCombineField)).compareTo(((Comparable) v2.getAs(preCombineField))) >= 0) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+            }\n+        ).map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+  }\n+\n+  private ExpressionEncoder getEncoder(StructType schema) {\n+    List<Attribute> attributes = JavaConversions.asJavaCollection(schema.toAttributes()).stream()\n+        .map(Attribute::toAttribute).collect(Collectors.toList());\n+    return RowEncoder.apply(schema)", "originalCommit": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDcwODY0Mw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664708643", "bodyText": "its been quite sometime I put up this patch :). Will do a round of testing and will update for both spark versions.", "author": "nsivabalan", "createdAt": "2021-07-06T16:29:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDg4NDgzMw==", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664884833", "bodyText": "yes, tested both spark2 and spark3.", "author": "nsivabalan", "createdAt": "2021-07-06T21:18:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA=="}], "type": "inlineReview", "revised_code": {"commit": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "chunk": "diff --git a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\nindex fecf64d383..6f5dd3713d 100644\n--- a/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n+++ b/hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java\n\n@@ -47,7 +47,6 @@ public class SparkRowWriteHelper {\n   }\n \n   private static class WriteHelperHolder {\n-\n     private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n   }\n \n"}}, {"oid": "80ae3446670e4012ef3896477964bb916b71a864", "url": "https://github.com/apache/hudi/commit/80ae3446670e4012ef3896477964bb916b71a864", "message": "Addressing feedback", "committedDate": "2021-07-06T19:42:12Z", "type": "forcePushed"}, {"oid": "0b5cdce9cea59681d7e604c6f6706a4e96a19861", "url": "https://github.com/apache/hudi/commit/0b5cdce9cea59681d7e604c6f6706a4e96a19861", "message": "Addressing feedback", "committedDate": "2021-07-06T21:11:45Z", "type": "forcePushed"}, {"oid": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "url": "https://github.com/apache/hudi/commit/ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "message": "Addressing feedback", "committedDate": "2021-07-06T21:20:22Z", "type": "forcePushed"}, {"oid": "1fa675d9739656c45850d60fa49b66970b18f5ac", "url": "https://github.com/apache/hudi/commit/1fa675d9739656c45850d60fa49b66970b18f5ac", "message": "Addressing feedback", "committedDate": "2021-07-07T04:00:13Z", "type": "forcePushed"}, {"oid": "bf73f313cdfb5e640c16e40eb017151dcd464b41", "url": "https://github.com/apache/hudi/commit/bf73f313cdfb5e640c16e40eb017151dcd464b41", "message": "trigger rebuild", "committedDate": "2021-07-07T15:16:04Z", "type": "commit"}, {"oid": "ab8b925db2387548626d762dae3c8458371b43f3", "url": "https://github.com/apache/hudi/commit/ab8b925db2387548626d762dae3c8458371b43f3", "message": "[HUDI-1156] Remove unused dependencies from HoodieDeltaStreamerWrapper Class (#1927)", "committedDate": "2021-07-07T15:16:04Z", "type": "commit"}, {"oid": "b436ba3ce44ad7d393e98f11bda8e986888f31fa", "url": "https://github.com/apache/hudi/commit/b436ba3ce44ad7d393e98f11bda8e986888f31fa", "message": "Adding Dedup support for BulkInsert with Rows", "committedDate": "2021-07-07T15:25:35Z", "type": "commit"}, {"oid": "280aecde2d0a426616144becf8cfeb75bb86166b", "url": "https://github.com/apache/hudi/commit/280aecde2d0a426616144becf8cfeb75bb86166b", "message": "Fixing dedup support", "committedDate": "2021-07-07T15:29:51Z", "type": "commit"}, {"oid": "5785cf93a8a4214e8d2bad920edaa00e0d775874", "url": "https://github.com/apache/hudi/commit/5785cf93a8a4214e8d2bad920edaa00e0d775874", "message": "Fixing build failure", "committedDate": "2021-07-07T15:34:05Z", "type": "commit"}, {"oid": "28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "url": "https://github.com/apache/hudi/commit/28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "message": "Refactoring and minor fixes", "committedDate": "2021-07-07T15:35:03Z", "type": "commit"}, {"oid": "8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "url": "https://github.com/apache/hudi/commit/8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "message": "Some refactoring and fixing tests", "committedDate": "2021-07-07T15:36:48Z", "type": "commit"}, {"oid": "e4813604f6757ecc09eb117e7e35754d564ef004", "url": "https://github.com/apache/hudi/commit/e4813604f6757ecc09eb117e7e35754d564ef004", "message": "Removing PreCombine interface", "committedDate": "2021-07-07T15:41:31Z", "type": "commit"}, {"oid": "c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "url": "https://github.com/apache/hudi/commit/c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "message": "Fixing tests", "committedDate": "2021-07-07T15:41:33Z", "type": "commit"}, {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "message": "Addressing feedback", "committedDate": "2021-07-07T15:54:26Z", "type": "commit"}, {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "message": "Addressing feedback", "committedDate": "2021-07-07T15:54:26Z", "type": "forcePushed"}]}