{"pr_number": 1484, "pr_title": "[HUDI-316] : Hbase qps repartition writestatus", "pr_createdAt": "2020-04-03T20:18:17Z", "pr_url": "https://github.com/apache/hudi/pull/1484", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODYxMQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r403398611", "bodyText": "Could we please avoid using this? Should not be too hard to roll our own..", "author": "vinothchandar", "createdAt": "2020-04-04T00:47:58Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE4MzAzNQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r423183035", "bodyText": "@vinothchandar Re implemented for most part and stripped unnecessary methods of RateLimiter. And removed Ticker class as well (that was from Google/Guava).", "author": "v3nkatesh", "createdAt": "2020-05-11T16:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODYxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java b/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\nindex 21a4c473b0..e156ccffdb 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\n\n@@ -18,228 +18,74 @@\n \n package org.apache.hudi.common.util;\n \n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.Semaphore;\n import java.util.concurrent.TimeUnit;\n-import javax.annotation.Nullable;\n import javax.annotation.concurrent.ThreadSafe;\n \n-/*\n- * Note: Based on RateLimiter implementation in Google/Guava.\n- *         - adopted from com.google.common.util.concurrent\n- *           Copyright (C) 2012 The Guava Authors\n- *           Home page: https://github.com/google/guava\n- *           License: http://www.apache.org/licenses/LICENSE-2.0\n- */\n-\n @ThreadSafe\n-public abstract class RateLimiter {\n-  private final RateLimiter.SleepingTicker ticker;\n-  private final long offsetNanos;\n-  double storedPermits;\n-  double maxPermits;\n-  volatile double stableIntervalMicros;\n-  private final Object mutex;\n-  private long nextFreeTicketMicros;\n-\n-  public static RateLimiter create(double permitsPerSecond) {\n-    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n-  }\n-\n-  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n-    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n-    rateLimiter.setRate(permitsPerSecond);\n-    return rateLimiter;\n-  }\n-\n-  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n-    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n-  }\n-\n-  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n-    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n-    rateLimiter.setRate(permitsPerSecond);\n-    return rateLimiter;\n-  }\n-\n-  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n-    this.mutex = new Object();\n-    this.nextFreeTicketMicros = 0L;\n-    this.ticker = ticker;\n-    this.offsetNanos = ticker.read();\n-  }\n-\n-  public final void setRate(double permitsPerSecond) {\n-    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n-    Object var3 = this.mutex;\n-    synchronized (this.mutex) {\n-      this.resync(this.readSafeMicros());\n-      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n-      this.stableIntervalMicros = stableIntervalMicros;\n-      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n-    }\n-  }\n-\n-  abstract void doSetRate(double var1, double var3);\n-\n-  public final double getRate() {\n-    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n-  }\n+public class RateLimiter {\n \n-  public void acquire() {\n-    this.acquire(1);\n-  }\n+  private final Semaphore semaphore;\n+  private final int maxPermits;\n+  private final TimeUnit timePeriod;\n+  private ScheduledExecutorService scheduler;\n+  private static final long RELEASE_PERMITS_PERIOD_IN_SECONDS = 1L;\n+  private static final long WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS = 5;\n+  private static final int SCHEDULER_CORE_THREAD_POOL_SIZE = 1;\n \n-  public void acquire(int permits) {\n-    checkPermits(permits);\n-    Object var4 = this.mutex;\n-    long microsToWait;\n-    synchronized (this.mutex) {\n-      microsToWait = this.reserveNextTicket((double)permits, this.readSafeMicros());\n-    }\n+  private static final Logger LOG = LogManager.getLogger(RateLimiter.class);\n \n-    this.ticker.sleepMicrosUninterruptibly(microsToWait);\n+  public static RateLimiter create(int permits, TimeUnit timePeriod) {\n+    final RateLimiter limiter = new RateLimiter(permits, timePeriod);\n+    limiter.releasePermitsPeriodically();\n+    return limiter;\n   }\n \n-  private static void checkPermits(int permits) {\n-    checkArgument(permits > 0, \"Requested permits must be positive\");\n+  private RateLimiter(int permits, TimeUnit timePeriod) {\n+    this.semaphore = new Semaphore(permits);\n+    this.maxPermits = permits;\n+    this.timePeriod = timePeriod;\n   }\n \n-  private long reserveNextTicket(double requiredPermits, long nowMicros) {\n-    this.resync(nowMicros);\n-    long microsToNextFreeTicket = this.nextFreeTicketMicros - nowMicros;\n-    double storedPermitsToSpend = Math.min(requiredPermits, this.storedPermits);\n-    double freshPermits = requiredPermits - storedPermitsToSpend;\n-    long waitMicros = this.storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long)(freshPermits * this.stableIntervalMicros);\n-    this.nextFreeTicketMicros += waitMicros;\n-    this.storedPermits -= storedPermitsToSpend;\n-    return microsToNextFreeTicket;\n-  }\n-\n-  abstract long storedPermitsToWaitTime(double var1, double var3);\n-\n-  private void resync(long nowMicros) {\n-    if (nowMicros > this.nextFreeTicketMicros) {\n-      this.storedPermits = Math.min(this.maxPermits, this.storedPermits + (double)(nowMicros - this.nextFreeTicketMicros) / this.stableIntervalMicros);\n-      this.nextFreeTicketMicros = nowMicros;\n+  public boolean tryAcquire(int numPermits) {\n+    if (numPermits > maxPermits) {\n+      acquire(maxPermits);\n+      return tryAcquire(numPermits - maxPermits);\n+    } else {\n+      return acquire(numPermits);\n     }\n-\n-  }\n-\n-  private long readSafeMicros() {\n-    return TimeUnit.NANOSECONDS.toMicros(this.ticker.read() - this.offsetNanos);\n-  }\n-\n-  public String toString() {\n-    return String.format(\"RateLimiter[stableRate=%3.1fqps]\", 1000000.0D / this.stableIntervalMicros);\n   }\n \n-  abstract static class SleepingTicker extends Ticker {\n-    static final RateLimiter.SleepingTicker SYSTEM_TICKER = new RateLimiter.SleepingTicker() {\n-      public long read() {\n-        return systemTicker().read();\n+  public boolean acquire(int numOps) {\n+    try {\n+      if (!semaphore.tryAcquire(numOps)) {\n+        Thread.sleep(WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS);\n+        return acquire(numOps);\n       }\n-\n-      public void sleepMicrosUninterruptibly(long micros) {\n-        if (micros > 0L) {\n-          sleepUninterruptibly(micros, TimeUnit.MICROSECONDS);\n-        }\n-\n-      }\n-    };\n-\n-    SleepingTicker() {\n+      LOG.debug(String.format(\"acquire permits: %s, maxPremits: %s\", numOps, maxPermits));\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(\"Unable to acquire permits\", e);\n     }\n-\n-    static void sleepUninterruptibly(long sleepFor, TimeUnit unit) {\n-      boolean interrupted = false;\n-\n-      try {\n-        long remainingNanos = unit.toNanos(sleepFor);\n-        long end = System.nanoTime() + remainingNanos;\n-\n-        while (true) {\n-          try {\n-            TimeUnit.NANOSECONDS.sleep(remainingNanos);\n-            return;\n-          } catch (InterruptedException var12) {\n-            interrupted = true;\n-            remainingNanos = end - System.nanoTime();\n-          }\n-        }\n-      } finally {\n-        if (interrupted) {\n-          Thread.currentThread().interrupt();\n-        }\n-\n-      }\n-    }\n-\n-    abstract void sleepMicrosUninterruptibly(long var1);\n+    return true;\n   }\n \n-  private static class Bursty extends RateLimiter {\n-    final double maxBurstSeconds;\n-\n-    Bursty(RateLimiter.SleepingTicker ticker, double maxBurstSeconds) {\n-      super(ticker);\n-      this.maxBurstSeconds = maxBurstSeconds;\n-    }\n-\n-    void doSetRate(double permitsPerSecond, double stableIntervalMicros) {\n-      double oldMaxPermits = this.maxPermits;\n-      this.maxPermits = this.maxBurstSeconds * permitsPerSecond;\n-      this.storedPermits = oldMaxPermits == 0.0D ? 0.0D : this.storedPermits * this.maxPermits / oldMaxPermits;\n-    }\n-\n-    long storedPermitsToWaitTime(double storedPermits, double permitsToTake) {\n-      return 0L;\n-    }\n+  public void stop() {\n+    scheduler.shutdownNow();\n   }\n \n-  private static class WarmingUp extends RateLimiter {\n-    final long warmupPeriodMicros;\n-    private double slope;\n-    private double halfPermits;\n-\n-    WarmingUp(RateLimiter.SleepingTicker ticker, long warmupPeriod, TimeUnit timeUnit) {\n-      super(ticker);\n-      this.warmupPeriodMicros = timeUnit.toMicros(warmupPeriod);\n-    }\n-\n-    void doSetRate(double permitsPerSecond, double stableIntervalMicros) {\n-      double oldMaxPermits = this.maxPermits;\n-      this.maxPermits = (double)this.warmupPeriodMicros / stableIntervalMicros;\n-      this.halfPermits = this.maxPermits / 2.0D;\n-      double coldIntervalMicros = stableIntervalMicros * 3.0D;\n-      this.slope = (coldIntervalMicros - stableIntervalMicros) / this.halfPermits;\n-      if (oldMaxPermits == 1.0D / 0.0) {\n-        this.storedPermits = 0.0D;\n-      } else {\n-        this.storedPermits = oldMaxPermits == 0.0D ? this.maxPermits : this.storedPermits * this.maxPermits / oldMaxPermits;\n-      }\n-\n-    }\n+  public void releasePermitsPeriodically() {\n+    scheduler = Executors.newScheduledThreadPool(SCHEDULER_CORE_THREAD_POOL_SIZE);\n+    scheduler.scheduleAtFixedRate(() -> {\n+      LOG.debug(String.format(\"Release permits: maxPremits: %s, available: %s\", maxPermits,\n+          semaphore.availablePermits()));\n+      semaphore.release(maxPermits - semaphore.availablePermits());\n+    }, RELEASE_PERMITS_PERIOD_IN_SECONDS, RELEASE_PERMITS_PERIOD_IN_SECONDS, timePeriod);\n \n-    long storedPermitsToWaitTime(double storedPermits, double permitsToTake) {\n-      double availablePermitsAboveHalf = storedPermits - this.halfPermits;\n-      long micros = 0L;\n-      if (availablePermitsAboveHalf > 0.0D) {\n-        double permitsAboveHalfToTake = Math.min(availablePermitsAboveHalf, permitsToTake);\n-        micros = (long)(permitsAboveHalfToTake * (this.permitsToTime(availablePermitsAboveHalf) + this.permitsToTime(availablePermitsAboveHalf - permitsAboveHalfToTake)) / 2.0D);\n-        permitsToTake -= permitsAboveHalfToTake;\n-      }\n-\n-      micros = (long)((double)micros + this.stableIntervalMicros * permitsToTake);\n-      return micros;\n-    }\n-\n-    private double permitsToTime(double permits) {\n-      return this.stableIntervalMicros + permits * this.slope;\n-    }\n   }\n \n-  public static void checkArgument(boolean expression, @Nullable Object errorMessage) {\n-    if (!expression) {\n-      throw new IllegalArgumentException(String.valueOf(errorMessage));\n-    }\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjA2Mw==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766063", "bodyText": "Do you need this map as instance variable? Looks like there is one HBaseIndex object per client. We don't seem to be clearing entries from this map also.  So, over time, this map can get pretty large and can cause increased memory utilization? Please correct me if I'm misreading.", "author": "satishkotha", "createdAt": "2020-04-13T22:55:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;\n+  Map<String, Integer> fileIdPartitionMap = new HashMap<>();", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NzE5Mg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418387192", "bodyText": "Yeah we don't need a class variable for this. Moved it inside updateLocation() method.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjA2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -92,12 +93,9 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n-  private int maxPutsPerSec;\n   private long totalNumInserts;\n   private int numWriteStatusWithInserts;\n-  Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n \n   /**\n    * multiPutBatchSize will be computed and re-set in updateLocation if\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjQzNg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766436", "bodyText": "I dont see how this is used. could you please add a comment for all these instance variables? It seems like they can be local variables specific to the operation being performed.", "author": "satishkotha", "createdAt": "2020-04-13T22:56:24Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NzY1Mg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418387652", "bodyText": "Yeah can be removed actually, I am anyway recalculating this inside getBatchSize(). Also cleaned up other variables that are not used.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:19:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjQzNg=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -92,12 +93,9 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n-  private int maxPutsPerSec;\n   private long totalNumInserts;\n   private int numWriteStatusWithInserts;\n-  Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n \n   /**\n    * multiPutBatchSize will be computed and re-set in updateLocation if\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNTUwNg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407805506", "bodyText": "Can you help me understand this code? why do we need to force trigger here? Is this just to releaseQPSResources? releaseQPSResources seems to be doing nothing (at least default implementation, are there other implementations outside hoodie?). Is it really important to release here as opposed to doing it in 'close()' (earlier behavior)?", "author": "satishkotha", "createdAt": "2020-04-14T01:03:56Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4OTgwMg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418389802", "bodyText": "Correct, releaseQPSResources() has implementation outside of hoodie. This ensures that we release any resources acquired for HBase operations are released right after the spark stage is done (i.e. because of forced spark action)", "author": "v3nkatesh", "createdAt": "2020-05-01T02:30:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNTUwNg=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -354,31 +352,36 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n     // report number of operations to account per second with rate limiter.\n     // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n     // for within that second\n-    limiter.acquire(mutations.size());\n+    limiter.tryAcquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n   }\n \n-  @Override\n-  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) {\n-    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+  public Map<String, Integer> mapFileWithInsertsToUniquePartition(JavaRDD<WriteStatus> writeStatusRDD) {\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+    int partitionIndex = 0;\n     // Map each fileId that has inserts to a unique partition Id. This will be used while\n     // repartitioning RDD<WriteStatus>\n-    int partitionIndex = 0;\n     final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n                                    .map(w -> w.getFileId()).collect();\n     for (final String fileId : fileIds) {\n-      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+      fileIdPartitionMap.put(fileId, partitionIndex++);\n     }\n+    return fileIdPartitionMap;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+      HoodieTable<T> hoodieTable) {\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD);\n+    final Map<String, Integer> fileIdPartitionMap = mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n                                           writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                            .partitionBy(new WriteStatusPartitioner(fileIdPartitionMap,\n                                               this.numWriteStatusWithInserts))\n                                             .map(w -> w._2());\n     acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n-    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n     JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n         true);\n     // caching the index updated status RDD\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNzkyOQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407807929", "bodyText": "Not directly related to your change, so feel free to ignore this comment. But hBaseIndexQPSResourceAllocator  is instance variable. why is this again passed as argument. This seems like a consistent pattern in this class. Because we are also using exact same name for local variable, it masks instance variable and can become easily error prone if the two variables evolve to mean different things.", "author": "satishkotha", "createdAt": "2020-04-14T01:12:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();\n     return writeStatusJavaRDD;\n   }\n \n-  private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,\n-      HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator, final JavaSparkContext jsc) {\n+  private Option<Float> calculateQPSFraction(JavaRDD<WriteStatus> writeStatusRDD,\n+                                               HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator) {", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MDYxOQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418390619", "bodyText": "Yeah I think it makes sense to refactor it here. Let me update other parts of this class where hBaseIndexQPSResourceAllocator is passed around.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:34:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNzkyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -354,31 +352,36 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n     // report number of operations to account per second with rate limiter.\n     // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n     // for within that second\n-    limiter.acquire(mutations.size());\n+    limiter.tryAcquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n   }\n \n-  @Override\n-  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) {\n-    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+  public Map<String, Integer> mapFileWithInsertsToUniquePartition(JavaRDD<WriteStatus> writeStatusRDD) {\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+    int partitionIndex = 0;\n     // Map each fileId that has inserts to a unique partition Id. This will be used while\n     // repartitioning RDD<WriteStatus>\n-    int partitionIndex = 0;\n     final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n                                    .map(w -> w.getFileId()).collect();\n     for (final String fileId : fileIds) {\n-      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+      fileIdPartitionMap.put(fileId, partitionIndex++);\n     }\n+    return fileIdPartitionMap;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+      HoodieTable<T> hoodieTable) {\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD);\n+    final Map<String, Integer> fileIdPartitionMap = mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n                                           writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                            .partitionBy(new WriteStatusPartitioner(fileIdPartitionMap,\n                                               this.numWriteStatusWithInserts))\n                                             .map(w -> w._2());\n     acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n-    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n     JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n         true);\n     // caching the index updated status RDD\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMjgzNw==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407812837", "bodyText": "Can you share the context on why we created HBaseIndexQPSResourceAllocator?  Do you think calls to RateLimiter#acquire can be made inside HBaseIndexQPSResourceAllocator#acquireQPSResources to simplify?", "author": "satishkotha", "createdAt": "2020-04-14T01:30:24Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTA3Mg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391072", "bodyText": "Synced offline to go over use-case outside of hoodie and why we need to rate limit here.\nJust to summarize, we need to rate limit here because the actual hbase operations are handled here. And HBaseIndexQPSResourceAllocator#acquireQPSResources is mostly meant to manage metadata like checking for available resources before an operation, releasing meta resources etc.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:36:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMjgzNw=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -92,12 +93,9 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n-  private int maxPutsPerSec;\n   private long totalNumInserts;\n   private int numWriteStatusWithInserts;\n-  Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n \n   /**\n    * multiPutBatchSize will be computed and re-set in updateLocation if\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTIwNA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407815204", "bodyText": "could you return the time spent waiting here? I think adding metrics on time taken is very important for debugging any potential performance issues. Also, would be useful to log if time taken is greater than some threshold (say, 300ms?)", "author": "satishkotha", "createdAt": "2020-04-14T01:39:05Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.\n+ *         - adopted from com.google.common.util.concurrent\n+ *           Copyright (C) 2012 The Guava Authors\n+ *           Home page: https://github.com/google/guava\n+ *           License: http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+@ThreadSafe\n+public abstract class RateLimiter {\n+  private final RateLimiter.SleepingTicker ticker;\n+  private final long offsetNanos;\n+  double storedPermits;\n+  double maxPermits;\n+  volatile double stableIntervalMicros;\n+  private final Object mutex;\n+  private long nextFreeTicketMicros;\n+\n+  public static RateLimiter create(double permitsPerSecond) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n+    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n+    this.mutex = new Object();\n+    this.nextFreeTicketMicros = 0L;\n+    this.ticker = ticker;\n+    this.offsetNanos = ticker.read();\n+  }\n+\n+  public final void setRate(double permitsPerSecond) {\n+    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n+    Object var3 = this.mutex;\n+    synchronized (this.mutex) {\n+      this.resync(this.readSafeMicros());\n+      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n+      this.stableIntervalMicros = stableIntervalMicros;\n+      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n+    }\n+  }\n+\n+  abstract void doSetRate(double var1, double var3);\n+\n+  public final double getRate() {\n+    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n+  }\n+\n+  public void acquire() {\n+    this.acquire(1);\n+  }\n+\n+  public void acquire(int permits) {", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MDUzOA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418690538", "bodyText": "any chance we can add these as metrics/logs?", "author": "satishkotha", "createdAt": "2020-05-01T19:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTIwNA=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java b/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\nindex 21a4c473b0..e156ccffdb 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java\n\n@@ -18,228 +18,74 @@\n \n package org.apache.hudi.common.util;\n \n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.Semaphore;\n import java.util.concurrent.TimeUnit;\n-import javax.annotation.Nullable;\n import javax.annotation.concurrent.ThreadSafe;\n \n-/*\n- * Note: Based on RateLimiter implementation in Google/Guava.\n- *         - adopted from com.google.common.util.concurrent\n- *           Copyright (C) 2012 The Guava Authors\n- *           Home page: https://github.com/google/guava\n- *           License: http://www.apache.org/licenses/LICENSE-2.0\n- */\n-\n @ThreadSafe\n-public abstract class RateLimiter {\n-  private final RateLimiter.SleepingTicker ticker;\n-  private final long offsetNanos;\n-  double storedPermits;\n-  double maxPermits;\n-  volatile double stableIntervalMicros;\n-  private final Object mutex;\n-  private long nextFreeTicketMicros;\n-\n-  public static RateLimiter create(double permitsPerSecond) {\n-    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n-  }\n-\n-  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n-    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n-    rateLimiter.setRate(permitsPerSecond);\n-    return rateLimiter;\n-  }\n-\n-  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n-    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n-  }\n-\n-  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n-    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n-    rateLimiter.setRate(permitsPerSecond);\n-    return rateLimiter;\n-  }\n-\n-  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n-    this.mutex = new Object();\n-    this.nextFreeTicketMicros = 0L;\n-    this.ticker = ticker;\n-    this.offsetNanos = ticker.read();\n-  }\n-\n-  public final void setRate(double permitsPerSecond) {\n-    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n-    Object var3 = this.mutex;\n-    synchronized (this.mutex) {\n-      this.resync(this.readSafeMicros());\n-      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n-      this.stableIntervalMicros = stableIntervalMicros;\n-      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n-    }\n-  }\n-\n-  abstract void doSetRate(double var1, double var3);\n-\n-  public final double getRate() {\n-    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n-  }\n+public class RateLimiter {\n \n-  public void acquire() {\n-    this.acquire(1);\n-  }\n+  private final Semaphore semaphore;\n+  private final int maxPermits;\n+  private final TimeUnit timePeriod;\n+  private ScheduledExecutorService scheduler;\n+  private static final long RELEASE_PERMITS_PERIOD_IN_SECONDS = 1L;\n+  private static final long WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS = 5;\n+  private static final int SCHEDULER_CORE_THREAD_POOL_SIZE = 1;\n \n-  public void acquire(int permits) {\n-    checkPermits(permits);\n-    Object var4 = this.mutex;\n-    long microsToWait;\n-    synchronized (this.mutex) {\n-      microsToWait = this.reserveNextTicket((double)permits, this.readSafeMicros());\n-    }\n+  private static final Logger LOG = LogManager.getLogger(RateLimiter.class);\n \n-    this.ticker.sleepMicrosUninterruptibly(microsToWait);\n+  public static RateLimiter create(int permits, TimeUnit timePeriod) {\n+    final RateLimiter limiter = new RateLimiter(permits, timePeriod);\n+    limiter.releasePermitsPeriodically();\n+    return limiter;\n   }\n \n-  private static void checkPermits(int permits) {\n-    checkArgument(permits > 0, \"Requested permits must be positive\");\n+  private RateLimiter(int permits, TimeUnit timePeriod) {\n+    this.semaphore = new Semaphore(permits);\n+    this.maxPermits = permits;\n+    this.timePeriod = timePeriod;\n   }\n \n-  private long reserveNextTicket(double requiredPermits, long nowMicros) {\n-    this.resync(nowMicros);\n-    long microsToNextFreeTicket = this.nextFreeTicketMicros - nowMicros;\n-    double storedPermitsToSpend = Math.min(requiredPermits, this.storedPermits);\n-    double freshPermits = requiredPermits - storedPermitsToSpend;\n-    long waitMicros = this.storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long)(freshPermits * this.stableIntervalMicros);\n-    this.nextFreeTicketMicros += waitMicros;\n-    this.storedPermits -= storedPermitsToSpend;\n-    return microsToNextFreeTicket;\n-  }\n-\n-  abstract long storedPermitsToWaitTime(double var1, double var3);\n-\n-  private void resync(long nowMicros) {\n-    if (nowMicros > this.nextFreeTicketMicros) {\n-      this.storedPermits = Math.min(this.maxPermits, this.storedPermits + (double)(nowMicros - this.nextFreeTicketMicros) / this.stableIntervalMicros);\n-      this.nextFreeTicketMicros = nowMicros;\n+  public boolean tryAcquire(int numPermits) {\n+    if (numPermits > maxPermits) {\n+      acquire(maxPermits);\n+      return tryAcquire(numPermits - maxPermits);\n+    } else {\n+      return acquire(numPermits);\n     }\n-\n-  }\n-\n-  private long readSafeMicros() {\n-    return TimeUnit.NANOSECONDS.toMicros(this.ticker.read() - this.offsetNanos);\n-  }\n-\n-  public String toString() {\n-    return String.format(\"RateLimiter[stableRate=%3.1fqps]\", 1000000.0D / this.stableIntervalMicros);\n   }\n \n-  abstract static class SleepingTicker extends Ticker {\n-    static final RateLimiter.SleepingTicker SYSTEM_TICKER = new RateLimiter.SleepingTicker() {\n-      public long read() {\n-        return systemTicker().read();\n+  public boolean acquire(int numOps) {\n+    try {\n+      if (!semaphore.tryAcquire(numOps)) {\n+        Thread.sleep(WAIT_BEFORE_NEXT_ACQUIRE_PERMIT_IN_MS);\n+        return acquire(numOps);\n       }\n-\n-      public void sleepMicrosUninterruptibly(long micros) {\n-        if (micros > 0L) {\n-          sleepUninterruptibly(micros, TimeUnit.MICROSECONDS);\n-        }\n-\n-      }\n-    };\n-\n-    SleepingTicker() {\n+      LOG.debug(String.format(\"acquire permits: %s, maxPremits: %s\", numOps, maxPermits));\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(\"Unable to acquire permits\", e);\n     }\n-\n-    static void sleepUninterruptibly(long sleepFor, TimeUnit unit) {\n-      boolean interrupted = false;\n-\n-      try {\n-        long remainingNanos = unit.toNanos(sleepFor);\n-        long end = System.nanoTime() + remainingNanos;\n-\n-        while (true) {\n-          try {\n-            TimeUnit.NANOSECONDS.sleep(remainingNanos);\n-            return;\n-          } catch (InterruptedException var12) {\n-            interrupted = true;\n-            remainingNanos = end - System.nanoTime();\n-          }\n-        }\n-      } finally {\n-        if (interrupted) {\n-          Thread.currentThread().interrupt();\n-        }\n-\n-      }\n-    }\n-\n-    abstract void sleepMicrosUninterruptibly(long var1);\n+    return true;\n   }\n \n-  private static class Bursty extends RateLimiter {\n-    final double maxBurstSeconds;\n-\n-    Bursty(RateLimiter.SleepingTicker ticker, double maxBurstSeconds) {\n-      super(ticker);\n-      this.maxBurstSeconds = maxBurstSeconds;\n-    }\n-\n-    void doSetRate(double permitsPerSecond, double stableIntervalMicros) {\n-      double oldMaxPermits = this.maxPermits;\n-      this.maxPermits = this.maxBurstSeconds * permitsPerSecond;\n-      this.storedPermits = oldMaxPermits == 0.0D ? 0.0D : this.storedPermits * this.maxPermits / oldMaxPermits;\n-    }\n-\n-    long storedPermitsToWaitTime(double storedPermits, double permitsToTake) {\n-      return 0L;\n-    }\n+  public void stop() {\n+    scheduler.shutdownNow();\n   }\n \n-  private static class WarmingUp extends RateLimiter {\n-    final long warmupPeriodMicros;\n-    private double slope;\n-    private double halfPermits;\n-\n-    WarmingUp(RateLimiter.SleepingTicker ticker, long warmupPeriod, TimeUnit timeUnit) {\n-      super(ticker);\n-      this.warmupPeriodMicros = timeUnit.toMicros(warmupPeriod);\n-    }\n-\n-    void doSetRate(double permitsPerSecond, double stableIntervalMicros) {\n-      double oldMaxPermits = this.maxPermits;\n-      this.maxPermits = (double)this.warmupPeriodMicros / stableIntervalMicros;\n-      this.halfPermits = this.maxPermits / 2.0D;\n-      double coldIntervalMicros = stableIntervalMicros * 3.0D;\n-      this.slope = (coldIntervalMicros - stableIntervalMicros) / this.halfPermits;\n-      if (oldMaxPermits == 1.0D / 0.0) {\n-        this.storedPermits = 0.0D;\n-      } else {\n-        this.storedPermits = oldMaxPermits == 0.0D ? this.maxPermits : this.storedPermits * this.maxPermits / oldMaxPermits;\n-      }\n-\n-    }\n+  public void releasePermitsPeriodically() {\n+    scheduler = Executors.newScheduledThreadPool(SCHEDULER_CORE_THREAD_POOL_SIZE);\n+    scheduler.scheduleAtFixedRate(() -> {\n+      LOG.debug(String.format(\"Release permits: maxPremits: %s, available: %s\", maxPermits,\n+          semaphore.availablePermits()));\n+      semaphore.release(maxPermits - semaphore.availablePermits());\n+    }, RELEASE_PERMITS_PERIOD_IN_SECONDS, RELEASE_PERMITS_PERIOD_IN_SECONDS, timePeriod);\n \n-    long storedPermitsToWaitTime(double storedPermits, double permitsToTake) {\n-      double availablePermitsAboveHalf = storedPermits - this.halfPermits;\n-      long micros = 0L;\n-      if (availablePermitsAboveHalf > 0.0D) {\n-        double permitsAboveHalfToTake = Math.min(availablePermitsAboveHalf, permitsToTake);\n-        micros = (long)(permitsAboveHalfToTake * (this.permitsToTime(availablePermitsAboveHalf) + this.permitsToTime(availablePermitsAboveHalf - permitsAboveHalfToTake)) / 2.0D);\n-        permitsToTake -= permitsAboveHalfToTake;\n-      }\n-\n-      micros = (long)((double)micros + this.stableIntervalMicros * permitsToTake);\n-      return micros;\n-    }\n-\n-    private double permitsToTime(double permits) {\n-      return this.stableIntervalMicros + permits * this.slope;\n-    }\n   }\n \n-  public static void checkArgument(boolean expression, @Nullable Object errorMessage) {\n-    if (!expression) {\n-      throw new IllegalArgumentException(String.valueOf(errorMessage));\n-    }\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODIyMw==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407868223", "bodyText": "Could you please make this a static class if its not using any instance variables of outer class", "author": "satishkotha", "createdAt": "2020-04-14T05:01:26Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -498,4 +554,37 @@ public boolean isImplicitWithStorage() {\n   public void setHbaseConnection(Connection hbaseConnection) {\n     HBaseIndex.hbaseConnection = hbaseConnection;\n   }\n+\n+  /**\n+   * Partitions each WriteStatus with inserts into a unique single partition. WriteStatus without inserts will be\n+   * assigned to random partitions. This partitioner will be useful to utilize max parallelism with spark operations\n+   * that are based on inserts in each WriteStatus.\n+   */\n+  public class WriteStatusPartitioner extends Partitioner {", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTIwNA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391204", "bodyText": "done.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:37:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODIyMw=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -560,7 +557,7 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n    * assigned to random partitions. This partitioner will be useful to utilize max parallelism with spark operations\n    * that are based on inserts in each WriteStatus.\n    */\n-  public class WriteStatusPartitioner extends Partitioner {\n+  public static class WriteStatusPartitioner extends Partitioner {\n     private int totalPartitions;\n     final Map<String, Integer> fileIdPartitionMap;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2OTU2NQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407869565", "bodyText": "nit: looks like this is logged in the above method call too. so i think this can be removed.", "author": "satishkotha", "createdAt": "2020-04-14T05:06:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTU2Mg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391562", "bodyText": "done.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2OTU2NQ=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -354,31 +352,36 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n     // report number of operations to account per second with rate limiter.\n     // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n     // for within that second\n-    limiter.acquire(mutations.size());\n+    limiter.tryAcquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n   }\n \n-  @Override\n-  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) {\n-    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+  public Map<String, Integer> mapFileWithInsertsToUniquePartition(JavaRDD<WriteStatus> writeStatusRDD) {\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+    int partitionIndex = 0;\n     // Map each fileId that has inserts to a unique partition Id. This will be used while\n     // repartitioning RDD<WriteStatus>\n-    int partitionIndex = 0;\n     final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n                                    .map(w -> w.getFileId()).collect();\n     for (final String fileId : fileIds) {\n-      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+      fileIdPartitionMap.put(fileId, partitionIndex++);\n     }\n+    return fileIdPartitionMap;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+      HoodieTable<T> hoodieTable) {\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD);\n+    final Map<String, Integer> fileIdPartitionMap = mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n                                           writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                            .partitionBy(new WriteStatusPartitioner(fileIdPartitionMap,\n                                               this.numWriteStatusWithInserts))\n                                             .map(w -> w._2());\n     acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n-    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n     JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n         true);\n     // caching the index updated status RDD\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkwNTk1NA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407905954", "bodyText": "Consider redoing this logic, because if this.numWriteStatusWithInserts == 0 , we still go through the process of generating fileIdPartitionMap which is not ideal.\nAlso, curious, if you did any performance measurements before and after this change. It is worth highlighting in release notes if this improvement is significant", "author": "satishkotha", "createdAt": "2020-04-14T06:55:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTg1NQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391855", "bodyText": "When this.numWriteStatusWithInserts == 0, fileIds will be empty. So fileIdPartitionMap will also be empty in this case.", "author": "v3nkatesh", "createdAt": "2020-05-01T02:41:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkwNTk1NA=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -354,31 +352,36 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n     // report number of operations to account per second with rate limiter.\n     // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n     // for within that second\n-    limiter.acquire(mutations.size());\n+    limiter.tryAcquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n   }\n \n-  @Override\n-  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) {\n-    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+  public Map<String, Integer> mapFileWithInsertsToUniquePartition(JavaRDD<WriteStatus> writeStatusRDD) {\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+    int partitionIndex = 0;\n     // Map each fileId that has inserts to a unique partition Id. This will be used while\n     // repartitioning RDD<WriteStatus>\n-    int partitionIndex = 0;\n     final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n                                    .map(w -> w.getFileId()).collect();\n     for (final String fileId : fileIds) {\n-      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+      fileIdPartitionMap.put(fileId, partitionIndex++);\n     }\n+    return fileIdPartitionMap;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+      HoodieTable<T> hoodieTable) {\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD);\n+    final Map<String, Integer> fileIdPartitionMap = mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n                                           writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                            .partitionBy(new WriteStatusPartitioner(fileIdPartitionMap,\n                                               this.numWriteStatusWithInserts))\n                                             .map(w -> w._2());\n     acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n-    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n     JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n         true);\n     // caching the index updated status RDD\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODk5NA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408328994", "bodyText": "nit: can we also move hTable.get(keys) inside this if?  do we need to invoke hTable.get if keys is empty?", "author": "satishkotha", "createdAt": "2020-04-14T17:57:27Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -252,8 +263,10 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n     };\n   }\n \n-  private Result[] doGet(HTable hTable, List<Get> keys) throws IOException {\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n+  private Result[] doGet(HTable hTable, List<Get> keys, RateLimiter limiter) throws IOException {\n+    if (keys.size() > 0) {", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5NzMwMA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418397300", "bodyText": "Invoking hTable.get on empty keys is returning an empty Result array. But anyway, I changed it to explicitly return empty array now.", "author": "v3nkatesh", "createdAt": "2020-05-01T03:08:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODk5NA=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -265,9 +262,10 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n \n   private Result[] doGet(HTable hTable, List<Get> keys, RateLimiter limiter) throws IOException {\n     if (keys.size() > 0) {\n-      limiter.acquire(keys.size());\n+      limiter.tryAcquire(keys.size());\n+      return hTable.get(keys);\n     }\n-    return hTable.get(keys);\n+    return new Result[keys.size()];\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0NTAwNw==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408345007", "bodyText": "lot of code in this test seems like repetition from source code. consider refactoring this part into a library to reuse in tests if needed", "author": "satishkotha", "createdAt": "2020-04-14T18:24:24Z", "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java", "diffHunk": "@@ -329,47 +332,140 @@ public void testPutBatchSizeCalculation() {\n     // All asserts cases below are derived out of the first\n     // example below, with change in one parameter at a time.\n \n-    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.1f);\n-    // Expected batchSize is 8 because in that case, total request sent in one second is below\n-    // 8 (batchSize) * 200 (parallelism) * 10 (maxReqsInOneSecond) * 10 (numRegionServers) * 0.1 (qpsFraction)) => 16000\n-    // We assume requests get distributed to Region Servers uniformly, so each RS gets 1600 request\n-    // 1600 happens to be 10% of 16667 (maxQPSPerRegionServer) as expected.\n-    Assert.assertEquals(putBatchSize, 8);\n+    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.1f);\n+    // Total puts that can be sent  in 1 second = (10 * 16667 * 0.1) = 16,667\n+    // Total puts per batch will be (16,667 / parallelism) = 83.335, where 200 is the maxExecutors\n+    Assert.assertEquals(putBatchSize, 83);\n \n     // Number of Region Servers are halved, total requests sent in a second are also halved, so batchSize is also halved\n-    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize2, 4);\n+    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize2, 41);\n \n     // If the parallelism is halved, batchSize has to double\n-    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize3, 16);\n+    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 0.1f);\n+    Assert.assertEquals(putBatchSize3, 166);\n \n     // If the parallelism is halved, batchSize has to double.\n     // This time parallelism is driven by numTasks rather than numExecutors\n-    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize4, 16);\n+    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize4, 166);\n \n     // If sleepTimeMs is halved, batchSize has to halve\n-    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.05f);\n-    Assert.assertEquals(putBatchSize5, 4);\n+    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.05f);\n+    Assert.assertEquals(putBatchSize5, 41);\n \n     // If maxQPSPerRegionServer is doubled, batchSize also doubles\n-    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize6, 16);\n+    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize6, 166);\n   }\n \n   @Test\n   public void testsHBasePutAccessParallelism() {\n     HoodieWriteConfig config = getConfig();\n     HBaseIndex index = new HBaseIndex(config);\n     final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n-        Arrays.asList(getSampleWriteStatus(1, 2), getSampleWriteStatus(0, 3), getSampleWriteStatus(10, 0)), 10);\n+        Arrays.asList(\n+          getSampleWriteStatus(0, 2),\n+          getSampleWriteStatus(2, 3),\n+          getSampleWriteStatus(4, 3),\n+          getSampleWriteStatus(6, 3),\n+          getSampleWriteStatus(8, 0)),\n+        10);\n     final Tuple2<Long, Integer> tuple = index.getHBasePutAccessParallelism(writeStatusRDD);\n     final int hbasePutAccessParallelism = Integer.parseInt(tuple._2.toString());\n     final int hbaseNumPuts = Integer.parseInt(tuple._1.toString());\n     Assert.assertEquals(10, writeStatusRDD.getNumPartitions());\n-    Assert.assertEquals(2, hbasePutAccessParallelism);\n-    Assert.assertEquals(11, hbaseNumPuts);\n+    Assert.assertEquals(4, hbasePutAccessParallelism);\n+    Assert.assertEquals(20, hbaseNumPuts);\n+  }\n+\n+  @Test\n+  public void testsWriteStatusPartitioner() {\n+    HoodieWriteConfig config = getConfig();\n+    HBaseIndex index = new HBaseIndex(config);\n+    int parallelism = 4;\n+    final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n+        Arrays.asList(\n+          getSampleWriteStatusWithFileId(0, 2),\n+          getSampleWriteStatusWithFileId(2, 3),\n+          getSampleWriteStatusWithFileId(4, 3),\n+          getSampleWriteStatusWithFileId(0, 3),\n+          getSampleWriteStatusWithFileId(11, 0)), parallelism);\n+    int partitionIndex = 0;\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQwMjM5MA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418402390", "bodyText": "Sure, I have moved this logic inside HBaseIndex to a separate method instead of a different class, let me know. Using this util method from tests.", "author": "v3nkatesh", "createdAt": "2020-05-01T03:36:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0NTAwNw=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java b/hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java\nindex 8e31ccb3d6..c7c0bd03ff 100644\n--- a/hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java\n+++ b/hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java\n\n@@ -391,17 +390,11 @@ public class TestHbaseIndex extends HoodieClientTestHarness {\n           getSampleWriteStatusWithFileId(4, 3),\n           getSampleWriteStatusWithFileId(0, 3),\n           getSampleWriteStatusWithFileId(11, 0)), parallelism);\n-    int partitionIndex = 0;\n-    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n \n-    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n-                                   .map(w -> w.getFileId()).collect();\n-    for (final String fileId : fileIds) {\n-      fileIdPartitionMap.put(fileId, partitionIndex++);\n-    }\n+    final Map<String, Integer> fileIdPartitionMap = index.mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     int numWriteStatusWithInserts = (int) index.getHBasePutAccessParallelism(writeStatusRDD)._2;\n     JavaRDD<WriteStatus> partitionedRDD = writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(index.new WriteStatusPartitioner(fileIdPartitionMap,\n+                                            .partitionBy(new HBaseIndex.WriteStatusPartitioner(fileIdPartitionMap,\n                                               numWriteStatusWithInserts)).map(w -> w._2());\n     Assert.assertEquals(numWriteStatusWithInserts, partitionedRDD.getNumPartitions());\n     int[] partitionIndexesBeforeRepartition = writeStatusRDD.partitions().stream().mapToInt(p -> p.index()).toArray();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r411749174", "bodyText": "another question, what is the typical latency of these mutate operations? If time taken here combined with time taken to collect 'multiPutBatchSize' is > 1 second, then it seems like limiter would generate enough tokens for next run and would not wait at all.", "author": "satishkotha", "createdAt": "2020-04-20T23:03:34Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "originalCommit": "d10bc048bc9f5e705b309d843bc89c86c00ae64a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQyNTU5MQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418425591", "bodyText": "Because of the synchronous nature, we will not acquire more permits until the operation is done even if it takes more than a second.", "author": "v3nkatesh", "createdAt": "2020-05-01T05:43:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5Njg4OQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418696889", "bodyText": "example to help clarify what i meant:\nlets say, mutator.mutate() + flush +clear takes 2 seconds as minimum. limiter.acquire would never wait because it generates mutations.size() tokens every second. So we would never wait. looks like this is expected and we dont see it as a problem. So I'm fine with it. (if possible, having metrics on per operation wait time would help us debug any potential issues)", "author": "satishkotha", "createdAt": "2020-05-01T19:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ5Njk3Mw==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425496973", "bodyText": "I think we synced offline for this, but let me try to summarize.\nToken will be acquired only after previous batch is done, so it won't flood the system or over-utilize the cluster as planned. Though the side effect is hbase operation running slower than intended. Yes metrics on operation will be useful, will create follow up ticket.", "author": "v3nkatesh", "createdAt": "2020-05-15T00:11:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}], "type": "inlineReview", "revised_code": {"commit": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nindex e4e8409816..3616ca8985 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n\n@@ -354,31 +352,36 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n     // report number of operations to account per second with rate limiter.\n     // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n     // for within that second\n-    limiter.acquire(mutations.size());\n+    limiter.tryAcquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n   }\n \n-  @Override\n-  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) {\n-    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+  public Map<String, Integer> mapFileWithInsertsToUniquePartition(JavaRDD<WriteStatus> writeStatusRDD) {\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+    int partitionIndex = 0;\n     // Map each fileId that has inserts to a unique partition Id. This will be used while\n     // repartitioning RDD<WriteStatus>\n-    int partitionIndex = 0;\n     final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n                                    .map(w -> w.getFileId()).collect();\n     for (final String fileId : fileIds) {\n-      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+      fileIdPartitionMap.put(fileId, partitionIndex++);\n     }\n+    return fileIdPartitionMap;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+      HoodieTable<T> hoodieTable) {\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD);\n+    final Map<String, Integer> fileIdPartitionMap = mapFileWithInsertsToUniquePartition(writeStatusRDD);\n     JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n                                           writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n-                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                            .partitionBy(new WriteStatusPartitioner(fileIdPartitionMap,\n                                               this.numWriteStatusWithInserts))\n                                             .map(w -> w._2());\n     acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n-    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n     JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n         true);\n     // caching the index updated status RDD\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418692391", "bodyText": "these two also seem like related to the operation being performed and not really need to be instance variables. If we can find a way to move them to local variables, that would make it cleaner.", "author": "satishkotha", "createdAt": "2020-05-01T19:10:35Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,14 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;", "originalCommit": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ4OTczNQ==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425489735", "bodyText": "@v3nkatesh can you address or respond to this comment ?", "author": "n3nash", "createdAt": "2020-05-14T23:45:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ5ODI0Mg==", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425498242", "bodyText": "These 2 are actually used at multiple places inside the class, so did not refactor. It's possible to move them inside methods, but it will mean re-calculating.", "author": "v3nkatesh", "createdAt": "2020-05-15T00:16:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "chunk": "diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java\nsimilarity index 83%\nrename from hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\nrename to hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java\nindex daab4d9951..77659b7251 100644\n--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/SparkHoodieHBaseIndex.java\n\n@@ -89,7 +91,7 @@ public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n \n-  private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n+  private static final Logger LOG = LogManager.getLogger(SparkHoodieHBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private int maxQpsPerRegionServer;\n"}}, {"oid": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "url": "https://github.com/apache/hudi/commit/5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "message": "Refactor RateLimiter and remove Ticker class", "committedDate": "2020-09-04T07:47:23Z", "type": "forcePushed"}, {"oid": "6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "url": "https://github.com/apache/hudi/commit/6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes", "committedDate": "2020-10-29T07:28:49Z", "type": "forcePushed"}, {"oid": "1bdf773ad1851843337674f9cc036926ef2d8ccb", "url": "https://github.com/apache/hudi/commit/1bdf773ad1851843337674f9cc036926ef2d8ccb", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes", "committedDate": "2020-10-29T18:36:18Z", "type": "commit"}, {"oid": "1bdf773ad1851843337674f9cc036926ef2d8ccb", "url": "https://github.com/apache/hudi/commit/1bdf773ad1851843337674f9cc036926ef2d8ccb", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes", "committedDate": "2020-10-29T18:36:18Z", "type": "forcePushed"}]}