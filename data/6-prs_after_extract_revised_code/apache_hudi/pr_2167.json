{"pr_number": 2167, "pr_title": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "pr_createdAt": "2020-10-11T01:51:40Z", "pr_url": "https://github.com/apache/hudi/pull/2167", "timeline": [{"oid": "863bd643f32db542e3895e331a5febfb702be9d8", "url": "https://github.com/apache/hudi/commit/863bd643f32db542e3895e331a5febfb702be9d8", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable\n\nRemove APIs in `HoodieTestUtils`\n- `createCommitFiles`\n- `createDataFile`\n- `createNewLogFile`\n- `createCompactionRequest`\n\nMigrated usages in `TestCleaner#testPendingCompactions`.\n\nAlso improved some API names in `HoodieTestTable`.", "committedDate": "2020-10-11T01:48:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg1NDE3MA==", "url": "https://github.com/apache/hudi/pull/2167#discussion_r502854170", "bodyText": "@yanghua The original logic of test prep is incredibly difficult to understand. Please avoid reading it. I simply compared the output files in temp directory and made sure they were equivalent before and after the change.", "author": "xushiyan", "createdAt": "2020-10-11T02:02:28Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java", "diffHunk": "@@ -1098,73 +1097,82 @@ public void testCleanPreviousCorruptedCleanFiles() throws IOException {\n    * @param expNumFilesDeleted Number of files deleted\n    */\n   private void testPendingCompactions(HoodieWriteConfig config, int expNumFilesDeleted,\n-      int expNumFilesUnderCompactionDeleted, boolean retryFailure) throws IOException {\n+      int expNumFilesUnderCompactionDeleted, boolean retryFailure) throws Exception {\n     HoodieTableMetaClient metaClient =\n         HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n-    String[] instants = new String[] {\"000\", \"001\", \"003\", \"005\", \"007\", \"009\", \"011\", \"013\"};\n-    String[] compactionInstants = new String[] {\"002\", \"004\", \"006\", \"008\", \"010\"};\n-    Map<String, String> expFileIdToPendingCompaction = new HashMap<>();\n-    Map<String, String> fileIdToLatestInstantBeforeCompaction = new HashMap<>();\n-    Map<String, List<FileSlice>> compactionInstantsToFileSlices = new HashMap<>();\n-\n-    for (String instant : instants) {\n-      HoodieTestUtils.createCommitFiles(basePath, instant);\n-    }\n+    final String partition = \"2016/03/15\";\n+    Map<String, String> expFileIdToPendingCompaction = new HashMap<String, String>() {\n+      {\n+        put(\"fileId2\", \"004\");\n+        put(\"fileId3\", \"006\");\n+        put(\"fileId4\", \"008\");\n+        put(\"fileId5\", \"010\");\n+      }\n+    };\n+    Map<String, String> fileIdToLatestInstantBeforeCompaction = new HashMap<String, String>() {\n+      {\n+        put(\"fileId1\", \"000\");\n+        put(\"fileId2\", \"000\");\n+        put(\"fileId3\", \"001\");\n+        put(\"fileId4\", \"003\");\n+        put(\"fileId5\", \"005\");\n+        put(\"fileId6\", \"009\");\n+        put(\"fileId7\", \"011\");\n+      }\n+    };\n \n     // Generate 7 file-groups. First one has only one slice and no pending compaction. File Slices (2 - 5) has\n     // multiple versions with pending compaction. File Slices (6 - 7) have multiple file-slices but not under\n     // compactions\n     // FileIds 2-5 will be under compaction\n-    int maxNumFileIds = 7;\n-    String[] fileIds = new String[] {\"fileId1\", \"fileId2\", \"fileId3\", \"fileId4\", \"fileId5\", \"fileId6\", \"fileId7\"};\n-    int maxNumFileIdsForCompaction = 4;\n-    for (int i = 0; i < maxNumFileIds; i++) {\n-      final String fileId = HoodieTestUtils.createDataFile(basePath,\n-          HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0], fileIds[i]);\n-      HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0],\n-          fileId, Option.empty());\n-      HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0],\n-          fileId, Option.of(2));\n-      fileIdToLatestInstantBeforeCompaction.put(fileId, instants[0]);\n-      for (int j = 1; j <= i; j++) {\n-        if (j == i && j <= maxNumFileIdsForCompaction) {\n-          expFileIdToPendingCompaction.put(fileId, compactionInstants[j]);\n-          metaClient = HoodieTableMetaClient.reload(metaClient);\n-          HoodieTable table = HoodieSparkTable.create(config, context, metaClient);\n-          FileSlice slice =\n-              table.getSliceView().getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n-                  .filter(fs -> fs.getFileId().equals(fileId)).findFirst().get();\n-          List<FileSlice> slices = new ArrayList<>();\n-          if (compactionInstantsToFileSlices.containsKey(compactionInstants[j])) {\n-            slices = compactionInstantsToFileSlices.get(compactionInstants[j]);\n-          }\n-          slices.add(slice);\n-          compactionInstantsToFileSlices.put(compactionInstants[j], slices);\n-          // Add log-files to simulate delta-commits after pending compaction\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              compactionInstants[j], fileId, Option.empty());\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              compactionInstants[j], fileId, Option.of(2));\n-        } else {\n-          HoodieTestUtils.createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[j],\n-              fileId);\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              instants[j], fileId, Option.empty());\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              instants[j], fileId, Option.of(2));\n-          fileIdToLatestInstantBeforeCompaction.put(fileId, instants[j]);\n-        }\n-      }\n-    }\n-\n-    // Setup pending compaction plans\n-    for (String instant : compactionInstants) {\n-      List<FileSlice> fileSliceList = compactionInstantsToFileSlices.get(instant);\n-      if (null != fileSliceList) {\n-        HoodieTestUtils.createCompactionRequest(metaClient, instant, fileSliceList.stream()\n-            .map(fs -> Pair.of(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fs)).collect(Collectors.toList()));\n-      }\n-    }", "originalCommit": "863bd643f32db542e3895e331a5febfb702be9d8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}