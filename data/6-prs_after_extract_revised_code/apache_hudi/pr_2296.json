{"pr_number": 2296, "pr_title": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "pr_createdAt": "2020-12-04T08:47:57Z", "pr_url": "https://github.com/apache/hudi/pull/2296", "timeline": [{"oid": "39c0618daffe76038d5691caa358ac5a75bdcd8b", "url": "https://github.com/apache/hudi/commit/39c0618daffe76038d5691caa358ac5a75bdcd8b", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2020-12-10T03:15:10Z", "type": "forcePushed"}, {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d", "url": "https://github.com/apache/hudi/commit/f86f5b5619394a9e99ccf6c855a1a82b3a095b7d", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case", "committedDate": "2020-12-14T09:46:02Z", "type": "forcePushed"}, {"oid": "d39721bb754db6d114fafd8eaa7a342c090199b1", "url": "https://github.com/apache/hudi/commit/d39721bb754db6d114fafd8eaa7a342c090199b1", "message": "skip empty commit", "committedDate": "2021-01-30T14:02:58Z", "type": "forcePushed"}, {"oid": "9dffc5ac02d7beedba9f694cb028d86a5438391c", "url": "https://github.com/apache/hudi/commit/9dffc5ac02d7beedba9f694cb028d86a5438391c", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case\n\nskip empty commit", "committedDate": "2021-01-30T14:04:08Z", "type": "forcePushed"}, {"oid": "69b232a039ef3829667b4887f1509c7763ce5f3f", "url": "https://github.com/apache/hudi/commit/69b232a039ef3829667b4887f1509c7763ce5f3f", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case\n\nskip empty commit\n\nfix test case", "committedDate": "2021-02-01T16:20:12Z", "type": "forcePushed"}, {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7", "url": "https://github.com/apache/hudi/commit/673e8e7b1b2893676a6b507b66b12e2625409ab7", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case\n\nskip empty commit\n\nfix test case\n\nadd comment", "committedDate": "2021-02-02T02:11:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568692323", "bodyText": "Will this check impact the performance of non-empty commit? Will the previous stages being cached?\nI personally feel it's better to do the empty check before run into hudi's operation.", "author": "garyli1019", "createdAt": "2021-02-02T15:26:39Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "originalCommit": "673e8e7b1b2893676a6b507b66b12e2625409ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcyODI5OA==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568728298", "bodyText": "Hi @garyli1019 , it is just a java list#isEmpty here ,so it should has no impact for the performance. In our previous implement, we use the RDD#isEmpty to skip the empty commit, It is a heavy operation.", "author": "pengzhiwei2018", "createdAt": "2021-02-02T16:10:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTIyMDgxNg==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r569220816", "bodyText": "What I was not sure about is if this will change the Spark DAGs. But looks like it will not.\n@vinothchandar @nsivabalan can you guys take a look as well?", "author": "garyli1019", "createdAt": "2021-02-03T08:34:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDIyNDAyMQ==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r570224021", "bodyText": "Guess this is code path when auto commit is disabled. Here is the path when autoCommit is set to true. May be we need to fix here as well. But would be nice if we clean the commit property rather than just returning( .requested, .inflight etc). Basically rollback this pending commit.\nbut let's hear @vinothchandar thoughts as well whether can we delay an empty dataset to this extent or should we fail fast before itself.", "author": "nsivabalan", "createdAt": "2021-02-04T13:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDQ3NjYwMw==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r570476603", "bodyText": "I think there was an explicit ask to allow the empty commit before. Lets take deltastreamer which stores the offset of the kafka checkpoints in the commit metadata. If we don't commit when stats are empty the checkpoint will never advance. The transformer  in delta streamer could filter out all records read in that batch for e.g and lead to an empty commit. but the kafka offsets would have advanced. So its not good to do this IMO", "author": "vinothchandar", "createdAt": "2021-02-04T19:10:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMTU3Ng==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r665831576", "bodyText": "lets control this using a new config?", "author": "vinothchandar", "createdAt": "2021-07-08T02:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2OTcwMDU0MA==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r669700540", "bodyText": "yes, it is better for this.  And by default we can allow empty commits.", "author": "pengzhiwei2018", "createdAt": "2021-07-14T15:02:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}], "type": "inlineReview", "revised_code": {"commit": "04142242b1687b3fd37523dcf78edd18bcd71157", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\nindex a8a93e789..aab4bcb36 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java\n\n@@ -173,30 +172,31 @@ public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n-    // Skip the empty commit\n-    if (stats.isEmpty()) {\n+    // Skip the empty commit if not allowed\n+    if (!config.allowEmptyCommit() && stats.isEmpty()) {\n       return true;\n     }\n     LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable table = createTable(config, hadoopConf);\n-\n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata, operationType, config.getSchema(), commitActionType);\n-    // Finalize write\n-    finalizeWrite(table, instantTime, stats);\n-\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds,\n+        extraMetadata, operationType, config.getWriteSchema(), commitActionType);\n+    HeartbeatUtils.abortIfHeartbeatExpired(instantTime, table, heartbeatClient, config);\n+    this.txnManager.beginTransaction(Option.of(new HoodieInstant(State.INFLIGHT, table.getMetaClient().getCommitActionType(), instantTime)),\n+        lastCompletedTxnAndMetadata.isPresent() ? Option.of(lastCompletedTxnAndMetadata.get().getLeft()) : Option.empty());\n     try {\n-      activeTimeline.saveAsComplete(new HoodieInstant(true, commitActionType, instantTime),\n-          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n+      preCommit(instantTime, metadata);\n+      commit(table, commitActionType, instantTime, metadata, stats);\n       postCommit(table, metadata, instantTime, extraMetadata);\n-      emitCommitMetrics(instantTime, metadata, commitActionType);\n       LOG.info(\"Committed \" + instantTime);\n     } catch (IOException e) {\n-      throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n-          e);\n+      throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime, e);\n+    } finally {\n+      this.txnManager.endTransaction();\n     }\n-\n+    // do this outside of lock since compaction, clustering can be time taking and we don't need a lock for the entire execution period\n+    runTableServicesInline(table, metadata, extraMetadata);\n+    emitCommitMetrics(instantTime, metadata, commitActionType);\n     // callback if needed.\n     if (config.writeCommitCallbackOn()) {\n       if (null == commitCallback) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MzAxNw==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568693017", "bodyText": "should remain unchanged?", "author": "garyli1019", "createdAt": "2021-02-02T15:27:28Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java", "diffHunk": "@@ -186,7 +191,7 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {\n       for (String partitionPath : dataGen.getPartitionPaths()) {\n         List<WriteStatus> writeStatuses = result.collect();\n         assertTrue(writeStatuses.stream()\n-            .filter(writeStatus -> writeStatus.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);\n+            .filter(writeStatus1 -> writeStatus1.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);", "originalCommit": "673e8e7b1b2893676a6b507b66b12e2625409ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcyOTI1Mg==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568729252", "bodyText": "Thanks for the suggestion!", "author": "pengzhiwei2018", "createdAt": "2021-02-02T16:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MzAxNw=="}], "type": "inlineReview", "revised_code": {"commit": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java\nindex ae05a2ba1..cbbd406a1 100644\n--- a/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java\n+++ b/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java\n\n@@ -191,7 +191,7 @@ public class TestHoodieCompactor extends HoodieClientTestHarness {\n       for (String partitionPath : dataGen.getPartitionPaths()) {\n         List<WriteStatus> writeStatuses = result.collect();\n         assertTrue(writeStatuses.stream()\n-            .filter(writeStatus1 -> writeStatus1.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);\n+            .filter(writeStatus -> writeStatus.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);\n       }\n     }\n   }\n"}}, {"oid": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6", "url": "https://github.com/apache/hudi/commit/d4e3d89e0c02f1b9a704a61627f7aafec14e67e6", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case\n\nskip empty commit\n\nfix test case\n\nadd comment\n\nremove useless change", "committedDate": "2021-02-03T02:16:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMjIxNg==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r665832216", "bodyText": "we should actually have this generate an empty commit and test. If we don't then we checkpoints won't move.\nConsider this scenario, when deltastreamer reads from kafka using a custom transformer. If the transformer filters out all records from Kafka, we will have empty input for write, but the kafka offsets have to move ahead.", "author": "vinothchandar", "createdAt": "2021-07-08T02:50:55Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -932,15 +932,10 @@ public void testFilterDupes() throws Exception {\n     ds2.sync();\n     mClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), tableBasePath, true);\n     HoodieInstant newLastFinished = mClient.getCommitsTimeline().filterCompletedInstants().lastInstant().get();\n-    assertTrue(HoodieTimeline.compareTimestamps(newLastFinished.getTimestamp(), HoodieTimeline.GREATER_THAN, lastFinished.getTimestamp()\n+    // there is not new commit generate for empty commits", "originalCommit": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2OTcwMTQxMg==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r669701412", "bodyText": "Yes, can understand this. Will add config to control this and by default we allow the empty commits.", "author": "pengzhiwei2018", "createdAt": "2021-07-14T15:03:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMjIxNg=="}], "type": "inlineReview", "revised_code": {"commit": "04142242b1687b3fd37523dcf78edd18bcd71157", "chunk": "diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java\nindex ddf23780a..642c6664b 100644\n--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java\n+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java\n\n@@ -921,21 +1287,26 @@ public class TestHoodieDeltaStreamer extends UtilitiesTestBase {\n     assertEquals(1000, counts.get(1).getLong(1));\n \n     // Test with empty commits\n-    HoodieTableMetaClient mClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), tableBasePath, true);\n+    HoodieTableMetaClient mClient = HoodieTableMetaClient.builder().setConf(jsc.hadoopConfiguration()).setBasePath(tableBasePath).setLoadActiveTimelineOnLoad(true).build();\n     HoodieInstant lastFinished = mClient.getCommitsTimeline().filterCompletedInstants().lastInstant().get();\n     HoodieDeltaStreamer.Config cfg2 = TestHelpers.makeDropAllConfig(tableBasePath, WriteOperationType.UPSERT);\n     cfg2.filterDupes = false;\n     cfg2.sourceLimit = 2000;\n     cfg2.operation = WriteOperationType.UPSERT;\n-    cfg2.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    cfg2.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n     HoodieDeltaStreamer ds2 = new HoodieDeltaStreamer(cfg2, jsc);\n     ds2.sync();\n-    mClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), tableBasePath, true);\n+    mClient = HoodieTableMetaClient.builder().setConf(jsc.hadoopConfiguration()).setBasePath(tableBasePath).setLoadActiveTimelineOnLoad(true).build();\n     HoodieInstant newLastFinished = mClient.getCommitsTimeline().filterCompletedInstants().lastInstant().get();\n-    // there is not new commit generate for empty commits\n-    assertTrue(HoodieTimeline.compareTimestamps(newLastFinished.getTimestamp(), HoodieTimeline.EQUALS, lastFinished.getTimestamp()\n+    assertTrue(HoodieTimeline.compareTimestamps(newLastFinished.getTimestamp(), HoodieTimeline.GREATER_THAN, lastFinished.getTimestamp()\n     ));\n \n+    // Ensure it is empty\n+    HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n+        .fromBytes(mClient.getActiveTimeline().getInstantDetails(newLastFinished).get(), HoodieCommitMetadata.class);\n+    System.out.println(\"New Commit Metadata=\" + commitMetadata);\n+    assertTrue(commitMetadata.getPartitionToWriteStats().isEmpty());\n+\n     // Try UPSERT with filterDupes true. Expect exception\n     cfg2.filterDupes = true;\n     cfg2.operation = WriteOperationType.UPSERT;\n"}}, {"oid": "04142242b1687b3fd37523dcf78edd18bcd71157", "url": "https://github.com/apache/hudi/commit/04142242b1687b3fd37523dcf78edd18bcd71157", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write\n\nadd some test case\n\nskip empty commit\n\nfix test case\n\nadd comment\n\nremove useless change", "committedDate": "2021-07-15T11:33:06Z", "type": "forcePushed"}, {"oid": "380fc82fc31f5ebdede6bfef992177e8cc09016c", "url": "https://github.com/apache/hudi/commit/380fc82fc31f5ebdede6bfef992177e8cc09016c", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-15T11:33:51Z", "type": "forcePushed"}, {"oid": "dc36883644cf88971ecf13dc3642d60e60a1e6ec", "url": "https://github.com/apache/hudi/commit/dc36883644cf88971ecf13dc3642d60e60a1e6ec", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-15T11:59:27Z", "type": "forcePushed"}, {"oid": "7ebc9ff5395d8790339586ae3ff7e56b05a6a569", "url": "https://github.com/apache/hudi/commit/7ebc9ff5395d8790339586ae3ff7e56b05a6a569", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-15T12:07:35Z", "type": "forcePushed"}, {"oid": "1abea6c4b0431492eccdecb6aa3a3deab503b36c", "url": "https://github.com/apache/hudi/commit/1abea6c4b0431492eccdecb6aa3a3deab503b36c", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-15T12:11:10Z", "type": "forcePushed"}, {"oid": "244d86b04feb328d55597223bdf33f50a17c708f", "url": "https://github.com/apache/hudi/commit/244d86b04feb328d55597223bdf33f50a17c708f", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-15T12:12:16Z", "type": "forcePushed"}, {"oid": "6ac9d507dc541a7e17741c3e1fcd8a6bb28aecc6", "url": "https://github.com/apache/hudi/commit/6ac9d507dc541a7e17741c3e1fcd8a6bb28aecc6", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-16T03:58:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3ODczMjc0OA==", "url": "https://github.com/apache/hudi/pull/2296#discussion_r678732748", "bodyText": "reword: Whether to allow generation of empty commits, even if no data was written in the commit. It's useful in cases where extra metadata needs to be published regardless e.g tracking source offsets when ingesting data", "author": "vinothchandar", "createdAt": "2021-07-29T00:11:38Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -366,6 +366,11 @@\n       .withDocumentation(\"When enabled, records in older schema are rewritten into newer schema during upsert,delete and background\"\n           + \" compaction,clustering operations.\");\n \n+  public static final ConfigProperty<Boolean> ALLOW_EMPTY_COMMIT = ConfigProperty\n+       .key(\"hoodie.allow.empty.commmit\")\n+       .defaultValue(true)\n+       .withDocumentation(\"Whether to allow generate empty commit when the input is empty.\");", "originalCommit": "6ac9d507dc541a7e17741c3e1fcd8a6bb28aecc6", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f04fd3e1c0f889389c08f533f6621eb843695e88", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\nindex c97faa1c4..d64279669 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java\n\n@@ -367,9 +367,10 @@ public class HoodieWriteConfig extends HoodieConfig {\n           + \" compaction,clustering operations.\");\n \n   public static final ConfigProperty<Boolean> ALLOW_EMPTY_COMMIT = ConfigProperty\n-       .key(\"hoodie.allow.empty.commmit\")\n+       .key(\"hoodie.allow.empty.commit\")\n        .defaultValue(true)\n-       .withDocumentation(\"Whether to allow generate empty commit when the input is empty.\");\n+       .withDocumentation(\"Whether to allow generation of empty commits, even if no data was written in the commit. \"\n+          + \"It's useful in cases where extra metadata needs to be published regardless e.g tracking source offsets when ingesting data\");\n \n   private ConsistencyGuardConfig consistencyGuardConfig;\n \n"}}, {"oid": "f04fd3e1c0f889389c08f533f6621eb843695e88", "url": "https://github.com/apache/hudi/commit/f04fd3e1c0f889389c08f533f6621eb843695e88", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-29T02:24:50Z", "type": "commit"}, {"oid": "f04fd3e1c0f889389c08f533f6621eb843695e88", "url": "https://github.com/apache/hudi/commit/f04fd3e1c0f889389c08f533f6621eb843695e88", "message": "[HUDI-1425] Performance loss with the additional hoodieRecords.isEmpty() in HoodieSparkSqlWriter#write", "committedDate": "2021-07-29T02:24:50Z", "type": "forcePushed"}]}