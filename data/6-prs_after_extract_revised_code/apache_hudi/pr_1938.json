{"pr_number": 1938, "pr_title": "[HUDI-920] Support Incremental query for MOR table", "pr_createdAt": "2020-08-09T00:11:50Z", "pr_url": "https://github.com/apache/hudi/pull/1938", "timeline": [{"oid": "80f21122584fe10e07cb6cd4597958750b722c4c", "url": "https://github.com/apache/hudi/commit/80f21122584fe10e07cb6cd4597958750b722c4c", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2020-08-09T02:51:13Z", "type": "forcePushed"}, {"oid": "3caed0144c9cb7c5f1986d29699a14df5fe7c3c7", "url": "https://github.com/apache/hudi/commit/3caed0144c9cb7c5f1986d29699a14df5fe7c3c7", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2020-08-09T03:16:44Z", "type": "forcePushed"}, {"oid": "44f571fee63a8deab642a3accb4410b99428895d", "url": "https://github.com/apache/hudi/commit/44f571fee63a8deab642a3accb4410b99428895d", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2020-08-09T23:13:03Z", "type": "forcePushed"}, {"oid": "9e275c2d4677d1dd8221d2cf8e900b1f9bb0d67c", "url": "https://github.com/apache/hudi/commit/9e275c2d4677d1dd8221d2cf8e900b1f9bb0d67c", "message": "Update file listing", "committedDate": "2020-09-08T04:24:05Z", "type": "forcePushed"}, {"oid": "eb251d8ecaa5fc9d926b5cfe3092b01353ca2f20", "url": "https://github.com/apache/hudi/commit/eb251d8ecaa5fc9d926b5cfe3092b01353ca2f20", "message": "Update file listing", "committedDate": "2020-09-08T06:08:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDcwMTkzMg==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r484701932", "bodyText": "Copied and changed this method from #1817 , will apply this change to that PR after merged.", "author": "garyli1019", "createdAt": "2020-09-08T07:18:24Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -443,4 +444,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(", "originalCommit": "eb251d8ecaa5fc9d926b5cfe3092b01353ca2f20", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\nindex c15a2b4a25..019b558dcb 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n\n@@ -445,40 +472,39 @@ public class HoodieInputFormatUtils {\n   }\n \n   /**\n-   * List affected file status based on given commits.\n+   * Iterate through a list of commits in ascending order, and extract the file status of\n+   * all affected files from the commits metadata grouping by partition path. If the files has\n+   * been touched multiple times in the given commits, the return value will keep the one\n+   * from the latest commit.\n    * @param basePath\n    * @param commitsToCheck\n    * @param timeline\n    * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n    * @throws IOException\n    */\n-  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+  public static HashMap<String, HashMap<String, FileStatus>> listAffectedFilesForCommits(\n       Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n-    // Extract files touched by these commits.\n-    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    // TODO: Use HoodieMetaTable to extract affected file directly.\n     HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n-    for (HoodieInstant commit: commitsToCheck) {\n+    List<HoodieInstant> sortedCommitsToCheck = new ArrayList<>(commitsToCheck);\n+    sortedCommitsToCheck.sort(HoodieInstant::compareTo);\n+    // Iterate through the given commits.\n+    for (HoodieInstant commit: sortedCommitsToCheck) {\n       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n           HoodieCommitMetadata.class);\n+      // Iterate through all the affected partitions of a commit.\n       for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n         if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n           partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n         }\n+        // Iterate through all the written files of this partition.\n         for (HoodieWriteStat stat : entry.getValue()) {\n           String relativeFilePath = stat.getPath();\n           Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n           if (fullPath != null) {\n-            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n-              // If filesystem support Append. Update the FileStatus of log file if being appended.\n-              FileStatus prevFileStatus = partitionToFileStatusesMap.get(entry.getKey()).get(fullPath.getName());\n-              FileStatus combinedFs = new FileStatus(prevFileStatus.getLen() + stat.getTotalWriteBytes(),\n-                  false, 0, 0, 0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), combinedFs);\n-            } else {\n-              FileStatus fs = new FileStatus(stat.getTotalWriteBytes(), false, 0, 0,\n-                  0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n-            }\n+            FileStatus fs = new FileStatus(stat.getFileSizeInBytes(), false, 0, 0,\n+                0, fullPath);\n+            partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n           }\n         }\n       }\n"}}, {"oid": "1847bd3803acf3aa415f7c6a3dd686cfd92ebc12", "url": "https://github.com/apache/hudi/commit/1847bd3803acf3aa415f7c6a3dd686cfd92ebc12", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2020-12-31T06:57:10Z", "type": "forcePushed"}, {"oid": "452be51154c5b4302469e9810b5569884d61c238", "url": "https://github.com/apache/hudi/commit/452be51154c5b4302469e9810b5569884d61c238", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2021-01-07T15:37:18Z", "type": "commit"}, {"oid": "518917a10fcbd3166892f74975a080e7ea232f98", "url": "https://github.com/apache/hudi/commit/518917a10fcbd3166892f74975a080e7ea232f98", "message": "Update file listing", "committedDate": "2020-12-05T11:13:21Z", "type": "forcePushed"}, {"oid": "452be51154c5b4302469e9810b5569884d61c238", "url": "https://github.com/apache/hudi/commit/452be51154c5b4302469e9810b5569884d61c238", "message": "[HUDI-920] Support Incremental query for MOR table", "committedDate": "2021-01-07T15:37:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MTUzMw==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553481533", "bodyText": "can we just have this in hudi-spark for now. thats the only module that needs to call this.", "author": "vinothchandar", "createdAt": "2021-01-07T17:41:49Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI5MTc2MQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554291761", "bodyText": "This can be shared by other engines later. If we move this to spark we need to switch to scala code, then move it back later when supporting other engines. We could save some effort if we leave it here?", "author": "garyli1019", "createdAt": "2021-01-09T04:57:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MTUzMw=="}], "type": "inlineReview", "revised_code": {"commit": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\nindex 20c1ef1fb7..019b558dcb 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n\n@@ -472,40 +472,39 @@ public class HoodieInputFormatUtils {\n   }\n \n   /**\n-   * List affected file status based on given commits.\n+   * Iterate through a list of commits in ascending order, and extract the file status of\n+   * all affected files from the commits metadata grouping by partition path. If the files has\n+   * been touched multiple times in the given commits, the return value will keep the one\n+   * from the latest commit.\n    * @param basePath\n    * @param commitsToCheck\n    * @param timeline\n    * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n    * @throws IOException\n    */\n-  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+  public static HashMap<String, HashMap<String, FileStatus>> listAffectedFilesForCommits(\n       Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n-    // Extract files touched by these commits.\n-    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    // TODO: Use HoodieMetaTable to extract affected file directly.\n     HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n-    for (HoodieInstant commit: commitsToCheck) {\n+    List<HoodieInstant> sortedCommitsToCheck = new ArrayList<>(commitsToCheck);\n+    sortedCommitsToCheck.sort(HoodieInstant::compareTo);\n+    // Iterate through the given commits.\n+    for (HoodieInstant commit: sortedCommitsToCheck) {\n       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n           HoodieCommitMetadata.class);\n+      // Iterate through all the affected partitions of a commit.\n       for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n         if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n           partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n         }\n+        // Iterate through all the written files of this partition.\n         for (HoodieWriteStat stat : entry.getValue()) {\n           String relativeFilePath = stat.getPath();\n           Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n           if (fullPath != null) {\n-            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n-              // If filesystem support Append. Update the FileStatus of log file if being appended.\n-              FileStatus prevFileStatus = partitionToFileStatusesMap.get(entry.getKey()).get(fullPath.getName());\n-              FileStatus combinedFs = new FileStatus(prevFileStatus.getLen() + stat.getTotalWriteBytes(),\n-                  false, 0, 0, 0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), combinedFs);\n-            } else {\n-              FileStatus fs = new FileStatus(stat.getTotalWriteBytes(), false, 0, 0,\n-                  0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n-            }\n+            FileStatus fs = new FileStatus(stat.getFileSizeInBytes(), false, 0, 0,\n+                0, fullPath);\n+            partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n           }\n         }\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MjA3OA==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553482078", "bodyText": "Can we redo this such that it can use the metadata table for obtaining the listing? You can see how this is done in HoodieParquetInputFormat.", "author": "vinothchandar", "createdAt": "2021-01-07T17:42:52Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+      Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n+    // Extract files touched by these commits.\n+    // TODO This might need to be done in parallel like listStatus parallelism ?", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI4NzI2Mg==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554287262", "bodyText": "Are you referring to RFC-15 that not being landed yet? The current implementation of HoodieParquetInputFormat is listing all files of affected partitions and then do the filtering later.", "author": "garyli1019", "createdAt": "2021-01-09T04:05:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MjA3OA=="}], "type": "inlineReview", "revised_code": {"commit": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\nindex 20c1ef1fb7..019b558dcb 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n\n@@ -472,40 +472,39 @@ public class HoodieInputFormatUtils {\n   }\n \n   /**\n-   * List affected file status based on given commits.\n+   * Iterate through a list of commits in ascending order, and extract the file status of\n+   * all affected files from the commits metadata grouping by partition path. If the files has\n+   * been touched multiple times in the given commits, the return value will keep the one\n+   * from the latest commit.\n    * @param basePath\n    * @param commitsToCheck\n    * @param timeline\n    * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n    * @throws IOException\n    */\n-  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+  public static HashMap<String, HashMap<String, FileStatus>> listAffectedFilesForCommits(\n       Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n-    // Extract files touched by these commits.\n-    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    // TODO: Use HoodieMetaTable to extract affected file directly.\n     HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n-    for (HoodieInstant commit: commitsToCheck) {\n+    List<HoodieInstant> sortedCommitsToCheck = new ArrayList<>(commitsToCheck);\n+    sortedCommitsToCheck.sort(HoodieInstant::compareTo);\n+    // Iterate through the given commits.\n+    for (HoodieInstant commit: sortedCommitsToCheck) {\n       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n           HoodieCommitMetadata.class);\n+      // Iterate through all the affected partitions of a commit.\n       for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n         if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n           partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n         }\n+        // Iterate through all the written files of this partition.\n         for (HoodieWriteStat stat : entry.getValue()) {\n           String relativeFilePath = stat.getPath();\n           Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n           if (fullPath != null) {\n-            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n-              // If filesystem support Append. Update the FileStatus of log file if being appended.\n-              FileStatus prevFileStatus = partitionToFileStatusesMap.get(entry.getKey()).get(fullPath.getName());\n-              FileStatus combinedFs = new FileStatus(prevFileStatus.getLen() + stat.getTotalWriteBytes(),\n-                  false, 0, 0, 0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), combinedFs);\n-            } else {\n-              FileStatus fs = new FileStatus(stat.getTotalWriteBytes(), false, 0, 0,\n-                  0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n-            }\n+            FileStatus fs = new FileStatus(stat.getFileSizeInBytes(), false, 0, 0,\n+                0, fullPath);\n+            partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n           }\n         }\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4Mjc0Ng==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553482746", "bodyText": "a little bit more clearer doc? does this method obtain all the file status that were affected by the list of commits to check?", "author": "vinothchandar", "createdAt": "2021-01-07T17:43:58Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\nindex 20c1ef1fb7..019b558dcb 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n\n@@ -472,40 +472,39 @@ public class HoodieInputFormatUtils {\n   }\n \n   /**\n-   * List affected file status based on given commits.\n+   * Iterate through a list of commits in ascending order, and extract the file status of\n+   * all affected files from the commits metadata grouping by partition path. If the files has\n+   * been touched multiple times in the given commits, the return value will keep the one\n+   * from the latest commit.\n    * @param basePath\n    * @param commitsToCheck\n    * @param timeline\n    * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n    * @throws IOException\n    */\n-  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+  public static HashMap<String, HashMap<String, FileStatus>> listAffectedFilesForCommits(\n       Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n-    // Extract files touched by these commits.\n-    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    // TODO: Use HoodieMetaTable to extract affected file directly.\n     HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n-    for (HoodieInstant commit: commitsToCheck) {\n+    List<HoodieInstant> sortedCommitsToCheck = new ArrayList<>(commitsToCheck);\n+    sortedCommitsToCheck.sort(HoodieInstant::compareTo);\n+    // Iterate through the given commits.\n+    for (HoodieInstant commit: sortedCommitsToCheck) {\n       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n           HoodieCommitMetadata.class);\n+      // Iterate through all the affected partitions of a commit.\n       for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n         if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n           partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n         }\n+        // Iterate through all the written files of this partition.\n         for (HoodieWriteStat stat : entry.getValue()) {\n           String relativeFilePath = stat.getPath();\n           Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n           if (fullPath != null) {\n-            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n-              // If filesystem support Append. Update the FileStatus of log file if being appended.\n-              FileStatus prevFileStatus = partitionToFileStatusesMap.get(entry.getKey()).get(fullPath.getName());\n-              FileStatus combinedFs = new FileStatus(prevFileStatus.getLen() + stat.getTotalWriteBytes(),\n-                  false, 0, 0, 0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), combinedFs);\n-            } else {\n-              FileStatus fs = new FileStatus(stat.getTotalWriteBytes(), false, 0, 0,\n-                  0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n-            }\n+            FileStatus fs = new FileStatus(stat.getFileSizeInBytes(), false, 0, 0,\n+                0, fullPath);\n+            partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n           }\n         }\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4NDIwMQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r553484201", "bodyText": "you can just pick the latest such file now. it will have the latest log size using getFileSizeInBytes(). No need to do the addition here.", "author": "vinothchandar", "createdAt": "2021-01-07T17:46:36Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -470,4 +471,45 @@ private static HoodieBaseFile refreshFileStatus(Configuration conf, HoodieBaseFi\n     }\n   }\n \n+  /**\n+   * List affected file status based on given commits.\n+   * @param basePath\n+   * @param commitsToCheck\n+   * @param timeline\n+   * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n+   * @throws IOException\n+   */\n+  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+      Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n+    // Extract files touched by these commits.\n+    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n+    for (HoodieInstant commit: commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n+        if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n+          partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n+        }\n+        for (HoodieWriteStat stat : entry.getValue()) {\n+          String relativeFilePath = stat.getPath();\n+          Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n+          if (fullPath != null) {\n+            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n+              // If filesystem support Append. Update the FileStatus of log file if being appended.", "originalCommit": "452be51154c5b4302469e9810b5569884d61c238", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI5MTUwNQ==", "url": "https://github.com/apache/hudi/pull/1938#discussion_r554291505", "bodyText": "fixed.", "author": "garyli1019", "createdAt": "2021-01-09T04:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4NDIwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "chunk": "diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\nindex 20c1ef1fb7..019b558dcb 100644\n--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java\n\n@@ -472,40 +472,39 @@ public class HoodieInputFormatUtils {\n   }\n \n   /**\n-   * List affected file status based on given commits.\n+   * Iterate through a list of commits in ascending order, and extract the file status of\n+   * all affected files from the commits metadata grouping by partition path. If the files has\n+   * been touched multiple times in the given commits, the return value will keep the one\n+   * from the latest commit.\n    * @param basePath\n    * @param commitsToCheck\n    * @param timeline\n    * @return HashMap<partitionPath, HashMap<fileName, FileStatus>>\n    * @throws IOException\n    */\n-  public static HashMap<String, HashMap<String, FileStatus>> listStatusForAffectedPartitions(\n+  public static HashMap<String, HashMap<String, FileStatus>> listAffectedFilesForCommits(\n       Path basePath, List<HoodieInstant> commitsToCheck, HoodieTimeline timeline) throws IOException {\n-    // Extract files touched by these commits.\n-    // TODO This might need to be done in parallel like listStatus parallelism ?\n+    // TODO: Use HoodieMetaTable to extract affected file directly.\n     HashMap<String, HashMap<String, FileStatus>> partitionToFileStatusesMap = new HashMap<>();\n-    for (HoodieInstant commit: commitsToCheck) {\n+    List<HoodieInstant> sortedCommitsToCheck = new ArrayList<>(commitsToCheck);\n+    sortedCommitsToCheck.sort(HoodieInstant::compareTo);\n+    // Iterate through the given commits.\n+    for (HoodieInstant commit: sortedCommitsToCheck) {\n       HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n           HoodieCommitMetadata.class);\n+      // Iterate through all the affected partitions of a commit.\n       for (Map.Entry<String, List<HoodieWriteStat>> entry: commitMetadata.getPartitionToWriteStats().entrySet()) {\n         if (!partitionToFileStatusesMap.containsKey(entry.getKey())) {\n           partitionToFileStatusesMap.put(entry.getKey(), new HashMap<>());\n         }\n+        // Iterate through all the written files of this partition.\n         for (HoodieWriteStat stat : entry.getValue()) {\n           String relativeFilePath = stat.getPath();\n           Path fullPath = relativeFilePath != null ? FSUtils.getPartitionPath(basePath, relativeFilePath) : null;\n           if (fullPath != null) {\n-            if (partitionToFileStatusesMap.get(entry.getKey()).containsKey(fullPath.getName())) {\n-              // If filesystem support Append. Update the FileStatus of log file if being appended.\n-              FileStatus prevFileStatus = partitionToFileStatusesMap.get(entry.getKey()).get(fullPath.getName());\n-              FileStatus combinedFs = new FileStatus(prevFileStatus.getLen() + stat.getTotalWriteBytes(),\n-                  false, 0, 0, 0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), combinedFs);\n-            } else {\n-              FileStatus fs = new FileStatus(stat.getTotalWriteBytes(), false, 0, 0,\n-                  0, fullPath);\n-              partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n-            }\n+            FileStatus fs = new FileStatus(stat.getFileSizeInBytes(), false, 0, 0,\n+                0, fullPath);\n+            partitionToFileStatusesMap.get(entry.getKey()).put(fullPath.getName(), fs);\n           }\n         }\n       }\n"}}, {"oid": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "url": "https://github.com/apache/hudi/commit/71299741c0c42a5def873d2d51dfd4e9bb5880cb", "message": "address comments", "committedDate": "2021-01-09T04:59:12Z", "type": "commit"}, {"oid": "71299741c0c42a5def873d2d51dfd4e9bb5880cb", "url": "https://github.com/apache/hudi/commit/71299741c0c42a5def873d2d51dfd4e9bb5880cb", "message": "address comments", "committedDate": "2021-01-09T04:59:12Z", "type": "forcePushed"}]}