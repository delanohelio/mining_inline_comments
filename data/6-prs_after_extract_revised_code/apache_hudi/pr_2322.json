{"pr_number": 2322, "pr_title": "[HUDI-1437] support more accurate spark JobGroup for better performan tracking", "pr_createdAt": "2020-12-10T15:23:01Z", "pr_url": "https://github.com/apache/hudi/pull/2322", "timeline": [{"oid": "d7be7c992e707884206c835356068c28b079034b", "url": "https://github.com/apache/hudi/commit/d7be7c992e707884206c835356068c28b079034b", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-11T01:54:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4MjMwMQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r540982301", "bodyText": ",\"Delete replaced file groups\" -> , \"Delete replaced file groups\"", "author": "leesf", "createdAt": "2020-12-11T14:23:32Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java", "diffHunk": "@@ -68,7 +68,7 @@\n   public static boolean deleteReplacedFileGroups(HoodieEngineContext context, HoodieTableMetaClient metaClient,\n                                                  TableFileSystemView fileSystemView,\n                                                  HoodieInstant instant, List<String> replacedPartitions) {\n-\n+    context.setJobStatus(ReplaceArchivalHelper.class.getSimpleName(),\"Delete replaced file groups\");", "originalCommit": "d7be7c992e707884206c835356068c28b079034b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTU4Nzk3NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541587974", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-12T14:36:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4MjMwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java\nindex c85543db7..891f4a863 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/ReplaceArchivalHelper.java\n\n@@ -68,7 +68,7 @@ public class ReplaceArchivalHelper implements Serializable {\n   public static boolean deleteReplacedFileGroups(HoodieEngineContext context, HoodieTableMetaClient metaClient,\n                                                  TableFileSystemView fileSystemView,\n                                                  HoodieInstant instant, List<String> replacedPartitions) {\n-    context.setJobStatus(ReplaceArchivalHelper.class.getSimpleName(),\"Delete replaced file groups\");\n+    context.setJobStatus(ReplaceArchivalHelper.class.getSimpleName(), \"Delete replaced file groups\");\n     List<Boolean> f = context.map(replacedPartitions, partition -> {\n       Stream<FileSlice> fileSlices =  fileSystemView.getReplacedFileGroupsBeforeOrOn(instant.getTimestamp(), partition)\n           .flatMap(HoodieFileGroup::getAllRawFileSlices);\n"}}, {"oid": "ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "url": "https://github.com/apache/hudi/commit/ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-12T14:36:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg3MDA5MQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541870091", "bodyText": "this would be inneccessary?", "author": "leesf", "createdAt": "2020-12-13T07:52:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java", "diffHunk": "@@ -42,9 +42,10 @@ public IteratorBasedQueueProducer(Iterator<I> inputIterator) {\n   @Override\n   public void produce(BoundedInMemoryQueue<I, ?> queue) throws Exception {\n     LOG.info(\"starting to buffer records\");\n+    long start = System.currentTimeMillis();\n     while (inputIterator.hasNext()) {\n       queue.insertRecord(inputIterator.next());\n     }\n-    LOG.info(\"finished buffering records\");\n+    LOG.info(\"finished buffering records, cost = \" + ((System.currentTimeMillis() - start) / 1000));", "originalCommit": "ab96d884a4fa8b8ce1657bbed5b9a5b3267bdad7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTkzOTUzNg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r541939536", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-13T14:46:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTg3MDA5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "chunk": "diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java b/hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java\nindex f6a2ad4f1..3d11f38e5 100644\n--- a/hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java\n+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/queue/IteratorBasedQueueProducer.java\n\n@@ -42,10 +42,9 @@ public class IteratorBasedQueueProducer<I> implements BoundedInMemoryQueueProduc\n   @Override\n   public void produce(BoundedInMemoryQueue<I, ?> queue) throws Exception {\n     LOG.info(\"starting to buffer records\");\n-    long start = System.currentTimeMillis();\n     while (inputIterator.hasNext()) {\n       queue.insertRecord(inputIterator.next());\n     }\n-    LOG.info(\"finished buffering records, cost = \" + ((System.currentTimeMillis() - start) / 1000));\n+    LOG.info(\"finished buffering records\");\n   }\n }\n"}}, {"oid": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "url": "https://github.com/apache/hudi/commit/4181d08d22184035dcb6f73a1103fedcb95eaef3", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-13T14:45:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542250943", "bodyText": "Should we clear the jobGroup after the job finish?If not, the next job will reuse the group name.", "author": "pengzhiwei2018", "createdAt": "2020-12-14T09:53:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -52,6 +52,7 @@ public SparkMarkerBasedRollbackStrategy(HoodieTable<T, JavaRDD<HoodieRecord<T>>,\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3OTc0MA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542479740", "bodyText": "@pengzhiwei2018 Since each job should have been set their own job group name, so it should be ok?", "author": "leesf", "createdAt": "2020-12-14T15:35:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk3MTY4NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542971684", "bodyText": "Should we clear the jobGroup after the job finish?If not, the next job will reuse the group name.\n\n@pengzhiwei2018 Some method to do one specific operation of hudi, may be contain not only one spark action. it can reuse the jobGroup. In this scenario , We don\u2019t have to set it so detailed. Also it will be more suitable just like @leesf mention,  follow the original mechanism of Hudi.", "author": "lw309637554", "createdAt": "2020-12-15T01:19:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjk5OTgyMQ==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r542999821", "bodyText": "@leesf @lw309637554 Maybe we can make all of the spark job in hoodie have a job group name. But if user does some spark action after the hoodie job.The user's  job may reuse the hoodie's job group name.It is hard to control the user's behavior. In my use case, the reused job group name make me really confused in performance profile.", "author": "pengzhiwei2018", "createdAt": "2020-12-15T02:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAzODM3Nw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543038377", "bodyText": "yeah, as your case  you can set job group in you spark job. just like hudi do .", "author": "lw309637554", "createdAt": "2020-12-15T04:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MTk3NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543091974", "bodyText": "may be we can set the jobGroup to \"\" upon writeClient.close() or something?", "author": "vinothchandar", "createdAt": "2020-12-15T06:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDA5NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204094", "bodyText": "this option make sense", "author": "lw309637554", "createdAt": "2020-12-16T10:58:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjI1MDk0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\nindex 8c0c22ce5..04fe5aca5 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\n\n@@ -52,7 +52,7 @@ public class SparkMarkerBasedRollbackStrategy<T extends HoodieRecordPayload> ext\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n-      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Rolling back using marker files\");\n       return jsc.parallelize(markerFilePaths, parallelism)\n           .map(markerFilePath -> {\n             String typeStr = markerFilePath.substring(markerFilePath.lastIndexOf(\".\") + 1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5Mjk0OA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543092948", "bodyText": "change message to : \"Delete invalid files generated during the write operation\" ?", "author": "vinothchandar", "createdAt": "2020-12-15T06:59:06Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -403,6 +403,7 @@ public void finalizeWrite(HoodieEngineContext context, String instantTs, List<Ho\n \n   private void deleteInvalidFilesByPartitions(HoodieEngineContext context, Map<String, List<Pair<String, String>>> invalidFilesByPartition) {\n     // Now delete partially written files\n+    context.setJobStatus(this.getClass().getSimpleName(), \"Delete invalid files by partitions\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDUwMg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204502", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-16T10:59:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5Mjk0OA=="}], "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\nindex 29afa5507..1220e8f62 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java\n\n@@ -403,7 +403,7 @@ public abstract class HoodieTable<T extends HoodieRecordPayload, I, K, O> implem\n \n   private void deleteInvalidFilesByPartitions(HoodieEngineContext context, Map<String, List<Pair<String, String>>> invalidFilesByPartition) {\n     // Now delete partially written files\n-    context.setJobStatus(this.getClass().getSimpleName(), \"Delete invalid files by partitions\");\n+    context.setJobStatus(this.getClass().getSimpleName(), \"Delete invalid files generated during the write operation\");\n     context.map(new ArrayList<>(invalidFilesByPartition.values()), partitionWithFileList -> {\n       final FileSystem fileSystem = metaClient.getFs();\n       LOG.info(\"Deleting invalid data files=\" + partitionWithFileList);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MzM4NA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543093384", "bodyText": "change to : Obtaining marker files for all created, merged paths", "author": "vinothchandar", "createdAt": "2020-12-15T07:00:03Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -135,6 +135,7 @@ public boolean doesMarkerDirExist() throws IOException {\n     if (subDirectories.size() > 0) {\n       parallelism = Math.min(subDirectories.size(), parallelism);\n       SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+      context.setJobStatus(this.getClass().getSimpleName(), \"MarkerFiles created and merged data paths\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDIwNDgxMA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r544204810", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-16T11:00:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5MzM4NA=="}], "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java\nindex ea7865017..24ab3db3d 100644\n--- a/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java\n+++ b/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/MarkerFiles.java\n\n@@ -135,7 +135,7 @@ public class MarkerFiles implements Serializable {\n     if (subDirectories.size() > 0) {\n       parallelism = Math.min(subDirectories.size(), parallelism);\n       SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n-      context.setJobStatus(this.getClass().getSimpleName(), \"MarkerFiles created and merged data paths\");\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Obtaining marker files for all created, merged paths\");\n       dataFiles.addAll(context.flatMap(subDirectories, directory -> {\n         Path path = new Path(directory);\n         FileSystem fileSystem = path.getFileSystem(serializedConf.get());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NDI2Ng==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543094266", "bodyText": "Change to : Compute all comparisons needed between records and files", "author": "vinothchandar", "createdAt": "2020-12-15T07:01:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -137,12 +137,14 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n    */\n   private Map<String, Long> computeComparisonsPerFileGroup(final Map<String, Long> recordsPerPartition,\n                                                            final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo,\n-                                                           JavaPairRDD<String, String> partitionRecordKeyPairRDD) {\n+                                                           JavaPairRDD<String, String> partitionRecordKeyPairRDD,\n+                                                           final HoodieEngineContext context) {\n \n     Map<String, Long> fileToComparisons;\n     if (config.getBloomIndexPruneByRanges()) {\n       // we will just try exploding the input and then count to determine comparisons\n       // FIX(vc): Only do sampling here and extrapolate?\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Explode recordRDD with file comparisons\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java\nindex 960d401ed..2e653f01e 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java\n\n@@ -144,7 +144,7 @@ public class SparkHoodieBloomIndex<T extends HoodieRecordPayload> extends SparkH\n     if (config.getBloomIndexPruneByRanges()) {\n       // we will just try exploding the input and then count to determine comparisons\n       // FIX(vc): Only do sampling here and extrapolate?\n-      context.setJobStatus(this.getClass().getSimpleName(), \"Explode recordRDD with file comparisons\");\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Compute all comparisons needed between records and files\");\n       fileToComparisons = explodeRecordRDDWithFileComparisons(partitionToFileInfo, partitionRecordKeyPairRDD)\n           .mapToPair(t -> t).countByKey();\n     } else {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NDQ0OA==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543094448", "bodyText": "Change to : Building workload profile", "author": "vinothchandar", "createdAt": "2020-12-15T07:02:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -101,6 +101,7 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n \n     WorkloadProfile profile = null;\n     if (isWorkloadProfileNeeded()) {\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Build workload profile\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\nindex 8ceac228a..73be8d412 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java\n\n@@ -101,7 +101,7 @@ public abstract class BaseSparkCommitActionExecutor<T extends HoodieRecordPayloa\n \n     WorkloadProfile profile = null;\n     if (isWorkloadProfileNeeded()) {\n-      context.setJobStatus(this.getClass().getSimpleName(), \"Build workload profile\");\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Building workload profile\");\n       profile = new WorkloadProfile(buildProfile(inputRecordsRDD));\n       LOG.info(\"Workload profile :\" + profile);\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NTEwMw==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543095103", "bodyText": "Change to : Preparing compaction metadata", "author": "vinothchandar", "createdAt": "2020-12-15T07:03:54Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java", "diffHunk": "@@ -76,6 +76,7 @@ public SparkRunCompactionActionExecutor(HoodieSparkEngineContext context,\n       JavaRDD<WriteStatus> statuses = compactor.compact(context, compactionPlan, table, config, instantTime);\n \n       statuses.persist(SparkMemoryUtils.getWriteStatusStorageLevel(config.getProps()));\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Collect compaction metadata status\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java\nindex 5f7b3e7ae..5851b08c6 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkRunCompactionActionExecutor.java\n\n@@ -76,7 +76,7 @@ public class SparkRunCompactionActionExecutor<T extends HoodieRecordPayload> ext\n       JavaRDD<WriteStatus> statuses = compactor.compact(context, compactionPlan, table, config, instantTime);\n \n       statuses.persist(SparkMemoryUtils.getWriteStatusStorageLevel(config.getProps()));\n-      context.setJobStatus(this.getClass().getSimpleName(), \"Collect compaction metadata status\");\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Preparing compaction metadata\");\n       List<HoodieWriteStat> updateStatusMap = statuses.map(WriteStatus::getStat).collect();\n       HoodieCommitMetadata metadata = new HoodieCommitMetadata(true);\n       for (HoodieWriteStat stat : updateStatusMap) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5NTQ0Mg==", "url": "https://github.com/apache/hudi/pull/2322#discussion_r543095442", "bodyText": "Change to:  Rolling back using marker files", "author": "vinothchandar", "createdAt": "2020-12-15T07:04:26Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -52,6 +52,7 @@ public SparkMarkerBasedRollbackStrategy(HoodieTable<T, JavaRDD<HoodieRecord<T>>,\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");", "originalCommit": "4181d08d22184035dcb6f73a1103fedcb95eaef3", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "chunk": "diff --git a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\nindex 8c0c22ce5..04fe5aca5 100644\n--- a/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\n+++ b/hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/rollback/SparkMarkerBasedRollbackStrategy.java\n\n@@ -52,7 +52,7 @@ public class SparkMarkerBasedRollbackStrategy<T extends HoodieRecordPayload> ext\n       MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n       List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n       int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n-      jsc.setJobGroup(this.getClass().getSimpleName(), \"Marker files rollback\");\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Rolling back using marker files\");\n       return jsc.parallelize(markerFilePaths, parallelism)\n           .map(markerFilePath -> {\n             String typeStr = markerFilePath.substring(markerFilePath.lastIndexOf(\".\") + 1);\n"}}, {"oid": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "url": "https://github.com/apache/hudi/commit/65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-16T11:03:15Z", "type": "commit"}, {"oid": "65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "url": "https://github.com/apache/hudi/commit/65ed0cb2eb072e4a91dd5ebb6332e20b238d291e", "message": "[HUDI-1437]  support more accurate spark JobGroup for better performance tracking", "committedDate": "2020-12-16T11:03:15Z", "type": "forcePushed"}, {"oid": "9eef5e89c314a0e99cfc41a91be94e457c7279cf", "url": "https://github.com/apache/hudi/commit/9eef5e89c314a0e99cfc41a91be94e457c7279cf", "message": "Merge branch 'master' into HUDI-1437-3", "committedDate": "2020-12-16T11:05:55Z", "type": "commit"}]}