{"pr_number": 6263, "pr_title": "Move Hadoop file system cache into Trino", "pr_createdAt": "2020-12-08T21:15:09Z", "pr_url": "https://github.com/trinodb/trino/pull/6263", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc5NDQyOQ==", "url": "https://github.com/trinodb/trino/pull/6263#discussion_r539794429", "bodyText": "sometime back we had added requireHadoopNative call at one more place where hadoop code is initialized: #5076 We should register the cache there as well?", "author": "phd3", "createdAt": "2020-12-10T02:22:08Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/HdfsEnvironment.java", "diffHunk": "@@ -37,6 +39,7 @@\n {\n     static {\n         HadoopNative.requireHadoopNative();\n+        FileSystemManager.registerCache(PrestoFileSystemCache.INSTANCE);", "originalCommit": "0f37b83ebba61e9d0b2348f1860411326a5e4163", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYxMjAzNw==", "url": "https://github.com/trinodb/trino/pull/6263#discussion_r540612037", "bodyText": "This is a good point, but it shouldn't be needed there, since the Kerberos code doesn't touch any file system related code.", "author": "electrum", "createdAt": "2020-12-11T00:59:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc5NDQyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "7949c3226295f5674463aba58ac98bcf995b594e", "chunk": "diff --git a/presto-hive/src/main/java/io/prestosql/plugin/hive/HdfsEnvironment.java b/presto-hive/src/main/java/io/prestosql/plugin/hive/HdfsEnvironment.java\ndeleted file mode 100644\nindex 1b1ed63934..0000000000\n--- a/presto-hive/src/main/java/io/prestosql/plugin/hive/HdfsEnvironment.java\n+++ /dev/null\n\n@@ -1,185 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.plugin.hive;\n-\n-import com.google.common.primitives.Shorts;\n-import io.prestosql.hadoop.HadoopNative;\n-import io.prestosql.plugin.hive.authentication.GenericExceptionAction;\n-import io.prestosql.plugin.hive.authentication.HdfsAuthentication;\n-import io.prestosql.plugin.hive.fs.PrestoFileSystemCache;\n-import io.prestosql.spi.connector.ConnectorSession;\n-import io.prestosql.spi.security.ConnectorIdentity;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.FileSystemManager;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.fs.permission.FsPermission;\n-\n-import javax.inject.Inject;\n-\n-import java.io.IOException;\n-import java.util.Optional;\n-\n-import static com.google.common.base.MoreObjects.toStringHelper;\n-import static java.lang.Integer.parseUnsignedInt;\n-import static java.util.Objects.requireNonNull;\n-\n-public class HdfsEnvironment\n-{\n-    static {\n-        HadoopNative.requireHadoopNative();\n-        FileSystemManager.registerCache(PrestoFileSystemCache.INSTANCE);\n-    }\n-\n-    private final HdfsConfiguration hdfsConfiguration;\n-    private final HdfsAuthentication hdfsAuthentication;\n-    private final FsPermission newDirectoryPermissions;\n-    private final boolean newFileInheritOwnership;\n-    private final boolean verifyChecksum;\n-\n-    @Inject\n-    public HdfsEnvironment(\n-            HdfsConfiguration hdfsConfiguration,\n-            HdfsConfig config,\n-            HdfsAuthentication hdfsAuthentication)\n-    {\n-        this.hdfsConfiguration = requireNonNull(hdfsConfiguration, \"hdfsConfiguration is null\");\n-        requireNonNull(config, \"config is null\");\n-        this.newDirectoryPermissions = FsPermission.createImmutable(Shorts.checkedCast(parseUnsignedInt(config.getNewDirectoryPermissions(), 8)));\n-        this.newFileInheritOwnership = config.isNewFileInheritOwnership();\n-        this.verifyChecksum = config.isVerifyChecksum();\n-        this.hdfsAuthentication = requireNonNull(hdfsAuthentication, \"hdfsAuthentication is null\");\n-    }\n-\n-    public Configuration getConfiguration(HdfsContext context, Path path)\n-    {\n-        return hdfsConfiguration.getConfiguration(context, path.toUri());\n-    }\n-\n-    public FileSystem getFileSystem(HdfsContext context, Path path)\n-            throws IOException\n-    {\n-        return getFileSystem(context.getIdentity().getUser(), path, getConfiguration(context, path));\n-    }\n-\n-    public FileSystem getFileSystem(String user, Path path, Configuration configuration)\n-            throws IOException\n-    {\n-        return hdfsAuthentication.doAs(user, () -> {\n-            FileSystem fileSystem = path.getFileSystem(configuration);\n-            fileSystem.setVerifyChecksum(verifyChecksum);\n-            return fileSystem;\n-        });\n-    }\n-\n-    public FsPermission getNewDirectoryPermissions()\n-    {\n-        return newDirectoryPermissions;\n-    }\n-\n-    public boolean isNewFileInheritOwnership()\n-    {\n-        return newFileInheritOwnership;\n-    }\n-\n-    public <R, E extends Exception> R doAs(String user, GenericExceptionAction<R, E> action)\n-            throws E\n-    {\n-        return hdfsAuthentication.doAs(user, action);\n-    }\n-\n-    public void doAs(String user, Runnable action)\n-    {\n-        hdfsAuthentication.doAs(user, action);\n-    }\n-\n-    public static class HdfsContext\n-    {\n-        private final ConnectorIdentity identity;\n-        private final Optional<String> source;\n-        private final Optional<String> queryId;\n-        private final Optional<String> schemaName;\n-        private final Optional<String> tableName;\n-\n-        public HdfsContext(ConnectorIdentity identity)\n-        {\n-            this.identity = requireNonNull(identity, \"identity is null\");\n-            this.source = Optional.empty();\n-            this.queryId = Optional.empty();\n-            this.schemaName = Optional.empty();\n-            this.tableName = Optional.empty();\n-        }\n-\n-        public HdfsContext(ConnectorSession session, String schemaName)\n-        {\n-            requireNonNull(session, \"session is null\");\n-            requireNonNull(schemaName, \"schemaName is null\");\n-            this.identity = requireNonNull(session.getIdentity(), \"session.getIdentity() is null\");\n-            this.source = requireNonNull(session.getSource(), \"session.getSource()\");\n-            this.queryId = Optional.of(session.getQueryId());\n-            this.schemaName = Optional.of(schemaName);\n-            this.tableName = Optional.empty();\n-        }\n-\n-        public HdfsContext(ConnectorSession session, String schemaName, String tableName)\n-        {\n-            requireNonNull(session, \"session is null\");\n-            requireNonNull(schemaName, \"schemaName is null\");\n-            requireNonNull(tableName, \"tableName is null\");\n-            this.identity = requireNonNull(session.getIdentity(), \"session.getIdentity() is null\");\n-            this.source = requireNonNull(session.getSource(), \"session.getSource()\");\n-            this.queryId = Optional.of(session.getQueryId());\n-            this.schemaName = Optional.of(schemaName);\n-            this.tableName = Optional.of(tableName);\n-        }\n-\n-        public ConnectorIdentity getIdentity()\n-        {\n-            return identity;\n-        }\n-\n-        public Optional<String> getSource()\n-        {\n-            return source;\n-        }\n-\n-        public Optional<String> getQueryId()\n-        {\n-            return queryId;\n-        }\n-\n-        public Optional<String> getSchemaName()\n-        {\n-            return schemaName;\n-        }\n-\n-        public Optional<String> getTableName()\n-        {\n-            return tableName;\n-        }\n-\n-        @Override\n-        public String toString()\n-        {\n-            return toStringHelper(this)\n-                    .omitNullValues()\n-                    .add(\"user\", identity)\n-                    .add(\"source\", source.orElse(null))\n-                    .add(\"queryId\", queryId.orElse(null))\n-                    .add(\"schemaName\", schemaName.orElse(null))\n-                    .add(\"tableName\", tableName.orElse(null))\n-                    .toString();\n-        }\n-    }\n-}\n"}}, {"oid": "7949c3226295f5674463aba58ac98bcf995b594e", "url": "https://github.com/trinodb/trino/commit/7949c3226295f5674463aba58ac98bcf995b594e", "message": "Move Hadoop file system cache into Trino", "committedDate": "2021-06-08T02:09:09Z", "type": "commit"}, {"oid": "7949c3226295f5674463aba58ac98bcf995b594e", "url": "https://github.com/trinodb/trino/commit/7949c3226295f5674463aba58ac98bcf995b594e", "message": "Move Hadoop file system cache into Trino", "committedDate": "2021-06-08T02:09:09Z", "type": "forcePushed"}]}