{"pr_number": 6063, "pr_title": "Added ability to have unique table location for each iceberg table", "pr_createdAt": "2020-11-23T18:22:45Z", "pr_url": "https://github.com/trinodb/trino/pull/6063", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5ODcxOQ==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529798719", "bodyText": "We don't need to say \"if true\" for boolean properties, as that's redundant. Let's phrase this as\n\nInclude UUID in the table location", "author": "electrum", "createdAt": "2020-11-24T18:41:23Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java", "diffHunk": "@@ -52,4 +54,18 @@ public IcebergConfig setCompressionCodec(HiveCompressionCodec compressionCodec)\n         this.compressionCodec = compressionCodec;\n         return this;\n     }\n+\n+    @NotNull\n+    public boolean isUniqueTableLocation()\n+    {\n+        return uniqueTableLocation;\n+    }\n+\n+    @Config(\"iceberg.unique-table-location\")\n+    @ConfigDescription(\"If true UUID will be added to the table location\")", "originalCommit": "e039d5d9586b9204d0fac111279822be52d24fbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgzNzc0MA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529837740", "bodyText": "looks really better, thanks", "author": "sshkvar", "createdAt": "2020-11-24T19:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5ODcxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java b/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java\ndeleted file mode 100644\nindex 2ed95f6faf..0000000000\n--- a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java\n+++ /dev/null\n\n@@ -1,71 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.plugin.iceberg;\n-\n-import io.airlift.configuration.Config;\n-import io.airlift.configuration.ConfigDescription;\n-import io.prestosql.plugin.hive.HiveCompressionCodec;\n-import org.apache.iceberg.FileFormat;\n-\n-import javax.validation.constraints.NotNull;\n-\n-import static io.prestosql.plugin.hive.HiveCompressionCodec.GZIP;\n-import static io.prestosql.plugin.iceberg.IcebergFileFormat.ORC;\n-\n-public class IcebergConfig\n-{\n-    private IcebergFileFormat fileFormat = ORC;\n-    private HiveCompressionCodec compressionCodec = GZIP;\n-    private boolean uniqueTableLocation;\n-\n-    @NotNull\n-    public FileFormat getFileFormat()\n-    {\n-        return FileFormat.valueOf(fileFormat.name());\n-    }\n-\n-    @Config(\"iceberg.file-format\")\n-    public IcebergConfig setFileFormat(IcebergFileFormat fileFormat)\n-    {\n-        this.fileFormat = fileFormat;\n-        return this;\n-    }\n-\n-    @NotNull\n-    public HiveCompressionCodec getCompressionCodec()\n-    {\n-        return compressionCodec;\n-    }\n-\n-    @Config(\"iceberg.compression-codec\")\n-    public IcebergConfig setCompressionCodec(HiveCompressionCodec compressionCodec)\n-    {\n-        this.compressionCodec = compressionCodec;\n-        return this;\n-    }\n-\n-    @NotNull\n-    public boolean isUniqueTableLocation()\n-    {\n-        return uniqueTableLocation;\n-    }\n-\n-    @Config(\"iceberg.unique-table-location\")\n-    @ConfigDescription(\"If true UUID will be added to the table location\")\n-    public IcebergConfig setUniqueTableLocation(boolean uniqueTableLocation)\n-    {\n-        this.uniqueTableLocation = uniqueTableLocation;\n-        return this;\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5ODc4MA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529798780", "bodyText": "Not needed since this returns a primitive", "author": "electrum", "createdAt": "2020-11-24T18:41:29Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java", "diffHunk": "@@ -52,4 +54,18 @@ public IcebergConfig setCompressionCodec(HiveCompressionCodec compressionCodec)\n         this.compressionCodec = compressionCodec;\n         return this;\n     }\n+\n+    @NotNull", "originalCommit": "e039d5d9586b9204d0fac111279822be52d24fbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgzODMyNQ==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529838325", "bodyText": "removed, thanks", "author": "sshkvar", "createdAt": "2020-11-24T19:50:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc5ODc4MA=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java b/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java\ndeleted file mode 100644\nindex 2ed95f6faf..0000000000\n--- a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergConfig.java\n+++ /dev/null\n\n@@ -1,71 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.plugin.iceberg;\n-\n-import io.airlift.configuration.Config;\n-import io.airlift.configuration.ConfigDescription;\n-import io.prestosql.plugin.hive.HiveCompressionCodec;\n-import org.apache.iceberg.FileFormat;\n-\n-import javax.validation.constraints.NotNull;\n-\n-import static io.prestosql.plugin.hive.HiveCompressionCodec.GZIP;\n-import static io.prestosql.plugin.iceberg.IcebergFileFormat.ORC;\n-\n-public class IcebergConfig\n-{\n-    private IcebergFileFormat fileFormat = ORC;\n-    private HiveCompressionCodec compressionCodec = GZIP;\n-    private boolean uniqueTableLocation;\n-\n-    @NotNull\n-    public FileFormat getFileFormat()\n-    {\n-        return FileFormat.valueOf(fileFormat.name());\n-    }\n-\n-    @Config(\"iceberg.file-format\")\n-    public IcebergConfig setFileFormat(IcebergFileFormat fileFormat)\n-    {\n-        this.fileFormat = fileFormat;\n-        return this;\n-    }\n-\n-    @NotNull\n-    public HiveCompressionCodec getCompressionCodec()\n-    {\n-        return compressionCodec;\n-    }\n-\n-    @Config(\"iceberg.compression-codec\")\n-    public IcebergConfig setCompressionCodec(HiveCompressionCodec compressionCodec)\n-    {\n-        this.compressionCodec = compressionCodec;\n-        return this;\n-    }\n-\n-    @NotNull\n-    public boolean isUniqueTableLocation()\n-    {\n-        return uniqueTableLocation;\n-    }\n-\n-    @Config(\"iceberg.unique-table-location\")\n-    @ConfigDescription(\"If true UUID will be added to the table location\")\n-    public IcebergConfig setUniqueTableLocation(boolean uniqueTableLocation)\n-    {\n-        this.uniqueTableLocation = uniqueTableLocation;\n-        return this;\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgwMTMwOA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529801308", "bodyText": "Should we remove the dashes? Compare\n\nmy_table-1ec297aa2e8511eb9dfe7bf8d2aea93a\nmy_table_1ec297aa-2e85-11eb-9dfe-7bf8d2aea93a", "author": "electrum", "createdAt": "2020-11-24T18:45:50Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -393,7 +397,8 @@ public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, Con\n         HiveIdentity identity = new HiveIdentity(session);\n         String targetPath = getTableLocation(tableMetadata.getProperties());\n         if (targetPath == null) {\n-            targetPath = getTableDefaultLocation(database, hdfsContext, hdfsEnvironment, schemaName, tableName).toString();\n+            String uniqueTableName = useUniqueTableLocation ? String.format(\"%s_%s\", tableName, UUID.randomUUID()) : tableName;", "originalCommit": "e039d5d9586b9204d0fac111279822be52d24fbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgzOTA4Mg==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529839082", "bodyText": "yes, without dashes looks better. Removed dashes and moved generation of unique table name to separate private method", "author": "sshkvar", "createdAt": "2020-11-24T19:52:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgwMTMwOA=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java b/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java\ndeleted file mode 100644\nindex 323164511b..0000000000\n--- a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadata.java\n+++ /dev/null\n\n@@ -1,704 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.plugin.iceberg;\n-\n-import com.google.common.base.VerifyException;\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.ImmutableMap;\n-import io.airlift.json.JsonCodec;\n-import io.airlift.slice.Slice;\n-import io.prestosql.plugin.base.classloader.ClassLoaderSafeSystemTable;\n-import io.prestosql.plugin.hive.HdfsEnvironment;\n-import io.prestosql.plugin.hive.HdfsEnvironment.HdfsContext;\n-import io.prestosql.plugin.hive.HiveSchemaProperties;\n-import io.prestosql.plugin.hive.HiveWrittenPartitions;\n-import io.prestosql.plugin.hive.TableAlreadyExistsException;\n-import io.prestosql.plugin.hive.authentication.HiveIdentity;\n-import io.prestosql.plugin.hive.metastore.Database;\n-import io.prestosql.plugin.hive.metastore.HiveMetastore;\n-import io.prestosql.plugin.hive.metastore.HivePrincipal;\n-import io.prestosql.plugin.hive.metastore.Table;\n-import io.prestosql.spi.PrestoException;\n-import io.prestosql.spi.connector.CatalogSchemaName;\n-import io.prestosql.spi.connector.ColumnHandle;\n-import io.prestosql.spi.connector.ColumnMetadata;\n-import io.prestosql.spi.connector.ConnectorInsertTableHandle;\n-import io.prestosql.spi.connector.ConnectorMetadata;\n-import io.prestosql.spi.connector.ConnectorNewTableLayout;\n-import io.prestosql.spi.connector.ConnectorOutputMetadata;\n-import io.prestosql.spi.connector.ConnectorOutputTableHandle;\n-import io.prestosql.spi.connector.ConnectorSession;\n-import io.prestosql.spi.connector.ConnectorTableHandle;\n-import io.prestosql.spi.connector.ConnectorTableMetadata;\n-import io.prestosql.spi.connector.ConnectorTableProperties;\n-import io.prestosql.spi.connector.Constraint;\n-import io.prestosql.spi.connector.ConstraintApplicationResult;\n-import io.prestosql.spi.connector.SchemaNotFoundException;\n-import io.prestosql.spi.connector.SchemaTableName;\n-import io.prestosql.spi.connector.SchemaTablePrefix;\n-import io.prestosql.spi.connector.SystemTable;\n-import io.prestosql.spi.connector.TableNotFoundException;\n-import io.prestosql.spi.predicate.Domain;\n-import io.prestosql.spi.predicate.TupleDomain;\n-import io.prestosql.spi.security.PrestoPrincipal;\n-import io.prestosql.spi.statistics.ComputedStatistics;\n-import io.prestosql.spi.statistics.TableStatistics;\n-import io.prestosql.spi.type.TypeManager;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.iceberg.AppendFiles;\n-import org.apache.iceberg.DataFiles;\n-import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.PartitionField;\n-import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.PartitionSpecParser;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.SchemaParser;\n-import org.apache.iceberg.Snapshot;\n-import org.apache.iceberg.TableMetadata;\n-import org.apache.iceberg.TableOperations;\n-import org.apache.iceberg.Transaction;\n-import org.apache.iceberg.types.Type;\n-import org.apache.iceberg.types.TypeUtil;\n-import org.apache.iceberg.types.Types;\n-import org.apache.iceberg.types.Types.NestedField;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.OptionalLong;\n-import java.util.Set;\n-import java.util.UUID;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.atomic.AtomicInteger;\n-import java.util.function.BiPredicate;\n-\n-import static com.google.common.base.Verify.verify;\n-import static com.google.common.collect.ImmutableList.toImmutableList;\n-import static com.google.common.collect.ImmutableMap.toImmutableMap;\n-import static com.google.common.collect.ImmutableSet.toImmutableSet;\n-import static io.prestosql.plugin.hive.HiveMetadata.TABLE_COMMENT;\n-import static io.prestosql.plugin.hive.util.HiveWriteUtils.getTableDefaultLocation;\n-import static io.prestosql.plugin.iceberg.ExpressionConverter.toIcebergExpression;\n-import static io.prestosql.plugin.iceberg.IcebergSchemaProperties.getSchemaLocation;\n-import static io.prestosql.plugin.iceberg.IcebergTableProperties.FILE_FORMAT_PROPERTY;\n-import static io.prestosql.plugin.iceberg.IcebergTableProperties.PARTITIONING_PROPERTY;\n-import static io.prestosql.plugin.iceberg.IcebergTableProperties.getFileFormat;\n-import static io.prestosql.plugin.iceberg.IcebergTableProperties.getPartitioning;\n-import static io.prestosql.plugin.iceberg.IcebergTableProperties.getTableLocation;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.getColumns;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.getDataPath;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.getFileFormat;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.getIcebergTable;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.getTableComment;\n-import static io.prestosql.plugin.iceberg.IcebergUtil.isIcebergTable;\n-import static io.prestosql.plugin.iceberg.PartitionFields.parsePartitionFields;\n-import static io.prestosql.plugin.iceberg.PartitionFields.toPartitionFields;\n-import static io.prestosql.plugin.iceberg.TableType.DATA;\n-import static io.prestosql.plugin.iceberg.TypeConverter.toIcebergType;\n-import static io.prestosql.plugin.iceberg.TypeConverter.toPrestoType;\n-import static io.prestosql.spi.StandardErrorCode.INVALID_SCHEMA_PROPERTY;\n-import static io.prestosql.spi.StandardErrorCode.NOT_SUPPORTED;\n-import static io.prestosql.spi.StandardErrorCode.SCHEMA_NOT_EMPTY;\n-import static io.prestosql.spi.type.BigintType.BIGINT;\n-import static java.util.Collections.singletonList;\n-import static java.util.Objects.requireNonNull;\n-import static java.util.function.Function.identity;\n-import static java.util.stream.Collectors.toList;\n-import static org.apache.iceberg.BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE;\n-import static org.apache.iceberg.BaseMetastoreTableOperations.TABLE_TYPE_PROP;\n-import static org.apache.iceberg.TableMetadata.newTableMetadata;\n-import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n-import static org.apache.iceberg.Transactions.createTableTransaction;\n-\n-public class IcebergMetadata\n-        implements ConnectorMetadata\n-{\n-    private final HiveMetastore metastore;\n-    private final HdfsEnvironment hdfsEnvironment;\n-    private final TypeManager typeManager;\n-    private final JsonCodec<CommitTaskData> commitTaskCodec;\n-\n-    private final Map<String, Optional<Long>> snapshotIds = new ConcurrentHashMap<>();\n-    private final boolean useUniqueTableLocation;\n-\n-    private Transaction transaction;\n-\n-    public IcebergMetadata(\n-            HiveMetastore metastore,\n-            HdfsEnvironment hdfsEnvironment,\n-            TypeManager typeManager,\n-            JsonCodec<CommitTaskData> commitTaskCodec,\n-            boolean useUniqueTableLocation)\n-    {\n-        this.metastore = requireNonNull(metastore, \"metastore is null\");\n-        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n-        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n-        this.commitTaskCodec = requireNonNull(commitTaskCodec, \"commitTaskCodec is null\");\n-        this.useUniqueTableLocation = useUniqueTableLocation;\n-    }\n-\n-    @Override\n-    public List<String> listSchemaNames(ConnectorSession session)\n-    {\n-        return metastore.getAllDatabases();\n-    }\n-\n-    @Override\n-    public Map<String, Object> getSchemaProperties(ConnectorSession session, CatalogSchemaName schemaName)\n-    {\n-        Optional<Database> db = metastore.getDatabase(schemaName.getSchemaName());\n-        if (db.isPresent()) {\n-            return HiveSchemaProperties.fromDatabase(db.get());\n-        }\n-\n-        throw new SchemaNotFoundException(schemaName.getSchemaName());\n-    }\n-\n-    @Override\n-    public Optional<PrestoPrincipal> getSchemaOwner(ConnectorSession session, CatalogSchemaName schemaName)\n-    {\n-        Optional<Database> database = metastore.getDatabase(schemaName.getSchemaName());\n-        if (database.isPresent()) {\n-            return database.flatMap(db -> Optional.of(new PrestoPrincipal(db.getOwnerType(), db.getOwnerName())));\n-        }\n-\n-        throw new SchemaNotFoundException(schemaName.getSchemaName());\n-    }\n-\n-    @Override\n-    public IcebergTableHandle getTableHandle(ConnectorSession session, SchemaTableName tableName)\n-    {\n-        IcebergTableName name = IcebergTableName.from(tableName.getTableName());\n-        verify(name.getTableType() == DATA, \"Wrong table type: \" + name.getTableType());\n-\n-        Optional<Table> hiveTable = metastore.getTable(new HiveIdentity(session), tableName.getSchemaName(), name.getTableName());\n-        if (hiveTable.isEmpty()) {\n-            return null;\n-        }\n-        if (!isIcebergTable(hiveTable.get())) {\n-            throw new UnknownTableTypeException(tableName);\n-        }\n-\n-        org.apache.iceberg.Table table = getIcebergTable(metastore, hdfsEnvironment, session, hiveTable.get().getSchemaTableName());\n-        Optional<Long> snapshotId = getSnapshotId(table, name.getSnapshotId());\n-\n-        return new IcebergTableHandle(\n-                tableName.getSchemaName(),\n-                name.getTableName(),\n-                name.getTableType(),\n-                snapshotId,\n-                TupleDomain.all());\n-    }\n-\n-    @Override\n-    public Optional<SystemTable> getSystemTable(ConnectorSession session, SchemaTableName tableName)\n-    {\n-        return getRawSystemTable(session, tableName)\n-                .map(systemTable -> new ClassLoaderSafeSystemTable(systemTable, getClass().getClassLoader()));\n-    }\n-\n-    private Optional<SystemTable> getRawSystemTable(ConnectorSession session, SchemaTableName tableName)\n-    {\n-        IcebergTableName name = IcebergTableName.from(tableName.getTableName());\n-\n-        Optional<Table> hiveTable = metastore.getTable(new HiveIdentity(session), tableName.getSchemaName(), name.getTableName());\n-        if (hiveTable.isEmpty() || !isIcebergTable(hiveTable.get())) {\n-            return Optional.empty();\n-        }\n-\n-        org.apache.iceberg.Table table = getIcebergTable(metastore, hdfsEnvironment, session, hiveTable.get().getSchemaTableName());\n-\n-        SchemaTableName systemTableName = new SchemaTableName(tableName.getSchemaName(), name.getTableNameWithType());\n-        switch (name.getTableType()) {\n-            case HISTORY:\n-                if (name.getSnapshotId().isPresent()) {\n-                    throw new PrestoException(NOT_SUPPORTED, \"Snapshot ID not supported for history table: \" + systemTableName);\n-                }\n-                return Optional.of(new HistoryTable(systemTableName, table));\n-            case SNAPSHOTS:\n-                if (name.getSnapshotId().isPresent()) {\n-                    throw new PrestoException(NOT_SUPPORTED, \"Snapshot ID not supported for snapshots table: \" + systemTableName);\n-                }\n-                return Optional.of(new SnapshotsTable(systemTableName, typeManager, table));\n-            case PARTITIONS:\n-                return Optional.of(new PartitionTable(systemTableName, typeManager, table, getSnapshotId(table, name.getSnapshotId())));\n-            case MANIFESTS:\n-                return Optional.of(new ManifestsTable(systemTableName, table, getSnapshotId(table, name.getSnapshotId())));\n-            case FILES:\n-                return Optional.of(new FilesTable(systemTableName, typeManager, table, getSnapshotId(table, name.getSnapshotId())));\n-        }\n-        return Optional.empty();\n-    }\n-\n-    @Override\n-    public ConnectorTableProperties getTableProperties(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        return new ConnectorTableProperties();\n-    }\n-\n-    @Override\n-    public ConnectorTableMetadata getTableMetadata(ConnectorSession session, ConnectorTableHandle table)\n-    {\n-        return getTableMetadata(session, ((IcebergTableHandle) table).getSchemaTableName());\n-    }\n-\n-    @Override\n-    public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n-    {\n-        return schemaName.map(Collections::singletonList)\n-                .orElseGet(metastore::getAllDatabases)\n-                .stream()\n-                .flatMap(schema -> metastore.getTablesWithParameter(schema, TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE).stream()\n-                        .map(table -> new SchemaTableName(schema, table))\n-                        .collect(toList())\n-                        .stream())\n-                .collect(toList());\n-    }\n-\n-    @Override\n-    public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n-        return getColumns(icebergTable.schema(), typeManager).stream()\n-                .collect(toImmutableMap(IcebergColumnHandle::getName, identity()));\n-    }\n-\n-    @Override\n-    public ColumnMetadata getColumnMetadata(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle columnHandle)\n-    {\n-        IcebergColumnHandle column = (IcebergColumnHandle) columnHandle;\n-        return ColumnMetadata.builder()\n-                .setName(column.getName())\n-                .setType(column.getType())\n-                .setComment(column.getComment())\n-                .build();\n-    }\n-\n-    @Override\n-    public Map<SchemaTableName, List<ColumnMetadata>> listTableColumns(ConnectorSession session, SchemaTablePrefix prefix)\n-    {\n-        List<SchemaTableName> tables = prefix.getTable()\n-                .map(ignored -> singletonList(prefix.toSchemaTableName()))\n-                .orElseGet(() -> listTables(session, prefix.getSchema()));\n-\n-        ImmutableMap.Builder<SchemaTableName, List<ColumnMetadata>> columns = ImmutableMap.builder();\n-        for (SchemaTableName table : tables) {\n-            try {\n-                columns.put(table, getTableMetadata(session, table).getColumns());\n-            }\n-            catch (TableNotFoundException e) {\n-                // table disappeared during listing operation\n-            }\n-            catch (UnknownTableTypeException e) {\n-                // ignore table of unknown type\n-            }\n-        }\n-        return columns.build();\n-    }\n-\n-    @Override\n-    public void createSchema(ConnectorSession session, String schemaName, Map<String, Object> properties, PrestoPrincipal owner)\n-    {\n-        Optional<String> location = getSchemaLocation(properties).map(uri -> {\n-            try {\n-                hdfsEnvironment.getFileSystem(new HdfsContext(session, schemaName), new Path(uri));\n-            }\n-            catch (IOException | IllegalArgumentException e) {\n-                throw new PrestoException(INVALID_SCHEMA_PROPERTY, \"Invalid location URI: \" + uri, e);\n-            }\n-            return uri;\n-        });\n-\n-        Database database = Database.builder()\n-                .setDatabaseName(schemaName)\n-                .setLocation(location)\n-                .setOwnerType(owner.getType())\n-                .setOwnerName(owner.getName())\n-                .build();\n-\n-        metastore.createDatabase(new HiveIdentity(session), database);\n-    }\n-\n-    @Override\n-    public void dropSchema(ConnectorSession session, String schemaName)\n-    {\n-        // basic sanity check to provide a better error message\n-        if (!listTables(session, Optional.of(schemaName)).isEmpty() ||\n-                !listViews(session, Optional.of(schemaName)).isEmpty()) {\n-            throw new PrestoException(SCHEMA_NOT_EMPTY, \"Schema not empty: \" + schemaName);\n-        }\n-        metastore.dropDatabase(new HiveIdentity(session), schemaName);\n-    }\n-\n-    @Override\n-    public void renameSchema(ConnectorSession session, String source, String target)\n-    {\n-        metastore.renameDatabase(new HiveIdentity(session), source, target);\n-    }\n-\n-    @Override\n-    public void setSchemaAuthorization(ConnectorSession session, String source, PrestoPrincipal principal)\n-    {\n-        metastore.setDatabaseOwner(new HiveIdentity(session), source, HivePrincipal.from(principal));\n-    }\n-\n-    @Override\n-    public void createTable(ConnectorSession session, ConnectorTableMetadata tableMetadata, boolean ignoreExisting)\n-    {\n-        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n-        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n-    }\n-\n-    @Override\n-    public void setTableComment(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<String> comment)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-        metastore.commentTable(new HiveIdentity(session), handle.getSchemaName(), handle.getTableName(), comment);\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n-        if (comment.isEmpty()) {\n-            icebergTable.updateProperties().remove(TABLE_COMMENT).commit();\n-        }\n-        else {\n-            icebergTable.updateProperties().set(TABLE_COMMENT, comment.get()).commit();\n-        }\n-    }\n-\n-    @Override\n-    public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, ConnectorTableMetadata tableMetadata, Optional<ConnectorNewTableLayout> layout)\n-    {\n-        SchemaTableName schemaTableName = tableMetadata.getTable();\n-        String schemaName = schemaTableName.getSchemaName();\n-        String tableName = schemaTableName.getTableName();\n-\n-        Schema schema = toIcebergSchema(tableMetadata.getColumns());\n-\n-        PartitionSpec partitionSpec = parsePartitionFields(schema, getPartitioning(tableMetadata.getProperties()));\n-\n-        Database database = metastore.getDatabase(schemaName)\n-                .orElseThrow(() -> new SchemaNotFoundException(schemaName));\n-\n-        HdfsContext hdfsContext = new HdfsContext(session, schemaName, tableName);\n-        HiveIdentity identity = new HiveIdentity(session);\n-        String targetPath = getTableLocation(tableMetadata.getProperties());\n-        if (targetPath == null) {\n-            String uniqueTableName = useUniqueTableLocation ? String.format(\"%s_%s\", tableName, UUID.randomUUID()) : tableName;\n-            targetPath = getTableDefaultLocation(database, hdfsContext, hdfsEnvironment, schemaName, uniqueTableName).toString();\n-        }\n-\n-        TableOperations operations = new HiveTableOperations(metastore, hdfsEnvironment, hdfsContext, identity, schemaName, tableName, session.getUser(), targetPath);\n-        if (operations.current() != null) {\n-            throw new TableAlreadyExistsException(schemaTableName);\n-        }\n-\n-        ImmutableMap.Builder<String, String> propertiesBuilder = ImmutableMap.builderWithExpectedSize(2);\n-        FileFormat fileFormat = getFileFormat(tableMetadata.getProperties());\n-        propertiesBuilder.put(DEFAULT_FILE_FORMAT, fileFormat.toString());\n-        if (tableMetadata.getComment().isPresent()) {\n-            propertiesBuilder.put(TABLE_COMMENT, tableMetadata.getComment().get());\n-        }\n-\n-        TableMetadata metadata = newTableMetadata(schema, partitionSpec, targetPath, propertiesBuilder.build());\n-\n-        transaction = createTableTransaction(tableName, operations, metadata);\n-\n-        return new IcebergWritableTableHandle(\n-                schemaName,\n-                tableName,\n-                SchemaParser.toJson(metadata.schema()),\n-                PartitionSpecParser.toJson(metadata.spec()),\n-                getColumns(metadata.schema(), typeManager),\n-                targetPath,\n-                fileFormat);\n-    }\n-\n-    @Override\n-    public Optional<ConnectorOutputMetadata> finishCreateTable(ConnectorSession session, ConnectorOutputTableHandle tableHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n-    {\n-        return finishInsert(session, (IcebergWritableTableHandle) tableHandle, fragments, computedStatistics);\n-    }\n-\n-    @Override\n-    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n-\n-        transaction = icebergTable.newTransaction();\n-\n-        return new IcebergWritableTableHandle(\n-                table.getSchemaName(),\n-                table.getTableName(),\n-                SchemaParser.toJson(icebergTable.schema()),\n-                PartitionSpecParser.toJson(icebergTable.spec()),\n-                getColumns(icebergTable.schema(), typeManager),\n-                getDataPath(icebergTable.location()),\n-                getFileFormat(icebergTable));\n-    }\n-\n-    @Override\n-    public Optional<ConnectorOutputMetadata> finishInsert(ConnectorSession session, ConnectorInsertTableHandle insertHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n-    {\n-        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n-        org.apache.iceberg.Table icebergTable = transaction.table();\n-\n-        List<CommitTaskData> commitTasks = fragments.stream()\n-                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n-                .collect(toImmutableList());\n-\n-        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n-                .map(field -> field.transform().getResultType(\n-                        icebergTable.schema().findType(field.sourceId())))\n-                .toArray(Type[]::new);\n-\n-        AppendFiles appendFiles = transaction.newFastAppend();\n-        for (CommitTaskData task : commitTasks) {\n-            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n-\n-            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n-                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n-                    .withFormat(table.getFileFormat())\n-                    .withMetrics(task.getMetrics().metrics());\n-\n-            if (!icebergTable.spec().fields().isEmpty()) {\n-                String partitionDataJson = task.getPartitionDataJson()\n-                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n-                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n-            }\n-\n-            appendFiles.appendFile(builder.build());\n-        }\n-\n-        appendFiles.commit();\n-        transaction.commitTransaction();\n-\n-        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n-                .map(CommitTaskData::getPath)\n-                .collect(toImmutableList())));\n-    }\n-\n-    @Override\n-    public ColumnHandle getUpdateRowIdColumnHandle(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        return new IcebergColumnHandle(0, \"$row_id\", BIGINT, Optional.empty());\n-    }\n-\n-    @Override\n-    public Optional<Object> getInfo(ConnectorTableHandle tableHandle)\n-    {\n-        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n-        return Optional.of(new IcebergInputInfo(table.getSnapshotId()));\n-    }\n-\n-    @Override\n-    public void dropTable(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-        metastore.dropTable(new HiveIdentity(session), handle.getSchemaName(), handle.getTableName(), true);\n-    }\n-\n-    @Override\n-    public void renameTable(ConnectorSession session, ConnectorTableHandle tableHandle, SchemaTableName newTable)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-        metastore.renameTable(new HiveIdentity(session), handle.getSchemaName(), handle.getTableName(), newTable.getSchemaName(), newTable.getTableName());\n-    }\n-\n-    @Override\n-    public void addColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnMetadata column)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n-        icebergTable.updateSchema().addColumn(column.getName(), toIcebergType(column.getType())).commit();\n-    }\n-\n-    @Override\n-    public void dropColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle column)\n-    {\n-        IcebergTableHandle icebergTableHandle = (IcebergTableHandle) tableHandle;\n-        IcebergColumnHandle handle = (IcebergColumnHandle) column;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, icebergTableHandle.getSchemaTableName());\n-        icebergTable.updateSchema().deleteColumn(handle.getName()).commit();\n-    }\n-\n-    @Override\n-    public void renameColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle source, String target)\n-    {\n-        IcebergTableHandle icebergTableHandle = (IcebergTableHandle) tableHandle;\n-        IcebergColumnHandle columnHandle = (IcebergColumnHandle) source;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, icebergTableHandle.getSchemaTableName());\n-        icebergTable.updateSchema().renameColumn(columnHandle.getName(), target).commit();\n-    }\n-\n-    private ConnectorTableMetadata getTableMetadata(ConnectorSession session, SchemaTableName table)\n-    {\n-        if (metastore.getTable(new HiveIdentity(session), table.getSchemaName(), table.getTableName()).isEmpty()) {\n-            throw new TableNotFoundException(table);\n-        }\n-\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table);\n-\n-        List<ColumnMetadata> columns = getColumnMetadatas(icebergTable);\n-\n-        ImmutableMap.Builder<String, Object> properties = ImmutableMap.builder();\n-        properties.put(FILE_FORMAT_PROPERTY, getFileFormat(icebergTable));\n-        if (!icebergTable.spec().fields().isEmpty()) {\n-            properties.put(PARTITIONING_PROPERTY, toPartitionFields(icebergTable.spec()));\n-        }\n-\n-        return new ConnectorTableMetadata(table, columns, properties.build(), getTableComment(icebergTable));\n-    }\n-\n-    private List<ColumnMetadata> getColumnMetadatas(org.apache.iceberg.Table table)\n-    {\n-        return table.schema().columns().stream()\n-                .map(column -> {\n-                    return ColumnMetadata.builder()\n-                            .setName(column.name())\n-                            .setType(toPrestoType(column.type(), typeManager))\n-                            .setNullable(column.isOptional())\n-                            .setComment(Optional.ofNullable(column.doc()))\n-                            .build();\n-                })\n-                .collect(toImmutableList());\n-    }\n-\n-    private static Schema toIcebergSchema(List<ColumnMetadata> columns)\n-    {\n-        List<NestedField> icebergColumns = new ArrayList<>();\n-        for (ColumnMetadata column : columns) {\n-            if (!column.isHidden()) {\n-                int index = icebergColumns.size();\n-                Type type = toIcebergType(column.getType());\n-                NestedField field = column.isNullable()\n-                        ? NestedField.optional(index, column.getName(), type, column.getComment())\n-                        : NestedField.required(index, column.getName(), type, column.getComment());\n-                icebergColumns.add(field);\n-            }\n-        }\n-        Type icebergSchema = Types.StructType.of(icebergColumns);\n-        AtomicInteger nextFieldId = new AtomicInteger(1);\n-        icebergSchema = TypeUtil.assignFreshIds(icebergSchema, nextFieldId::getAndIncrement);\n-        return new Schema(icebergSchema.asStructType().fields());\n-    }\n-\n-    @Override\n-    public Optional<ConnectorTableHandle> applyDelete(ConnectorSession session, ConnectorTableHandle handle)\n-    {\n-        return Optional.of(handle);\n-    }\n-\n-    @Override\n-    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n-    }\n-\n-    @Override\n-    public OptionalLong executeDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n-\n-        icebergTable.newDelete()\n-                .deleteFromRowFilter(toIcebergExpression(handle.getPredicate()))\n-                .commit();\n-\n-        // TODO: it should be possible to return number of deleted records\n-        return OptionalLong.empty();\n-    }\n-\n-    @Override\n-    public boolean usesLegacyTableLayouts()\n-    {\n-        return false;\n-    }\n-\n-    public HiveMetastore getMetastore()\n-    {\n-        return metastore;\n-    }\n-\n-    public void rollback()\n-    {\n-        // TODO: cleanup open transaction\n-    }\n-\n-    @Override\n-    public Optional<ConstraintApplicationResult<ConnectorTableHandle>> applyFilter(ConnectorSession session, ConnectorTableHandle handle, Constraint constraint)\n-    {\n-        IcebergTableHandle table = (IcebergTableHandle) handle;\n-\n-        // TODO: Remove TupleDomain#simplify once Iceberg supports IN expression\n-        TupleDomain<IcebergColumnHandle> newDomain = constraint.getSummary()\n-                .transform(IcebergColumnHandle.class::cast)\n-                .intersect(table.getPredicate());\n-\n-        if (newDomain.isNone()) {\n-            return Optional.empty();\n-        }\n-\n-        if (newDomain.equals(table.getPredicate())) {\n-            return Optional.empty();\n-        }\n-\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n-\n-        List<PartitionField> fields = icebergTable.spec().fields().stream()\n-                .filter(field -> field.transform().isIdentity())\n-                .collect(toImmutableList());\n-\n-        // Ensure partition specs in all manifests contain the identity fields from the predicate\n-        if (!icebergTable.specs().values().stream().allMatch(spec -> spec.fields().containsAll(fields))) {\n-            return Optional.empty();\n-        }\n-\n-        Set<Integer> partitionSourceIds = icebergTable.spec().fields().stream()\n-                .filter(field -> field.transform().isIdentity())\n-                .map(PartitionField::sourceId)\n-                .collect(toImmutableSet());\n-\n-        BiPredicate<IcebergColumnHandle, Domain> contains = (column, domain) -> partitionSourceIds.contains(column.getId());\n-        TupleDomain<ColumnHandle> remainingTupleDomain = newDomain.filter(contains.negate()).transform(ColumnHandle.class::cast);\n-        TupleDomain<IcebergColumnHandle> enforcedTupleDomain = newDomain.filter(contains);\n-\n-        return Optional.of(new ConstraintApplicationResult<>(\n-                new IcebergTableHandle(table.getSchemaName(),\n-                        table.getTableName(),\n-                        table.getTableType(),\n-                        table.getSnapshotId(),\n-                        enforcedTupleDomain),\n-                remainingTupleDomain));\n-    }\n-\n-    @Override\n-    public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTableHandle tableHandle, Constraint constraint)\n-    {\n-        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n-        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n-        return TableStatisticsMaker.getTableStatistics(typeManager, constraint, handle, icebergTable);\n-    }\n-\n-    private Optional<Long> getSnapshotId(org.apache.iceberg.Table table, Optional<Long> snapshotId)\n-    {\n-        return snapshotIds.computeIfAbsent(table.toString(), ignored -> snapshotId\n-                .map(id -> IcebergUtil.resolveSnapshotId(table, id))\n-                .or(() -> Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId)));\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgwMzE1Mw==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529803153", "bodyText": "Wrap the argument, since all other arguments are wrapped", "author": "electrum", "createdAt": "2020-11-24T18:49:19Z", "path": "presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadataFactory.java", "diffHunk": "@@ -37,23 +38,24 @@ public IcebergMetadataFactory(\n             TypeManager typeManager,\n             JsonCodec<CommitTaskData> commitTaskDataJsonCodec)\n     {\n-        this(metastore, hdfsEnvironment, typeManager, commitTaskDataJsonCodec);\n+        this(metastore, hdfsEnvironment, typeManager, commitTaskDataJsonCodec, config.isUniqueTableLocation());\n     }\n \n     public IcebergMetadataFactory(\n             HiveMetastore metastore,\n             HdfsEnvironment hdfsEnvironment,\n             TypeManager typeManager,\n-            JsonCodec<CommitTaskData> commitTaskCodec)\n+            JsonCodec<CommitTaskData> commitTaskCodec, boolean useUniqueTableLocation)", "originalCommit": "e039d5d9586b9204d0fac111279822be52d24fbd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgzOTY5NA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r529839694", "bodyText": "Not sure that I understand comment correct, but moved  boolean useUniqueTableLocation to the next line", "author": "sshkvar", "createdAt": "2020-11-24T19:53:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgwMzE1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadataFactory.java b/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadataFactory.java\ndeleted file mode 100644\nindex 73bd4cad89..0000000000\n--- a/presto-iceberg/src/main/java/io/prestosql/plugin/iceberg/IcebergMetadataFactory.java\n+++ /dev/null\n\n@@ -1,61 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package io.prestosql.plugin.iceberg;\n-\n-import io.airlift.json.JsonCodec;\n-import io.prestosql.plugin.hive.HdfsEnvironment;\n-import io.prestosql.plugin.hive.metastore.HiveMetastore;\n-import io.prestosql.spi.type.TypeManager;\n-\n-import javax.inject.Inject;\n-\n-import static java.util.Objects.requireNonNull;\n-\n-public class IcebergMetadataFactory\n-{\n-    private final HiveMetastore metastore;\n-    private final HdfsEnvironment hdfsEnvironment;\n-    private final TypeManager typeManager;\n-    private final JsonCodec<CommitTaskData> commitTaskCodec;\n-    private final boolean useUniqueTableLocation;\n-\n-    @Inject\n-    public IcebergMetadataFactory(\n-            IcebergConfig config,\n-            HiveMetastore metastore,\n-            HdfsEnvironment hdfsEnvironment,\n-            TypeManager typeManager,\n-            JsonCodec<CommitTaskData> commitTaskDataJsonCodec)\n-    {\n-        this(metastore, hdfsEnvironment, typeManager, commitTaskDataJsonCodec, config.isUniqueTableLocation());\n-    }\n-\n-    public IcebergMetadataFactory(\n-            HiveMetastore metastore,\n-            HdfsEnvironment hdfsEnvironment,\n-            TypeManager typeManager,\n-            JsonCodec<CommitTaskData> commitTaskCodec, boolean useUniqueTableLocation)\n-    {\n-        this.metastore = requireNonNull(metastore, \"metastore is null\");\n-        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n-        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n-        this.commitTaskCodec = requireNonNull(commitTaskCodec, \"commitTaskCodec is null\");\n-        this.useUniqueTableLocation = useUniqueTableLocation;\n-    }\n-\n-    public IcebergMetadata create()\n-    {\n-        return new IcebergMetadata(metastore, hdfsEnvironment, typeManager, commitTaskCodec, useUniqueTableLocation);\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzNDAwOTE5NA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r634009194", "bodyText": "tableName + \"-\" + UUID.randomUUID().toString().replace(\"-\", \"\")", "author": "raunaqmorarka", "createdAt": "2021-05-18T03:14:54Z", "path": "plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -1074,6 +1078,11 @@ else if (strings.size() != 2) {\n         return viewToken;\n     }\n \n+    private String generateUniqueTableName(String tableName)\n+    {\n+        return String.format(\"%s-%s\", tableName, UUID.randomUUID().toString().replace(\"-\", \"\"));", "originalCommit": "647298023da5fc86990dcb0d8c2dadc92a83c85a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\nindex 42e2ff2857..6d1ede4313 100644\n--- a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\n+++ b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\n\n@@ -1078,11 +1325,6 @@ public class IcebergMetadata\n         return viewToken;\n     }\n \n-    private String generateUniqueTableName(String tableName)\n-    {\n-        return String.format(\"%s-%s\", tableName, UUID.randomUUID().toString().replace(\"-\", \"\"));\n-    }\n-\n     private static class TableToken\n     {\n         // Current Snapshot ID of the table\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzNDAxNzc4NA==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r634017784", "bodyText": "Append  randomUUID to table location", "author": "raunaqmorarka", "createdAt": "2021-05-18T03:46:26Z", "path": "plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergConfig.java", "diffHunk": "@@ -74,4 +76,17 @@ public IcebergConfig setUseFileSizeFromMetadata(boolean useFileSizeFromMetadata)\n         this.useFileSizeFromMetadata = useFileSizeFromMetadata;\n         return this;\n     }\n+\n+    public boolean isUniqueTableLocation()\n+    {\n+        return uniqueTableLocation;\n+    }\n+\n+    @Config(\"iceberg.unique-table-location\")\n+    @ConfigDescription(\"Include UUID in the table location\")", "originalCommit": "647298023da5fc86990dcb0d8c2dadc92a83c85a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NzMzMzgxNQ==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r677333815", "bodyText": "\"Use randomized, unique table locations\"", "author": "findepi", "createdAt": "2021-07-27T10:47:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzNDAxNzc4NA=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergConfig.java b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergConfig.java\nindex 25b19b1123..d2fb2b02d8 100644\n--- a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergConfig.java\n+++ b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergConfig.java\n\n@@ -77,13 +79,27 @@ public class IcebergConfig\n         return this;\n     }\n \n+    @Min(1)\n+    public int getMaxPartitionsPerWriter()\n+    {\n+        return maxPartitionsPerWriter;\n+    }\n+\n+    @Config(\"iceberg.max-partitions-per-writer\")\n+    @ConfigDescription(\"Maximum number of partitions per writer\")\n+    public IcebergConfig setMaxPartitionsPerWriter(int maxPartitionsPerWriter)\n+    {\n+        this.maxPartitionsPerWriter = maxPartitionsPerWriter;\n+        return this;\n+    }\n+\n     public boolean isUniqueTableLocation()\n     {\n         return uniqueTableLocation;\n     }\n \n     @Config(\"iceberg.unique-table-location\")\n-    @ConfigDescription(\"Include UUID in the table location\")\n+    @ConfigDescription(\"Use randomized, unique table locations\")\n     public IcebergConfig setUniqueTableLocation(boolean uniqueTableLocation)\n     {\n         this.uniqueTableLocation = uniqueTableLocation;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NzMzNzUzMg==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r677337532", "bodyText": "The uniqueTableName variable name suggests we're making the table name unique, while we're only changing table location.\nLet's call the variable tableNameForLocation\nSince same naming problem applies to generateUniqueTableName, let's inline that method.\nString tableNameForLocation = tableName;\nif (useUniqueTableLocation) {\n    tableNameForLocation += \"-\" + randomUUID().toString().replace(\"-\", \"\")\n}", "author": "findepi", "createdAt": "2021-07-27T10:53:17Z", "path": "plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java", "diffHunk": "@@ -495,7 +498,8 @@ public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, Con\n         HiveIdentity identity = new HiveIdentity(session);\n         String targetPath = getTableLocation(tableMetadata.getProperties());\n         if (targetPath == null) {\n-            targetPath = getTableDefaultLocation(database, hdfsContext, hdfsEnvironment, schemaName, tableName).toString();\n+            String uniqueTableName = useUniqueTableLocation ? generateUniqueTableName(tableName) : tableName;", "originalCommit": "647298023da5fc86990dcb0d8c2dadc92a83c85a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NzYwMzQ4Nw==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r677603487", "bodyText": "@findepi sure, I will rebase this brach, add code fixes and tests in next few days", "author": "sshkvar", "createdAt": "2021-07-27T16:15:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NzMzNzUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3ODg3NzE3OQ==", "url": "https://github.com/trinodb/trino/pull/6063#discussion_r678877179", "bodyText": "@findepi I have rebased this brach.\nAdded few tests to check that uuid suffix added to a table location, and also added tests for check that table correctly dropped .\nWill be really appreciate for the review", "author": "sshkvar", "createdAt": "2021-07-29T06:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NzMzNzUzMg=="}], "type": "inlineReview", "revised_code": {"commit": "0a30f7939798f0263cf2594cfa9f39f238afb456", "chunk": "diff --git a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\nindex 42e2ff2857..6d1ede4313 100644\n--- a/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\n+++ b/plugin/trino-iceberg/src/main/java/io/trino/plugin/iceberg/IcebergMetadata.java\n\n@@ -494,15 +574,26 @@ public class IcebergMetadata\n         Database database = metastore.getDatabase(schemaName)\n                 .orElseThrow(() -> new SchemaNotFoundException(schemaName));\n \n-        HdfsContext hdfsContext = new HdfsContext(session, schemaName, tableName);\n+        HdfsContext hdfsContext = new HdfsContext(session);\n         HiveIdentity identity = new HiveIdentity(session);\n         String targetPath = getTableLocation(tableMetadata.getProperties());\n         if (targetPath == null) {\n-            String uniqueTableName = useUniqueTableLocation ? generateUniqueTableName(tableName) : tableName;\n-            targetPath = getTableDefaultLocation(database, hdfsContext, hdfsEnvironment, schemaName, uniqueTableName).toString();\n+            String tableNameForLocation = tableName;\n+            if (useUniqueTableLocation) {\n+                tableNameForLocation += \"-\" + randomUUID().toString().replace(\"-\", \"\");\n+            }\n+            targetPath = getTableDefaultLocation(database, hdfsContext, hdfsEnvironment, schemaName, tableNameForLocation).toString();\n         }\n \n-        TableOperations operations = new HiveTableOperations(metastore, hdfsEnvironment, hdfsContext, identity, schemaName, tableName, session.getUser(), targetPath);\n+        TableOperations operations = tableOperationsProvider.createTableOperations(\n+                hdfsContext,\n+                session.getQueryId(),\n+                identity,\n+                schemaName,\n+                tableName,\n+                Optional.of(session.getUser()),\n+                Optional.of(targetPath));\n+\n         if (operations.current() != null) {\n             throw new TableAlreadyExistsException(schemaTableName);\n         }\n"}}, {"oid": "0a30f7939798f0263cf2594cfa9f39f238afb456", "url": "https://github.com/trinodb/trino/commit/0a30f7939798f0263cf2594cfa9f39f238afb456", "message": "rebase: Added ability to have unique table location for each iceberg table", "committedDate": "2021-08-03T12:08:57Z", "type": "commit"}, {"oid": "0a30f7939798f0263cf2594cfa9f39f238afb456", "url": "https://github.com/trinodb/trino/commit/0a30f7939798f0263cf2594cfa9f39f238afb456", "message": "rebase: Added ability to have unique table location for each iceberg table", "committedDate": "2021-08-03T12:08:57Z", "type": "forcePushed"}]}