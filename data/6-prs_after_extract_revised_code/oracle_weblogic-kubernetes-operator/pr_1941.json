{"pr_number": 1941, "pr_title": "Owls 84594", "pr_createdAt": "2020-09-23T16:37:18Z", "pr_url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941", "timeline": [{"oid": "81341307a55f6a09ba285ab620a65150e6f25a9f", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/81341307a55f6a09ba285ab620a65150e6f25a9f", "message": "Test for reproducing bug owls-84594", "committedDate": "2020-09-22T20:50:48Z", "type": "commit"}, {"oid": "bd86c734807bf32bd2fc3eb358cb08a017bb7873", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/bd86c734807bf32bd2fc3eb358cb08a017bb7873", "message": "clean up cluster comparison", "committedDate": "2020-09-23T15:36:41Z", "type": "commit"}, {"oid": "9be918d8e7ac8a6f34d77ff8613dd7372a5542e4", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/9be918d8e7ac8a6f34d77ff8613dd7372a5542e4", "message": "Fix checkstyle issues", "committedDate": "2020-09-23T15:55:39Z", "type": "commit"}, {"oid": "f9d2786d625ef0ebba49cda6c200a9a3c32cb788", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/f9d2786d625ef0ebba49cda6c200a9a3c32cb788", "message": "Verify logs from ms1", "committedDate": "2020-09-23T16:45:23Z", "type": "commit"}, {"oid": "7955cdcf119e0d2a9bf5477bea791e1b24ac385e", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/7955cdcf119e0d2a9bf5477bea791e1b24ac385e", "message": "Merge branch 'owls-84594' of https://github.com/oracle/weblogic-kubernetes-operator into owls-84594", "committedDate": "2020-09-23T16:46:37Z", "type": "commit"}, {"oid": "765d366d804d48fc7efae98d5135a4ffa8d5bcec", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/765d366d804d48fc7efae98d5135a4ffa8d5bcec", "message": "fix the expected ignore sessions attribute for ms1", "committedDate": "2020-09-23T17:20:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc2NTg4Mg==", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941#discussion_r493765882", "bodyText": "Since the issue we are fixing here involves addin 2 non-clustered managed servers, should we add a test case where 2 non-clustered managed servers are added and make sure that both will be started?", "author": "alai8", "createdAt": "2020-09-23T17:28:14Z", "path": "operator/src/test/java/oracle/kubernetes/operator/steps/ManagedServerUpIteratorStepTest.java", "diffHunk": "@@ -293,6 +293,27 @@ public void maxClusterConcurrentStartup_doesNotApplyToNonClusteredServers() {\n     assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n   }\n \n+  @Test\n+  public void whenClusteredServersAlreadyScheduled_canStartNonclusteredServer() {", "originalCommit": "7955cdcf119e0d2a9bf5477bea791e1b24ac385e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc4NDU2Mg==", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941#discussion_r493784562", "bodyText": "I believe that maxClusterConcurrentStartup_doesNotApplyToNonClusteredServers covers that case.", "author": "russgold", "createdAt": "2020-09-23T17:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc2NTg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5NTUzOA==", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941#discussion_r493795538", "bodyText": "that test doesn't seem to verify MS4 will be started.", "author": "alai8", "createdAt": "2020-09-23T18:17:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc2NTg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NTI2Nw==", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941#discussion_r493895267", "bodyText": "You're correct. I am going to do some cleanup of the tests in that class. There is more going on than is obvious.", "author": "russgold", "createdAt": "2020-09-23T21:06:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc2NTg4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5NDU3Mg==", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1941#discussion_r493994572", "bodyText": "The revised test is nonClusteredServers_ignoreConcurrencyLimit", "author": "russgold", "createdAt": "2020-09-24T01:58:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc2NTg4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "0c02624975d472a9e9f775581720b7c7ae1b9e2a", "chunk": "diff --git a/operator/src/test/java/oracle/kubernetes/operator/steps/ManagedServerUpIteratorStepTest.java b/operator/src/test/java/oracle/kubernetes/operator/steps/ManagedServerUpIteratorStepTest.java\nindex 3d835ae7de..ab2698e145 100644\n--- a/operator/src/test/java/oracle/kubernetes/operator/steps/ManagedServerUpIteratorStepTest.java\n+++ b/operator/src/test/java/oracle/kubernetes/operator/steps/ManagedServerUpIteratorStepTest.java\n\n@@ -159,188 +166,166 @@ public class ManagedServerUpIteratorStepTest {\n     testSupport\n             .addToPacket(ProcessingConstants.DOMAIN_TOPOLOGY, domainConfig)\n             .addDomainPresenceInfo(domainPresenceInfo);\n+    testSupport.doOnCreate(KubernetesTestSupport.POD, p -> schedulePodUpdates((V1Pod) p));\n+  }\n+\n+  // Invoked when a pod is created to simulate the Kubernetes behavior in which a pod is scheduled on a node\n+  // very quickly, and then takes much longer actually to become ready.\n+  void schedulePodUpdates(V1Pod pod) {\n+    testSupport.schedule(() -> setPodScheduled(getServerName(pod)), SCHEDULING_DELAY_MSEC, TimeUnit.MILLISECONDS);\n+    testSupport.schedule(() -> setPodReady(getServerName(pod)), POD_READY_DELAY_SEC, TimeUnit.SECONDS);\n+  }\n+\n+  // Marks the pod with the specified server name as having been scheduled on a Kubernetes node.\n+  private void setPodScheduled(String serverName) {\n+    Objects.requireNonNull(domainPresenceInfo.getServerPod(serverName).getSpec()).setNodeName(\"aNode\");\n+  }\n+\n+  // Marks the pod with the specified server name as having become ready.\n+  private void setPodReady(String serverName) {\n+    domainPresenceInfo.getServerPod(serverName).status(createPodReadyStatus());\n+  }\n+\n+  private V1PodStatus createPodReadyStatus() {\n+    return new V1PodStatus()\n+          .phase(\"Running\")\n+          .addConditionsItem(new V1PodCondition().status(\"True\").type(\"Ready\"));\n   }\n \n-  /**\n-   * Cleanup env after tests.\n-   * @throws Exception if test support failed\n-   */\n   @After\n   public void tearDown() throws Exception {\n-    for (Memento memento : mementos) {\n-      memento.revert();\n-    }\n+    mementos.forEach(Memento::revert);\n \n     testSupport.throwOnCompletionFailure();\n   }\n \n-  private void makePodReady(String serverName) {\n-    domainPresenceInfo.getServerPod(serverName).status(new V1PodStatus().phase(\"Running\"));\n-    Objects.requireNonNull(domainPresenceInfo.getServerPod(serverName).getStatus())\n-            .addConditionsItem(new V1PodCondition().status(\"True\").type(\"Ready\"));\n-  }\n+  @Test\n+  public void withMultipleServersAvailableToStart_onlyOneForEachClusterInitiallyStarts() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(0);\n+    configureCluster(CLUSTER2).withMaxConcurrentStartup(1);\n+    addWlsCluster(CLUSTER1, MS1, MS2);\n+    addWlsCluster(CLUSTER2, MS3, MS4);\n+\n+    invokeStepWithServerStartupInfos();\n \n-  private void schedulePod(String serverName, String nodeName) {\n-    Objects.requireNonNull(domainPresenceInfo.getServerPod(serverName).getSpec()).setNodeName(nodeName);\n+    assertThat(getStartedManagedServers().size(), equalTo(NUM_CLUSTERS));\n   }\n \n-  @Test\n-  public void withConcurrencyOf1_bothClusteredServersScheduleAndStartSequentially() {\n-    configureCluster(CLUSTER).withMaxConcurrentStartup(1);\n-    addWlsCluster(CLUSTER, 8001, MS1, MS2);\n+  @Nonnull\n+  private List<String> getStartedManagedServers() {\n+    return domainPresenceInfo.getServerPods()\n+          .map(this::getServerName)\n+          .filter(name -> !ADMIN.equals(name))\n+          .collect(Collectors.toList());\n+  }\n \n-    invokeStepWithServerStartupInfos(createServerStartupInfosForCluster(CLUSTER, MS1, MS2));\n+  private String getServerName(V1Pod pod) {\n+    return Optional.of(pod).map(V1Pod::getMetadata).map(V1ObjectMeta::getLabels).map(this::getServerName).orElse(null);\n+  }\n \n-    assertThat(MS1 + \" pod\", domainPresenceInfo.getServerPod(MS1), notNullValue());\n-    schedulePod(MS1, \"Node1\");\n-    testSupport.setTime(100, TimeUnit.MILLISECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), nullValue());\n-    makePodReady(MS1);\n-    testSupport.setTime(10, TimeUnit.SECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), notNullValue());\n+  private String getServerName(@Nonnull Map<String,String> labels) {\n+    return labels.get(SERVERNAME_LABEL);\n   }\n \n   @Test\n-  public void withConcurrencyOf0_clusteredServersScheduleSequentiallyAndStartConcurrently() {\n-    configureCluster(CLUSTER).withMaxConcurrentStartup(0);\n-    addWlsCluster(CLUSTER, PORT, MS1, MS2);\n+  public void whenConcurrencyLimitDisabled_additionalClusteredServersStartsAfterPreviousIsScheduled() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(0);\n+    addWlsCluster(CLUSTER1, MS1, MS2, MS3);\n \n-    invokeStepWithServerStartupInfos(createServerStartupInfosForCluster(CLUSTER,MS1, MS2));\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(2 * SCHEDULING_DETECTION_DELAY, TimeUnit.MILLISECONDS);\n \n-    assertThat(MS1 + \" pod\", domainPresenceInfo.getServerPod(MS1), notNullValue());\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), nullValue());\n-    schedulePod(MS1, \"Node1\");\n-    testSupport.setTime(100, TimeUnit.MILLISECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), notNullValue());\n+    assertThat(getStartedManagedServers(), containsInAnyOrder(MS1, MS2, MS3));\n   }\n \n   @Test\n-  public void withConcurrencyOf2_clusteredServersScheduleSequentiallyAndStartConcurrently() {\n-    configureCluster(CLUSTER).withMaxConcurrentStartup(2);\n-    addWlsCluster(CLUSTER, PORT, MS1, MS2);\n+  public void whenConcurrencyLimitIs1_secondClusteredServerDoesNotStartIfFirstIsNotReady() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(1);\n+    addWlsCluster(CLUSTER1, MS1, MS2);\n \n-    invokeStepWithServerStartupInfos(createServerStartupInfosForCluster(CLUSTER, MS1, MS2));\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(SCHEDULING_DETECTION_DELAY, TimeUnit.MILLISECONDS);\n \n-    assertThat(MS1 + \" pod\", domainPresenceInfo.getServerPod(MS1), notNullValue());\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), nullValue());\n-    schedulePod(MS1, \"Node1\");\n-    testSupport.setTime(100, TimeUnit.MILLISECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), notNullValue());\n+    assertThat(getStartedManagedServers(), hasSize(1));\n   }\n \n   @Test\n-  public void withConcurrencyOf2_4clusteredServersScheduleSequentiallyAndStartIn2Threads() {\n-    configureCluster(CLUSTER).withMaxConcurrentStartup(2);\n-    addWlsCluster(CLUSTER, PORT, MS1, MS2, MS3, MS4);\n-\n-    invokeStepWithServerStartupInfos(createServerStartupInfosForCluster(CLUSTER, MS1, MS2, MS3, MS4));\n-    assertThat(MS1 + \" pod\", domainPresenceInfo.getServerPod(MS1), notNullValue());\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), nullValue());\n-    schedulePod(MS1, \"Node1\");\n-    testSupport.setTime(100, TimeUnit.MILLISECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), notNullValue());\n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), nullValue());\n-    schedulePod(MS2, \"Node2\");\n-    testSupport.setTime(200, TimeUnit.MILLISECONDS);\n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), nullValue());\n-    makePodReady(MS1);\n-    testSupport.setTime(10, TimeUnit.SECONDS);\n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n-    assertThat(MS4 + \" pod\", domainPresenceInfo.getServerPod(MS4), nullValue());\n-    makePodReady(MS2);\n-    schedulePod(MS3, \"Node3\");\n-    testSupport.setTime(20, TimeUnit.SECONDS);\n-    assertThat(MS4 + \" pod\", domainPresenceInfo.getServerPod(MS4), notNullValue());\n+  public void whenConcurrencyLimitIs1_secondClusteredServerStartsAfterFirstIsReady() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(1);\n+    addWlsCluster(CLUSTER1, MS1, MS2);\n+\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(READY_DETECTION_DELAY, TimeUnit.SECONDS);\n+\n+    assertThat(getStartedManagedServers(), hasSize(2));\n   }\n \n   @Test\n-  public void withMultipleClusters_differentClusterScheduleAndStartDifferently() {\n-    final String CLUSTER2 = \"cluster2\";\n+  public void whenConcurrencyLimitIs2_secondClusteredServerStartsAfterFirstIsScheduledButNotThird() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(2);\n+    addWlsCluster(CLUSTER1, MS1, MS2, MS3, MS4);\n \n-    configureCluster(CLUSTER).withMaxConcurrentStartup(0);\n-    configureCluster(CLUSTER2).withMaxConcurrentStartup(1);\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(2 * SCHEDULING_DETECTION_DELAY, TimeUnit.MILLISECONDS);\n \n-    addWlsCluster(CLUSTER, PORT, MS1, MS2);\n-    addWlsCluster(CLUSTER2, PORT, MS3, MS4);\n+    assertThat(getStartedManagedServers(), containsInAnyOrder(MS1, MS2));\n+  }\n \n-    Collection<ServerStartupInfo> serverStartupInfos = createServerStartupInfosForCluster(CLUSTER, MS1, MS2);\n-    serverStartupInfos.addAll(createServerStartupInfosForCluster(CLUSTER2, MS3, MS4));\n-    invokeStepWithServerStartupInfos(serverStartupInfos);\n+  @Test\n+  public void whenConcurrencyLimitIs2_nextTwoStartAfterFirstTwoAreReady() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(2);\n+    addWlsCluster(CLUSTER1, MS1, MS2, MS3, MS4);\n \n-    assertThat(MS1 + \" pod\", domainPresenceInfo.getServerPod(MS1), notNullValue());\n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n-    schedulePod(MS1, \"Node1\");\n-    schedulePod(MS3, \"Node2\");\n-    testSupport.setTime(100, TimeUnit.MILLISECONDS);\n-    assertThat(MS2 + \" pod\", domainPresenceInfo.getServerPod(MS2), notNullValue());\n-    assertThat(MS4 + \" pod\", domainPresenceInfo.getServerPod(MS4), nullValue());\n-    //makePodReady(MS3);\n-    //k8sTestSupport.setTime(10, TimeUnit.SECONDS);\n-    //assertThat(MS4 + \" pod\", domainPresenceInfo.getServerPod(MS4), notNullValue());\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(READY_DETECTION_DELAY, TimeUnit.SECONDS);\n+\n+    assertThat(getStartedManagedServers(), containsInAnyOrder(MS1, MS2, MS3, MS4));\n   }\n \n   @Test\n-  public void maxClusterConcurrentStartup_doesNotApplyToNonClusteredServers() {\n+  public void nonClusteredServers_ignoreConcurrencyLimit() {\n     domain.getSpec().setMaxClusterConcurrentStartup(1);\n+    addWlsServers(MS1, MS2, MS3);\n \n-    addWlsServers(MS3, MS4);\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(2 * SCHEDULING_DETECTION_DELAY, TimeUnit.MILLISECONDS);\n \n-    invokeStepWithServerStartupInfos(createServerStartupInfos(MS3, MS4));\n+    assertThat(getStartedManagedServers(), containsInAnyOrder(MS1, MS2, MS3));\n+  }\n \n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n-    schedulePod(MS3, \"Node2\");\n-    testSupport.setTime(200, TimeUnit.MILLISECONDS);\n-    assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n+  @Test\n+  public void withMultipleClusters_differentClusterScheduleAndStartDifferently() {\n+    configureCluster(CLUSTER1).withMaxConcurrentStartup(0);\n+    configureCluster(CLUSTER2).withMaxConcurrentStartup(1);\n+    addWlsCluster(CLUSTER1, MS1, MS2);\n+    addWlsCluster(CLUSTER2, MS3, MS4);\n+\n+    invokeStepWithServerStartupInfos();\n+    testSupport.setTime(SCHEDULING_DETECTION_DELAY, TimeUnit.MILLISECONDS);\n+\n+    assertThat(getStartedManagedServers(), containsInAnyOrder(MS1, MS2, MS3));\n   }\n \n   @Test\n   public void whenClusteredServersAlreadyScheduled_canStartNonclusteredServer() {\n     domain.getSpec().setMaxClusterConcurrentStartup(1);\n-    Arrays.asList(MS1, MS2).forEach(serverName -> addScheduledClusteredServer(domainPresenceInfo, serverName));\n-\n-    addWlsCluster(CLUSTER, PORT, MS1, MS2);\n+    Arrays.asList(MS1, MS2).forEach(this::addScheduledClusteredServer);\n     addWlsServer(MS3);\n \n-    invokeStepWithServerStartupInfos(createServerStartupInfos(MS3));\n+    invokeStepWithServerStartupInfos();\n \n     assertThat(MS3 + \" pod\", domainPresenceInfo.getServerPod(MS3), notNullValue());\n   }\n \n-  private void addScheduledClusteredServer(DomainPresenceInfo info, String serverName) {\n+  private void addScheduledClusteredServer(String serverName) {\n     domainPresenceInfo.setServerPod(serverName,\n           new V1Pod().metadata(\n-                withNames(new V1ObjectMeta().namespace(NS).putLabelsItem(CLUSTERNAME_LABEL, CLUSTER), serverName))\n+                withNames(new V1ObjectMeta().namespace(NS).putLabelsItem(CLUSTERNAME_LABEL, CLUSTER1), serverName))\n                       .spec(new V1PodSpec().nodeName(\"scheduled\")));\n   }\n \n \n-  @NotNull\n-  private Collection<ServerStartupInfo> createServerStartupInfosForCluster(String clusterName, String... servers) {\n-    Collection<ServerStartupInfo> serverStartupInfos = new ArrayList<>();\n-    Arrays.stream(servers).forEach(server ->\n-            serverStartupInfos.add(\n-                new ServerStartupInfo(configSupport.getWlsServer(clusterName, server),\n-                    clusterName,\n-                    domain.getServer(server, clusterName))\n-            )\n-    );\n-    return serverStartupInfos;\n-  }\n-\n-  @NotNull\n-  private Collection<ServerStartupInfo> createServerStartupInfos(String... servers) {\n-    Collection<ServerStartupInfo> serverStartupInfos = new ArrayList<>();\n-    Arrays.stream(servers).forEach(server ->\n-        serverStartupInfos.add(\n-            new ServerStartupInfo(configSupport.getWlsServer(server),\n-                null,\n-                domain.getServer(server, null))\n-        )\n-    );\n-    return serverStartupInfos;\n-  }\n-\n-  private void invokeStepWithServerStartupInfos(Collection<ServerStartupInfo> startupInfos) {\n+  private void invokeStepWithServerStartupInfos() {\n     ManagedServerUpIteratorStep step = new ManagedServerUpIteratorStep(startupInfos, nextStep);\n     testSupport.runSteps(step);\n   }\n"}}, {"oid": "0c02624975d472a9e9f775581720b7c7ae1b9e2a", "url": "https://github.com/oracle/weblogic-kubernetes-operator/commit/0c02624975d472a9e9f775581720b7c7ae1b9e2a", "message": "Simplify unit tests", "committedDate": "2020-09-24T01:55:45Z", "type": "commit"}]}