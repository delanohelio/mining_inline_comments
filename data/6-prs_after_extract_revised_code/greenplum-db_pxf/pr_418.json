{"pr_number": 418, "pr_title": "Parquet performance improvements for write", "pr_createdAt": "2020-08-11T00:45:30Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/418", "timeline": [{"oid": "a478752c0aeceff5402ea692876db124c72b349e", "url": "https://github.com/greenplum-db/pxf/commit/a478752c0aeceff5402ea692876db124c72b349e", "message": "Parquet performance improvements for write\n\nImprove performance during Parquet write. We no longer split files that\ngo over 128MB into multiple files, we also use new parquet libraries.\nFinally, we refactor the parquet write code to use non-deprecated API\ncalls.", "committedDate": "2020-08-11T13:49:51Z", "type": "forcePushed"}, {"oid": "49f0f75067657ddd0240285ec6f257fea2da3254", "url": "https://github.com/greenplum-db/pxf/commit/49f0f75067657ddd0240285ec6f257fea2da3254", "message": "Parquet performance improvements for write\n\nImprove performance during Parquet write. We no longer split files that\ngo over 128MB into multiple files, we also use new parquet libraries.\nFinally, we refactor the parquet write code to use non-deprecated API\ncalls.\n\nA new parameter `ENABLE_DICTIONARY` is also added. Parquet dictionaries\nare enabled by default (no behavior change from previous versions) but\nthe dictionary can be disabled by providing `ENABLE_DICTIONARY=false`", "committedDate": "2020-08-11T13:52:01Z", "type": "commit"}, {"oid": "49f0f75067657ddd0240285ec6f257fea2da3254", "url": "https://github.com/greenplum-db/pxf/commit/49f0f75067657ddd0240285ec6f257fea2da3254", "message": "Parquet performance improvements for write\n\nImprove performance during Parquet write. We no longer split files that\ngo over 128MB into multiple files, we also use new parquet libraries.\nFinally, we refactor the parquet write code to use non-deprecated API\ncalls.\n\nA new parameter `ENABLE_DICTIONARY` is also added. Parquet dictionaries\nare enabled by default (no behavior change from previous versions) but\nthe dictionary can be disabled by providing `ENABLE_DICTIONARY=false`", "committedDate": "2020-08-11T13:52:01Z", "type": "forcePushed"}, {"oid": "57acc645f569c6748b2fadda5a471a62769a902a", "url": "https://github.com/greenplum-db/pxf/commit/57acc645f569c6748b2fadda5a471a62769a902a", "message": "Parquet Write: Backfill write tests for parquet\n\nTest writing all supported formats for Parquet. We test the options that\nare exposed for users.\n\nCo-authored-by: Ashuka Xue <axue@vmware.com>", "committedDate": "2020-08-11T22:32:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE4NzI1MA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474187250", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            context.getOption(\"ENABLE_DICTIONARY\", String.valueOf(ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED)));\n          \n          \n            \n                            context.getOption(\"ENABLE_DICTIONARY\", ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED);\n          \n      \n    \n    \n  \n\nWe should introduce a function in RequestContext:\npublic boolean getOption(String option, boolean defaultValue)", "author": "denalex", "createdAt": "2020-08-20T18:27:44Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -228,13 +243,15 @@ public boolean openForWrite() throws IOException {\n         codecName = codecFactory.getCodec(compressCodec, DEFAULT_COMPRESSION);\n \n         // Options for parquet write\n-        pageSize = context.getOption(\"PAGE_SIZE\", DEFAULT_PAGE_SIZE);\n+        pageSize = context.getOption(\"PAGE_SIZE\", ParquetProperties.DEFAULT_PAGE_SIZE);\n         rowGroupSize = context.getOption(\"ROWGROUP_SIZE\", DEFAULT_ROWGROUP_SIZE);\n-        dictionarySize = context.getOption(\"DICTIONARY_PAGE_SIZE\", DEFAULT_DICTIONARY_PAGE_SIZE);\n+        enableDictionary = StringUtils.equalsIgnoreCase(\"true\",\n+                context.getOption(\"ENABLE_DICTIONARY\", String.valueOf(ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED)));", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\nindex e06a73d1..97173dbd 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n\n@@ -243,13 +247,12 @@ public class ParquetFileAccessor extends BasePlugin implements Accessor {\n         codecName = codecFactory.getCodec(compressCodec, DEFAULT_COMPRESSION);\n \n         // Options for parquet write\n-        pageSize = context.getOption(\"PAGE_SIZE\", ParquetProperties.DEFAULT_PAGE_SIZE);\n+        pageSize = context.getOption(\"PAGE_SIZE\", DEFAULT_PAGE_SIZE);\n         rowGroupSize = context.getOption(\"ROWGROUP_SIZE\", DEFAULT_ROWGROUP_SIZE);\n-        enableDictionary = StringUtils.equalsIgnoreCase(\"true\",\n-                context.getOption(\"ENABLE_DICTIONARY\", String.valueOf(ParquetProperties.DEFAULT_IS_DICTIONARY_ENABLED)));\n-        dictionarySize = context.getOption(\"DICTIONARY_PAGE_SIZE\", ParquetProperties.DEFAULT_DICTIONARY_PAGE_SIZE);\n+        enableDictionary = context.getOption(\"ENABLE_DICTIONARY\", DEFAULT_IS_DICTIONARY_ENABLED);\n+        dictionarySize = context.getOption(\"DICTIONARY_PAGE_SIZE\", DEFAULT_DICTIONARY_PAGE_SIZE);\n         String parquetVerStr = context.getOption(\"PARQUET_VERSION\");\n-        parquetVersion = parquetVerStr != null ? WriterVersion.fromString(parquetVerStr.toLowerCase()) : ParquetProperties.DEFAULT_WRITER_VERSION;\n+        parquetVersion = parquetVerStr != null ? WriterVersion.fromString(parquetVerStr.toLowerCase()) : DEFAULT_WRITER_VERSION;\n         LOG.debug(\"{}-{}: Parquet options: PAGE_SIZE = {}, ROWGROUP_SIZE = {}, DICTIONARY_PAGE_SIZE = {}, PARQUET_VERSION = {}, ENABLE_DICTIONARY = {}\",\n                 context.getTransactionId(), context.getSegmentId(), pageSize, rowGroupSize, dictionarySize, parquetVersion, enableDictionary);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5MzE0NQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474193145", "bodyText": "we should have a backlog story to have an option to write this as INT64 for non-Hive consumers (like Spark). Avro-Parquet libraries do not even support times in INT96", "author": "denalex", "createdAt": "2020-08-20T18:38:23Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -490,33 +493,29 @@ private MessageType generateParquetSchema(List<ColumnDescriptor> columns) {\n                         precision = columnTypeModifiers[0];\n                         scale = columnTypeModifiers[1];\n                     }\n-                    length = PRECISION_TO_BYTE_COUNT[precision - 1];\n-                    dmt = new DecimalMetadata(precision, scale);\n+                    builder = Types\n+                            .optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)\n+                            .length(PRECISION_TO_BYTE_COUNT[precision - 1])\n+                            .as(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n                     break;\n                 case TIMESTAMP:\n                 case TIMESTAMP_WITH_TIME_ZONE:\n-                    typeName = PrimitiveTypeName.INT96;\n+                    builder = Types.optional(PrimitiveTypeName.INT96);", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDI1MzYyNw==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474253627", "bodyText": "we have a story called INT96 deprecated in Parquet 2.5.0", "author": "frankgh", "createdAt": "2020-08-20T20:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5MzE0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\nindex e06a73d1..97173dbd 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n\n@@ -496,7 +499,7 @@ public class ParquetFileAccessor extends BasePlugin implements Accessor {\n                     builder = Types\n                             .optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)\n                             .length(PRECISION_TO_BYTE_COUNT[precision - 1])\n-                            .as(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n+                            .as(DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n                     break;\n                 case TIMESTAMP:\n                 case TIMESTAMP_WITH_TIME_ZONE:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5NDIxNg==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474194216", "bodyText": "also dates / times often are written as ints, not strings. also, timestamp-with-timezone will be converted to UTC by which logic ?", "author": "denalex", "createdAt": "2020-08-20T18:40:17Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java", "diffHunk": "@@ -490,33 +493,29 @@ private MessageType generateParquetSchema(List<ColumnDescriptor> columns) {\n                         precision = columnTypeModifiers[0];\n                         scale = columnTypeModifiers[1];\n                     }\n-                    length = PRECISION_TO_BYTE_COUNT[precision - 1];\n-                    dmt = new DecimalMetadata(precision, scale);\n+                    builder = Types\n+                            .optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)\n+                            .length(PRECISION_TO_BYTE_COUNT[precision - 1])\n+                            .as(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n                     break;\n                 case TIMESTAMP:\n                 case TIMESTAMP_WITH_TIME_ZONE:\n-                    typeName = PrimitiveTypeName.INT96;\n+                    builder = Types.optional(PrimitiveTypeName.INT96);\n                     break;\n                 case DATE:\n                 case TIME:", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDI0MDYyMA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474240620", "bodyText": "We'll need to double check dates / times. I will investigate and address in a follow up PR. Timestamp with timezone is converted to UTC here ParquetTypeConverter.getBinaryFromTimestampWithTimeZone", "author": "frankgh", "createdAt": "2020-08-20T20:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5NDIxNg=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\nindex e06a73d1..97173dbd 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetFileAccessor.java\n\n@@ -496,7 +499,7 @@ public class ParquetFileAccessor extends BasePlugin implements Accessor {\n                     builder = Types\n                             .optional(PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY)\n                             .length(PRECISION_TO_BYTE_COUNT[precision - 1])\n-                            .as(LogicalTypeAnnotation.DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n+                            .as(DecimalLogicalTypeAnnotation.decimalType(scale, precision));\n                     break;\n                 case TIMESTAMP:\n                 case TIMESTAMP_WITH_TIME_ZONE:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5NTQ4Mw==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474195483", "bodyText": "can we statically import LogicalTypeAnnotation.* ? this is too verbose", "author": "denalex", "createdAt": "2020-08-20T18:42:22Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetResolver.java", "diffHunk": "@@ -136,8 +145,10 @@ private void fillGroup(int index, OneField field, Group group, Type type) throws\n             case FIXED_LEN_BYTE_ARRAY:\n                 // From org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.DecimalDataWriter#decimalToBinary\n                 String value = (String) field.val;\n-                int precision = Math.min(HiveDecimal.MAX_PRECISION, type.asPrimitiveType().getDecimalMetadata().getPrecision());\n-                int scale = Math.min(HiveDecimal.MAX_SCALE, type.asPrimitiveType().getDecimalMetadata().getScale());\n+                LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalTypeAnnotation =", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5NTg3NQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474195875", "bodyText": "also decimalLogicalTypeAnnotation -> typeAnnotation", "author": "denalex", "createdAt": "2020-08-20T18:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE5NTQ4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetResolver.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetResolver.java\nindex 5bc1261f..079021e2 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetResolver.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/ParquetResolver.java\n\n@@ -145,10 +147,9 @@ public class ParquetResolver extends BasePlugin implements Resolver {\n             case FIXED_LEN_BYTE_ARRAY:\n                 // From org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.DecimalDataWriter#decimalToBinary\n                 String value = (String) field.val;\n-                LogicalTypeAnnotation.DecimalLogicalTypeAnnotation decimalLogicalTypeAnnotation =\n-                        (LogicalTypeAnnotation.DecimalLogicalTypeAnnotation) type.getLogicalTypeAnnotation();\n-                int precision = Math.min(HiveDecimal.MAX_PRECISION, decimalLogicalTypeAnnotation.getPrecision());\n-                int scale = Math.min(HiveDecimal.MAX_SCALE, decimalLogicalTypeAnnotation.getScale());\n+                DecimalLogicalTypeAnnotation typeAnnotation = (DecimalLogicalTypeAnnotation) type.getLogicalTypeAnnotation();\n+                int precision = Math.min(HiveDecimal.MAX_PRECISION, typeAnnotation.getPrecision());\n+                int scale = Math.min(HiveDecimal.MAX_SCALE, typeAnnotation.getScale());\n                 HiveDecimal hiveDecimal = HiveDecimal.enforcePrecisionScale(\n                         HiveDecimal.create(value),\n                         precision,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwMjAxNA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474202014", "bodyText": "should we be comparing types using instanceof or using LogicalTypeToken getType() methods and comparing == of type enums ?", "author": "denalex", "createdAt": "2020-08-20T18:54:36Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java", "diffHunk": "@@ -32,17 +32,15 @@\n     BINARY {\n         @Override\n         public DataType getDataType(Type type) {\n-            OriginalType originalType = type.getOriginalType();\n+            LogicalTypeAnnotation originalType = type.getLogicalTypeAnnotation();\n             if (originalType == null) {\n                 return DataType.BYTEA;\n-            }\n-            switch (originalType) {\n-                case DATE:\n-                    return DataType.DATE;\n-                case TIMESTAMP_MILLIS:\n-                    return DataType.TIMESTAMP;\n-                default:\n-                    return DataType.TEXT;\n+            } else if (originalType instanceof LogicalTypeAnnotation.DateLogicalTypeAnnotation) {", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIzMDY3NQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474230675", "bodyText": "LogicalTypeToken.getType() is package-private unfortunately, so we have to use instanceof. For example:\n    @Override\n    LogicalTypeToken getType() {\n      return LogicalTypeToken.DATE;\n    }", "author": "frankgh", "createdAt": "2020-08-20T19:41:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwMjAxNA=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java\nindex ac8dc546..09e55ded 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/parquet/ParquetTypeConverter.java\n\n@@ -35,9 +40,9 @@ public enum ParquetTypeConverter {\n             LogicalTypeAnnotation originalType = type.getLogicalTypeAnnotation();\n             if (originalType == null) {\n                 return DataType.BYTEA;\n-            } else if (originalType instanceof LogicalTypeAnnotation.DateLogicalTypeAnnotation) {\n+            } else if (originalType instanceof DateLogicalTypeAnnotation) {\n                 return DataType.DATE;\n-            } else if (originalType instanceof LogicalTypeAnnotation.TimestampLogicalTypeAnnotation) {\n+            } else if (originalType instanceof TimestampLogicalTypeAnnotation) {\n                 return DataType.TIMESTAMP;\n             } else {\n                 return DataType.TEXT;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNDMxMQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474204311", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public void testSettingPAGE_SIZEOption() throws Exception {\n          \n          \n            \n                public void testSetting_PAGE_SIZE_Option() throws Exception {\n          \n      \n    \n    \n  \n\nand in other methods for better readability", "author": "denalex", "createdAt": "2020-08-20T18:58:52Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzE1MA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474207150", "bodyText": "I am not sure we are correctly storing DATE as STRING:\nhttps://github.com/apache/parquet-format/blob/master/LogicalTypes.md#date", "author": "denalex", "createdAt": "2020-08-20T19:04:37Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIzMzA5Mg==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474233092", "bodyText": "I can investigate and fix in a follow up PR", "author": "frankgh", "createdAt": "2020-08-20T19:46:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzE1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM1MzA5OA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474353098", "bodyText": "I just verified that PXF is storing date as String whereas Hive stores date as int32 with logical type DATE.", "author": "frankgh", "createdAt": "2020-08-21T01:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzE1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM1MzI1MQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474353251", "bodyText": "Hive\n gpadmin@mdw \ue0b0 ~/parquet \ue0b0 java -jar parquet-tools-1.12.0-SNAPSHOT.jar schema /tmp/000000_0\nmessage hive_schema {\n  optional int32 dt (DATE);\n}\n\nPXF\n gpadmin@mdw \ue0b0 ~/parquet \ue0b0 java -jar parquet-tools-1.12.0-SNAPSHOT.jar schema /tmp/1597879570-0000000014_2.0.snappy.parquet\nmessage hive_schema {\n  optional binary dt (STRING);\n}", "author": "frankgh", "createdAt": "2020-08-21T01:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzE1MA=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzU2NA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474207564", "bodyText": "copy & paste issue ?", "author": "denalex", "createdAt": "2020-08-20T19:05:18Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-02\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-03\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-04\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-05\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-06\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-07\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-08\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-09\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-10\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteFloat8() throws Exception {\n+        String path = temp.getRoot() + \"/out/float/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 0, \"float8\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123459\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.FLOAT8.getOID(), 1.1 * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzcyMA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474207720", "bodyText": "copy & paste ?", "author": "denalex", "createdAt": "2020-08-20T19:05:37Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-02\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-03\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-04\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-05\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-06\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-07\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-08\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-09\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-10\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteFloat8() throws Exception {\n+        String path = temp.getRoot() + \"/out/float/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 0, \"float8\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123459\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIzMzU0Ng==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474233546", "bodyText": "good catch", "author": "frankgh", "createdAt": "2020-08-20T19:47:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIwNzcyMA=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIxMTA5NA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474211094", "bodyText": "why there's an annotation if the physical type is already int ? I assume int32 ?", "author": "denalex", "createdAt": "2020-08-20T19:12:32Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-02\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-03\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-04\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-05\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-06\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-07\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-08\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-09\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-10\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteFloat8() throws Exception {\n+        String path = temp.getRoot() + \"/out/float/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 0, \"float8\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123459\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.FLOAT8.getOID(), 1.1 * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(1.1, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(2.2, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(3.3, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(4.4, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(5.5, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(6.6, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(7.7, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(8.8, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(9.9, fileReader.read().getDouble(0, 0), 0.01);\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBoolean() throws Exception {\n+        String path = temp.getRoot() + \"/out/boolean/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123460\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with boolean values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BOOLEAN.getOID(), i % 2 == 0));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is boolean\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteTimestamp() throws Exception {\n+        String path = temp.getRoot() + \"/out/timestamp/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 0, \"timestamp\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123462\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with timestamp values\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TIMESTAMP.getOID(), localTimestampString));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            assertEquals(localTimestampString, bytesToTimestamp(fileReader.read().getInt96(0, 0).getBytes()));\n+        }\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBigInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/bigint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 0, \"bigint\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123463\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            long value = (long) Integer.MAX_VALUE + i;\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BIGINT.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(2147483647L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483648L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483649L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483650L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483651L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483652L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483653L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483654L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483655L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483656L, fileReader.read().getLong(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBytea() throws Exception {\n+        String path = temp.getRoot() + \"/out/bytea/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 0, \"bytea\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123464\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bytea values\n+        for (int i = 0; i < 10; i++) {\n+            byte[] value = Binary.fromString(StringUtils.repeat(\"a\", i + 1)).getBytes();\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BYTEA.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(Binary.fromString(\"a\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteSmallInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/smallint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 0, \"int2\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123465\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.SMALLINT.getOID(), (short) i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.IntLogicalTypeAnnotation);", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIzNDIzOQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474234239", "bodyText": "Physical type is int32, but you can represent int32, int16 and int8 with the physical int32 type", "author": "frankgh", "createdAt": "2020-08-20T19:49:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIxMTA5NA=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIxMTQ5OQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474211499", "bodyText": "can we actually assert the actual type coming back for each of these methods ?", "author": "denalex", "createdAt": "2020-08-20T19:13:23Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-02\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-03\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-04\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-05\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-06\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-07\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-08\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-09\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-10\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteFloat8() throws Exception {\n+        String path = temp.getRoot() + \"/out/float/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 0, \"float8\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123459\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.FLOAT8.getOID(), 1.1 * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(1.1, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(2.2, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(3.3, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(4.4, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(5.5, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(6.6, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(7.7, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(8.8, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(9.9, fileReader.read().getDouble(0, 0), 0.01);\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBoolean() throws Exception {\n+        String path = temp.getRoot() + \"/out/boolean/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123460\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with boolean values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BOOLEAN.getOID(), i % 2 == 0));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is boolean\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteTimestamp() throws Exception {\n+        String path = temp.getRoot() + \"/out/timestamp/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 0, \"timestamp\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123462\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with timestamp values\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TIMESTAMP.getOID(), localTimestampString));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            assertEquals(localTimestampString, bytesToTimestamp(fileReader.read().getInt96(0, 0).getBytes()));\n+        }\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBigInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/bigint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 0, \"bigint\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123463\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            long value = (long) Integer.MAX_VALUE + i;\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BIGINT.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(2147483647L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483648L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483649L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483650L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483651L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483652L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483653L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483654L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483655L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483656L, fileReader.read().getLong(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBytea() throws Exception {\n+        String path = temp.getRoot() + \"/out/bytea/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 0, \"bytea\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123464\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bytea values\n+        for (int i = 0; i < 10; i++) {\n+            byte[] value = Binary.fromString(StringUtils.repeat(\"a\", i + 1)).getBytes();\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BYTEA.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(Binary.fromString(\"a\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteSmallInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/smallint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 0, \"int2\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123465\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.SMALLINT.getOID(), (short) i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.IntLogicalTypeAnnotation);\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteReal() throws Exception {\n+        String path = temp.getRoot() + \"/out/real/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 0, \"real\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123466\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with real values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.REAL.getOID(), 1.1F * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is FLOAT\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(1.1F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(2.2F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(3.3F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(4.4F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(5.5F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(6.6F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(7.7F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(8.8F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(9.9F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteVarchar() throws Exception {\n+        String path = temp.getRoot() + \"/out/varchar/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 0, \"varchar\", new Integer[]{5}));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123467\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with varchar values\n+        for (int i = 0; i < 10; i++) {\n+            String s = StringUtils.repeat(\"b\", i % 5);\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.VARCHAR.getOID(), s));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIzNDY3MQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474234671", "bodyText": "I'm not sure what you mean", "author": "frankgh", "createdAt": "2020-08-20T19:50:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIxMTQ5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDIxMzE3OA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r474213178", "bodyText": "would be nice to somehow push the precision boundary and make sure we do not lose precision. I had issues around this with Avro/Parquet in NiFi, where they didn't use BigDecomal and were losing precision when writing / reading decimal types. Would be good to prove we do not lose precision.", "author": "denalex", "createdAt": "2020-08-20T19:16:19Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,942 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingPAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingDICTIONARY_PAGE_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSettingENABLE_DICTIONARYOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSettingPARQUET_VERSIONOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSettingROWGROUP_SIZEOption() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteText() throws Exception {\n+        String path = temp.getRoot() + \"/out/text/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"name\", DataType.TEXT.getOID(), 0, \"text\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123457\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with TEXT values of a repeated i + 1 times\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TEXT.getOID(), StringUtils.repeat(\"a\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is String\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"a\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertEquals(\"aaaaaaaaaa\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteDate() throws Exception {\n+        String path = temp.getRoot() + \"/out/date/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"cdate\", DataType.DATE.getOID(), 0, \"date\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123458\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.DATE.getOID(), String.format(\"2020-08-%02d\", i + 1)));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"2020-08-01\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-02\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-03\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-04\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-05\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-06\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-07\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-08\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-09\", fileReader.read().getString(0, 0));\n+        assertEquals(\"2020-08-10\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteFloat8() throws Exception {\n+        String path = temp.getRoot() + \"/out/float/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"amt\", DataType.FLOAT8.getOID(), 0, \"float8\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123459\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with DATE from 2020-08-01 to 2020-08-10\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.FLOAT8.getOID(), 1.1 * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is binary, logical type is Date\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(1.1, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(2.2, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(3.3, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(4.4, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(5.5, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(6.6, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(7.7, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(8.8, fileReader.read().getDouble(0, 0), 0.01);\n+        assertEquals(9.9, fileReader.read().getDouble(0, 0), 0.01);\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBoolean() throws Exception {\n+        String path = temp.getRoot() + \"/out/boolean/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"b\", DataType.BOOLEAN.getOID(), 5, \"bool\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123460\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with boolean values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BOOLEAN.getOID(), i % 2 == 0));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is boolean\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertTrue(fileReader.read().getBoolean(0, 0));\n+        assertFalse(fileReader.read().getBoolean(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteTimestamp() throws Exception {\n+        String path = temp.getRoot() + \"/out/timestamp/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"tm\", DataType.TIMESTAMP.getOID(), 0, \"timestamp\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123462\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with timestamp values\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.TIMESTAMP.getOID(), localTimestampString));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+\n+        for (int i = 0; i < 10; i++) {\n+\n+            Instant timestamp = Instant.parse(String.format(\"2020-08-%02dT04:00:05Z\", i + 1)); // UTC\n+            ZonedDateTime localTime = timestamp.atZone(ZoneId.systemDefault());\n+            String localTimestampString = localTime.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); // should be \"2020-08-%02dT04:00:05Z\" in PST\n+\n+            assertEquals(localTimestampString, bytesToTimestamp(fileReader.read().getInt96(0, 0).getBytes()));\n+        }\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBigInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/bigint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bg\", DataType.BIGINT.getOID(), 0, \"bigint\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123463\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            long value = (long) Integer.MAX_VALUE + i;\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BIGINT.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT96\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(2147483647L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483648L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483649L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483650L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483651L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483652L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483653L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483654L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483655L, fileReader.read().getLong(0, 0));\n+        assertEquals(2147483656L, fileReader.read().getLong(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteBytea() throws Exception {\n+        String path = temp.getRoot() + \"/out/bytea/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"bin\", DataType.BYTEA.getOID(), 0, \"bytea\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123464\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bytea values\n+        for (int i = 0; i < 10; i++) {\n+            byte[] value = Binary.fromString(StringUtils.repeat(\"a\", i + 1)).getBytes();\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BYTEA.getOID(), value));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(Binary.fromString(\"a\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertEquals(Binary.fromString(\"aaaaaaaaaa\"), fileReader.read().getBinary(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteSmallInt() throws Exception {\n+        String path = temp.getRoot() + \"/out/smallint/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"sml\", DataType.SMALLINT.getOID(), 0, \"int2\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123465\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.SMALLINT.getOID(), (short) i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is INT\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.IntLogicalTypeAnnotation);\n+        assertEquals(0, fileReader.read().getInteger(0, 0));\n+        assertEquals(1, fileReader.read().getInteger(0, 0));\n+        assertEquals(2, fileReader.read().getInteger(0, 0));\n+        assertEquals(3, fileReader.read().getInteger(0, 0));\n+        assertEquals(4, fileReader.read().getInteger(0, 0));\n+        assertEquals(5, fileReader.read().getInteger(0, 0));\n+        assertEquals(6, fileReader.read().getInteger(0, 0));\n+        assertEquals(7, fileReader.read().getInteger(0, 0));\n+        assertEquals(8, fileReader.read().getInteger(0, 0));\n+        assertEquals(9, fileReader.read().getInteger(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteReal() throws Exception {\n+        String path = temp.getRoot() + \"/out/real/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"r\", DataType.REAL.getOID(), 0, \"real\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123466\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with real values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.REAL.getOID(), 1.1F * i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is FLOAT\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());\n+        assertEquals(0F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(1.1F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(2.2F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(3.3F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(4.4F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(5.5F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(6.6F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(7.7F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(8.8F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertEquals(9.9F, fileReader.read().getFloat(0, 0), 0.001);\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteVarchar() throws Exception {\n+        String path = temp.getRoot() + \"/out/varchar/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"vc1\", DataType.VARCHAR.getOID(), 0, \"varchar\", new Integer[]{5}));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123467\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with varchar values\n+        for (int i = 0; i < 10; i++) {\n+            String s = StringUtils.repeat(\"b\", i % 5);\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.VARCHAR.getOID(), s));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertEquals(\"b\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bb\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bbb\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bbbb\", fileReader.read().getString(0, 0));\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertEquals(\"b\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bb\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bbb\", fileReader.read().getString(0, 0));\n+        assertEquals(\"bbbb\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteChar() throws Exception {\n+        String path = temp.getRoot() + \"/out/char/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"c1\", DataType.BPCHAR.getOID(), 0, \"char\", new Integer[]{3}));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123468\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with char values\n+        for (int i = 0; i < 10; i++) {\n+            String s = StringUtils.repeat(\"c\", i % 3);\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.BPCHAR.getOID(), s));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.StringLogicalTypeAnnotation);\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertEquals(\"c\", fileReader.read().getString(0, 0));\n+        assertEquals(\"cc\", fileReader.read().getString(0, 0));\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertEquals(\"c\", fileReader.read().getString(0, 0));\n+        assertEquals(\"cc\", fileReader.read().getString(0, 0));\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertEquals(\"c\", fileReader.read().getString(0, 0));\n+        assertEquals(\"cc\", fileReader.read().getString(0, 0));\n+        assertEquals(\"\", fileReader.read().getString(0, 0));\n+        assertNull(fileReader.read());\n+        fileReader.close();\n+    }\n+\n+    @Test\n+    public void testWriteNumeric() throws Exception {\n+        String path = temp.getRoot() + \"/out/numeric/\";\n+        columnDescriptors.add(new ColumnDescriptor(\"dec1\", DataType.NUMERIC.getOID(), 0, \"numeric\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123469\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with bigint values\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.NUMERIC.getOID(), String.format(\"%d.%d\", (i + 1), (i + 2))));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // Physical type is BINARY\n+        assertTrue(schema.getType(0).getLogicalTypeAnnotation() instanceof LogicalTypeAnnotation.DecimalLogicalTypeAnnotation);\n+        assertEquals(new BigDecimal(\"1.2\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"2.3\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"3.4\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"4.5\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"5.6\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"6.7\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"7.8\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"8.9\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"9.10\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));\n+        assertEquals(new BigDecimal(\"10.11\").setScale(18, ROUND_UNNECESSARY), new BigDecimal(new BigInteger(fileReader.read().getBinary(0, 0).getBytes()), 18));", "originalCommit": "57acc645f569c6748b2fadda5a471a62769a902a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 907c84f0..fdd07c7e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -16,7 +16,6 @@ import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n-import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n"}}, {"oid": "b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "url": "https://github.com/greenplum-db/pxf/commit/b9ff1b8bb5c8d1ab53e8edfa710434bc71d036d3", "message": "Address PR feedback", "committedDate": "2020-08-20T20:26:31Z", "type": "commit"}, {"oid": "78f2bb0a1e0f4b874b6cb5598f94f9d28d42d123", "url": "https://github.com/greenplum-db/pxf/commit/78f2bb0a1e0f4b874b6cb5598f94f9d28d42d123", "message": "Additional PR feedback", "committedDate": "2020-08-21T11:37:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg5MTYyOA==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r475891628", "bodyText": "what I meant here and everywhere else is to add assertion of the actual type:\nassertEquals(<EXPECTED_PHYSICAL_TYPE>, schema.getType(0));", "author": "denalex", "createdAt": "2020-08-24T21:01:09Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java", "diffHunk": "@@ -0,0 +1,973 @@\n+package org.greenplum.pxf.plugins.hdfs;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.greenplum.pxf.api.OneField;\n+import org.greenplum.pxf.api.OneRow;\n+import org.greenplum.pxf.api.io.DataType;\n+import org.greenplum.pxf.api.model.Accessor;\n+import org.greenplum.pxf.api.model.ConfigurationFactory;\n+import org.greenplum.pxf.api.model.RequestContext;\n+import org.greenplum.pxf.api.model.Resolver;\n+import org.greenplum.pxf.api.utilities.ColumnDescriptor;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.IOException;\n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static java.math.BigDecimal.ROUND_UNNECESSARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.BLOCK_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.DICTIONARY_PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.ENABLE_DICTIONARY;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.PAGE_SIZE;\n+import static org.apache.parquet.hadoop.ParquetOutputFormat.WRITER_VERSION;\n+import static org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n+import static org.apache.parquet.schema.LogicalTypeAnnotation.IntLogicalTypeAnnotation;\n+import static org.apache.parquet.schema.LogicalTypeAnnotation.StringLogicalTypeAnnotation;\n+import static org.greenplum.pxf.plugins.hdfs.parquet.ParquetTypeConverter.bytesToTimestamp;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ParquetWriteTest {\n+\n+    private Accessor accessor;\n+    private Resolver resolver;\n+    private RequestContext context;\n+    private Configuration configuration;\n+\n+    protected List<ColumnDescriptor> columnDescriptors;\n+\n+    @Rule\n+    public TemporaryFolder temp = new TemporaryFolder();\n+\n+    @Before\n+    public void setup() {\n+\n+        columnDescriptors = new ArrayList<>();\n+\n+        ConfigurationFactory mockConfigurationFactory = mock(ConfigurationFactory.class);\n+\n+        accessor = new ParquetFileAccessor(mockConfigurationFactory);\n+        resolver = new ParquetResolver(mockConfigurationFactory);\n+        context = new RequestContext();\n+\n+        context.setConfig(\"fakeConfig\");\n+        context.setServerName(\"fakeServerName\");\n+        context.setUser(\"fakeUser\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setSegmentId(4);\n+        context.setRequestType(RequestContext.RequestType.WRITE_BRIDGE);\n+        context.setTupleDescription(columnDescriptors);\n+\n+        configuration = new Configuration();\n+\n+        when(mockConfigurationFactory.\n+                initConfiguration(context.getConfig(), context.getServerName(), context.getUser(), context.getAdditionalConfigProps()))\n+                .thenReturn(configuration);\n+    }\n+\n+    @Test\n+    public void testDefaultWriteOptions() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+        assertEquals(1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+        assertTrue(configuration.getBoolean(ENABLE_DICTIONARY, false));\n+        assertEquals(\"PARQUET_1_0\", configuration.get(WRITER_VERSION));\n+        assertEquals(8 * 1024 * 1024, configuration.getLong(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSetting_PAGE_SIZE_Option() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSetting_DICTIONARY_PAGE_SIZE_Option() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"DICTIONARY_PAGE_SIZE\", \"5242880\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(5 * 1024 * 1024, configuration.getInt(DICTIONARY_PAGE_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testSetting_ENABLE_DICTIONARY_Option() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ENABLE_DICTIONARY\", \"false\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertFalse(configuration.getBoolean(ENABLE_DICTIONARY, true));\n+    }\n+\n+    @Test\n+    public void testSetting_PARQUET_VERSION_Option() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"PARQUET_VERSION\", \"v2\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(\"PARQUET_2_0\", configuration.get(WRITER_VERSION));\n+    }\n+\n+    @Test\n+    public void testSetting_ROWGROUP_SIZE_Option() throws Exception {\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+        context.setDataSource(temp.getRoot() + \"/out/\");\n+        context.setTransactionId(\"XID-XYZ-123453\");\n+        context.addOption(\"ROWGROUP_SIZE\", \"33554432\");\n+\n+        accessor.initialize(context);\n+        assertTrue(accessor.openForWrite());\n+        accessor.closeForWrite();\n+\n+        assertEquals(32 * 1024 * 1024, configuration.getInt(BLOCK_SIZE, -1));\n+    }\n+\n+    @Test\n+    public void testWriteInt() throws Exception {\n+\n+        String path = temp.getRoot() + \"/out/int/\";\n+\n+        columnDescriptors.add(new ColumnDescriptor(\"id\", DataType.INTEGER.getOID(), 0, \"int4\", null));\n+\n+        context.setDataSource(path);\n+        context.setTransactionId(\"XID-XYZ-123456\");\n+\n+        accessor.initialize(context);\n+        resolver.initialize(context);\n+\n+        assertTrue(accessor.openForWrite());\n+\n+        // write parquet file with INT values from 0 to 9\n+        for (int i = 0; i < 10; i++) {\n+            List<OneField> record = Collections.singletonList(new OneField(DataType.INTEGER.getOID(), i));\n+            OneRow rowToWrite = resolver.setFields(record);\n+            assertTrue(accessor.writeNextObject(rowToWrite));\n+        }\n+\n+        accessor.closeForWrite();\n+\n+        // Validate write\n+        Path expectedFile = new Path(HcfsType.LOCALFILE.getUriForWrite(configuration, context, true) + \".snappy.parquet\");\n+        assertTrue(expectedFile.getFileSystem(configuration).exists(expectedFile));\n+\n+        MessageType schema = validateFooter(expectedFile);\n+\n+        ParquetReader<Group> fileReader = ParquetReader.builder(new GroupReadSupport(), expectedFile)\n+                .withConf(configuration)\n+                .build();\n+\n+        // There is no logical annotation, the physical type is INT32\n+        assertNull(schema.getType(0).getLogicalTypeAnnotation());", "originalCommit": "78f2bb0a1e0f4b874b6cb5598f94f9d28d42d123", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA2NzU2OQ==", "url": "https://github.com/greenplum-db/pxf/pull/418#discussion_r476067569", "bodyText": "ready!", "author": "frankgh", "createdAt": "2020-08-25T02:11:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTg5MTYyOA=="}], "type": "inlineReview", "revised_code": {"commit": "a559032675d994fe1e5903c4913164fc787c7441", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\nindex 70489a3c..7ba8d13e 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/ParquetWriteTest.java\n\n@@ -17,6 +17,8 @@ import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.io.api.Binary;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n import org.greenplum.pxf.api.OneField;\n import org.greenplum.pxf.api.OneRow;\n import org.greenplum.pxf.api.io.DataType;\n"}}, {"oid": "a559032675d994fe1e5903c4913164fc787c7441", "url": "https://github.com/greenplum-db/pxf/commit/a559032675d994fe1e5903c4913164fc787c7441", "message": "Address PR feedback", "committedDate": "2020-08-25T02:11:29Z", "type": "commit"}]}