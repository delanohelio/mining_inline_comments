{"pr_number": 333, "pr_title": "Hive: Support column projection on Hive Profiles", "pr_createdAt": "2020-04-03T13:39:10Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/333", "timeline": [{"oid": "ddf042dd3afac11f128a4a7e26fe22e9b413f6f2", "url": "https://github.com/greenplum-db/pxf/commit/ddf042dd3afac11f128a4a7e26fe22e9b413f6f2", "message": "Hive: Support column projection on Hive Profiles\n\nCurrently, Hive, HiveText and HiveRC profiles do not support column\nprojection. Add support for column projection for these profiles at the\nPXF level. PXF can determine the following scenarios:\n\n1. The number of columns of the file is higher than the number of\n   columns defined at the Greenplum table definition.\n2. The number of columns of the file is less than the number of columns\n   defined at the Greenplum table definition.\n3. A mixed number of columns is present in the files we are accessing.\n\nFor this to work, the column names on the Greenplum table definition\nmust match the column names on the Hive table definition, as the\nprojection is based on column names rather than column positions.", "committedDate": "2020-04-03T14:21:58Z", "type": "forcePushed"}, {"oid": "13e1a874a5ae106c6714df4ee7e73c5e2156f32b", "url": "https://github.com/greenplum-db/pxf/commit/13e1a874a5ae106c6714df4ee7e73c5e2156f32b", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-04T17:34:28Z", "type": "forcePushed"}, {"oid": "a02c2d9b3deb8d6369a694e82071253bed86df76", "url": "https://github.com/greenplum-db/pxf/commit/a02c2d9b3deb8d6369a694e82071253bed86df76", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-06T14:08:35Z", "type": "forcePushed"}, {"oid": "29cdc4f7b2efe3109a0a22fe68f66b6324a36b94", "url": "https://github.com/greenplum-db/pxf/commit/29cdc4f7b2efe3109a0a22fe68f66b6324a36b94", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-06T14:15:18Z", "type": "forcePushed"}, {"oid": "d6bc87a6fd21dfca9fa38b65a216b1c7a02fd48b", "url": "https://github.com/greenplum-db/pxf/commit/d6bc87a6fd21dfca9fa38b65a216b1c7a02fd48b", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-06T17:01:15Z", "type": "forcePushed"}, {"oid": "c053af137976484fbcdab70a73b84bebc29c5292", "url": "https://github.com/greenplum-db/pxf/commit/c053af137976484fbcdab70a73b84bebc29c5292", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-06T20:37:04Z", "type": "forcePushed"}, {"oid": "7e857c34ee6143779e452cd0fbe5b9c1f29adc79", "url": "https://github.com/greenplum-db/pxf/commit/7e857c34ee6143779e452cd0fbe5b9c1f29adc79", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-06T20:48:34Z", "type": "forcePushed"}, {"oid": "a4094d0afe12ecb7fd5d66f6221b7b4940ac87bb", "url": "https://github.com/greenplum-db/pxf/commit/a4094d0afe12ecb7fd5d66f6221b7b4940ac87bb", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-07T11:40:58Z", "type": "forcePushed"}, {"oid": "76146b25e3110d77a84b0ed034e56b7226925a47", "url": "https://github.com/greenplum-db/pxf/commit/76146b25e3110d77a84b0ed034e56b7226925a47", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-07T13:07:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NTY3NQ==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r407665675", "bodyText": "can this be uncommented now ? If not, it's be better to mark it as @Ignore if it fails temporarily or just remove the test if the behavior changed.", "author": "denalex", "createdAt": "2020-04-13T19:20:32Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java", "diffHunk": "@@ -244,7 +331,7 @@ public void severalPartitionsDefaultSerde() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })\n+//    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })", "originalCommit": "4e363c4ef2bd2a488895586ca6cab8736069f162", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODUxMzE3OA==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408513178", "bodyText": "Removing this test as we now support querying a subset of Hive columns in Greenplum", "author": "frankgh", "createdAt": "2020-04-15T00:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NTY3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "chunk": "diff --git a/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java b/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java\nindex 5b0c2f0c..43936252 100755\n--- a/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java\n+++ b/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveRcTest.java\n\n@@ -331,7 +244,7 @@ public class HiveRcTest extends HiveBaseTest {\n      *\n      * @throws Exception if test fails to run\n      */\n-//    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })\n+    @Test(groups = { \"hive\", \"features\", \"gpdb\", \"security\" })\n     public void severalRcPartitionsNoPartitionColumn() throws Exception {\n \n         hiveTable = new HiveExternalTable(HIVE_REG_HETEROGEN_TABLE, HIVE_RC_COLS);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NzA5OQ==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r407667099", "bodyText": "better to remove the test if the behavior changed, @Ignore is usually used for temporarily failing tests that are supposed to work.", "author": "denalex", "createdAt": "2020-04-13T19:23:09Z", "path": "automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java", "diffHunk": "@@ -517,7 +577,8 @@ public void incorrectProfile() throws Exception {\n      *\n      * @throws Exception if test fails to run\n      */\n-    @Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    //@Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+    @Ignore(\"we now support column count mismatch for Hive\")", "originalCommit": "4e363c4ef2bd2a488895586ca6cab8736069f162", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODUxMzI0Mw==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408513243", "bodyText": "Removing this test as this is now supported", "author": "frankgh", "createdAt": "2020-04-15T00:32:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY2NzA5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "chunk": "diff --git a/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java b/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java\nindex 799086e2..f544ca2c 100755\n--- a/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java\n+++ b/automation/src/test/java/org/greenplum/pxf/automation/features/hive/HiveTest.java\n\n@@ -577,7 +537,7 @@ public class HiveTest extends HiveBaseTest {\n      *\n      * @throws Exception if test fails to run\n      */\n-    //@Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n+//    @Test(groups = {\"hive\", \"features\", \"gpdb\", \"security\"})\n     @Ignore(\"we now support column count mismatch for Hive\")\n     public void columnCountMisMatch() throws Exception {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5MTIwOQ==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408391209", "bodyText": "can we do lower case first and then always compare lower-cased strings ?", "author": "denalex", "createdAt": "2020-04-14T19:45:26Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);\n+\n+                if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n+                    // Skip partitioned columns\n+                    complexRecord.add(partitionField);\n+                } else if (i == null || structIndex >= structFields.size()) {\n+                    // This is a column not present in the file, but defined in greenplum.\n+                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n+                    addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n+                } else if (!columnDescriptor.isProjected()) {\n+                    // Non-projected fields will be sent as null values.\n+                    // This case is invoked only in the top level of fields and\n+                    // not when interpreting fields of type struct.\n+                    traverseTuple(null, fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                } else {\n+                    traverseTuple(structFields.get(structIndex), fields.get(i).getFieldObjectInspector(), complexRecord, false);\n+                }\n+            }\n         }\n+\n         return toFlatten ? structRecord : complexRecord;\n     }\n \n+    private OneField getPartitionField(String columnName) {\n+        OneField oneField = partitionColumnNames.get(columnName);\n+        if (oneField == null) {\n+            oneField = partitionColumnNames.get(columnName.toLowerCase());", "originalCommit": "4e363c4ef2bd2a488895586ca6cab8736069f162", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMTQ2Ng==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408801466", "bodyText": "This was mostly to support a very edge case. But I am reading that the Hive metastore stores the table schema in all lowercase. So it does not apply. I will fix it", "author": "frankgh", "createdAt": "2020-04-15T12:25:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5MTIwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "chunk": "diff --git a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\nindex c4ba3cd7..3c74a916 100644\n--- a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n+++ b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n\n@@ -487,19 +477,15 @@ public class HiveResolver extends HivePlugin implements Resolver {\n                             .boxed()\n                             .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n \n-            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n-            for (int j = 0; j < tupleDescription.size(); j++) {\n-                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+            for (ColumnDescriptor columnDescriptor : context.getTupleDescription()) {\n                 Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n                         columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n-                Integer structIndex = hiveIndexes.get(j);\n \n                 if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n                     // Skip partitioned columns\n                     complexRecord.add(partitionField);\n-                } else if (i == null || structIndex >= structFields.size()) {\n+                } else if (i == null) {\n                     // This is a column not present in the file, but defined in greenplum.\n-                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n                     addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n                 } else if (!columnDescriptor.isProjected()) {\n                     // Non-projected fields will be sent as null values.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5Mjk4Mw==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408392983", "bodyText": "are we sure we do not need column mapping inside struct here ?", "author": "denalex", "createdAt": "2020-04-14T19:48:53Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));", "originalCommit": "4e363c4ef2bd2a488895586ca6cab8736069f162", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMDIzOA==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408800238", "bodyText": "Yes, flattening does not happen for tuples \n  \n    \n      pxf/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n    \n    \n         Line 131\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           return traverseStruct(tuple, soi, false); \n        \n    \n  \n\n but rather on nested structs \n  \n    \n      pxf/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n    \n    \n         Line 413\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           List<OneField> structRecord = traverseStruct(obj, \n        \n    \n  \n\n . The flattening happens on columns of type collection for example here: \n  \n    \n      pxf/automation/tincrepo/main/pxf/features/hive/hive_collection_types/expected/query01.ans\n    \n    \n         Line 6\n      in\n      4e363c4\n    \n    \n    \n    \n\n        \n          \n           giraffe |  2 | [\"Nige\",\"Kenya\",\"Congo\"]     | {\"height\":4.0,\"wheight\":100.0} | {\"street\":\"agripas\",\"city\":\"aviv\",\"state\":\"israel\",\"zip\":56303}  | [[\"abc\"]] \n        \n    \n  \n\n Take a look at column t2, t3, t4, and t5, where t2 is a ARRAY, t3 a MAP<STRING, FLOAT>, t4 a STRUCT<street:STRING, city:STRING, state:STRING, zip:INT> (this column specifically will call the traverseStruct for it's value with a flatten = true value), t5 a UNIONTYPE<STRING, INT, ARRAY, ARRAY", "author": "frankgh", "createdAt": "2020-04-15T12:23:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5Mjk4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "chunk": "diff --git a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\nindex c4ba3cd7..3c74a916 100644\n--- a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n+++ b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n\n@@ -487,19 +477,15 @@ public class HiveResolver extends HivePlugin implements Resolver {\n                             .boxed()\n                             .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n \n-            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n-            for (int j = 0; j < tupleDescription.size(); j++) {\n-                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+            for (ColumnDescriptor columnDescriptor : context.getTupleDescription()) {\n                 Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n                         columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n-                Integer structIndex = hiveIndexes.get(j);\n \n                 if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n                     // Skip partitioned columns\n                     complexRecord.add(partitionField);\n-                } else if (i == null || structIndex >= structFields.size()) {\n+                } else if (i == null) {\n                     // This is a column not present in the file, but defined in greenplum.\n-                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n                     addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n                 } else if (!columnDescriptor.isProjected()) {\n                     // Non-projected fields will be sent as null values.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5ODA2Ng==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408398066", "bodyText": "why are we mapping GP column twice - once via column Name and once via hiveuserdata index mapping ?", "author": "denalex", "createdAt": "2020-04-14T19:57:59Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java", "diffHunk": "@@ -463,36 +467,62 @@ private void traverseTuple(Object obj, ObjectInspector objInspector,\n         List<? extends StructField> fields = soi.getAllStructFieldRefs();\n         List<Object> structFields = soi.getStructFieldsDataAsList(struct);\n         if (structFields == null) {\n-            throw new BadRecordException(\n-                    \"Illegal value NULL for Hive data type Struct\");\n+            throw new BadRecordException(\"Illegal value NULL for Hive data type Struct\");\n         }\n+\n         List<OneField> structRecord = new LinkedList<>();\n         List<OneField> complexRecord = new LinkedList<>();\n-        List<ColumnDescriptor> colData = context.getTupleDescription();\n-        for (int i = 0; i < structFields.size(); i++) {\n-            if (toFlatten) {\n-                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\n-                        \"\\\"%s\\\"\", fields.get(i).getFieldName())));\n-            } else if (!colData.get(i).isProjected()) {\n-                // Non-projected fields will be sent as null values.\n-                // This case is invoked only in the top level of fields and\n-                // not when interpreting fields of type struct.\n-                traverseTuple(null, fields.get(i).getFieldObjectInspector(),\n-                        complexRecord, toFlatten);\n-                continue;\n-            }\n-            traverseTuple(structFields.get(i),\n-                    fields.get(i).getFieldObjectInspector(), complexRecord,\n-                    toFlatten);\n-            if (toFlatten) {\n-                addOneFieldToRecord(structRecord, DataType.TEXT,\n-                        HdfsUtilities.toString(complexRecord, mapkeyDelim));\n+        OneField partitionField;\n+\n+        if (toFlatten) {\n+            for (int i = 0; i < structFields.size(); i++) {\n+                complexRecord.add(new OneField(DataType.TEXT.getOID(), String.format(\"\\\"%s\\\"\", fields.get(i).getFieldName())));\n+                traverseTuple(structFields.get(i), fields.get(i).getFieldObjectInspector(), complexRecord, true);\n+                addOneFieldToRecord(structRecord, DataType.TEXT, HdfsUtilities.toString(complexRecord, mapkeyDelim));\n                 complexRecord.clear();\n             }\n+        } else {\n+            Map<String, Integer> columnNameToStructIndexMap =\n+                    IntStream.range(0, fields.size())\n+                            .boxed()\n+                            .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n+\n+            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n+            for (int j = 0; j < tupleDescription.size(); j++) {\n+                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+                Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n+                        columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n+                Integer structIndex = hiveIndexes.get(j);", "originalCommit": "4e363c4ef2bd2a488895586ca6cab8736069f162", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgwMTg2OA==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r408801868", "bodyText": "good question, I will add a note here, in case we need both", "author": "frankgh", "createdAt": "2020-04-15T12:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5ODA2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "chunk": "diff --git a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\nindex c4ba3cd7..3c74a916 100644\n--- a/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n+++ b/server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveResolver.java\n\n@@ -487,19 +477,15 @@ public class HiveResolver extends HivePlugin implements Resolver {\n                             .boxed()\n                             .collect(Collectors.toMap(i -> fields.get(i).getFieldName(), i -> i));\n \n-            List<ColumnDescriptor> tupleDescription = context.getTupleDescription();\n-            for (int j = 0; j < tupleDescription.size(); j++) {\n-                ColumnDescriptor columnDescriptor = tupleDescription.get(j);\n+            for (ColumnDescriptor columnDescriptor : context.getTupleDescription()) {\n                 Integer i = defaultIfNull(columnNameToStructIndexMap.get(columnDescriptor.columnName()),\n                         columnNameToStructIndexMap.get(columnDescriptor.columnName().toLowerCase()));\n-                Integer structIndex = hiveIndexes.get(j);\n \n                 if ((partitionField = getPartitionField(columnDescriptor.columnName())) != null) {\n                     // Skip partitioned columns\n                     complexRecord.add(partitionField);\n-                } else if (i == null || structIndex >= structFields.size()) {\n+                } else if (i == null) {\n                     // This is a column not present in the file, but defined in greenplum.\n-                    LOG.warn(\"Column {} is not present in the source file, but it is defined in the table\", columnDescriptor.columnName());\n                     addOneFieldToRecord(complexRecord, columnDescriptor.getDataType(), null);\n                 } else if (!columnDescriptor.isProjected()) {\n                     // Non-projected fields will be sent as null values.\n"}}, {"oid": "7cd642a458a10710d0405a5c27bfca41dfa5dca5", "url": "https://github.com/greenplum-db/pxf/commit/7cd642a458a10710d0405a5c27bfca41dfa5dca5", "message": "Hive: Support column projection on Hive Profiles\n\nCurrently, Hive, HiveText and HiveRC profiles do not support column\nprojection. Add support for column projection for these profiles at the\nPXF level. PXF can determine the following scenarios:\n\n1. The number of columns of the file is higher than the number of\n   columns defined at the Greenplum table definition.\n2. The number of columns of the file is less than the number of columns\n   defined at the Greenplum table definition.\n3. A mixed number of columns is present in the files we are accessing.\n\nFor this to work, the column names on the Greenplum table definition\nmust match the column names on the Hive table definition, as the\nprojection is based on column names rather than column positions.", "committedDate": "2020-04-15T17:51:40Z", "type": "commit"}, {"oid": "b89d1e1dbba2b8fc511c135479df8e37659ba7a0", "url": "https://github.com/greenplum-db/pxf/commit/b89d1e1dbba2b8fc511c135479df8e37659ba7a0", "message": "Log warn when the column is not present in the file, but defined in greenplum", "committedDate": "2020-04-15T17:51:40Z", "type": "commit"}, {"oid": "c757252734e735fa4c171cfa0ffd41cbc4b0adab", "url": "https://github.com/greenplum-db/pxf/commit/c757252734e735fa4c171cfa0ffd41cbc4b0adab", "message": "Allow a subset of the Hive schema to be defined\n\nWe allow a subset of the Hive schema columns to be defined in the List\nof ColumnDescriptors, but we validate that all the names in the\nColumnDescriptor list is present in the Hive schema. If not present we\nerror out", "committedDate": "2020-04-15T17:51:41Z", "type": "commit"}, {"oid": "812b1060d66c64d2464b860988161d0df18129a3", "url": "https://github.com/greenplum-db/pxf/commit/812b1060d66c64d2464b860988161d0df18129a3", "message": "Hive: Validate that ALTERED hive tables can still be read\n\nAdd tests that:\n1. Create a hive table\n2. Read hive table\n3. Alter hive table (add column)\n4. Insert new records in hive table\n5. Read Greenplum external table accessing the hive table\n6. ALTER Greenplum external table with new column from hive\n7. Read Greenplum external table with new column from hive", "committedDate": "2020-04-15T17:51:41Z", "type": "commit"}, {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "url": "https://github.com/greenplum-db/pxf/commit/a18d488e9827cd56e87239d3c0c94b74d9471fe1", "message": "Remove Hive tests that are no longer used\n\n- Remove toLowerCase checking because Hive metastore always lowercases its schema\n- Add clarification notes about why we need both i, j and structIndex", "committedDate": "2020-04-15T17:51:41Z", "type": "commit"}, {"oid": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "url": "https://github.com/greenplum-db/pxf/commit/a18d488e9827cd56e87239d3c0c94b74d9471fe1", "message": "Remove Hive tests that are no longer used\n\n- Remove toLowerCase checking because Hive metastore always lowercases its schema\n- Add clarification notes about why we need both i, j and structIndex", "committedDate": "2020-04-15T17:51:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0NTQ5Mw==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417045493", "bodyText": "I wonder why include the entire array? Can we just include the 1 up to last elements?", "author": "oliverralbertini", "createdAt": "2020-04-29T03:19:39Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveColumnarSerdeResolver.java", "diffHunk": "@@ -65,29 +75,51 @@\n     private static final Log LOG = LogFactory.getLog(HiveColumnarSerdeResolver.class);\n     private boolean firstColumn;\n     private StringBuilder builder;\n-    private StringBuilder parts;\n     private String serdeType;\n-\n+    private String allColumnNames;\n+    private String allColumnTypes;\n+    private Map<String, String[]> partitionColumnNames;\n+    \n     /* read the data supplied by the fragmenter: inputformat name, serde name, partition keys */\n     @Override\n-    void parseUserData(RequestContext input) throws Exception {\n+    void parseUserData(RequestContext input) {\n         HiveUserData hiveUserData = HiveUtilities.parseHiveUserData(input);\n \n+        partitionColumnNames = new HashMap<>();\n         serdeType = hiveUserData.getSerdeClassName();\n-        parts = new StringBuilder();\n         partitionKeys = hiveUserData.getPartitionKeys();\n+        hiveIndexes = hiveUserData.getHiveIndexes();\n+        allColumnNames = hiveUserData.getAllColumnNames();\n+        allColumnTypes = hiveUserData.getAllColumnTypes();\n         parseDelimiterChar(input);\n     }\n \n     @Override\n     void initPartitionFields() {\n         if (context.getOutputFormat() == OutputFormat.TEXT) {\n-            initTextPartitionFields(parts);\n+            initTextPartitionFields(builder);\n         } else {\n             super.initPartitionFields();\n         }\n     }\n \n+    /*\n+     * The partition fields are initialized one time based on userData provided\n+     * by the fragmenter.\n+     */\n+    @Override\n+    void initTextPartitionFields(StringBuilder parts) {\n+        if (partitionKeys.equals(HiveDataFragmenter.HIVE_NO_PART_TBL)) {\n+            return;\n+        }\n+\n+        String[] partitionLevels = partitionKeys.split(HiveDataFragmenter.HIVE_PARTITIONS_DELIM);\n+        for (String partLevel : partitionLevels) {\n+            String[] levelKey = partLevel.split(HiveDataFragmenter.HIVE_1_PART_DELIM);\n+            partitionColumnNames.put(StringUtils.lowerCase(levelKey[0]), levelKey);", "originalCommit": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0ODQ0Ng==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417048446", "bodyText": "This looks very similar to the code in HiveColumnarSerdeResolver above.", "author": "oliverralbertini", "createdAt": "2020-04-29T03:33:23Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "diffHunk": "@@ -166,7 +172,23 @@ private void fetchTableMetaData(Metadata.Item tblDesc) throws Exception {\n         hiveClientWrapper.getSchema(tbl, metadata);\n         boolean hasComplexTypes = hiveClientWrapper.hasComplexTypes(metadata);\n \n-        verifySchema(tbl);\n+        // Keep a list of indices from the Hive schema columns that we need to\n+        // retrieve\n+        List<Integer> hiveIndexes = verifySchema(tbl);\n+        List<FieldSchema> fieldSchemaList = tbl.getSd().getCols();\n+\n+        // Get the column names and column types\n+        StringBuilder allColumnNames = new StringBuilder();\n+        StringBuilder allColumnTypes = new StringBuilder();\n+        String delim = \",\";\n+        for (FieldSchema fieldSchema : fieldSchemaList) {\n+            if (allColumnNames.length() > 0) {\n+                allColumnNames.append(delim);\n+                allColumnTypes.append(delim);\n+            }\n+            allColumnNames.append(fieldSchema.getName());\n+            allColumnTypes.append(fieldSchema.getType());", "originalCommit": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1ODc0OQ==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417058749", "bodyText": "I understand the need to have this metadata on each fragment, but it would be nice to not have to have so much duplicate data around.", "author": "oliverralbertini", "createdAt": "2020-04-29T04:21:33Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveDataFragmenter.java", "diffHunk": "@@ -324,8 +404,15 @@ private void fetchMetaData(HiveTablePartition tablePartition, boolean hasComplex\n             String filepath = fsp.getPath().toString();\n \n             byte[] locationInfo = HdfsUtilities.prepareFragmentMetadata(fsp);\n+            byte[] userData = hiveClientWrapper.makeUserData(\n+                    fragmenterForProfile,\n+                    tablePartition,\n+                    filterInFragmenter,\n+                    hiveIndexes,\n+                    allColumnNames,\n+                    allColumnTypes);\n             Fragment fragment = new Fragment(filepath, hosts, locationInfo,\n-                    hiveClientWrapper.makeUserData(fragmenterForProfile, tablePartition, filterInFragmenter), profile);\n+                    userData, profile);", "originalCommit": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA1OTM5Nw==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417059397", "bodyText": "What's the difference between this code and the one in the parent class? Does it just ignore partitioned columns? I don't have much context on the Hive input format.", "author": "oliverralbertini", "createdAt": "2020-04-29T04:24:33Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveInputFormatFragmenter.java", "diffHunk": "@@ -59,43 +65,57 @@\n         ORC_FILE_INPUT_FORMAT\n     }\n \n-    /*\n-     * Checks that hive fields and partitions match the GPDB schema. Throws an\n-     * exception if: - the number of fields (+ partitions) do not match the GPDB\n-     * table definition. - the hive fields types do not match the GPDB fields.\n+    /**\n+     * Checks that hive fields and partitions match the Greenplum schema.\n+     * Throws an exception if:\n+     * - A Greenplum column does not match any columns or partitions on the\n+     * Hive table definition\n+     * - The hive fields types do not match the Greenplum fields.\n+     * Then return a list of indexes corresponding to the matching columns in\n+     * Greenplum, ordered by the Greenplum schema order. It excludes any\n+     * partition column\n+     *\n+     * @param tbl the hive table\n+     * @return a list of indexes\n      */\n     @Override\n-    void verifySchema(Table tbl) throws Exception {\n+    List<Integer> verifySchema(Table tbl) {\n \n-        int columnsSize = context.getColumns();\n-        int hiveColumnsSize = tbl.getSd().getColsSize();\n-        int hivePartitionsSize = tbl.getPartitionKeysSize();\n+        List<Integer> indexes = new ArrayList<>();\n+        List<FieldSchema> hiveColumns = tbl.getSd().getCols();\n+        List<FieldSchema> hivePartitions = tbl.getPartitionKeys();\n \n-        LOG.debug(\"Hive table: {} fields, {} partitions. GPDB table: {} fields.\",\n-                hiveColumnsSize, hivePartitionsSize, columnsSize);\n+        Map<String, FieldSchema> columnNameToFieldSchema =\n+                Stream.concat(hiveColumns.stream(), hivePartitions.stream())\n+                        .collect(Collectors.toMap(FieldSchema::getName, fieldSchema -> fieldSchema));\n \n-        // check schema size\n-        if (columnsSize != (hiveColumnsSize + hivePartitionsSize)) {\n-            throw new IllegalArgumentException(\n-                    String.format(\"Hive table schema (%d fields, %d partitions) doesn't match PXF table (%d fields)\",\n-                            hiveColumnsSize, hivePartitionsSize, columnsSize));\n-        }\n+        Map<String, Integer> columnNameToColsIndexMap =\n+                IntStream.range(0, hiveColumns.size())\n+                        .boxed()\n+                        .collect(Collectors.toMap(i -> hiveColumns.get(i).getName(), i -> i));\n \n-        int index = 0;\n-        // check hive fields\n-        List<FieldSchema> hiveColumns = tbl.getSd().getCols();\n-        for (FieldSchema hiveCol : hiveColumns) {\n-            ColumnDescriptor colDesc = context.getColumn(index++);\n-            DataType colType = colDesc.getDataType();\n-            HiveUtilities.validateTypeCompatible(colType, colDesc.columnTypeModifiers(), hiveCol.getType(), colDesc.columnName());\n-        }\n-        // check partition fields\n-        List<FieldSchema> hivePartitions = tbl.getPartitionKeys();\n-        for (FieldSchema hivePart : hivePartitions) {\n-            ColumnDescriptor colDesc = context.getColumn(index++);\n-            DataType colType = colDesc.getDataType();\n-            HiveUtilities.validateTypeCompatible(colType, colDesc.columnTypeModifiers(), hivePart.getType(), colDesc.columnName());\n-        }\n+        FieldSchema fieldSchema;\n+        for (ColumnDescriptor cd : context.getTupleDescription()) {\n+            if ((fieldSchema = columnNameToFieldSchema.get(cd.columnName())) == null &&\n+                    (fieldSchema = columnNameToFieldSchema.get(cd.columnName().toLowerCase())) == null) {\n+                throw new IllegalArgumentException(\n+                        String.format(\"Column '%s' does not exist in the Hive schema or Hive Partition. \" +\n+                                        \"Ensure the column or partition exists and check the name spelling and case\",\n+                                cd.columnName()));\n+            }", "originalCommit": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA2MDEwNQ==", "url": "https://github.com/greenplum-db/pxf/pull/333#discussion_r417060105", "bodyText": "good catch!", "author": "oliverralbertini", "createdAt": "2020-04-29T04:27:33Z", "path": "server/pxf-hive/src/main/java/org/greenplum/pxf/plugins/hive/HiveORCAccessor.java", "diffHunk": "@@ -116,16 +116,18 @@ public boolean openForRead() throws Exception {\n     }\n \n     /**\n-     * Adds the table tuple description to JobConf ojbect\n+     * Adds the table tuple description to JobConf object", "originalCommit": "a18d488e9827cd56e87239d3c0c94b74d9471fe1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}