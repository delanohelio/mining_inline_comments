{"pr_number": 336, "pr_title": "Add User Option to allow invalid input paths", "pr_createdAt": "2020-04-12T12:17:21Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/336", "timeline": [{"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f", "url": "https://github.com/greenplum-db/pxf/commit/e7cedc5a58711a3b98269c6281ca030995229d3f", "message": "Add User Option to allow invalid input paths\n\nWhile running performance tests, some external tables did not have any\ndata, and when writting data to the partition, they did not create the\nexpected path while reading. To illustrate this better, this is a sample\ncode of what happened:\n\n    CREATE TABLE foo (id int, some_date date)\n    DISTRIBUTED BY (id)\n    PARTITION BY RANGE (some_date)\n    (start('1992-01-01') INCLUSIVE end ('1998-12-31') INCLUSIVE every\n    (30), default partition others);\n\nThe stamement above will create 87 partitions. In our experiment, some\nof the partitions did not have any data (there were no records for some\ndate ranges).\n\nWe then replace the internal partition with an external partition, as\nillustrated below:\n\n    CREATE EXTERNAL TABLE replace_partition_87 (id int, some_date date)\n    LOCATION (\n    'pxf://bucket/non-existent-path/?PROFILE=s3:parquet&SERVER=s3'\n    ) FORMAT 'CUSTOM' (formatter = 'pxfwritable_import');\n\n    ALTER TABLE foo\n    EXCHANGE PARTITION FOR (RANK(87)) WITH TABLE replace_partition_87\n    WITHOUT VALIDATION;\n\nBecause some of the external tables end up with no data, queries fail\nwhen reading the original partitioned table `foo`. This commit\nintroduces a new user property `IGNORE_INVALID_INPUT`, which when set to\ntrue, it will ignore the error and return an empty list of fragments.\nThe new table definition would look like this:\n\n    CREATE EXTERNAL TABLE replace_partition_87(id int, some_date date)\n    LOCATION (\n    'pxf://bucket/non-existent-path/?PROFLE=s3:parquet&IGNORE_INVALID_INPUT=true'\n    ) FORMAT 'CUSTOM' (formatter = 'pxfwritable_import');\n\nThis commit does not change the default behaviour of erroring out when\nthe partition is missing, but rather gives users an option to not error\nout when the input path is missing.", "committedDate": "2020-04-12T16:41:58Z", "type": "commit"}, {"oid": "e7cedc5a58711a3b98269c6281ca030995229d3f", "url": "https://github.com/greenplum-db/pxf/commit/e7cedc5a58711a3b98269c6281ca030995229d3f", "message": "Add User Option to allow invalid input paths\n\nWhile running performance tests, some external tables did not have any\ndata, and when writting data to the partition, they did not create the\nexpected path while reading. To illustrate this better, this is a sample\ncode of what happened:\n\n    CREATE TABLE foo (id int, some_date date)\n    DISTRIBUTED BY (id)\n    PARTITION BY RANGE (some_date)\n    (start('1992-01-01') INCLUSIVE end ('1998-12-31') INCLUSIVE every\n    (30), default partition others);\n\nThe stamement above will create 87 partitions. In our experiment, some\nof the partitions did not have any data (there were no records for some\ndate ranges).\n\nWe then replace the internal partition with an external partition, as\nillustrated below:\n\n    CREATE EXTERNAL TABLE replace_partition_87 (id int, some_date date)\n    LOCATION (\n    'pxf://bucket/non-existent-path/?PROFILE=s3:parquet&SERVER=s3'\n    ) FORMAT 'CUSTOM' (formatter = 'pxfwritable_import');\n\n    ALTER TABLE foo\n    EXCHANGE PARTITION FOR (RANK(87)) WITH TABLE replace_partition_87\n    WITHOUT VALIDATION;\n\nBecause some of the external tables end up with no data, queries fail\nwhen reading the original partitioned table `foo`. This commit\nintroduces a new user property `IGNORE_INVALID_INPUT`, which when set to\ntrue, it will ignore the error and return an empty list of fragments.\nThe new table definition would look like this:\n\n    CREATE EXTERNAL TABLE replace_partition_87(id int, some_date date)\n    LOCATION (\n    'pxf://bucket/non-existent-path/?PROFLE=s3:parquet&IGNORE_INVALID_INPUT=true'\n    ) FORMAT 'CUSTOM' (formatter = 'pxfwritable_import');\n\nThis commit does not change the default behaviour of erroring out when\nthe partition is missing, but rather gives users an option to not error\nout when the input path is missing.", "committedDate": "2020-04-12T16:41:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407645166", "bodyText": "maybe declare \"IGNORE_INVALID_INPUT\" as a constant on RequestContext ?", "author": "denalex", "createdAt": "2020-04-13T18:43:04Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java", "diffHunk": "@@ -64,7 +66,16 @@ public void initialize(RequestContext context) {\n     @Override\n     public List<Fragment> getFragments() throws Exception {\n         Path path = new Path(hcfsType.getDataUri(jobConf, context));\n-        List<InputSplit> splits = getSplits(path);\n+        List<InputSplit> splits;\n+        try {\n+            splits = getSplits(path);\n+        } catch (InvalidInputException e) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {", "originalCommit": "e7cedc5a58711a3b98269c6281ca030995229d3f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0Nzc5OA==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407647798", "bodyText": "can we think of a better name ? \"Invalid input\" is not smth exposed to end-users defining path in LOCATION Url, for them it is either \"path\" or \"resource\", in fact we call it in our docs \"path to data\" LOCATION('pxf://<path-to-data>?PROFILE=<profile_name>, so maybe at least IGNORE_INVALID_PATH. It is also not that the path is invalid (like in violation of syntax rules) but more as non-existent, but IGNORE_NON_EXISTENT_PATH is too long and prone to typing errors. I'd prefer 1-2 simple words max.", "author": "denalex", "createdAt": "2020-04-13T18:47:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY5ODY3MA==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407698670", "bodyText": "maybe something like IGNORE_MISSING_PATH could also work", "author": "frankgh", "createdAt": "2020-04-13T20:23:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczNzU1OA==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407737558", "bodyText": "Using IGNORE_MISSING_PATH", "author": "frankgh", "createdAt": "2020-04-13T21:40:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY0NTE2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "c9fc4c3580cfc6ca896cc05ea9ee6a0f5e7be2f2", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java\nindex ef9717b5..f08101fa 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsDataFragmenter.java\n\n@@ -70,7 +72,7 @@ public class HdfsDataFragmenter extends BaseFragmenter {\n         try {\n             splits = getSplits(path);\n         } catch (InvalidInputException e) {\n-            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(\"IGNORE_INVALID_INPUT\"))) {\n+            if (StringUtils.equalsIgnoreCase(\"true\", context.getOption(IGNORE_MISSING_PATH_OPTION))) {\n                 LOG.debug(\"Ignoring InvalidInputException\", e);\n                 return fragments;\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407651740", "bodyText": "what about other profiles ? If we introduce such an option, users might expect it to work similarly for other profiles (Hive, JDBC, etc). So maybe have other profiles proactively throw an error if they encounter this setting ? Or will this be too restrictive, since they will essentially ignore the option and error out if they determine actually missing path.", "author": "denalex", "createdAt": "2020-04-13T18:54:46Z", "path": "server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java", "diffHunk": "@@ -74,4 +74,37 @@ public void testFragmenterWilcardPath() throws Exception {\n         assertNotNull(fragmentList);\n         assertEquals(4, fragmentList.size());\n     }\n+\n+    @Test\n+    public void testInvalidInputPath() throws Exception {\n+        expectedException.expect(InvalidInputException.class);\n+        expectedException.expectMessage(\"Input Pattern file:/tmp/non-existent-path-on-disk/*.csv matches 0 files\");\n+\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+        fragmenter.getFragments();\n+    }\n+\n+    @Test\n+    public void testInvalidInputPathIgnored() throws Exception {\n+        RequestContext context = new RequestContext();\n+        context.setConfig(\"default\");\n+        context.setUser(\"test-user\");\n+        context.setProfileScheme(\"localfile\");\n+        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n+\n+        Fragmenter fragmenter = new HdfsFileFragmenter();\n+        fragmenter.initialize(context);\n+\n+        List<Fragment> fragmentList = fragmenter.getFragments();\n+        assertNotNull(fragmentList);\n+        assertEquals(0, fragmentList.size());\n+    }\n }", "originalCommit": "e7cedc5a58711a3b98269c6281ca030995229d3f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NTc1OQ==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407675759", "bodyText": "I thought about that, but I think this concept of PATH does not apply to other profiles. In essence, this feature allows supporting partitioned tables. Not in the Hive partitioned tables sense, but rather Greenplum partitioned tables backed by external tables accessing Hadoop Compatible Filesystems.", "author": "frankgh", "createdAt": "2020-04-13T19:39:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc5ODAxOA==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407798018", "bodyText": "Is it fair to say that the only profiles that don't use Hdfs{Data,File}Fragmenter are Hive and JDBC?", "author": "oliverralbertini", "createdAt": "2020-04-14T00:37:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNzU2Nw==", "url": "https://github.com/greenplum-db/pxf/pull/336#discussion_r407817567", "bodyText": "we also have Hbase and S3 Select", "author": "frankgh", "createdAt": "2020-04-14T01:47:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MTc0MA=="}], "type": "inlineReview", "revised_code": {"commit": "c9fc4c3580cfc6ca896cc05ea9ee6a0f5e7be2f2", "chunk": "diff --git a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java\nindex abdaaa42..88fb4124 100644\n--- a/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java\n+++ b/server/pxf-hdfs/src/test/java/org/greenplum/pxf/plugins/hdfs/HdfsFileFragmenterTest.java\n\n@@ -97,7 +97,7 @@ public class HdfsFileFragmenterTest {\n         context.setConfig(\"default\");\n         context.setUser(\"test-user\");\n         context.setProfileScheme(\"localfile\");\n-        context.addOption(\"IGNORE_INVALID_INPUT\", \"true\");\n+        context.addOption(\"IGNORE_MISSING_PATH\", \"true\");\n         context.setDataSource(\"/tmp/non-existent-path-on-disk/*.csv\");\n \n         Fragmenter fragmenter = new HdfsFileFragmenter();\n"}}, {"oid": "c9fc4c3580cfc6ca896cc05ea9ee6a0f5e7be2f2", "url": "https://github.com/greenplum-db/pxf/commit/c9fc4c3580cfc6ca896cc05ea9ee6a0f5e7be2f2", "message": "Address PR feedback", "committedDate": "2020-04-13T21:22:46Z", "type": "commit"}]}