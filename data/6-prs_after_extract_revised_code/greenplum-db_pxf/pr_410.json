{"pr_number": 410, "pr_title": "Allow skipping the header for *:text profiles", "pr_createdAt": "2020-07-13T01:50:25Z", "pr_url": "https://github.com/greenplum-db/pxf/pull/410", "timeline": [{"oid": "a206eefe2ff910731a82d2a1eb16364ec0301b4f", "url": "https://github.com/greenplum-db/pxf/commit/a206eefe2ff910731a82d2a1eb16364ec0301b4f", "message": "Allow skipping the header for *:text profiles\n\nWe introduce a new parameter \"SKIP_HEADER_COUNT\", which when provided\nfor *:text profiles, it will skip the N first lines of the first split\nof each file. For example, SKIP_HEADER_COUNT=2 will skip the first 2\nlines of each remote file and will only stream back starting from the\n3rd line. If SKIP_HEADER_COUNT is greater than the number of lines of a\nfile, no data will be streamed back to the client.", "committedDate": "2020-07-13T01:48:37Z", "type": "commit"}, {"oid": "4d0a4382e9ace6e232aa20c7d1e88860e8cbfc0c", "url": "https://github.com/greenplum-db/pxf/commit/4d0a4382e9ace6e232aa20c7d1e88860e8cbfc0c", "message": "Add test", "committedDate": "2020-07-13T02:54:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTA4Ng==", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453861086", "bodyText": "I think we can not rely on fragment index because:\n\nnot sure if fragment list coming in JSON guarantees order of splits\nisn't index maintained from 0 for each segment ?\nwhen reading multiple files, some beginning splits will not be the first fragments as other files would be read before them\n\nI think instead we should be looking at the offset in the fragment metadata, if the offset is 0, then we're reading the beginning of the file and can apply the header logic.", "author": "denalex", "createdAt": "2020-07-13T18:52:29Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java", "diffHunk": "@@ -56,17 +57,20 @@ public LineBreakAccessor() {\n     }\n \n     @Override\n-    public void initialize(RequestContext requestContext) {\n-        super.initialize(requestContext);\n+    public void initialize(RequestContext context) {\n+        super.initialize(context);\n         ((TextInputFormat) inputFormat).configure(jobConf);\n+        skipHeaderCount = context.getFragmentIndex() != 0 ? 0 :", "originalCommit": "4d0a4382e9ace6e232aa20c7d1e88860e8cbfc0c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk0MjM0MA==", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453942340", "bodyText": "agreed, pushing that change now.", "author": "frankgh", "createdAt": "2020-07-13T21:25:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTA4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "c4841abdfa1b9113f3962560ebd57d769912f372", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java\nindex 7c76ff13..f041c856 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/LineBreakAccessor.java\n\n@@ -60,8 +60,9 @@ public class LineBreakAccessor extends HdfsSplittableDataAccessor {\n     public void initialize(RequestContext context) {\n         super.initialize(context);\n         ((TextInputFormat) inputFormat).configure(jobConf);\n-        skipHeaderCount = context.getFragmentIndex() != 0 ? 0 :\n-                context.getOption(\"SKIP_HEADER_COUNT\", 0, true);\n+        skipHeaderCount = fileSplit.getStart() == 0\n+                ? context.getOption(\"SKIP_HEADER_COUNT\", 0, true)\n+                : 0;\n     }\n \n     @Override\n"}}, {"oid": "c4841abdfa1b9113f3962560ebd57d769912f372", "url": "https://github.com/greenplum-db/pxf/commit/c4841abdfa1b9113f3962560ebd57d769912f372", "message": "Address PR feedback", "committedDate": "2020-07-13T21:25:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng==", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r453997286", "bodyText": "this is confusing, if openForRead is used only for 1 Fragment and each Fragment is represented by a FileSplit, why are there iterators in this old logic ? Asking because if we can process multiple splits as the current logic implies, then we have to implement skipping lines inside readNextObject method.", "author": "denalex", "createdAt": "2020-07-13T23:08:02Z", "path": "server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java", "diffHunk": "@@ -80,7 +84,6 @@ public void initialize(RequestContext requestContext) {\n     @Override\n     public boolean openForRead() throws Exception {\n         LinkedList<InputSplit> requestSplits = new LinkedList<>();\n-        FileSplit fileSplit = HdfsUtilities.parseFileSplit(context);", "originalCommit": "c4841abdfa1b9113f3962560ebd57d769912f372", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxMTE1NA==", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r454011154", "bodyText": "Yes, this is confusing. I have seen this code before when we did the refactor, and I wasn't sure why it was there. I took at stab at removing this code in a subsequent commit and it looks like it was only being used by Avro, but there wasn't anything in avro that would add splits to the iterator. So it looks safe to remove. You can take a look at this commit for the cleanup: 24003f9", "author": "frankgh", "createdAt": "2020-07-13T23:41:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxNDYxOQ==", "url": "https://github.com/greenplum-db/pxf/pull/410#discussion_r454014619", "bodyText": "now waiting for pipelines to go green: https://ud.ci.gpdb.pivotal.io/teams/main/pipelines/dev:fguerrero-support-header-text", "author": "frankgh", "createdAt": "2020-07-13T23:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NzI4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "24003f9505aa679d861fc75277e26843f4705d17", "chunk": "diff --git a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java\nindex 431b07e3..f8d3cf47 100644\n--- a/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java\n+++ b/server/pxf-hdfs/src/main/java/org/greenplum/pxf/plugins/hdfs/HdfsSplittableDataAccessor.java\n\n@@ -82,13 +80,12 @@ public abstract class HdfsSplittableDataAccessor extends HcfsBaseAccessor {\n      * @return true if succeeded, false if no more splits to be read\n      */\n     @Override\n+    @SuppressWarnings(\"unchecked\")\n     public boolean openForRead() throws Exception {\n-        LinkedList<InputSplit> requestSplits = new LinkedList<>();\n-        requestSplits.add(fileSplit);\n-\n-        // Initialize record reader based on current split\n-        iter = requestSplits.listIterator(0);\n-        return getNextSplit();\n+        reader = (RecordReader<Object, Object>) getReader(jobConf, fileSplit);\n+        key = reader.createKey();\n+        data = reader.createValue();\n+        return true;\n     }\n \n     /**\n"}}, {"oid": "24003f9505aa679d861fc75277e26843f4705d17", "url": "https://github.com/greenplum-db/pxf/commit/24003f9505aa679d861fc75277e26843f4705d17", "message": "Remove iterator and getNextSplit", "committedDate": "2020-07-13T23:39:21Z", "type": "commit"}, {"oid": "0baabe7981b53905a6eef39e6c22be0579ea42c8", "url": "https://github.com/greenplum-db/pxf/commit/0baabe7981b53905a6eef39e6c22be0579ea42c8", "message": "Add automation test of reading multiple csv files with headers", "committedDate": "2020-07-14T01:09:40Z", "type": "forcePushed"}, {"oid": "d42dc3c0debc397ac30098936269a6ed1078164d", "url": "https://github.com/greenplum-db/pxf/commit/d42dc3c0debc397ac30098936269a6ed1078164d", "message": "Add automation test of reading multiple csv files with headers", "committedDate": "2020-07-14T01:11:39Z", "type": "commit"}, {"oid": "d42dc3c0debc397ac30098936269a6ed1078164d", "url": "https://github.com/greenplum-db/pxf/commit/d42dc3c0debc397ac30098936269a6ed1078164d", "message": "Add automation test of reading multiple csv files with headers", "committedDate": "2020-07-14T01:11:39Z", "type": "forcePushed"}, {"oid": "c74a2647484d6cc44604d6d0911e02ac511774bc", "url": "https://github.com/greenplum-db/pxf/commit/c74a2647484d6cc44604d6d0911e02ac511774bc", "message": "Make HiveAccessor code consistent with the logic of LineBreakAccessor", "committedDate": "2020-07-14T01:21:25Z", "type": "commit"}, {"oid": "66837fe81482946819a37aa79a3805690ad58242", "url": "https://github.com/greenplum-db/pxf/commit/66837fe81482946819a37aa79a3805690ad58242", "message": "Fix unit test and automation test", "committedDate": "2020-07-14T02:45:14Z", "type": "commit"}, {"oid": "2f5339a4eeaad0072520bf9b9f7ce6af33851f22", "url": "https://github.com/greenplum-db/pxf/commit/2f5339a4eeaad0072520bf9b9f7ce6af33851f22", "message": "fix test", "committedDate": "2020-07-14T03:10:23Z", "type": "commit"}, {"oid": "59d94f3aa34d1933ef76bd5b5039269d48d0f76e", "url": "https://github.com/greenplum-db/pxf/commit/59d94f3aa34d1933ef76bd5b5039269d48d0f76e", "message": "Use fragment index instead", "committedDate": "2020-07-14T11:36:05Z", "type": "commit"}, {"oid": "059fafc154a069d176c3b56117a6b802c2369e63", "url": "https://github.com/greenplum-db/pxf/commit/059fafc154a069d176c3b56117a6b802c2369e63", "message": "add unit test with skipHeaderCount on fragment with non zero index", "committedDate": "2020-07-14T12:46:01Z", "type": "commit"}]}