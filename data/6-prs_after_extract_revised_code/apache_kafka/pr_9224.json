{"pr_number": 9224, "pr_title": "KAFKA-10304: refactor MM2 integration tests", "pr_createdAt": "2020-08-26T17:20:07Z", "pr_url": "https://github.com/apache/kafka/pull/9224", "timeline": [{"oid": "08bd90651971370051ca9923bf8cb101d1acedcd", "url": "https://github.com/apache/kafka/commit/08bd90651971370051ca9923bf8cb101d1acedcd", "message": "refactor MM2 integration tests", "committedDate": "2020-09-12T18:03:01Z", "type": "forcePushed"}, {"oid": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "url": "https://github.com/apache/kafka/commit/bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "message": "refactor MM2 integration tests", "committedDate": "2020-11-23T05:00:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA5NTQyNg==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535095426", "bodyText": "This constant is unused. Should we use it instead of hardcoding 3 below when we create connect workers?", "author": "mimaison", "createdAt": "2020-12-03T10:52:57Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA5NTc1Nw==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535095757", "bodyText": "This is unused too", "author": "mimaison", "createdAt": "2020-12-03T10:53:22Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); ", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTA5ODAyMQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535098021", "bodyText": "Ideally we want to get rid of this method as it makes no sense in tests that are not SSL.", "author": "mimaison", "createdAt": "2020-12-03T10:55:28Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEwMDQxOQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535100419", "bodyText": "This should be done in the SSL class. The base class should not be aware of SSL and just use configurations from the concrete classes", "author": "mimaison", "createdAt": "2020-12-03T10:58:01Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();\n+\n+    protected void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props = basicMM2Config();\n+        \n+        final Map<String, Object> sslConfig = getSslConfig();\n+        if (sslConfig != null) {", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTEwMjY0NQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535102645", "bodyText": "Can we move this method to the base class?", "author": "mimaison", "createdAt": "2020-12-03T11:00:27Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.junit.Before;\n+import org.junit.After;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.Map;\n+\n+import org.apache.kafka.test.IntegrationTest;\n+\n+@Category(IntegrationTest.class)\n+public class MirrorConnectorsIntegrationTest extends MirrorConnectorsIntegrationBaseTest {\n+    \n+    @Before\n+    public void setup() throws InterruptedException {\n+        startClusters();\n+    }\n+\n+    @After", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java\nindex fd67828e90..a685e5d16a 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java\n\n@@ -16,29 +16,9 @@\n  */\n package org.apache.kafka.connect.mirror.integration;\n \n-import org.junit.Before;\n-import org.junit.After;\n import org.junit.experimental.categories.Category;\n-\n-import java.util.Map;\n-\n import org.apache.kafka.test.IntegrationTest;\n \n @Category(IntegrationTest.class)\n public class MirrorConnectorsIntegrationTest extends MirrorConnectorsIntegrationBaseTest {\n-    \n-    @Before\n-    public void setup() throws InterruptedException {\n-        startClusters();\n-    }\n-\n-    @After\n-    public void close() {\n-        shutdownClusters();\n-    }\n-    \n-    @Override\n-    Map<String, Object> getSslConfig() {\n-        return null;\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE2OTU5MQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535169591", "bodyText": "Do we really need to cast to Password and call value()? Can't we just pass brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)?", "author": "mimaison", "createdAt": "2020-12-03T12:06:43Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -161,6 +161,11 @@ private void start(int[] brokerPorts, String[] logDirs) {\n         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());\n         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n+        if (sslEnabled()) {\n+        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 274fe22575..84b961b42a 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -162,9 +162,9 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         if (sslEnabled()) {\n-        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n-        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n-        \tproducerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));\n+            producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n         }\n         producer = new KafkaProducer<>(producerProps);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE3MjIzNg==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535172236", "bodyText": "brokerConfig is a Properties, so we can call getProperty() to get back a String directly", "author": "mimaison", "createdAt": "2020-12-03T12:09:26Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -278,6 +283,14 @@ protected boolean hasState(KafkaServer server, Predicate<BrokerState> desiredSta\n             return false;\n         }\n     }\n+    \n+    public boolean sslEnabled() {\n+        final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 274fe22575..84b961b42a 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -285,11 +285,8 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n     }\n     \n     public boolean sslEnabled() {\n-        final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());\n-        if (listeners != null && listeners.toString().contains(\"SSL\")) {\n-            return true;\n-        }\n-        return false;\n+        final String listeners = brokerConfig.getProperty(KafkaConfig$.MODULE$.ListenersProp());\n+        return (listeners != null && listeners.contains(\"SSL\")) ? true : false;\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE3MzU2MA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535173560", "bodyText": "Instead of doing:\nif (condition) {\n  return true;\n} else {\n  return false;\n}\n\nYou can do:\nreturn condition;", "author": "mimaison", "createdAt": "2020-12-03T12:10:38Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -278,6 +283,14 @@ protected boolean hasState(KafkaServer server, Predicate<BrokerState> desiredSta\n             return false;\n         }\n     }\n+    \n+    public boolean sslEnabled() {\n+        final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());\n+        if (listeners != null && listeners.toString().contains(\"SSL\")) {", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 274fe22575..84b961b42a 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -285,11 +285,8 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n     }\n     \n     public boolean sslEnabled() {\n-        final Object listeners = brokerConfig.get(KafkaConfig$.MODULE$.ListenersProp());\n-        if (listeners != null && listeners.toString().contains(\"SSL\")) {\n-            return true;\n-        }\n-        return false;\n+        final String listeners = brokerConfig.getProperty(KafkaConfig$.MODULE$.ListenersProp());\n+        return (listeners != null && listeners.contains(\"SSL\")) ? true : false;\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE3NDMzMA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535174330", "bodyText": "Same as above, I think we can just pass brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)?", "author": "mimaison", "createdAt": "2020-12-03T12:11:20Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -444,7 +457,11 @@ public Admin createAdminClient() {\n         putIfAbsent(props, AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n         putIfAbsent(props, KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n         putIfAbsent(props, VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n-\n+        if (sslEnabled()) {\n+        \tputIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+        \tputIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 274fe22575..84b961b42a 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -458,9 +455,9 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n         putIfAbsent(props, KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n         putIfAbsent(props, VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArrayDeserializer\");\n         if (sslEnabled()) {\n-        \tputIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n-        \tputIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n-        \tputIfAbsent(props,CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            putIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            putIfAbsent(props,SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));\n+            putIfAbsent(props,CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n         }\n         KafkaConsumer<byte[], byte[]> consumer;\n         try {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE3NjEwNQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535176105", "bodyText": "A few lines in this file (like this one) contain tabs, we use spaces in Kafka", "author": "mimaison", "createdAt": "2020-12-03T12:13:05Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -161,6 +161,11 @@ private void start(int[] brokerPorts, String[] logDirs) {\n         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());\n         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n+        if (sslEnabled()) {\n+        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 274fe22575..84b961b42a 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -162,9 +162,9 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n         if (sslEnabled()) {\n-        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n-        \tproducerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n-        \tproducerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            producerProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            producerProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, brokerConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG));\n+            producerProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n         }\n         producer = new KafkaProducer<>(producerProps);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4MjM1NA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535182354", "bodyText": "I find it strange that this method closes the consumer it received.", "author": "mimaison", "createdAt": "2020-12-03T12:20:16Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();\n+\n+    protected void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props = basicMM2Config();\n+        \n+        final Map<String, Object> sslConfig = getSslConfig();\n+        if (sslConfig != null) {\n+            Properties sslProps = new Properties();\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n+            sslProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            \n+            // set SSL config for kafka connect worker\n+            backupWorkerProps.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            \n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            // set SSL config for producer used by source task in MM2\n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".producer.\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+        }\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    public void shutdownClusters() {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(primaryConsumer, 0);\n+\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(backupConsumer, 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        primaryConsumer.assign(backupOffsets.keySet());\n+        backupOffsets.forEach(primaryConsumer::seek);\n+        primaryConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        primaryConsumer.commitAsync();\n+\n+        assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+\n+        primaryConsumer.close();\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+            } catch (Throwable e) {\n+                return false;\n+            }\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+            } catch (Throwable e) {\n+                return false;\n+            }\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n+\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+ \n+        // Failback consumer group to primary cluster\n+        backupConsumer = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        backupConsumer.assign(primaryOffsets.keySet());\n+        primaryOffsets.forEach(backupConsumer::seek);\n+        backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        backupConsumer.commitAsync();\n+        \n+        assertTrue(\"Consumer failedback to zero upstream offset.\", backupConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback to zero downstream offset.\", backupConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback beyond expected upstream offset.\", backupConsumer.position(\n+            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Consumer failedback beyond expected downstream offset.\", backupConsumer.position(\n+            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        \n+        backupConsumer.close();\n+      \n+        // create more matching topics\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n+\n+        // only produce messages to the first partition\n+        produceMessages(primary, \"test-topic-2\", 1);\n+        produceMessages(backup, \"test-topic-3\", 1);\n+        \n+        // expect total consumed messages equals to NUM_RECORDS_PER_PARTITION\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+\n+    }\n+    \n+    @Test\n+    public void testReplicationWithEmptyPartition() throws Exception {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = Collections.singletonMap(\"group.id\", consumerGroupName);\n+\n+        // create topic\n+        String topic = \"test-topic-with-empty-partition\";\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+\n+        // produce to all test-topic-empty's partitions, except the last partition\n+        produceMessages(primary, topic, NUM_PARTITIONS - 1);\n+        \n+        // consume before starting the connectors so we don't need to wait for discovery\n+        int expectedRecords = NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1);\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic)) {\n+            waitForConsumingAllRecords(primaryConsumer, expectedRecords);\n+        }\n+        \n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        \n+        // sleep few seconds to have MM2 finish replication so that \"end\" consumer will consume some record\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(3));\n+\n+        // consume all records from backup cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                PRIMARY_CLUSTER_ALIAS + \".\" + topic)) {\n+            waitForConsumingAllRecords(backupConsumer, expectedRecords);\n+        }\n+        \n+        Admin backupClient = backup.kafka().createAdminClient();\n+        // retrieve the consumer group offset from backup cluster\n+        Map<TopicPartition, OffsetAndMetadata> remoteOffsets =\n+                backupClient.listConsumerGroupOffsets(consumerGroupName).partitionsToOffsetAndMetadata().get();\n+        // pinpoint the offset of the last partition which does not receive records \n+        OffsetAndMetadata offset = remoteOffsets.get(new TopicPartition(PRIMARY_CLUSTER_ALIAS + \".\" + topic, NUM_PARTITIONS - 1));\n+        // offset of the last partition should exist, but its value should be 0\n+        assertNotNull(\"Offset of last partition was not replicated\", offset);\n+        assertEquals(\"Offset of last partition is not zero\", 0, offset.offset());\n+    }\n+    \n+    @Test\n+    public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testOneWayReplicationWithAutoOffsetSync\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"earliest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                \"test-topic-1\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(primaryConsumer, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // enable automated consumer group offset sync\n+        mm2Props.put(\"sync.group.offsets.enabled\", \"true\");\n+        mm2Props.put(\"sync.group.offsets.interval.seconds\", \"1\");\n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(\n+            consumerProps, \"primary.test-topic-1\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Collections.singletonList(\"primary.test-topic-1\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        ConsumerRecords<byte[], byte[]> records = backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+\n+        // the size of consumer record should be zero, because the offsets of the same consumer group\n+        // have been automatically synchronized from primary to backup by the background job, so no\n+        // more records to consume from the replicated topic by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+\n+        // now create a new topic in primary cluster\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"primary.test-topic-2\", 1);\n+        // produce some records to the new topic in primary cluster\n+        produceMessages(primary, \"test-topic-2\");\n+\n+        // create a consumer at primary cluster to consume the new topic\n+        try (Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+                \"group.id\", \"consumer-group-1\"), \"test-topic-2\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(consumer1, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n+        backupConsumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+            \"group.id\", consumerGroupName), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Arrays.asList(\"primary.test-topic-1\", \"primary.test-topic-2\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        records = backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+        backupConsumer.close();\n+    }\n+    \n+    /*\n+     * launch the connectors on kafka connect cluster and check if they are running\n+     */\n+    private static void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster, \n+            List<Class<? extends Connector>> connectorClasses, MirrorMakerConfig mm2Config, \n+            String primary, String backup) throws InterruptedException {\n+        for (int i = 0; i < connectorClasses.size(); i++) {\n+            String connector = connectorClasses.get(i).getSimpleName();\n+            connectCluster.configureConnector(connector, mm2Config.connectorBaseConfig(\n+                new SourceAndTarget(primary, backup), connectorClasses.get(i)));\n+        }\n+        \n+        // we wait for the connector and tasks to come up for each connector, so that when we do the\n+        // actual testing, we are certain that the tasks are up and running; this will prevent\n+        // flaky tests where the connector and tasks didn't start up in time for the tests to be run\n+        List<String> connectorNames = connectorClasses.stream().map(x -> x.getSimpleName())\n+                .collect(Collectors.toList());\n+        for (String connector : connectorNames) {\n+            connectCluster.assertions().assertConnectorAndAtLeastNumTasksAreRunning(connector, 1,\n+                    \"Connector \" + connector + \" tasks did not start in time on cluster: \" + connectCluster);\n+        }\n+    }\n+ \n+    /*\n+     * delete all topics of the input kafka cluster\n+     */\n+    private static void deleteAllTopics(EmbeddedKafkaCluster cluster) {\n+        Admin client = cluster.createAdminClient();\n+        try {\n+            client.deleteTopics(client.listTopics().names().get());\n+        } catch (Throwable e) {\n+            // should not run into exception normally. In case of Exception, \n+            // simply fail the test and investigate\n+        }\n+    }\n+    \n+    /*\n+     * retrieve the config value based on the input cluster, topic and config name\n+     */\n+    private static String getTopicConfig(EmbeddedKafkaCluster cluster, String topic, String configName) {\n+        Admin client = cluster.createAdminClient();\n+        Collection<ConfigResource> cr =  Collections.singleton(\n+                new ConfigResource(ConfigResource.Type.TOPIC, topic)); \n+        try {\n+            DescribeConfigsResult configsResult = client.describeConfigs(cr);\n+            Config allConfigs = (Config) configsResult.all().get().values().toArray()[0];\n+            Iterator<ConfigEntry> configIterator = allConfigs.entries().iterator();\n+            while (configIterator.hasNext()) {\n+                ConfigEntry currentConfig = configIterator.next();     \n+                if (currentConfig.name().equals(configName)) {\n+                    return currentConfig.value();\n+                }\n+            }\n+        } catch (Throwable e) {\n+            // should not run into exception normally. In case of Exception, \n+            // simply fail the test and investigate\n+        }\n+        return null;\n+    }\n+    \n+    /*\n+     *  produce messages to the cluster and topic \n+     */\n+    protected void produceMessages(EmbeddedConnectCluster cluster, String topicName) {\n+        Map<String, String> recordSent = generateRecords(NUM_RECORDS_PRODUCED);\n+        for (Map.Entry<String, String> entry : recordSent.entrySet()) {\n+            cluster.kafka().produce(topicName, entry.getKey(), entry.getValue());\n+        }\n+    }\n+\n+    /*\n+     * produce messages to the cluster and topic partition less than numPartitions \n+     */\n+    protected void produceMessages(EmbeddedConnectCluster cluster, String topicName, int numPartitions) {\n+        int cnt = 0;\n+        for (int r = 0; r < NUM_RECORDS_PER_PARTITION; r++)\n+            for (int p = 0; p < numPartitions; p++)\n+                cluster.kafka().produce(topicName, p, \"key\", \"value-\" + cnt++);\n+    }\n+    \n+    /*\n+     * given consumer group, topics and expected number of records, make sure the consumer group\n+     * offsets are eventually synced to the expected offset numbers\n+     */\n+    private static <T> void waitForConsumerGroupOffsetSync(EmbeddedConnectCluster connect, \n+            Consumer<T, T> consumer, List<String> topics, String consumerGroupId, int numRecords)\n+            throws InterruptedException {\n+        Admin adminClient = connect.kafka().createAdminClient();\n+        List<TopicPartition> tps = new ArrayList<>(NUM_PARTITIONS * topics.size());\n+        for (int partitionIndex = 0; partitionIndex < NUM_PARTITIONS; partitionIndex++) {\n+            for (String topic : topics) {\n+                tps.add(new TopicPartition(topic, partitionIndex));\n+            }\n+        }\n+        long expectedTotalOffsets = numRecords * topics.size();\n+\n+        waitForCondition(() -> {\n+            Map<TopicPartition, OffsetAndMetadata> consumerGroupOffsets =\n+                    adminClient.listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();\n+            long consumerGroupOffsetTotal = consumerGroupOffsets.values().stream()\n+                    .mapToLong(metadata -> metadata.offset()).sum();\n+\n+            Map<TopicPartition, Long> offsets = consumer.endOffsets(tps, Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+            long totalOffsets = offsets.values().stream().mapToLong(l -> l).sum();\n+\n+            // make sure the consumer group offsets are synced to expected number\n+            return totalOffsets == expectedTotalOffsets && consumerGroupOffsetTotal > 0;\n+        }, OFFSET_SYNC_DURATION_MS, \"Consumer group offset sync is not complete in time\");\n+    }\n+\n+    /*\n+     * make sure the consumer to consume expected number of records\n+     */\n+    private static <T> void waitForConsumingAllRecords(Consumer<T, T> consumer, int numExpectedRecords) \n+            throws InterruptedException {\n+        final AtomicInteger totalConsumedRecords = new AtomicInteger(0);\n+        waitForCondition(() -> {\n+            ConsumerRecords<T, T> records = consumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+            return numExpectedRecords == totalConsumedRecords.addAndGet(records.count());\n+        }, RECORD_CONSUME_DURATION_MS, \"Consumer cannot consume all records in time\");\n+        consumer.commitSync();\n+        consumer.close();", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4NjQ0NA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535386444", "bodyText": "Closing the consumer will make the re-use of the same consumer instance in a clean state. I tried to remove consumer.close(); and it caused test failures.", "author": "ning2008wisc", "createdAt": "2020-12-03T16:29:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4MjM1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg3NzA4Ng==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538877086", "bodyText": "Yes but the code that created the consumer should close it. If I call waitForConsumingAllRecords(), I'd not expect it to close my consumer instance.", "author": "mimaison", "createdAt": "2020-12-08T23:09:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4MjM1NA=="}], "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4NTI1NA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535185254", "bodyText": "Can we use allConfigs.get() to find the configs we want instead of searching for them?", "author": "mimaison", "createdAt": "2020-12-03T12:24:50Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();\n+\n+    protected void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props = basicMM2Config();\n+        \n+        final Map<String, Object> sslConfig = getSslConfig();\n+        if (sslConfig != null) {\n+            Properties sslProps = new Properties();\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n+            sslProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            \n+            // set SSL config for kafka connect worker\n+            backupWorkerProps.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            \n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            // set SSL config for producer used by source task in MM2\n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".producer.\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+        }\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    public void shutdownClusters() {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(primaryConsumer, 0);\n+\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(backupConsumer, 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        primaryConsumer.assign(backupOffsets.keySet());\n+        backupOffsets.forEach(primaryConsumer::seek);\n+        primaryConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        primaryConsumer.commitAsync();\n+\n+        assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+\n+        primaryConsumer.close();\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+            } catch (Throwable e) {\n+                return false;\n+            }\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+            } catch (Throwable e) {\n+                return false;\n+            }\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n+\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+ \n+        // Failback consumer group to primary cluster\n+        backupConsumer = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        backupConsumer.assign(primaryOffsets.keySet());\n+        primaryOffsets.forEach(backupConsumer::seek);\n+        backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        backupConsumer.commitAsync();\n+        \n+        assertTrue(\"Consumer failedback to zero upstream offset.\", backupConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback to zero downstream offset.\", backupConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedback beyond expected upstream offset.\", backupConsumer.position(\n+            new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Consumer failedback beyond expected downstream offset.\", backupConsumer.position(\n+            new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        \n+        backupConsumer.close();\n+      \n+        // create more matching topics\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n+\n+        // only produce messages to the first partition\n+        produceMessages(primary, \"test-topic-2\", 1);\n+        produceMessages(backup, \"test-topic-3\", 1);\n+        \n+        // expect total consumed messages equals to NUM_RECORDS_PER_PARTITION\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+\n+    }\n+    \n+    @Test\n+    public void testReplicationWithEmptyPartition() throws Exception {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = Collections.singletonMap(\"group.id\", consumerGroupName);\n+\n+        // create topic\n+        String topic = \"test-topic-with-empty-partition\";\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+\n+        // produce to all test-topic-empty's partitions, except the last partition\n+        produceMessages(primary, topic, NUM_PARTITIONS - 1);\n+        \n+        // consume before starting the connectors so we don't need to wait for discovery\n+        int expectedRecords = NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1);\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic)) {\n+            waitForConsumingAllRecords(primaryConsumer, expectedRecords);\n+        }\n+        \n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        \n+        // sleep few seconds to have MM2 finish replication so that \"end\" consumer will consume some record\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(3));\n+\n+        // consume all records from backup cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                PRIMARY_CLUSTER_ALIAS + \".\" + topic)) {\n+            waitForConsumingAllRecords(backupConsumer, expectedRecords);\n+        }\n+        \n+        Admin backupClient = backup.kafka().createAdminClient();\n+        // retrieve the consumer group offset from backup cluster\n+        Map<TopicPartition, OffsetAndMetadata> remoteOffsets =\n+                backupClient.listConsumerGroupOffsets(consumerGroupName).partitionsToOffsetAndMetadata().get();\n+        // pinpoint the offset of the last partition which does not receive records \n+        OffsetAndMetadata offset = remoteOffsets.get(new TopicPartition(PRIMARY_CLUSTER_ALIAS + \".\" + topic, NUM_PARTITIONS - 1));\n+        // offset of the last partition should exist, but its value should be 0\n+        assertNotNull(\"Offset of last partition was not replicated\", offset);\n+        assertEquals(\"Offset of last partition is not zero\", 0, offset.offset());\n+    }\n+    \n+    @Test\n+    public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testOneWayReplicationWithAutoOffsetSync\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"earliest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                \"test-topic-1\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(primaryConsumer, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // enable automated consumer group offset sync\n+        mm2Props.put(\"sync.group.offsets.enabled\", \"true\");\n+        mm2Props.put(\"sync.group.offsets.interval.seconds\", \"1\");\n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(\n+            consumerProps, \"primary.test-topic-1\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Collections.singletonList(\"primary.test-topic-1\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        ConsumerRecords<byte[], byte[]> records = backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+\n+        // the size of consumer record should be zero, because the offsets of the same consumer group\n+        // have been automatically synchronized from primary to backup by the background job, so no\n+        // more records to consume from the replicated topic by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+\n+        // now create a new topic in primary cluster\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"primary.test-topic-2\", 1);\n+        // produce some records to the new topic in primary cluster\n+        produceMessages(primary, \"test-topic-2\");\n+\n+        // create a consumer at primary cluster to consume the new topic\n+        try (Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+                \"group.id\", \"consumer-group-1\"), \"test-topic-2\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(consumer1, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n+        backupConsumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+            \"group.id\", consumerGroupName), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Arrays.asList(\"primary.test-topic-1\", \"primary.test-topic-2\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        records = backupConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+        backupConsumer.close();\n+    }\n+    \n+    /*\n+     * launch the connectors on kafka connect cluster and check if they are running\n+     */\n+    private static void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster, \n+            List<Class<? extends Connector>> connectorClasses, MirrorMakerConfig mm2Config, \n+            String primary, String backup) throws InterruptedException {\n+        for (int i = 0; i < connectorClasses.size(); i++) {\n+            String connector = connectorClasses.get(i).getSimpleName();\n+            connectCluster.configureConnector(connector, mm2Config.connectorBaseConfig(\n+                new SourceAndTarget(primary, backup), connectorClasses.get(i)));\n+        }\n+        \n+        // we wait for the connector and tasks to come up for each connector, so that when we do the\n+        // actual testing, we are certain that the tasks are up and running; this will prevent\n+        // flaky tests where the connector and tasks didn't start up in time for the tests to be run\n+        List<String> connectorNames = connectorClasses.stream().map(x -> x.getSimpleName())\n+                .collect(Collectors.toList());\n+        for (String connector : connectorNames) {\n+            connectCluster.assertions().assertConnectorAndAtLeastNumTasksAreRunning(connector, 1,\n+                    \"Connector \" + connector + \" tasks did not start in time on cluster: \" + connectCluster);\n+        }\n+    }\n+ \n+    /*\n+     * delete all topics of the input kafka cluster\n+     */\n+    private static void deleteAllTopics(EmbeddedKafkaCluster cluster) {\n+        Admin client = cluster.createAdminClient();\n+        try {\n+            client.deleteTopics(client.listTopics().names().get());\n+        } catch (Throwable e) {\n+            // should not run into exception normally. In case of Exception, \n+            // simply fail the test and investigate\n+        }\n+    }\n+    \n+    /*\n+     * retrieve the config value based on the input cluster, topic and config name\n+     */\n+    private static String getTopicConfig(EmbeddedKafkaCluster cluster, String topic, String configName) {\n+        Admin client = cluster.createAdminClient();\n+        Collection<ConfigResource> cr =  Collections.singleton(\n+                new ConfigResource(ConfigResource.Type.TOPIC, topic)); \n+        try {\n+            DescribeConfigsResult configsResult = client.describeConfigs(cr);\n+            Config allConfigs = (Config) configsResult.all().get().values().toArray()[0];\n+            Iterator<ConfigEntry> configIterator = allConfigs.entries().iterator();", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg0NjE0NQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535846145", "bodyText": "allConfigs is a java.util.Collection object and seems does not directly support get() https://docs.oracle.com/javase/8/docs/api/java/util/Collection.html", "author": "ning2008wisc", "createdAt": "2020-12-04T05:29:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4NTI1NA=="}], "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4NjkyMg==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535186922", "bodyText": "This is always used with Duration.ofMillis(). Should we store the Duration object directly?", "author": "mimaison", "createdAt": "2020-12-03T12:27:28Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE4OTM1NA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535189354", "bodyText": "I understand this was probably already there but we should avoid catching Throwable. I'm not sure the try/catch block is necessarily helping in this cases", "author": "mimaison", "createdAt": "2020-12-03T12:31:38Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();\n+\n+    protected void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props = basicMM2Config();\n+        \n+        final Map<String, Object> sslConfig = getSslConfig();\n+        if (sslConfig != null) {\n+            Properties sslProps = new Properties();\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n+            sslProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            \n+            // set SSL config for kafka connect worker\n+            backupWorkerProps.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            \n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            // set SSL config for producer used by source task in MM2\n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".producer.\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+        }\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    public void shutdownClusters() {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(primaryConsumer, 0);\n+\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(backupConsumer, 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        primaryConsumer.assign(backupOffsets.keySet());\n+        backupOffsets.forEach(primaryConsumer::seek);\n+        primaryConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        primaryConsumer.commitAsync();\n+\n+        assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+\n+        primaryConsumer.close();\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+            } catch (Throwable e) {\n+                return false;\n+            }\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            try {\n+                return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+            } catch (Throwable e) {", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE5MDM3MA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535190370", "bodyText": "Closing a client in the middle of a test can be confusing. I wonder if we could wrap this whole block about primaryConsumer in a try with resource block. WDYT?", "author": "mimaison", "createdAt": "2020-12-03T12:33:20Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,617 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final int CONSUMER_POLL_TIMEOUT_MS = 500;\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    private static final long DEFAULT_PRODUCE_SEND_DURATION_MS = TimeUnit.SECONDS.toMillis(120); \n+    private static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    private static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    private Map<String, String> mm2Props;\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    private Map<String, String> primaryWorkerProps = new HashMap<>();\n+    private Map<String, String> backupWorkerProps = new HashMap<>(); \n+    abstract Map<String, Object> getSslConfig();\n+\n+    protected void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props = basicMM2Config();\n+        \n+        final Map<String, Object> sslConfig = getSslConfig();\n+        if (sslConfig != null) {\n+            Properties sslProps = new Properties();\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG));\n+            sslProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, ((Password) sslConfig.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG)).value());\n+            sslProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");\n+            \n+            // set SSL config for kafka connect worker\n+            backupWorkerProps.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            \n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+            // set SSL config for producer used by source task in MM2\n+            mm2Props.putAll(sslProps.entrySet().stream().collect(Collectors.toMap(\n+                e -> BACKUP_CLUSTER_ALIAS + \".producer.\" + String.valueOf(e.getKey()), e ->  String.valueOf(e.getValue()))));\n+        }\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(3)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(3,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    public void shutdownClusters() {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(primaryConsumer, 0);\n+\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n+        waitForConsumingAllRecords(backupConsumer, 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName));\n+        primaryConsumer.assign(backupOffsets.keySet());\n+        backupOffsets.forEach(primaryConsumer::seek);\n+        primaryConsumer.poll(Duration.ofMillis(CONSUMER_POLL_TIMEOUT_MS));\n+        primaryConsumer.commitAsync();\n+\n+        assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+        assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+\n+        primaryConsumer.close();", "originalCommit": "bd6ee1a200438b938d0a2a1e8ccc501e7171eb0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1NjkzNQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r535856935", "bodyText": "yeah - I think we can do \"try with resource block\", updated", "author": "ning2008wisc", "createdAt": "2020-12-04T06:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTE5MDM3MA=="}], "type": "inlineReview", "revised_code": {"commit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 1cbc118949..867c57a0a8 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -23,11 +23,8 @@ import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n-import org.apache.kafka.common.config.SslConfigs;\n-import org.apache.kafka.common.config.types.Password;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.connect.connector.Connector;\n"}}, {"oid": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "url": "https://github.com/apache/kafka/commit/ff7de40fcd6d370354d032b00f6d9ea656d15e22", "message": "refactor MM2 integration tests", "committedDate": "2020-12-04T06:26:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg3NjA5MQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538876091", "bodyText": "We can return listeners != null && listeners.contains(\"SSL\")", "author": "mimaison", "createdAt": "2020-12-08T23:06:56Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java", "diffHunk": "@@ -278,6 +283,11 @@ protected boolean hasState(KafkaServer server, Predicate<BrokerState> desiredSta\n             return false;\n         }\n     }\n+    \n+    public boolean sslEnabled() {\n+        final String listeners = brokerConfig.getProperty(KafkaConfig$.MODULE$.ListenersProp());\n+        return (listeners != null && listeners.contains(\"SSL\")) ? true : false;", "originalCommit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a4af47b3f582ca86f9720057768e60f1632613a4", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\nindex 84b961b42a..9da1c23106 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java\n\n@@ -286,7 +286,7 @@ public class EmbeddedKafkaCluster extends ExternalResource {\n     \n     public boolean sslEnabled() {\n         final String listeners = brokerConfig.getProperty(KafkaConfig$.MODULE$.ListenersProp());\n-        return (listeners != null && listeners.contains(\"SSL\")) ? true : false;\n+        return listeners != null && listeners.contains(\"SSL\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg4MzMwMQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538883301", "bodyText": "We can use Config.get() to directly access the configuration we want, see https://kafka.apache.org/26/javadoc/org/apache/kafka/clients/admin/Config.html#get-java.lang.String-", "author": "mimaison", "createdAt": "2020-12-08T23:19:44Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,577 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final Duration CONSUMER_POLL_TIMEOUT_MS = Duration.ofMillis(500);\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    protected static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    protected static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    protected Map<String, String> mm2Props = new HashMap<>();\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    protected Map<String, String> primaryWorkerProps = new HashMap<>();\n+    protected Map<String, String> backupWorkerProps = new HashMap<>(); \n+    \n+    @Before\n+    public void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props.putAll(basicMM2Config());\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    @After\n+    public void shutdownClusters() throws Exception {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws Exception {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        waitForConsumingAllRecords(primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        waitForConsumingAllRecords(backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        try (Consumer<byte[], byte[]> primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            primaryConsumer.assign(backupOffsets.keySet());\n+            backupOffsets.forEach(primaryConsumer::seek);\n+            primaryConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            primaryConsumer.commitAsync();\n+\n+            assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+                new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+                CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+        }\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n+\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+ \n+        // Failback consumer group to primary cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            backupConsumer.assign(primaryOffsets.keySet());\n+            primaryOffsets.forEach(backupConsumer::seek);\n+            backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            backupConsumer.commitAsync();\n+        \n+            assertTrue(\"Consumer failedback to zero upstream offset.\", backupConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback to zero downstream offset.\", backupConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback beyond expected upstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Consumer failedback beyond expected downstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        }\n+      \n+        // create more matching topics\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n+\n+        // only produce messages to the first partition\n+        produceMessages(primary, \"test-topic-2\", 1);\n+        produceMessages(backup, \"test-topic-3\", 1);\n+        \n+        // expect total consumed messages equals to NUM_RECORDS_PER_PARTITION\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+\n+    }\n+    \n+    @Test\n+    public void testReplicationWithEmptyPartition() throws Exception {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = Collections.singletonMap(\"group.id\", consumerGroupName);\n+\n+        // create topic\n+        String topic = \"test-topic-with-empty-partition\";\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+\n+        // produce to all test-topic-empty's partitions, except the last partition\n+        produceMessages(primary, topic, NUM_PARTITIONS - 1);\n+        \n+        // consume before starting the connectors so we don't need to wait for discovery\n+        int expectedRecords = NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1);\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic)) {\n+            waitForConsumingAllRecords(primaryConsumer, expectedRecords);\n+        }\n+        \n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        \n+        // sleep few seconds to have MM2 finish replication so that \"end\" consumer will consume some record\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(3));\n+\n+        // consume all records from backup cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                PRIMARY_CLUSTER_ALIAS + \".\" + topic)) {\n+            waitForConsumingAllRecords(backupConsumer, expectedRecords);\n+        }\n+        \n+        Admin backupClient = backup.kafka().createAdminClient();\n+        // retrieve the consumer group offset from backup cluster\n+        Map<TopicPartition, OffsetAndMetadata> remoteOffsets =\n+                backupClient.listConsumerGroupOffsets(consumerGroupName).partitionsToOffsetAndMetadata().get();\n+        // pinpoint the offset of the last partition which does not receive records \n+        OffsetAndMetadata offset = remoteOffsets.get(new TopicPartition(PRIMARY_CLUSTER_ALIAS + \".\" + topic, NUM_PARTITIONS - 1));\n+        // offset of the last partition should exist, but its value should be 0\n+        assertNotNull(\"Offset of last partition was not replicated\", offset);\n+        assertEquals(\"Offset of last partition is not zero\", 0, offset.offset());\n+    }\n+    \n+    @Test\n+    public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testOneWayReplicationWithAutoOffsetSync\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"earliest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                \"test-topic-1\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(primaryConsumer, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // enable automated consumer group offset sync\n+        mm2Props.put(\"sync.group.offsets.enabled\", \"true\");\n+        mm2Props.put(\"sync.group.offsets.interval.seconds\", \"1\");\n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(\n+            consumerProps, \"primary.test-topic-1\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Collections.singletonList(\"primary.test-topic-1\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        ConsumerRecords<byte[], byte[]> records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+\n+        // the size of consumer record should be zero, because the offsets of the same consumer group\n+        // have been automatically synchronized from primary to backup by the background job, so no\n+        // more records to consume from the replicated topic by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+\n+        // now create a new topic in primary cluster\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"primary.test-topic-2\", 1);\n+        // produce some records to the new topic in primary cluster\n+        produceMessages(primary, \"test-topic-2\");\n+\n+        // create a consumer at primary cluster to consume the new topic\n+        try (Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+                \"group.id\", \"consumer-group-1\"), \"test-topic-2\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(consumer1, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n+        backupConsumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+            \"group.id\", consumerGroupName), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Arrays.asList(\"primary.test-topic-1\", \"primary.test-topic-2\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+        // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+        backupConsumer.close();\n+    }\n+    \n+    /*\n+     * launch the connectors on kafka connect cluster and check if they are running\n+     */\n+    private static void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster, \n+            List<Class<? extends Connector>> connectorClasses, MirrorMakerConfig mm2Config, \n+            String primary, String backup) throws InterruptedException {\n+        for (int i = 0; i < connectorClasses.size(); i++) {\n+            String connector = connectorClasses.get(i).getSimpleName();\n+            connectCluster.configureConnector(connector, mm2Config.connectorBaseConfig(\n+                new SourceAndTarget(primary, backup), connectorClasses.get(i)));\n+        }\n+        \n+        // we wait for the connector and tasks to come up for each connector, so that when we do the\n+        // actual testing, we are certain that the tasks are up and running; this will prevent\n+        // flaky tests where the connector and tasks didn't start up in time for the tests to be run\n+        List<String> connectorNames = connectorClasses.stream().map(x -> x.getSimpleName())\n+                .collect(Collectors.toList());\n+        for (String connector : connectorNames) {\n+            connectCluster.assertions().assertConnectorAndAtLeastNumTasksAreRunning(connector, 1,\n+                    \"Connector \" + connector + \" tasks did not start in time on cluster: \" + connectCluster);\n+        }\n+    }\n+ \n+    /*\n+     * delete all topics of the input kafka cluster\n+     */\n+    private static void deleteAllTopics(EmbeddedKafkaCluster cluster) throws Exception {\n+        Admin client = cluster.createAdminClient();\n+        client.deleteTopics(client.listTopics().names().get());\n+    }\n+    \n+    /*\n+     * retrieve the config value based on the input cluster, topic and config name\n+     */\n+    private static String getTopicConfig(EmbeddedKafkaCluster cluster, String topic, String configName) \n+        throws Exception {\n+        Admin client = cluster.createAdminClient();\n+        Collection<ConfigResource> cr =  Collections.singleton(\n+                new ConfigResource(ConfigResource.Type.TOPIC, topic)); \n+\n+        DescribeConfigsResult configsResult = client.describeConfigs(cr);\n+        Config allConfigs = (Config) configsResult.all().get().values().toArray()[0];", "originalCommit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "80019cf4bd61f08f452dc6032f3201dda884feda", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 867c57a0a8..51a5b12b15 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -36,6 +36,7 @@ import org.apache.kafka.connect.mirror.SourceAndTarget;\n import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.connect.util.clusters.UngracefulShutdownException;\n import org.apache.kafka.test.IntegrationTest;\n import static org.apache.kafka.test.TestUtils.waitForCondition;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg4NTY1MQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538885651", "bodyText": "We could use a \"for each\" loop here, something like:\nfor (Class<? extends Connector> connector : connectorClasses) {\n    connectCluster.configureConnector(connector.getSimpleName(), mm2Config.connectorBaseConfig(\n        new SourceAndTarget(primary, backup), connector));\n}", "author": "mimaison", "createdAt": "2020-12-08T23:24:54Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,577 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final Duration CONSUMER_POLL_TIMEOUT_MS = Duration.ofMillis(500);\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    protected static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    protected static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    protected Map<String, String> mm2Props = new HashMap<>();\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    protected Map<String, String> primaryWorkerProps = new HashMap<>();\n+    protected Map<String, String> backupWorkerProps = new HashMap<>(); \n+    \n+    @Before\n+    public void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props.putAll(basicMM2Config());\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    @After\n+    public void shutdownClusters() throws Exception {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws Exception {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        waitForConsumingAllRecords(primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        waitForConsumingAllRecords(backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        try (Consumer<byte[], byte[]> primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            primaryConsumer.assign(backupOffsets.keySet());\n+            backupOffsets.forEach(primaryConsumer::seek);\n+            primaryConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            primaryConsumer.commitAsync();\n+\n+            assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+                new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+                CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+        }\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n+\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+ \n+        // Failback consumer group to primary cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            backupConsumer.assign(primaryOffsets.keySet());\n+            primaryOffsets.forEach(backupConsumer::seek);\n+            backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            backupConsumer.commitAsync();\n+        \n+            assertTrue(\"Consumer failedback to zero upstream offset.\", backupConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback to zero downstream offset.\", backupConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback beyond expected upstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Consumer failedback beyond expected downstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        }\n+      \n+        // create more matching topics\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n+\n+        // only produce messages to the first partition\n+        produceMessages(primary, \"test-topic-2\", 1);\n+        produceMessages(backup, \"test-topic-3\", 1);\n+        \n+        // expect total consumed messages equals to NUM_RECORDS_PER_PARTITION\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+\n+    }\n+    \n+    @Test\n+    public void testReplicationWithEmptyPartition() throws Exception {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = Collections.singletonMap(\"group.id\", consumerGroupName);\n+\n+        // create topic\n+        String topic = \"test-topic-with-empty-partition\";\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+\n+        // produce to all test-topic-empty's partitions, except the last partition\n+        produceMessages(primary, topic, NUM_PARTITIONS - 1);\n+        \n+        // consume before starting the connectors so we don't need to wait for discovery\n+        int expectedRecords = NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1);\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic)) {\n+            waitForConsumingAllRecords(primaryConsumer, expectedRecords);\n+        }\n+        \n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        \n+        // sleep few seconds to have MM2 finish replication so that \"end\" consumer will consume some record\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(3));\n+\n+        // consume all records from backup cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                PRIMARY_CLUSTER_ALIAS + \".\" + topic)) {\n+            waitForConsumingAllRecords(backupConsumer, expectedRecords);\n+        }\n+        \n+        Admin backupClient = backup.kafka().createAdminClient();\n+        // retrieve the consumer group offset from backup cluster\n+        Map<TopicPartition, OffsetAndMetadata> remoteOffsets =\n+                backupClient.listConsumerGroupOffsets(consumerGroupName).partitionsToOffsetAndMetadata().get();\n+        // pinpoint the offset of the last partition which does not receive records \n+        OffsetAndMetadata offset = remoteOffsets.get(new TopicPartition(PRIMARY_CLUSTER_ALIAS + \".\" + topic, NUM_PARTITIONS - 1));\n+        // offset of the last partition should exist, but its value should be 0\n+        assertNotNull(\"Offset of last partition was not replicated\", offset);\n+        assertEquals(\"Offset of last partition is not zero\", 0, offset.offset());\n+    }\n+    \n+    @Test\n+    public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testOneWayReplicationWithAutoOffsetSync\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"earliest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                \"test-topic-1\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(primaryConsumer, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // enable automated consumer group offset sync\n+        mm2Props.put(\"sync.group.offsets.enabled\", \"true\");\n+        mm2Props.put(\"sync.group.offsets.interval.seconds\", \"1\");\n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(\n+            consumerProps, \"primary.test-topic-1\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Collections.singletonList(\"primary.test-topic-1\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        ConsumerRecords<byte[], byte[]> records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+\n+        // the size of consumer record should be zero, because the offsets of the same consumer group\n+        // have been automatically synchronized from primary to backup by the background job, so no\n+        // more records to consume from the replicated topic by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+\n+        // now create a new topic in primary cluster\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"primary.test-topic-2\", 1);\n+        // produce some records to the new topic in primary cluster\n+        produceMessages(primary, \"test-topic-2\");\n+\n+        // create a consumer at primary cluster to consume the new topic\n+        try (Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+                \"group.id\", \"consumer-group-1\"), \"test-topic-2\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(consumer1, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n+        backupConsumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+            \"group.id\", consumerGroupName), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Arrays.asList(\"primary.test-topic-1\", \"primary.test-topic-2\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+        // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+        backupConsumer.close();\n+    }\n+    \n+    /*\n+     * launch the connectors on kafka connect cluster and check if they are running\n+     */\n+    private static void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster, \n+            List<Class<? extends Connector>> connectorClasses, MirrorMakerConfig mm2Config, \n+            String primary, String backup) throws InterruptedException {\n+        for (int i = 0; i < connectorClasses.size(); i++) {", "originalCommit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "80019cf4bd61f08f452dc6032f3201dda884feda", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 867c57a0a8..51a5b12b15 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -36,6 +36,7 @@ import org.apache.kafka.connect.mirror.SourceAndTarget;\n import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.connect.util.clusters.UngracefulShutdownException;\n import org.apache.kafka.test.IntegrationTest;\n import static org.apache.kafka.test.TestUtils.waitForCondition;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg4NzI2MA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538887260", "bodyText": "This intermediate List is not really useful. We could just change the loop below to iterate over the connector classes and call getSimpleName() on each of them", "author": "mimaison", "createdAt": "2020-12-08T23:27:17Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,577 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final Duration CONSUMER_POLL_TIMEOUT_MS = Duration.ofMillis(500);\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n+    protected static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n+    protected static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n+    private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n+            Arrays.asList(MirrorSourceConnector.class, MirrorCheckpointConnector.class, MirrorHeartbeatConnector.class);\n+\n+    protected Map<String, String> mm2Props = new HashMap<>();\n+    private MirrorMakerConfig mm2Config; \n+    private EmbeddedConnectCluster primary;\n+    private EmbeddedConnectCluster backup;\n+    \n+    private final AtomicBoolean exited = new AtomicBoolean(false);\n+    protected Properties primaryBrokerProps = new Properties();\n+    protected Properties backupBrokerProps = new Properties();\n+    protected Map<String, String> primaryWorkerProps = new HashMap<>();\n+    protected Map<String, String> backupWorkerProps = new HashMap<>(); \n+    \n+    @Before\n+    public void startClusters() throws InterruptedException {\n+        primaryBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        backupBrokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        \n+        mm2Props.putAll(basicMM2Config());\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props); \n+        primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS));\n+        backupWorkerProps.putAll(mm2Config.workerConfig(new SourceAndTarget(PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS)));\n+        \n+        primary = new EmbeddedConnectCluster.Builder()\n+                .name(PRIMARY_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(primaryBrokerProps)\n+                .workerProps(primaryWorkerProps)\n+                .build();\n+\n+        backup = new EmbeddedConnectCluster.Builder()\n+                .name(BACKUP_CLUSTER_ALIAS + \"-connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .numBrokers(1)\n+                .brokerProps(backupBrokerProps)\n+                .workerProps(backupWorkerProps)\n+                .build();\n+        \n+        primary.start();\n+        primary.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + PRIMARY_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+        \n+        backup.start();\n+        backup.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS,\n+                \"Workers of \" + BACKUP_CLUSTER_ALIAS + \"-connect-cluster did not start in time.\");\n+\n+        createTopics();\n+ \n+        warmUpConsumer();\n+        \n+        log.info(PRIMARY_CLUSTER_ALIAS + \" REST service: {}\", primary.endpointForResource(\"connectors\"));\n+        log.info(BACKUP_CLUSTER_ALIAS + \" REST service: {}\", backup.endpointForResource(\"connectors\"));\n+        log.info(PRIMARY_CLUSTER_ALIAS + \" brokers: {}\", primary.kafka().bootstrapServers());\n+        log.info(BACKUP_CLUSTER_ALIAS + \" brokers: {}\", backup.kafka().bootstrapServers());\n+        \n+        // now that the brokers are running, we can finish setting up the Connectors\n+        mm2Props.put(PRIMARY_CLUSTER_ALIAS + \".bootstrap.servers\", primary.kafka().bootstrapServers());\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \".bootstrap.servers\", backup.kafka().bootstrapServers());\n+        \n+        Exit.setExitProcedure((status, errorCode) -> exited.set(true));\n+    }\n+    \n+    @After\n+    public void shutdownClusters() throws Exception {\n+        for (String x : primary.connectors()) {\n+            primary.deleteConnector(x);\n+        }\n+        for (String x : backup.connectors()) {\n+            backup.deleteConnector(x);\n+        }\n+        deleteAllTopics(primary.kafka());\n+        deleteAllTopics(backup.kafka());\n+        primary.stop();\n+        backup.stop();\n+        try {\n+            assertFalse(exited.get());\n+        } finally {\n+            Exit.resetExitProcedure();\n+        }\n+    }\n+    \n+    @Test\n+    public void testReplication() throws Exception {\n+        produceMessages(primary, \"test-topic-1\");\n+        produceMessages(backup, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testReplication\";\n+        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"latest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        waitForConsumingAllRecords(primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        waitForConsumingAllRecords(backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\"), 0);\n+        \n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        waitUntilMirrorMakerIsRunning(primary, CONNECTOR_LIST, mm2Config, BACKUP_CLUSTER_ALIAS, PRIMARY_CLUSTER_ALIAS); \n+\n+        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(PRIMARY_CLUSTER_ALIAS));\n+        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(BACKUP_CLUSTER_ALIAS));\n+        \n+        assertEquals(\"topic config was not synced\", TopicConfig.CLEANUP_POLICY_COMPACT, \n+                getTopicConfig(backup.kafka(), \"primary.test-topic-1\", TopicConfig.CLEANUP_POLICY_CONFIG));\n+        \n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n+        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n+        \n+        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n+        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n+            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n+        \n+        assertTrue(\"Heartbeats were not emitted to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not emitted to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\", backup.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n+        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\", primary.kafka().consume(1,\n+            RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n+        \n+        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(PRIMARY_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(PRIMARY_CLUSTER_ALIAS));\n+        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(BACKUP_CLUSTER_ALIAS));\n+        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(BACKUP_CLUSTER_ALIAS));\n+        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\", backup.kafka().consume(1,\n+            CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n+\n+        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, PRIMARY_CLUSTER_ALIAS,\n+            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+\n+        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n+            new TopicPartition(\"primary.test-topic-1\", 0)));\n+\n+        // Failover consumer group to backup cluster.\n+        try (Consumer<byte[], byte[]> primaryConsumer = backup.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            primaryConsumer.assign(backupOffsets.keySet());\n+            backupOffsets.forEach(primaryConsumer::seek);\n+            primaryConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            primaryConsumer.commitAsync();\n+\n+            assertTrue(\"Consumer failedover to zero offset.\", primaryConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedover beyond expected offset.\", primaryConsumer.position(\n+                new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n+                CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n+        }\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n+\n+        waitForCondition(() -> {\n+            return primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n+        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n+\n+        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, BACKUP_CLUSTER_ALIAS,\n+                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n+ \n+        // Failback consumer group to primary cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = primary.kafka().createConsumer(Collections.singletonMap(\"group.id\", consumerGroupName))) {\n+            backupConsumer.assign(primaryOffsets.keySet());\n+            primaryOffsets.forEach(backupConsumer::seek);\n+            backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+            backupConsumer.commitAsync();\n+        \n+            assertTrue(\"Consumer failedback to zero upstream offset.\", backupConsumer.position(new TopicPartition(\"test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback to zero downstream offset.\", backupConsumer.position(new TopicPartition(\"backup.test-topic-1\", 0)) > 0);\n+            assertTrue(\"Consumer failedback beyond expected upstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+            assertTrue(\"Consumer failedback beyond expected downstream offset.\", backupConsumer.position(\n+                new TopicPartition(\"backup.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n+        }\n+      \n+        // create more matching topics\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"test-topic-3\", NUM_PARTITIONS);\n+\n+        // only produce messages to the first partition\n+        produceMessages(primary, \"test-topic-2\", 1);\n+        produceMessages(backup, \"test-topic-3\", 1);\n+        \n+        // expect total consumed messages equals to NUM_RECORDS_PER_PARTITION\n+        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-2\").count());\n+        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, RECORD_TRANSFER_DURATION_MS, \"test-topic-3\").count());\n+\n+        assertEquals(\"New topic was not replicated to primary cluster.\", NUM_RECORDS_PER_PARTITION,\n+            primary.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-3\").count());\n+        assertEquals(\"New topic was not replicated to backup cluster.\", NUM_RECORDS_PER_PARTITION,\n+            backup.kafka().consume(NUM_RECORDS_PER_PARTITION, 2 * RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-2\").count());\n+\n+    }\n+    \n+    @Test\n+    public void testReplicationWithEmptyPartition() throws Exception {\n+        String consumerGroupName = \"consumer-group-testReplicationWithEmptyPartition\";\n+        Map<String, Object> consumerProps  = Collections.singletonMap(\"group.id\", consumerGroupName);\n+\n+        // create topic\n+        String topic = \"test-topic-with-empty-partition\";\n+        primary.kafka().createTopic(topic, NUM_PARTITIONS);\n+\n+        // produce to all test-topic-empty's partitions, except the last partition\n+        produceMessages(primary, topic, NUM_PARTITIONS - 1);\n+        \n+        // consume before starting the connectors so we don't need to wait for discovery\n+        int expectedRecords = NUM_RECORDS_PER_PARTITION * (NUM_PARTITIONS - 1);\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, topic)) {\n+            waitForConsumingAllRecords(primaryConsumer, expectedRecords);\n+        }\n+        \n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+        \n+        // sleep few seconds to have MM2 finish replication so that \"end\" consumer will consume some record\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(3));\n+\n+        // consume all records from backup cluster\n+        try (Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                PRIMARY_CLUSTER_ALIAS + \".\" + topic)) {\n+            waitForConsumingAllRecords(backupConsumer, expectedRecords);\n+        }\n+        \n+        Admin backupClient = backup.kafka().createAdminClient();\n+        // retrieve the consumer group offset from backup cluster\n+        Map<TopicPartition, OffsetAndMetadata> remoteOffsets =\n+                backupClient.listConsumerGroupOffsets(consumerGroupName).partitionsToOffsetAndMetadata().get();\n+        // pinpoint the offset of the last partition which does not receive records \n+        OffsetAndMetadata offset = remoteOffsets.get(new TopicPartition(PRIMARY_CLUSTER_ALIAS + \".\" + topic, NUM_PARTITIONS - 1));\n+        // offset of the last partition should exist, but its value should be 0\n+        assertNotNull(\"Offset of last partition was not replicated\", offset);\n+        assertEquals(\"Offset of last partition is not zero\", 0, offset.offset());\n+    }\n+    \n+    @Test\n+    public void testOneWayReplicationWithAutoOffsetSync() throws InterruptedException {\n+        produceMessages(primary, \"test-topic-1\");\n+        String consumerGroupName = \"consumer-group-testOneWayReplicationWithAutoOffsetSync\";\n+        Map<String, Object> consumerProps  = new HashMap<String, Object>() {{\n+                put(\"group.id\", consumerGroupName);\n+                put(\"auto.offset.reset\", \"earliest\");\n+            }};\n+        // create consumers before starting the connectors so we don't need to wait for discovery\n+        try (Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \n+                \"test-topic-1\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(primaryConsumer, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // enable automated consumer group offset sync\n+        mm2Props.put(\"sync.group.offsets.enabled\", \"true\");\n+        mm2Props.put(\"sync.group.offsets.interval.seconds\", \"1\");\n+        // one way replication from primary to backup\n+        mm2Props.put(BACKUP_CLUSTER_ALIAS + \"->\" + PRIMARY_CLUSTER_ALIAS + \".enabled\", \"false\");\n+\n+        mm2Config = new MirrorMakerConfig(mm2Props);\n+\n+        waitUntilMirrorMakerIsRunning(backup, CONNECTOR_LIST, mm2Config, PRIMARY_CLUSTER_ALIAS, BACKUP_CLUSTER_ALIAS);\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume 1 topic\n+        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(\n+            consumerProps, \"primary.test-topic-1\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Collections.singletonList(\"primary.test-topic-1\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        ConsumerRecords<byte[], byte[]> records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+\n+        // the size of consumer record should be zero, because the offsets of the same consumer group\n+        // have been automatically synchronized from primary to backup by the background job, so no\n+        // more records to consume from the replicated topic by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+\n+        // now create a new topic in primary cluster\n+        primary.kafka().createTopic(\"test-topic-2\", NUM_PARTITIONS);\n+        backup.kafka().createTopic(\"primary.test-topic-2\", 1);\n+        // produce some records to the new topic in primary cluster\n+        produceMessages(primary, \"test-topic-2\");\n+\n+        // create a consumer at primary cluster to consume the new topic\n+        try (Consumer<byte[], byte[]> consumer1 = primary.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+                \"group.id\", \"consumer-group-1\"), \"test-topic-2\")) {\n+            // we need to wait for consuming all the records for MM2 replicating the expected offsets\n+            waitForConsumingAllRecords(consumer1, NUM_RECORDS_PRODUCED);\n+        }\n+\n+        // create a consumer at backup cluster with same consumer group Id to consume old and new topic\n+        backupConsumer = backup.kafka().createConsumerAndSubscribeTo(Collections.singletonMap(\n+            \"group.id\", consumerGroupName), \"primary.test-topic-1\", \"primary.test-topic-2\");\n+\n+        waitForConsumerGroupOffsetSync(backup, backupConsumer, Arrays.asList(\"primary.test-topic-1\", \"primary.test-topic-2\"), \n+            consumerGroupName, NUM_RECORDS_PRODUCED);\n+\n+        records = backupConsumer.poll(CONSUMER_POLL_TIMEOUT_MS);\n+        // similar reasoning as above, no more records to consume by the same consumer group at backup cluster\n+        assertEquals(\"consumer record size is not zero\", 0, records.count());\n+        backupConsumer.close();\n+    }\n+    \n+    /*\n+     * launch the connectors on kafka connect cluster and check if they are running\n+     */\n+    private static void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster, \n+            List<Class<? extends Connector>> connectorClasses, MirrorMakerConfig mm2Config, \n+            String primary, String backup) throws InterruptedException {\n+        for (int i = 0; i < connectorClasses.size(); i++) {\n+            String connector = connectorClasses.get(i).getSimpleName();\n+            connectCluster.configureConnector(connector, mm2Config.connectorBaseConfig(\n+                new SourceAndTarget(primary, backup), connectorClasses.get(i)));\n+        }\n+        \n+        // we wait for the connector and tasks to come up for each connector, so that when we do the\n+        // actual testing, we are certain that the tasks are up and running; this will prevent\n+        // flaky tests where the connector and tasks didn't start up in time for the tests to be run\n+        List<String> connectorNames = connectorClasses.stream().map(x -> x.getSimpleName())", "originalCommit": "ff7de40fcd6d370354d032b00f6d9ea656d15e22", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg4NzkwOA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r538887908", "bodyText": "I'm assuming it's worth keeping this 2nd loop separate from the first one for performance reasons. At first glance it looks strange to iterate over the same collection twice in a row", "author": "mimaison", "createdAt": "2020-12-08T23:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg4NzI2MA=="}], "type": "inlineReview", "revised_code": {"commit": "80019cf4bd61f08f452dc6032f3201dda884feda", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex 867c57a0a8..51a5b12b15 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -36,6 +36,7 @@ import org.apache.kafka.connect.mirror.SourceAndTarget;\n import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.connect.util.clusters.UngracefulShutdownException;\n import org.apache.kafka.test.IntegrationTest;\n import static org.apache.kafka.test.TestUtils.waitForCondition;\n \n"}}, {"oid": "80019cf4bd61f08f452dc6032f3201dda884feda", "url": "https://github.com/apache/kafka/commit/80019cf4bd61f08f452dc6032f3201dda884feda", "message": "refactor MM2 integration tests", "committedDate": "2020-12-09T00:37:43Z", "type": "forcePushed"}, {"oid": "a4af47b3f582ca86f9720057768e60f1632613a4", "url": "https://github.com/apache/kafka/commit/a4af47b3f582ca86f9720057768e60f1632613a4", "message": "refactor MM2 integration tests", "committedDate": "2020-12-09T01:27:52Z", "type": "forcePushed"}, {"oid": "cd77b18a39f39926291cf53ab595805b1dee2a56", "url": "https://github.com/apache/kafka/commit/cd77b18a39f39926291cf53ab595805b1dee2a56", "message": "refactor MM2 integration tests", "committedDate": "2020-12-09T01:36:21Z", "type": "forcePushed"}, {"oid": "7b8957abcd82571fd7677f3abedf2ffc2ff0075d", "url": "https://github.com/apache/kafka/commit/7b8957abcd82571fd7677f3abedf2ffc2ff0075d", "message": "refactor MM2 integration tests", "committedDate": "2020-12-09T23:24:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDIyNjM4OQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r540226389", "bodyText": "We don't need this field, this could be a local in startClusters()", "author": "mimaison", "createdAt": "2020-12-10T14:46:31Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.config.types.Password;\n+import org.apache.kafka.common.network.Mode;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestSslUtils;\n+import org.apache.kafka.test.TestUtils;\n+import kafka.server.KafkaConfig$;\n+\n+import org.junit.experimental.categories.Category;\n+import org.junit.Before;\n+\n+/**\n+ * Tests MM2 replication with SSL enabled at backup kafka cluster\n+ */\n+@Category(IntegrationTest.class)\n+public class MirrorConnectorsIntegrationSSLTest extends MirrorConnectorsIntegrationBaseTest {\n+    \n+    private static Map<String, Object> sslConfig;", "originalCommit": "7b8957abcd82571fd7677f3abedf2ffc2ff0075d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f57a8c9a494e10811068be1d4f26bbb6816292ee", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java\nindex fa7f0ddec5..0e268d428f 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java\n\n@@ -37,12 +37,10 @@ import org.junit.Before;\n  */\n @Category(IntegrationTest.class)\n public class MirrorConnectorsIntegrationSSLTest extends MirrorConnectorsIntegrationBaseTest {\n-    \n-    private static Map<String, Object> sslConfig;\n \n     @Before\n     public void startClusters() throws Exception {\n-        sslConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, TestUtils.tempFile(), \"testCert\");\n+        Map<String, Object> sslConfig = TestSslUtils.createSslConfig(false, true, Mode.SERVER, TestUtils.tempFile(), \"testCert\");\n         // enable SSL on backup kafka broker\n         backupBrokerProps.put(KafkaConfig$.MODULE$.ListenersProp(), \"SSL://localhost:0\");\n         backupBrokerProps.put(KafkaConfig$.MODULE$.InterBrokerListenerNameProp(), \"SSL\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDIyNjYxMQ==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r540226611", "bodyText": "This field is unused", "author": "mimaison", "createdAt": "2020-12-10T14:46:51Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.mirror.integration;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.DescribeConfigsResult;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.utils.Exit;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.mirror.MirrorClient;\n+import org.apache.kafka.connect.mirror.MirrorHeartbeatConnector;\n+import org.apache.kafka.connect.mirror.MirrorMakerConfig;\n+import org.apache.kafka.connect.mirror.MirrorSourceConnector;\n+import org.apache.kafka.connect.mirror.SourceAndTarget;\n+import org.apache.kafka.connect.mirror.MirrorCheckpointConnector;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n+import org.apache.kafka.connect.util.clusters.UngracefulShutdownException;\n+import org.apache.kafka.test.IntegrationTest;\n+import static org.apache.kafka.test.TestUtils.waitForCondition;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.After;\n+import org.junit.Before;\n+\n+import static org.apache.kafka.connect.mirror.TestUtils.generateRecords;\n+\n+/**\n+ * Tests MM2 replication and failover/failback logic.\n+ *\n+ * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n+ * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n+ * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n+ * between clusters during this failover and failback.\n+ */\n+@Category(IntegrationTest.class)\n+public abstract class MirrorConnectorsIntegrationBaseTest {\n+    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationBaseTest.class);\n+    \n+    private static final int NUM_RECORDS_PER_PARTITION = 10;\n+    private static final int NUM_PARTITIONS = 10;\n+    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n+    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n+    private static final int CHECKPOINT_DURATION_MS = 20_000;\n+    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n+    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n+    private static final int NUM_WORKERS = 3;\n+    private static final Duration CONSUMER_POLL_TIMEOUT_MS = Duration.ofMillis(500);\n+    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;", "originalCommit": "7b8957abcd82571fd7677f3abedf2ffc2ff0075d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f57a8c9a494e10811068be1d4f26bbb6816292ee", "chunk": "diff --git a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\nindex c056f53a9b..c2a8ab60da 100644\n--- a/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n+++ b/connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java\n\n@@ -85,7 +85,6 @@ public abstract class MirrorConnectorsIntegrationBaseTest {\n     private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n     private static final int NUM_WORKERS = 3;\n     private static final Duration CONSUMER_POLL_TIMEOUT_MS = Duration.ofMillis(500);\n-    private static final int BROKER_RESTART_TIMEOUT_MS = 10_000;\n     protected static final String PRIMARY_CLUSTER_ALIAS = \"primary\";\n     protected static final String BACKUP_CLUSTER_ALIAS = \"backup\";\n     private static final List<Class<? extends Connector>> CONNECTOR_LIST = \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDIzMDUxNA==", "url": "https://github.com/apache/kafka/pull/9224#discussion_r540230514", "bodyText": "It looks like we lost these lines and maybe some others too. These came from #9698. Can you make sure we don't miss any existing logic?", "author": "mimaison", "createdAt": "2020-12-10T14:51:30Z", "path": "connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java", "diffHunk": "@@ -1,595 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.connect.mirror;\n-\n-import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.consumer.Consumer;\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.clients.consumer.ConsumerRecords;\n-import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n-import org.apache.kafka.connect.util.clusters.EmbeddedKafkaCluster;\n-import org.apache.kafka.connect.util.clusters.UngracefulShutdownException;\n-import org.apache.kafka.test.IntegrationTest;\n-import org.apache.kafka.common.utils.Exit;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.experimental.categories.Category;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.Set;\n-import java.util.concurrent.TimeoutException;\n-import java.util.stream.Collectors;\n-import java.util.stream.IntStream;\n-\n-import static org.apache.kafka.test.TestUtils.waitForCondition;\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertTrue;\n-\n-/**\n- * Tests MM2 replication and failover/failback logic.\n- *\n- * MM2 is configured with active/active replication between two Kafka clusters. Tests validate that\n- * records sent to either cluster arrive at the other cluster. Then, a consumer group is migrated from\n- * one cluster to the other and back. Tests validate that consumer offsets are translated and replicated\n- * between clusters during this failover and failback.\n- */\n-@Category(IntegrationTest.class)\n-public class MirrorConnectorsIntegrationTest {\n-\n-    private static final Logger log = LoggerFactory.getLogger(MirrorConnectorsIntegrationTest.class);\n-\n-    private static final int NUM_RECORDS_PER_PARTITION = 10;\n-    private static final int NUM_PARTITIONS = 10;\n-    private static final int NUM_RECORDS_PRODUCED = NUM_PARTITIONS * NUM_RECORDS_PER_PARTITION;\n-    private static final int RECORD_TRANSFER_DURATION_MS = 30_000;\n-    private static final int CHECKPOINT_DURATION_MS = 20_000;\n-    private static final int RECORD_CONSUME_DURATION_MS = 20_000;\n-    private static final int OFFSET_SYNC_DURATION_MS = 30_000;\n-\n-    private volatile boolean shuttingDown;\n-    private Map<String, String> mm2Props;\n-    private MirrorMakerConfig mm2Config;\n-    private EmbeddedConnectCluster primary;\n-    private EmbeddedConnectCluster backup;\n-\n-    private Exit.Procedure exitProcedure;\n-    private Exit.Procedure haltProcedure;\n-\n-    @Before\n-    public void setup() throws InterruptedException {\n-        shuttingDown = false;\n-        exitProcedure = (code, message) -> {\n-            if (shuttingDown) {\n-                // ignore this since we're shutting down Connect and Kafka and timing isn't always great\n-                return;\n-            }\n-            if (code != 0) {\n-                String exitMessage = \"Abrupt service exit with code \" + code + \" and message \" + message;\n-                log.warn(exitMessage);\n-                throw new UngracefulShutdownException(exitMessage);\n-            }\n-        };\n-        haltProcedure = (code, message) -> {\n-            if (shuttingDown) {\n-                // ignore this since we're shutting down Connect and Kafka and timing isn't always great\n-                return;\n-            }\n-            if (code != 0) {\n-                String haltMessage = \"Abrupt service halt with code \" + code + \" and message \" + message;\n-                log.warn(haltMessage);\n-                throw new UngracefulShutdownException(haltMessage);\n-            }\n-        };\n-        // Override the exit and halt procedure that Connect and Kafka will use. For these integration tests,\n-        // we don't want to exit the JVM and instead simply want to fail the test\n-        Exit.setExitProcedure(exitProcedure);\n-        Exit.setHaltProcedure(haltProcedure);\n-\n-        Properties brokerProps = new Properties();\n-        brokerProps.put(\"auto.create.topics.enable\", \"false\");\n-\n-        mm2Props = new HashMap<>();\n-        mm2Props.put(\"clusters\", \"primary, backup\");\n-        mm2Props.put(\"max.tasks\", \"10\");\n-        mm2Props.put(\"topics\", \"test-topic-.*, primary.test-topic-.*, backup.test-topic-.*\");\n-        mm2Props.put(\"groups\", \"consumer-group-.*\");\n-        mm2Props.put(\"primary->backup.enabled\", \"true\");\n-        mm2Props.put(\"backup->primary.enabled\", \"true\");\n-        mm2Props.put(\"sync.topic.acls.enabled\", \"false\");\n-        mm2Props.put(\"emit.checkpoints.interval.seconds\", \"1\");\n-        mm2Props.put(\"emit.heartbeats.interval.seconds\", \"1\");\n-        mm2Props.put(\"refresh.topics.interval.seconds\", \"1\");\n-        mm2Props.put(\"refresh.groups.interval.seconds\", \"1\");\n-        mm2Props.put(\"checkpoints.topic.replication.factor\", \"1\");\n-        mm2Props.put(\"heartbeats.topic.replication.factor\", \"1\");\n-        mm2Props.put(\"offset-syncs.topic.replication.factor\", \"1\");\n-        mm2Props.put(\"config.storage.replication.factor\", \"1\");\n-        mm2Props.put(\"offset.storage.replication.factor\", \"1\");\n-        mm2Props.put(\"status.storage.replication.factor\", \"1\");\n-        mm2Props.put(\"replication.factor\", \"1\");\n-        \n-        mm2Config = new MirrorMakerConfig(mm2Props); \n-        Map<String, String> primaryWorkerProps = mm2Config.workerConfig(new SourceAndTarget(\"backup\", \"primary\"));\n-        Map<String, String> backupWorkerProps = mm2Config.workerConfig(new SourceAndTarget(\"primary\", \"backup\"));\n-\n-        primary = new EmbeddedConnectCluster.Builder()\n-                .name(\"primary-connect-cluster\")\n-                .numWorkers(3)\n-                .numBrokers(1)\n-                .brokerProps(brokerProps)\n-                .workerProps(primaryWorkerProps)\n-                .maskExitProcedures(false)\n-                .build();\n-\n-        backup = new EmbeddedConnectCluster.Builder()\n-                .name(\"backup-connect-cluster\")\n-                .numWorkers(3)\n-                .numBrokers(1)\n-                .brokerProps(brokerProps)\n-                .workerProps(backupWorkerProps)\n-                .maskExitProcedures(false)\n-                .build();\n-\n-        primary.start();\n-        primary.assertions().assertAtLeastNumWorkersAreUp(3,\n-                \"Workers of primary-connect-cluster did not start in time.\");\n-        backup.start();\n-        backup.assertions().assertAtLeastNumWorkersAreUp(3,\n-                \"Workers of backup-connect-cluster did not start in time.\");\n-\n-        // create these topics before starting the connectors so we don't need to wait for discovery\n-        primary.kafka().createTopic(\"test-topic-1\", NUM_PARTITIONS);\n-        primary.kafka().createTopic(\"backup.test-topic-1\", 1);\n-        primary.kafka().createTopic(\"heartbeats\", 1);\n-        backup.kafka().createTopic(\"test-topic-1\", NUM_PARTITIONS);\n-        backup.kafka().createTopic(\"primary.test-topic-1\", 1);\n-        backup.kafka().createTopic(\"heartbeats\", 1);\n-\n-        // produce to all partitions of test-topic-1\n-        produceMessages(primary, \"test-topic-1\", \"message-1-\");\n-        produceMessages(backup, \"test-topic-1\", \"message-2-\");\n-\n-        // Generate some consumer activity on both clusters to ensure the checkpoint connector always starts promptly\n-        Map<String, Object> dummyProps = Collections.singletonMap(\"group.id\", \"consumer-group-dummy\");\n-        Consumer<byte[], byte[]> dummyConsumer = primary.kafka().createConsumerAndSubscribeTo(dummyProps, \"test-topic-1\");\n-        consumeAllMessages(dummyConsumer);\n-        dummyConsumer.close();\n-        dummyConsumer = backup.kafka().createConsumerAndSubscribeTo(dummyProps, \"test-topic-1\");\n-        consumeAllMessages(dummyConsumer);\n-        dummyConsumer.close();\n-\n-        log.info(\"primary REST service: {}\", primary.endpointForResource(\"connectors\"));\n-        log.info(\"backup REST service: {}\", backup.endpointForResource(\"connectors\"));\n- \n-        log.info(\"primary brokers: {}\", primary.kafka().bootstrapServers());\n-        log.info(\"backup brokers: {}\", backup.kafka().bootstrapServers());\n-        \n-        // now that the brokers are running, we can finish setting up the Connectors\n-        mm2Props.put(\"primary.bootstrap.servers\", primary.kafka().bootstrapServers());\n-        mm2Props.put(\"backup.bootstrap.servers\", backup.kafka().bootstrapServers());\n-        mm2Config = new MirrorMakerConfig(mm2Props);\n-    }\n-\n-\n-    private void waitUntilMirrorMakerIsRunning(EmbeddedConnectCluster connectCluster,\n-        MirrorMakerConfig mm2Config, String primary, String backup) throws InterruptedException {\n-\n-        connectCluster.configureConnector(\"MirrorSourceConnector\",\n-                mm2Config.connectorBaseConfig(new SourceAndTarget(primary, backup), MirrorSourceConnector.class));\n-        connectCluster.configureConnector(\"MirrorCheckpointConnector\",\n-                mm2Config.connectorBaseConfig(new SourceAndTarget(primary, backup), MirrorCheckpointConnector.class));\n-        connectCluster.configureConnector(\"MirrorHeartbeatConnector\",\n-                mm2Config.connectorBaseConfig(new SourceAndTarget(primary, backup), MirrorHeartbeatConnector.class));\n-\n-        // we wait for the connector and tasks to come up for each connector, so that when we do the\n-        // actual testing, we are certain that the tasks are up and running; this will prevent\n-        // flaky tests where the connector and tasks didn't start up in time for the tests to be\n-        // run\n-        Set<String> connectorNames = new HashSet<>(Arrays.asList(\"MirrorSourceConnector\",\n-                                                                 \"MirrorCheckpointConnector\", \"MirrorHeartbeatConnector\"));\n-\n-        for (String connector : connectorNames) {\n-            connectCluster.assertions().assertConnectorAndAtLeastNumTasksAreRunning(connector, 1,\n-                    \"Connector \" + connector + \" tasks did not start in time on cluster: \" + connectCluster);\n-        }\n-    }\n-\n-    @After\n-    public void close() {\n-        try {\n-            for (String x : primary.connectors()) {\n-                primary.deleteConnector(x);\n-            }\n-            for (String x : backup.connectors()) {\n-                backup.deleteConnector(x);\n-            }\n-            deleteAllTopics(primary.kafka());\n-            deleteAllTopics(backup.kafka());\n-        } finally {\n-            shuttingDown = true;\n-            try {\n-                try {\n-                    primary.stop();\n-                } finally {\n-                    backup.stop();\n-                }\n-            } finally {\n-                Exit.resetExitProcedure();\n-                Exit.resetHaltProcedure();\n-            }\n-        }\n-    }\n-\n-    @Test\n-    public void testReplication() throws InterruptedException {\n-        String consumerGroupName = \"consumer-group-testReplication\";\n-        Map<String, Object> consumerProps = new HashMap<String, Object>() {{\n-                put(\"group.id\", consumerGroupName);\n-                put(\"auto.offset.reset\", \"latest\");\n-            }};\n-\n-        // create consumers before starting the connectors so we don't need to wait for discovery\n-        Consumer<byte[], byte[]> primaryConsumer = primary.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n-        consumeAllMessages(primaryConsumer, 0);\n-        primaryConsumer.close();\n-\n-        Consumer<byte[], byte[]> backupConsumer = backup.kafka().createConsumerAndSubscribeTo(consumerProps, \"test-topic-1\");\n-        consumeAllMessages(backupConsumer, 0);\n-        backupConsumer.close();\n-\n-        waitUntilMirrorMakerIsRunning(backup, mm2Config, \"primary\", \"backup\");\n-        waitUntilMirrorMakerIsRunning(primary, mm2Config, \"backup\", \"primary\");\n-        MirrorClient primaryClient = new MirrorClient(mm2Config.clientConfig(\"primary\"));\n-        MirrorClient backupClient = new MirrorClient(mm2Config.clientConfig(\"backup\"));\n-\n-        assertEquals(\"Records were not produced to primary cluster.\", NUM_RECORDS_PRODUCED,\n-            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n-        assertEquals(\"Records were not replicated to backup cluster.\", NUM_RECORDS_PRODUCED,\n-            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\").count());\n-        assertEquals(\"Records were not produced to backup cluster.\", NUM_RECORDS_PRODUCED,\n-            backup.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic-1\").count());\n-        assertEquals(\"Records were not replicated to primary cluster.\", NUM_RECORDS_PRODUCED,\n-            primary.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\").count());\n-\n-        assertEquals(\"Primary cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n-            primary.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"backup.test-topic-1\", \"test-topic-1\").count());\n-        assertEquals(\"Backup cluster doesn't have all records from both clusters.\", NUM_RECORDS_PRODUCED * 2,\n-            backup.kafka().consume(NUM_RECORDS_PRODUCED * 2, RECORD_TRANSFER_DURATION_MS, \"primary.test-topic-1\", \"test-topic-1\").count());\n-\n-        assertTrue(\"Heartbeats were not emitted to primary cluster.\",\n-            primary.kafka().consume(1, RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n-        assertTrue(\"Heartbeats were not emitted to backup cluster.\",\n-            backup.kafka().consume(1, RECORD_TRANSFER_DURATION_MS, \"heartbeats\").count() > 0);\n-        assertTrue(\"Heartbeats were not replicated downstream to backup cluster.\",\n-            backup.kafka().consume(1, RECORD_TRANSFER_DURATION_MS, \"primary.heartbeats\").count() > 0);\n-        assertTrue(\"Heartbeats were not replicated downstream to primary cluster.\",\n-            primary.kafka().consume(1, RECORD_TRANSFER_DURATION_MS, \"backup.heartbeats\").count() > 0);\n-\n-        assertTrue(\"Did not find upstream primary cluster.\", backupClient.upstreamClusters().contains(\"primary\"));\n-        assertEquals(\"Did not calculate replication hops correctly.\", 1, backupClient.replicationHops(\"primary\"));\n-        assertTrue(\"Did not find upstream backup cluster.\", primaryClient.upstreamClusters().contains(\"backup\"));\n-        assertEquals(\"Did not calculate replication hops correctly.\", 1, primaryClient.replicationHops(\"backup\"));\n-\n-        assertTrue(\"Checkpoints were not emitted downstream to backup cluster.\",\n-            backup.kafka().consume(1, CHECKPOINT_DURATION_MS, \"primary.checkpoints.internal\").count() > 0);\n-\n-        Map<TopicPartition, OffsetAndMetadata> backupOffsets = backupClient.remoteConsumerOffsets(consumerGroupName, \"primary\",\n-            Duration.ofMillis(CHECKPOINT_DURATION_MS));\n-\n-        assertTrue(\"Offsets not translated downstream to backup cluster. Found: \" + backupOffsets, backupOffsets.containsKey(\n-            new TopicPartition(\"primary.test-topic-1\", 0)));\n-\n-        // Failover consumer group to backup cluster.\n-        backupConsumer = backup.kafka().createConsumer(consumerProps);\n-        backupConsumer.assign(allPartitions(\"test-topic-1\", \"primary.test-topic-1\"));\n-        seek(backupConsumer, backupOffsets);\n-        consumeAllMessages(backupConsumer, 0);\n-\n-        assertTrue(\"Consumer failedover to zero offset.\", backupConsumer.position(new TopicPartition(\"primary.test-topic-1\", 0)) > 0);\n-        assertTrue(\"Consumer failedover beyond expected offset.\", backupConsumer.position(\n-            new TopicPartition(\"primary.test-topic-1\", 0)) <= NUM_RECORDS_PRODUCED);\n-        assertTrue(\"Checkpoints were not emitted upstream to primary cluster.\", primary.kafka().consume(1,\n-            CHECKPOINT_DURATION_MS, \"backup.checkpoints.internal\").count() > 0);\n-\n-        backupConsumer.close();\n-\n-        waitForCondition(() -> {\n-            try {\n-                return primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n-                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"backup.test-topic-1\", 0));\n-            } catch (Throwable e) {\n-                return false;\n-            }\n-        }, CHECKPOINT_DURATION_MS, \"Offsets not translated downstream to primary cluster.\");\n-\n-        waitForCondition(() -> {\n-            try {\n-                return primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n-                    Duration.ofMillis(CHECKPOINT_DURATION_MS)).containsKey(new TopicPartition(\"test-topic-1\", 0));\n-            } catch (Throwable e) {\n-                return false;\n-            }\n-        }, CHECKPOINT_DURATION_MS, \"Offsets not translated upstream to primary cluster.\");\n-\n-        Map<TopicPartition, OffsetAndMetadata> primaryOffsets = primaryClient.remoteConsumerOffsets(consumerGroupName, \"backup\",\n-                Duration.ofMillis(CHECKPOINT_DURATION_MS));\n-\n-        primaryClient.close();\n-        backupClient.close();", "originalCommit": "7b8957abcd82571fd7677f3abedf2ffc2ff0075d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "f57a8c9a494e10811068be1d4f26bbb6816292ee", "url": "https://github.com/apache/kafka/commit/f57a8c9a494e10811068be1d4f26bbb6816292ee", "message": "refactor MM2 integration tests", "committedDate": "2020-12-10T15:19:32Z", "type": "commit"}, {"oid": "f57a8c9a494e10811068be1d4f26bbb6816292ee", "url": "https://github.com/apache/kafka/commit/f57a8c9a494e10811068be1d4f26bbb6816292ee", "message": "refactor MM2 integration tests", "committedDate": "2020-12-10T15:19:32Z", "type": "forcePushed"}]}